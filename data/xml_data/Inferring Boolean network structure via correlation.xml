
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inferring Boolean network structure via correlation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011">2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Markus</forename>
								<surname>Maucher</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Research group Bioinformatics and Systems Biology</orgName>
								<orgName type="department" key="dep2">Clinic for Internal Medicine I</orgName>
								<orgName type="institution">University Medical Center Ulm</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Barbara</forename>
								<surname>Kracher</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Biochemistry and Molecular Biology</orgName>
								<orgName type="institution">Ulm University</orgName>
								<address>
									<settlement>and</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Michael</forename>
								<surname>Kühl</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Biochemistry and Molecular Biology</orgName>
								<orgName type="institution">Ulm University</orgName>
								<address>
									<settlement>and</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Hans</forename>
								<forename type="middle">A</forename>
								<surname>Kestler</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Research group Bioinformatics and Systems Biology</orgName>
								<orgName type="department" key="dep2">Clinic for Internal Medicine I</orgName>
								<orgName type="institution">University Medical Center Ulm</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for Neural Information Processing</orgName>
								<orgName type="laboratory">Research Group Bioinformatics and Systems Biology</orgName>
								<orgName type="institution">Ulm University</orgName>
								<address>
									<settlement>Ulm</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Olga</forename>
								<surname>Troyanskaya</surname>
							</persName>
						</author>
						<title level="a" type="main">Inferring Boolean network structure via correlation</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">27</biblScope>
							<biblScope unit="issue">11</biblScope>
							<biblScope unit="page" from="1529" to="1536"/>
							<date type="published" when="2011">2011</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btr166</idno>
					<note type="submission">Systems biology Advance Access publication April 5, 2011 Received on August 20, 2010; revised on February 28, 2011; accepted on March 25, 2011</note>
					<note>[15:00 16/5/2011 Bioinformatics-btr166.tex] Page: 1529 1529–1536 Associate Editor: Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Accurate, context-specific regulation of gene expression is essential for all organisms. Accordingly, it is very important to understand the complex relations within cellular gene regulatory networks. A tool to describe and analyze the behavior of such networks are Boolean models. The reconstruction of a Boolean network from biological data requires identification of dependencies within the network. This task becomes increasingly computationally demanding with large amounts of data created by recent high-throughput technologies. Thus, we developed a method that is especially suited for network structure reconstruction from large-scale data. In our approach, we took advantage of the fact that a specific transcription factor often will consistently either activate or inhibit a specific target gene, and this kind of regulatory behavior can be modeled using monotone functions. Results: To detect regulatory dependencies in a network, we examined how the expression of different genes correlates to successive network states. For this purpose, we used Pearson correlation as an elementary correlation measure. Given a Boolean network containing only monotone Boolean functions, we prove that the correlation of successive states can identify the dependencies in the network. This method not only finds dependencies in randomly created artificial networks to very high percentage, but also reconstructed large fractions of both a published Escherichia coli regulatory network from simulated data and a yeast cell cycle network from real microarray</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Boolean networks were popularized by Stuart Kauffman as models for genetic regulatory networks (<ref type="bibr" target="#b13">Kauffman, 1969</ref>). In this kind of model, only two states are discriminated (active/inactive) for each gene. The dynamics of the network can then be described by Boolean functions. This model works with a very small set of parameters and thus represents a very stringent application of Occam's Razor, which makes it especially suitable for modeling large genetic networks * To whom correspondence should be addressed. † The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.</p><p>(<ref type="bibr" target="#b2">Bornholdt, 2005</ref>). Nonetheless, it is powerful enough to model the structure of network motifs, basic network components frequently found in gene regulatory networks (<ref type="bibr" target="#b0">Alon, 2006;</ref><ref type="bibr" target="#b1">Babu et al., 2004</ref>). Gene transcription in eukaryotic cells has to be tightly regulated to ensure proper cell function, i.e. according to a particular cellular context (cell cycle phase, cell type, environmental conditions, developmental stage), a specific subset of genes is expressed while the expression of other genes has to be actively repressed. This regulation is generally mediated via certain proteins, called transcription factors, that bind to specific sequence motifs in the promoter region of a gene and either enhance or inhibit the transcription of this gene [reviewed e.g. in (<ref type="bibr" target="#b23">Orphanides and Reinberg, 2002;</ref><ref type="bibr" target="#b31">Venters and Pugh, 2009)]</ref>. If a promoter region contains binding motifs for different transcription factors, these factors can either cooperate in the regulation of a certain target gene or counteract each other. Moreover, in some cases, specific cofactors determine whether a transcription factor acts as activator or repressor. Despite the variety in the modes of transcriptional regulation, most transcriptional regulators will be either activators or inhibitors of a certain gene in a specific cell type. In this case, the activating or repressing effect of a transcription factor monotonically depends on its cellular concentration. In other words, an increase in the concentration of an activator will increase but never decrease transcription of its target, while an increase in the concentration of a repressor will decrease but never increase transcription of its target. This kind of transcriptional regulation can be modeled mathematically in a very simplistic manner by the use of monotone Boolean functions which describe exactly this monotonic relation. Besides the monotone Boolean functions applied in this work, there exist further related classes of functions describing such monotonic relations. Among them are, for example, nested canalyzing functions (<ref type="bibr" target="#b15">Kauffman et al., 2003</ref>), single-layer perceptrons (<ref type="bibr" target="#b26">Rani et al., 2007</ref>) and multilinear functions (<ref type="bibr" target="#b30">Tsukimoto and Hatano, 2003</ref>). Gene expression data can be analyzed and visualized on the basis of correlations identified in the observed expression patterns of the analyzed genes. Models from information theory correlate expression values from two genes directly and predict an interaction between two genes to be present if the correlation coefficient is exceeding a certain threshold. Algorithms based on this principle are for example CLR (<ref type="bibr" target="#b6">Faith et al., 2007</ref>) or ARACNE (<ref type="bibr" target="#b18">Margolin et al., 2006</ref>). However, these correlations generally reflect co-expression of genes and can, under some restrictions to the underlying network like e.g. absence of cyclic dependencies, also give information on direct causal relationships<ref type="bibr">[cf. (Pearl, 2000;</ref><ref type="bibr" target="#b27">Shipley, 2000;</ref><ref type="bibr" target="#b29">Spirtes et al., 2000</ref>Page: 1530 1529–1536</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M.Maucher et al.</head><p>to develop networks depicting dynamic relations from correlation data. Likewise, there exist algorithms for the inference of dynamic Boolean networks from gene expression data, like REVEAL (<ref type="bibr" target="#b17">Liang et al., 1998</ref>) or the best fit extension algorithm (<ref type="bibr">Lähdesmäki et al., 2003</ref>). These algorithms, however, perform reasonably well only if the datasets are not too large. In contrast to existing approaches, we do not correlate expression data of different genes directly, but instead correlate the states of particular genes with the successive states of potential target genes, thus assuming and examining directed causal dependencies. This article is organized as follows: we first prove properties of interaction reconstruction via correlation, assuming monotone Boolean functions. This is followed by a series of experiments successively advancing from entirely artificial through to real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS—CORRELATION AND MONOTONE FUNCTIONS</head><p>Boolean networks: a Boolean network consists of n nodes, numbered from 1 to n and n functions f 1 ,...,f n :{0,1} n →{0,1}. The state of the network can be described by a Boolean vector x ∈{0,1} n , where x i describes the state of the i-th node. The n functions describe the dynamics of the network: If the network is in state x at time t, it transforms into state (f 1 (x),f 2 (x),...,f n (x)) at time t +1. We can also combine f 1 ,...,f n into one function F :{0,1} n → {0,1} n such that F(x) i = f i (x). State x then transforms into state F(x). When considering successive states x (1) ,x (2) ,.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.., the term x</head><p>(j) i will denote the state of the i-th node in x (j). Relevance of a variable: a function f :{0,1} n →{0,1} depends on the i-th variable if there exist x 1 ,...,x i−1 ,x i+1 ,...,x n such that f (x 1 ,...,x i−1 ,0,x i+1 ,.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>..,x n )</head><p>= f (x 1 ,...,x i−1 ,1,x i+1 ,...,x n ). If f depends on the i-th input variable, we also say that the i-th variable is relevant for f. The set of all relevant variables of f is denoted as rel(f ). Reconstructing the dynamics of a Boolean network: in order to reconstruct the functions of a Boolean network, we are given a sequence of m states x (1) ,x (2) ,...,x (m) along with the corresponding successor states F(x (1) ),F(x (2) ),...,F(x (m) ). A tuple (x (i) ,F(x (i) )) is also called an example. From these examples, the task is to reconstruct the dependencies in the Boolean network, i.e. the set of variables each of the functions depends on. Monotonicity of functions: a Boolean function f :{0,1} n →{0,1} is monotonically increasing in the i-th variable if for all</p><formula>x 1 ,...,x i−1 ,x i+1 ,...,x n f (x 1 ,...,x i−1 ,0,x i+1 ,...,x n ) ≤ f (x 1 ,...,x i−1 ,1,x i+1 ,...,x n ),</formula><p>the function f is monotonically decreasing in the i-th variable if for all variables x 1 ,...,x i−1 ,x i+1 ,...,</p><formula>x n f (x 1 ,...,x i−1 ,0,x i+1 ,...,x n ) ≥ f (x 1 ,...,x i−1 ,1,x i+1 ,...,x n ).</formula><p>A function f is monotone if for every variable f is either monotonically increasing or monotonically decreasing in that variable. For example, the Boolean AND function is monotone, while the XOR function is not. Influence of a variable: a probability distribution D on {0,1} n is called product distribution if for any D-distributed random variable X the property</p><formula>P[X = (x 1 ,...,x n )]= n i=1 P[X i = x i ]</formula><p>holds, i.e. the X i are independent. Let D be a product distribution and X be a D-distributed random variable. The influence I D,i (f ) of a variable x i on a function f :{0,1} n →{0,1} is defined as the probability that a change in x i will also lead to a change in f (X), i.e.</p><formula>I D,i (f ) = P D f (X)| x i =0 = f (X)| x i =1 .</formula><p>Here, f (X)| x i =0 denotes a partial assignment, i.e.</p><formula>f (X 1 ,...,X n )| x i =b = f (X 1 ,...,X i−1 ,b,X i+1 ,...,X n ) .</formula><p>Pearson correlation: given two random variables X and Y , their Pearson correlation is defined as</p><formula>ρ(X,Y ) = E[(X −E[X])(Y −E[Y ])] σ X σ Y = Cov(X,Y ) σ X σ Y ,</formula><p>where σ Z denotes the SD of a random variable Z.</p><formula>x i. Then E D [(X i −E[X i ])(f (X)−E[f (X)])] σ X i σ f (X) = 1 σ X i σ f (X) E D i E X i [(X i −E[X i ])(f (X)−E[f (X)])] = 1 σ X i σ f (X) E D i (1−µ i )(−µ i ) f (X)| x i =0 −f (X) +µ i (1−µ i ) f (X)| x i =1 −f (X) = µ i (1−µ i ) σ X i σ f (X) E D i f (X)| x i =1 −f (X)| x i =0 = σ X i σ f (X) E D i f (X)| x i =1 −f (X)| x i =0 .</formula><p>The theorem then follows from the fact that f is monotone in x i .</p><p>As derived in Theorem 1, the influence of a variable can be estimated via a modified version of the Pearson correlatioñ</p><formula>correlatioñ ρ(X,Y ) = E[(X −E[X])(Y −E[Y ])] σ 2 X = Cov(X,Y ) σ 2 X .</formula><p>The Chernoff-Hoeffding bound (<ref type="bibr" target="#b12">Hoeffding, 1963</ref>): given independent random variables X 1 ,...,X m with a ≤ X i ≤ b for all i, the Chernoff-Hoeffding bound can be stated as follows:</p><formula>P m i=1 X i − m i=1 E[X i ] ≥ m ≤ e −2m 2 (b−a) 2 .</formula><p>Estimating the correlation coefficient: we will use the Chernoff-Hoeffding bound for the estimation of a variable's influence:Proof. As shown in Theorem 1, the influence of a variable X i on a function f can be estimated via a modified correlatioñand we can also show</p><formula>ρ(X i ,f (X)) = σ f (X) σ X ρ(X i ,f (X))</formula><formula>P 1 m m i=1 x i y i −E[XY ] ≥ r ≤ exp(−2m 2 r ) .</formula><p>We use the fact that the x i and y i are all Boolean to estimate the sample variance s 2 x = x(1−x). Let δ x denote the estimation error for µ x , i.e. x = µ x +δ x. That way,</p><formula>x(1−x) = (µ x +δ x )(1−µ x −δ x ) = µ x (1−µ x )+(1−2µ x −δ x )δ x .</formula><p>Since |(1−2µ x −δ x )|≤1, we can conclude that</p><formula>|x(1−x)−µ x (1−µ x )|≤|δ x |.</formula><p>This means that if the error for estimating µ x is at most x then the error for estimating σ 2 x is at most x , too. We can now combine the estimation errors above: estimating the modified correlation coefficient˜ρcoefficient˜ coefficient˜ρ viã</p><formula>viã r(X,Y ) = 1 m m i=1 x i y i −xy x(1−x)</formula><formula>and setting := ( r + x µ y + y µ x + x y + x )/(σ 2 x − x )</formula><p>, we can use Lemma 1 below to conclude that</p><formula>P[|˜r|˜r(X,Y )− ˜ ρ(X,Y )|≥] ≤ exp(−2m 2 x )+exp(−2m 2 y )+exp(−2m 2 r ) .</formula><p>In particular, the error probability shrinks exponentially in the number of samples m.</p><formula>A B − ˜ A ˜ B ≤ 1 + 2 B− 2 .</formula><p>Proof. It holds that</p><formula>A B − ˜ A ˜ B ≤ max A B − A− 1 B+ 2 , A+ 1 B− 2 − A B . With A B − A− 1 B+ 2 = A 2 +B 1 B(B+ 2 ) ≤ 1 + 2 B+ 2 and A+ 1 B− 2 − A B = A 2 +B 1 B(B− 2 ) ≤ 1 + 2 B− 2 the lemma follows.</formula><p>Inferring the structure of a Boolean network: combining the results above, we can infer the dependency relations within a Boolean graph with a fast algorithm: Algorithm 1 finds the relevant variables of a monotone function with a chosen minimum influence, given a sequence of independent examples, by calculating the correlation value of each variable and identifying all variables with this correlation exceeding a given value. Running that algorithm for the output function of each node of a Boolean network reveals the structure of the entire network.</p><p>Algorithm 1 Finding all relevant variables of f with influence exceeding a threshold T</p><formula>Input: m examples (x (1) ,f (x (1) )),...,(x (m) ,f (x (m)</formula><p>)) of a function f , drawn from a product distribution, threshold T Output: Approximation of rel(f )</p><formula>rel ←∅ y ← 1 m m j=1 f (x (j) ) for i ∈{1,...,n} do x ← 1 m m j=1 x</formula><p>(j) i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>if x(1−x) &gt; 0 then {ensures non zero sample variance}</head><formula>if 1 m m j=1 x (j) i f (x (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SIMULATION RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error rates in artificial networks with fixed in-degree: to</head><p>investigate the performance of our method for concrete values and for controlled noise levels, we analyzed the error probability of our inference algorithm in an artificial network consisting of monotone Boolean functions. For this purpose, we created a set of 50 random Boolean networks each containing 80 nodes. For every node in these networks, a monotone function with three input variables was constructed as follows: first, we created a random truth table and determined randomly for each input variable if the function should be monotonically increasing or decreasing in that variable. We then altered the initial truth table by correcting all inconsistencies. If this resulted in a function that did not depend on all of its input variables, we created a new random truth table and repeated the process until every function depended on all of its input variables. Additionally, for each of the networks generated we analyzed attractors, i.e. single states or sequences of states toward which the networks evolve. The network generation and attractor analysis was performed with the R package BoolNet (<ref type="bibr">Müssel et al., 2010</ref>), with the described modification to create monotone functions. We determined the number of attractors for each of these artificial networks via a random sampling of 10 000 starting states and their resulting attractors (function getAttractors of BoolNet). The number of attractors ranged from 1 to 59 (with a median of 5.5), with attractor lengths ranging from 1 to 258 (with median 6). From each of the 50 random networks, we subsequently generated a batch of 1000 simulated time-series datasets, with each dataset covering 2 time points. In this process, for the generation of each dataset a new starting configuration of the network was drawn from a uniform distribution. The datasets generated in this way for each of the 50 random networks represent synthetic 'gene expression' data of the 80 'genes', measured at two successive time points in an unperturbed system. These datasets were then used to reconstruct the dependencies in the underlying artificial networks. If a Boolean function depends on three variables, changing one of these variables from 0 to 1 must have an effect on f for at least one combination of the other two variables. Since the probability of such a combination of two variables is 0.25 under a uniform distribution, each of these variables has an influence of at least 0.25. A correlation value wasassumed to signify a dependency when it exceeded a threshold of 0.125, i.e. half the minimum influence.<ref type="figure" target="#fig_1">Figure 1</ref>shows the average identification rates of our method and Lähdesmäki and co-workers' best fit extension algorithm (<ref type="bibr">Lähdesmäki et al., 2003</ref>) for the reconstruction of 50 artificial Boolean networks. The network size of 80 nodes was chosen to be able to compare the performance of our approach with the best fit algorithm. Moreover, to obtain data that is closer to experimental results, we added a Gaussian noise term with mean 0 and SD 0.4 to the generated data, then rounded to the nearest value in {0,1}. This corresponds to a probability of about 0.1 for each bit to be changed to the wrong value. The effect of different noise levels on network reconstruction is shown in Supplementary<ref type="figure" target="#fig_1">Figure S1</ref>. For the best fit algorithm, we used the implementation in the BoolNet Rpackage. As seen in<ref type="figure" target="#fig_1">Figure 1</ref>, for a threshold of 0.125, the correlation approach detected 80% and more of the dependencies in the artificial network already when less than 200 datasets were used for network reconstruction, while the best fit algorithm identified a comparable portion of dependencies only when more than 200 datasets were used. However, for this threshold, the correlation approach also retrieved a fraction of false-positive dependencies, that were not actually present in the underlying networks. For example, when using 200 datasets, 25% of the non-present dependencies were wrongly classified as present. Thus, the rate of correctly classified non-present connections was lower than the true-positive rate, unless more than 400 datasets were used for network inference (<ref type="figure" target="#fig_1">Fig. 1</ref>). We further analyzed precision, i.e. the fraction of true dependencies among all predicted dependencies, and recall, i.e. the fraction of true dependencies that was correctly predicted, for our reconstruction approach and the best fit algorithm. As seen in Supplementary<ref type="figure">Figure S5</ref>, precision and recall increase for both algorithms with the number of datasets used for network inference. In the correlationapproach, moreover, the threshold for the correlation can be varied to determine whether the algorithm should preferably identify dependencies with a high precision, at the expense of a lower recall, or the other way round (see Supplementary<ref type="figure">Fig. S5</ref>). As additional measure for the reliability of network reconstruction, we further analyzed the F-score, which can be regarded as weighted average of precision and recall (see Supplementary<ref type="figure">Fig. S6</ref>). Runtime: in a second experiment, we measured the running time of the correlation method and the best fit algorithm. The average singlecore runtimes on a 3.0 GHz Xeon CPU are given in<ref type="figure" target="#tab_1">Table 1</ref>, a key advantage of the correlation method are the considerably shorter running times compared to the best fit algorithm. Thus, our correlation approach is especially useful for large networks in which the nodes have a relatively large indegree. For example, the runtime of the best fit algorithm for the reconstruction of a network of 200 nodes with a maximum in-degree of 4 (i.e. each Boolean function depends on no more than four input variables) is almost 3 h, whereas the correlation approach needs only about 15 s for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reconstruction of interactions from an E. coli regulatory network:</head><p>we next tested the method on a real biological network. For that purpose, we chose the integrated Escherichia coli gene regulatory and metabolic network published by<ref type="bibr" target="#b5">Covert et al. (2004)</ref>. Based on previously published information, extracted from literature and databases, the authors of this study constructed a network model that describes the transcriptional regulation of genes involved in<ref type="bibr" target="#b5">Covert et al. (2004)</ref>. Correlations signified a regulatory dependency if they exceeded a predefined threshold of 0.2 (circles) or 0.4 (squares). observed in biological data. Binarization was then performed by rounding to the nearest Boolean value 0 or 1. We then reconstructed the dependencies of the published E.coli network from the synthetic datasets using the described correlation method. To investigate the influence of the number of datasets on the reconstruction, we varied the number of used datasets in the range of 2 to the available 20 (<ref type="figure" target="#fig_3">Fig. 2</ref>). By comparing the predicted dependencies with the published network, finally, the fraction of correctly classified present (true positive) and non-present (true negative) dependencies was determined. Additionally, we analyzed precision and recall and F-scores for different thresholds (see Supplementary Figs S7 and S8). As seen in<ref type="figure" target="#fig_3">Figure 2</ref>, for a threshold value of 0.2, the true positive rate was close to 75% for network reconstruction from at least five datasets. The true negative rate, for this threshold, ranged between 50% and 80% depending on the number of datasets used for reconstruction. Thus, at this threshold a large fraction (∼ 75%) of the actual dependencies was found. To increase precision of the correlation approach, a higher threshold can be applied. So, for a threshold of 0.4, the true positive rate still was close to 60%, and in this case the true negative rate was higher than 80% already when five datasets were used for reconstruction (<ref type="figure" target="#fig_3">Fig. 2</ref>). Examples of reconstructed interactions for different dataset sizes and thresholds are given as Supplementary Material II. To further evaluate how reliably the correlation of successive states identified true relevant variables of the examined Boolean functions, we additionally computed the area under the receiver operator curve (AUC;<ref type="bibr" target="#b7">Fawcett, 2006</ref>). The AUC is equal to the probability that a randomly chosen present dependency has a higher absolute correlation value than a randomly chosen non-existing dependency. For computation of the AUC values, we performed 25) AUC values (583 genes are dependent). One AUC value is determined by calculating all correlations of one of the 583 dependent genes to the 685 variables that are potentially influencing them and utilizing all possible thresholds. From these values, we generated histograms separated by the number of either 1, 2, 3 or more than 3 input variables (<ref type="figure">Fig. 3</ref>). For Boolean functions with only one input variable (representing genes which are regulated by just one factor), the correlation method identified the relevant variables with very high reliability. For functions with several input variables (representing genes which are regulated by several factors), the fraction of smaller AUC values increased, but a large fraction is still close to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interactions of the yeast cell cycle transcriptional network</head><p>: finally, we used the correlation method to reconstruct a biological network from microarray data. For that purpose, we chose the cell cycle transcriptional network of budding yeast, as suggested by<ref type="bibr" target="#b22">Orlando et al. (2008)</ref>, and used published microarray results from three groups (<ref type="bibr" target="#b4">Cho et al., 1998;</ref><ref type="bibr" target="#b25">Pramila et al., 2006;</ref><ref type="bibr" target="#b28">Spellman et al., 1998</ref>) for network reconstruction. All three studies represent genome-wide analyses of gene expression during the cell cycle in synchronized yeast cells, but they differ in the applied synchronization methods and the time intervals at which transcript levels were measured. In a first step, we binarized each of these microarray datasets with the 2-means algorithm, the version of the k-means clustering algorithm that generates k = 2 clusters. Next, we created a set of</p><p>Page: 1534 1529–1536<ref type="bibr" target="#b22">Orlando et al. (2008)</ref>with the transcription factors arranged on the cell cycle time line (G1 →S→G2/M) on the basis of their peak transcript levels. Interactions shown as dashed lines are based on a publication by<ref type="bibr" target="#b25">Pramila et al. (2006)</ref>. Transcriptional activators are depicted as circles, repressors as rectangles and the cyclin Cln3 as octagon; activating interactions end in arrowheads and inhibitory interactions in squares. The interactions shown as solid and dashed lines were correctly identified by our approach, while interactions shown as dotted lines were not found. example pairs, i.e. each example contained the state of the network at a certain time point along with a succeeding state measured after a specific time period. As the time lag between regulator expression and target expression varies considerably in the data, we grouped the sample pairs according to the intervals: the short time lag group contained all examples where the following states were measured after a period of 5–10 min, the intermediate time lag group contained the examples with the following states measured after 14–Page: 1535 1529–1536</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M.Maucher et al.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Boolean network structure</head><p>time domain into the spectral domain, as the values are (in the case of the Fourier transform) hard to interpret. We think that the notion of a correlation value is much more intuitive. Our experiments on the randomly created artificial networks have shown that for noisy data and a low number of samples, Pearson correlation still finds a high percentage of the dependencies and, for a range of thresholds, even surpasses the best fit algorithm with regard to recall, precision and F-score (see<ref type="figure" target="#fig_1">Fig. 1</ref>, Supplementary Figs S5 and S6). A further advantage of our approach, besides the short running times, is the possibility to vary the threshold according to the desired outcome, i.e. whether a high recall or rather a high precision are requested. In case of little or no previous knowledge, for example, preferably a high threshold should be applied so that dependencies can be assumed with a high reliability, while a lower threshold can be applied, if more previous knowledge exists that can help to (pre-)select meaningful dependencies. For the simulated E.coli network, we also could reconstruct more than 50% of the present and non-present dependencies, already from a relatively low number of datasets (<ref type="figure" target="#fig_3">Fig. 2</ref>). The comparison of the areas under the ROCs indicate that especially for functions that depend only on one or two variables, dependencies can be found reliably. For functions depending on more variables, the identification of dependencies is more complicated, but the AUCs show that the correlation method still is able to provide information about these dependencies (<ref type="figure">Fig. 3</ref>). One reason for the increased difficulty when reconstructing functions with higher in-degree is the fact that for higher in-degree, variables can have a lower influence and dependencies are therefore harder to detect. In addition, the set of Boolean functions can be partitioned into sets of varying difficulty for a learning algorithm [cf. Gordon and<ref type="bibr" target="#b11">Peretto (1990)]</ref>. This partition, however, also depends to a large extent on the in-degree of the functions. Furthermore, we could show that Pearson correlation also performed reasonably well when reconstructing dependencies from real biological data, reaching identification rates (true positive, true negative) similar to those for the simulated E.coli datasets. Compared to the best fit algorithm, interactions were not only identified faster using correlation, but the identification was also more reliable. As seen also in<ref type="figure" target="#fig_7">Figure 4</ref>, almost 75% of the assumed dependencies could be correctly identified at the chosen threshold, in spite of the regulatory complexity of the yeast cell cycle network, even for components regulated by more than three factors (like Swi4, Swi6 or Yhp1). Further examining which of the expected dependencies were not identified by our method, we find that in these cases the regulatory mechanisms are based on a quite complex interplay of several factors involved. So, for example, Cln3 can activate transcription of Swi6 through inactivation of the transcriptional repressor Whi5 as well as independent of Whi5 (<ref type="bibr" target="#b32">Wittenberg and Reed, 2005</ref>), and the Whi5-independent mechanism might hamper identification of the inhibitory influence of Whi5 on Swi6. A second example is the transcriptional activator Mcm1, which can act in concert with an activating transcription factor complex consisting of Fkh2/Ndd1 or the transcriptional repressors Yhp1 and Yox1 (<ref type="bibr" target="#b32">Wittenberg and Reed, 2005</ref>). Accordingly, the influence of Mcm1, which was mostly not identified by our method, is strongly dependent on the presence of the respective cofactors, for which our approach correctly identified the respective dependencies. With regard to the reconstruction from real data, it has to be kept in mind that we cannot be completely sure that the published network whose interactions we are rebuilding exactly matches the real in vivo situation. Thus, for example, some identified dependencies, that were assumed to be false positives, might actually be real dependencies. In line with this reasoning, the precision of network reconstruction was indeed lower for the reconstruction from real data, for both the best-fit and the correlation algorithm. Furthermore, it has to be kept in mind, that correlations representing indirect effects within the cell cycle also were counted as false positives, while they do actually confer some biological meaning. In addition, Pearson correlation is not only suitable for measuring the dependencies between sequences of binarized values. Especially when real-valued examples are difficult to binarize, applying Pearson correlation directly on the raw, non-binarized data might already give some valuable information about a network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>For a Boolean network consisting only of monotone Boolean functions, we showed that Pearson correlation is a fast method to find dependencies in the network. This method makes it possible to analyze large networks that contain nodes with large input degree. We could show for both simulated and real microarray data that our approach could reconstruct large parts of published regulatory networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.1.</head><figDesc>Fig. 1. True positive and true negative rates of an experiment with artificial data and Gaussian noise with SD σ = 0.4. The correlation algorithm and the best fit extension algorithm were used to infer artificially created networks consisting of 80 nodes with an in-degree of 3. The plot shows average true positive and true negative rates for the reconstruction of 50 artificial networks from 50 to 1000 datasets each.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.2.</head><figDesc>Fig. 2. True positive (solid lines) and true negative (dashed lines) rates of the correlation approach for reconstruction of a published E.coli regulatory network with 685 nodes. The network was reconstructed from 2 to 20 synthetic time-series datasets (with 3 time steps), that had been generated from the network by Covert et al. (2004). Correlations signified a regulatory dependency if they exceeded a predefined threshold of 0.2 (circles) or 0.4 (squares).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig</head><figDesc>Fig. 3. Histograms for the areas under the receiver operator curves (AUC). Correlation values were computed from 10 time-series datasets each covering three discrete time steps. In total, 25 reconstruction runs were performed, each time creating new time-series. That way, every dependency in the E.coli network contributes 25 AUC values to the corresponding histogram. The AUC frequency distributions for functions that depend on 1, 2, 3, or more than 3 input variables are shown in four separate histograms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig.4.</head><figDesc>Fig. 4. Yeast cell cycle transcription factor network. The diagram shows the yeast transcription factor network as suggested by Orlando et al. (2008) with the transcription factors arranged on the cell cycle time line (G1 →S→G2/M) on the basis of their peak transcript levels. Interactions shown as dashed lines are based on a publication by Pramila et al. (2006). Transcriptional activators are depicted as circles, repressors as rectangles and the cyclin Cln3 as octagon; activating interactions end in arrowheads and inhibitory interactions in squares. The interactions shown as solid and dashed lines were correctly identified by our approach, while interactions shown as dotted lines were not found.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>) and references therein]. Moreover, Opgen-Rhein and Strimmer (2007) and Zoppoli et al. (2010) established algorithms</figDesc><table>[15:00 16/5/2011 Bioinformatics-btr166.tex] 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>Theorem 1. Given a monotone function f :{0,1} n →{0,1} and a random Proof. Let D i denote the product distribution D on all variables except</figDesc><table>variable X ∈{0,1} n that is distributed according to a product distribution D. 
Then the function f depends on the i-th variable if and only if the Pearson 
correlation of X i and f (X) is non-zero for non-zero variances. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>. −µ x | ≥ x ] ≤ exp(−2m 2 x ) and P y−µ y ≥ y ≤ exp(−2m 2 y ) , Page: 1531 1529–1536 Boolean network structure which implies P[|xy−µ x µ y |≥ x µ y + y µ x + x y ] ≤ exp(−2m 2 x )+exp(−2m 2 y ) ,</figDesc><table>We will show that when estimating this term the error probability shrinks 
exponentially in the number of samples. 
Via the Chernoff-Hoeffding bound, we can prove that 

P[|x </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>Lemma 1. Let A, ˜ A,B, ˜ B ∈[−1,1] with B &gt; |A|, |A−˜A||A−˜ |A−˜A| &lt;&lt; 1 and |B−˜B||B−˜ |B−˜B| &lt;&lt; 2. Then</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><figDesc>j) )−xy x(1−x) ≥ T then rel ← rel ∪{i} end if end if end for return rel</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><figDesc>Table 1. Runtimes of the correlation method and the best fit extension algorithm, averaged over 20 runs</figDesc><table>Network size 
Correlation (s) 
Best Fit 
Best fit 
(max k = 3) (s) 
(max k = 4) 

50 nodes 
0.98 
0.79 
10.2 s 
100 nodes 
3.86 
11.2 
324 s 
200 nodes 
15.4 
176 
2 h 50 min 

The Correlation approach is independent of setting a max k. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><figDesc>Table 1. The random networks for the reconstruction consisted of 50, 100 and 200 nodes, respectively. In each case, we measured the running times for reconstruction of the random network from 50 synthetic datasets. The values in the table show the running times averaged over 20 runs, where we created a new random Boolean network for each of these runs. As seen in</figDesc><table></table></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="21"> min, and the long time lag group contained the examples with the following states measured after 25–30 min. Thus, for network reconstruction we regarded subsequent states with a time lag of up to about one fourth of the yeast&apos;s cell cycle, which is assumed to span a time of about 135 min (Orlando et al., 2008). For each of the 16 genes with known dependencies in the published cell cycle transcriptional network, we computed the correlation to the other genes in the cell cycle network and also added 16 randomly chosen additional genes contained in all of the microarray datasets. This was done to distort the reconstruction process and to evaluate the detection of non-present dependencies. A computed correlation was assumed to represent a dependency, if it exceeded the mean of all correlations either by at least 1 SD for at least one of the time lag groups or by at least half the SD for at least two of the time lag groups. This evaluation was repeated for a total of 25 times, each time choosing a new set of 16 additional genes at random. For comparison, we also applied the best fit extension algorithm to the same data. This algorithm was run on the data corresponding to each of the three time lag groups, and for maximum in-degrees of 3, 4 and 5. Figure 4 shows the correctly identified dependencies in the yeast transcription factor network (Orlando et al., 2008). Assuming that this published network correctly and fully represents the real biological situation, our correlation inference approach correctly identified an average of 74.7% of the present dependencies and 59.1% of non-present dependencies for the described threshold. In comparison, Lähdesmäki and co-workers&apos; best fit algorithm found 61.1% of the present dependencies and 38.8% of the non-present dependencies. This best result was achieved with a maximum indegree of 5 and interpreting a dependency as present when it was found for at least one of the three time lag groups. The identification rates of the best fit algorithm for different maximum indegrees are given in the Supplementary Material I. Additionally, we analyzed precision and recall as well as F-scores for correlation and best fit algorithm, applying different thresholds for the correlation. In these analyses we found, that the initially chosen threshold of &apos;mean ±1SD&apos; provided a reasonable balance between recall and precision (see Supplementary Figs S9 and S10). An example of the interactions found by the described analysis are given in Supplementary Material III. Here, an interaction was included if it occurred in the majority of the 25 simulations. Finally, we examined differences in precision and recall, when each of the three time lag groups was used separately for network reconstruction (see Supplementary Fig. S12). 4 DISCUSSION Under the assumption that a Boolean network consists of monotone functions, we have shown that its dependencies can be reconstructed via Pearson correlation of subsequent states. Compared for instance to the best fit extension algorithm, correlations have the important advantage that they can be computed very quickly—for each pair of nodes, only two correlations for the two possible directed dependencies have to be computed, where the running time for the computation of a single correlation is linear in the number of samples. This leads to an overall running time of the order O(n 2 m), where n is the number of nodes and m the number of samples. In contrast, an exhaustive search algorithm like the best fit algorithm considers all subsets of genes up to a given size, assuming the functions of the network have an input degree of k, for each node of the network n k combinations of input nodes have to be considered, which leads to a running time of the order O(n k+1 m). This is also reflected in Table 1: for k = 3, doubling the number of nodes in the network increases the runtime of the best fit algorithm by a factor of 16, while it increases the runtime of the correlation method only by a factor of 4. So while the exhaustive search approach is only feasible for networks with a small number of nodes or low input degrees, the correlation method can also be applied to large networks. This is particularly interesting for biological applications, as in the context of microarray and deep-sequencing technologies this type of large-scale data becomes more available. Theoretically, dependencies in the examples violate the assumption of Theorem 1. In an experimental evaluation (Supplementary Figs S2–S4), we found that these dependencies only marginally influence the reconstruction process as long as these dependencies are not too strong (visually discernible only for bias probabilities greater than 0.8). While our new method is to some extent similar to using the Fourier transform, it does not need the Fourier transform&apos;s mathematical overhead (Bshouty and Tamon, 1996; Mossel et al., 2003). We do not see any specific advantage of moving from the</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors would especially like to thank Guido Adler for continuing support. The authors would also like to thank the anonymous reviewers for their valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<monogr>
		<title level="m" type="main">An Introduction to Systems Biology: Design Principles of Biological Circuits</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Alon</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Structure and evolution of transcriptional regulatory networks</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Babu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Opin. Struct. Biol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="283" to="291" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Systems biology: less is more in modeling large genetic networks</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bornholdt</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="449" to="451" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">On the fourier spectrum of monotone functions</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Bshouty</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Tamon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="747" to="770" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">A genome-wide transcriptional analysis of the mitotic cell cycle</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Cho</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Cell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="65" to="73" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Integrating high-throughput and computational data elucidates bacterial networks</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Covert</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">429</biblScope>
			<biblScope unit="page" from="92" to="96" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale mapping and validation of escherichia coli transcriptional regulation from a compendium of expression profiles</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Faith</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Biol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">An introduction to ROC analysis</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Fawcett</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="861" to="874" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="0" to="16" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName>
				<surname>Page</surname>
			</persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1536" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Maucher</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">The statistical distribution of Boolean gates in twoinputs, one-output multilayered neural networks</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Gordon</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Peretto</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phys. A Math. Gen</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">3061</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Probability inequalities for sums of bounded random variables</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Hoeffding</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="13" to="30" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Metabolic stability and epigenesis in randomly constructed genetic nets</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Kauffman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Theor. Biol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="437" to="467" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<monogr>
		<title level="m" type="main">The Origins of Order: Self-Organization and Selection in Evolution</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Kauffman</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Random Boolean network models and the yeast transcriptional network</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Kauffman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="14796" to="14799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">On learning gene regulatory networks under the boolean network model</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Lähdesmäki</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="147" to="167" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Reveal, a general reverse engineering algorithm for inference of genetic network architectures</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Liang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pac. Symp. Biocomput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="18" to="29" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Aracne: an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Margolin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning juntas</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Mossel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC &apos;03: Proceedings of the thirty-fifth annual ACM symposium on Theory of Computing</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="206" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">BoolNet-an R package for generation, reconstruction, and analysis of Boolean networks</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Müssel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1378" to="1380" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning causal networks from systems biology time course data: an effective model selection procedure for the vector autoregressive process</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Opgen-Rhein</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Strimmer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Suppl. . 2</note>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Global control of cell-cycle transcription by coupled CDK and network oscillators</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Orlando</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">453</biblScope>
			<biblScope unit="page" from="944" to="947" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">A unified theory of gene expression</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Orphanides</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Reinberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="439" to="451" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Pearl</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">The forkhead transcription factor hcm1 regulates chromosome segregation genes and fills the s-phase gap in the transcriptional circuitry of the cell cycle</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Pramila</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genes Dev</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2266" to="2278" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Analysis of E.coli promoter recognition problem in dinucleotide feature space</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">S</forename>
				<surname>Rani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="582" to="588" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<monogr>
		<title level="m" type="main">Cause and Correlation in Biology: A User&apos;s Guide to Path Analysis, Structural Equations and Causal Inference</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Shipley</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Comprehensive identification of cell cycle-regulated genes of the yeast Saccharomyces cerevisiae by microarray hybridization</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Spellman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Biol. Cell</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">3273</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<monogr>
		<title level="m" type="main">Causation, Prediction, and Search</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Spirtes</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd. edn</note>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">The functional localization of neural networks using genetic algorithms</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Tsukimoto</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Hatano</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">How eukaryotic genes are transcribed</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">J</forename>
				<surname>Venters</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">F</forename>
				<surname>Pugh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Crit. Rev. Biochem. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="117" to="141" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Cell cycle-dependent transcription in yeast: promoters, transcription factors, and transcriptomes</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Wittenberg</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Reed</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oncogene</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2746" to="2755" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Timedelay-aracne: Reverse engineering of gene networks from time-course data by an information theoretic approach</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Zoppoli</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">154</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>