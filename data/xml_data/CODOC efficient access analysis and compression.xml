
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequence analysis CODOC: efficient access, analysis and compression of depth of coverage signals</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014">18 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Niko</forename>
								<surname>Popitsch</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Integrative Bioinformatics Vienna (CIBIV)</orgName>
								<orgName type="department" key="dep2">Max F Perutz Laboratories</orgName>
								<orgName type="institution">University of Vienna and Medical University of Vienna</orgName>
								<address>
									<addrLine>Dr. Bohrgasse 9</addrLine>
									<postCode>1030</postCode>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">John</forename>
								<surname>Hancock</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Integrative Bioinformatics Vienna (CIBIV)</orgName>
								<orgName type="department" key="dep2">Max F Perutz Laboratories</orgName>
								<orgName type="institution">University of Vienna and Medical University of Vienna</orgName>
								<address>
									<addrLine>Dr. Bohrgasse 9</addrLine>
									<postCode>1030</postCode>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sequence analysis CODOC: efficient access, analysis and compression of depth of coverage signals</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">30</biblScope>
							<biblScope unit="page" from="2676" to="2677"/>
							<date type="published" when="2014">18 2014</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btu362</idno>
					<note type="submission">Received on March 16, 2014; revised on April 30, 2014; accepted on May 21, 2014</note>
					<note>Associate Editor: Contact: niko.popitsch@univie.ac.at Supplementary information: Supplementary data and usage exam-ples are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Current data formats for the representation of depth of coverage data (DOC), a central resource for interpreting, filtering or detecting novel features in high-throughput sequencing datasets, were primarily designed for visualization purposes. This limits their applicability in stand-alone analyses of these data, mainly owing to inaccurate representation or mediocre data compression. CODOC is a novel data format and comprehensive application programming interface for efficient representation, access and analysis of DOC data. CODOC compresses these data $4–32Â better than the best current comparable method by exploiting specific data characteristics while at the same time enabling more-exact signal recovery for lossy compression and very fast query answering times. Availability and implementation: Java source code and binaries are freely available for non-commercial use at http://purl.org/bgraph/codoc.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Depth of coverage (DOC) data are one-dimensional discrete genomic signals describing the number of reads from a highthroughput sequencing (HTS) experiment covering each position in a reference genome. DOC is determined by traversing shortread alignments column-wise and counting the number of mapped reads overlapping with each individual position (i.e. its coverage). DOC is a primary source of information for the detection of structural variants where it is assumed that a gain/loss of genetic material results in increased/decreased coverage. The same fundamental assumption is exploited in RNA sequencing where estimated expression rates are calculated from normalized coverage signals. DOC also plays a central role in, e.g. identifying novel transcripts, characterizing the alternative splicing landscape of a sample or comparing multiple SNP-calling datasets where it is required to determine what genomic regions are sufficiently covered in all considered datasets to be comparable among each other (<ref type="bibr">Ameur et al., 2011;</ref><ref type="bibr">Duan et al., 2013;</ref><ref type="bibr">Meynert et al., 2013;</ref><ref type="bibr">Teo et al., 2012</ref>). Current data formats—Broad's tiled data file format, TDF, and UCSC's BigWig format (<ref type="bibr">Kent et al., 2010;</ref><ref type="bibr">Thorvaldsd ottir et al., 2013</ref>)—were optimized for efficient display of DOC in genomic browsers but not for their stand-alone analysis. coverages) can be read from these external lists at decompression time (<ref type="figure">Fig. 1</ref>). This further reduces the number of required codewords while at the same time preserving (near-) exact DOC values at/close to variant positions. Blocked encoding and compression. Codewords are converted to byte representations using different encoders: chromosome identifiers are stored by standard RLE, position offsets by differential Golomb/Rice encoding (<ref type="bibr">Golomb, 1966</ref>), and coverage values are stored unencoded. Eventually, all encoded bytes are additionally compressed by a general-purpose compression algorithm (gzip or bzip2). The data are then split into blocks (BITs) that contain a configurable maximum of codewords (100 000 by default), which enables efficient random access at decompression time. Decompression. Signal reconstruction is done by linear interpolation at neighboring key positions. Decompression starts by constructing an in-memory interval tree (<ref type="bibr">Cormen et al., 2001</ref>) of BITs and their byte offsets. When a particular genomic position is queried, CODOC consults this index and loads the BIT containing the queried position from disk, finds the key positions and their coverage values by binary search and reports the interpolated result. CODOC caches accessed BITs using a leastrecently used strategy to exploit spatial locality of queries to close genomic positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS AND DISCUSSION</head><p>In our evaluation, CODOC outperforms BigWig ($50–70Â), lossless TDF ($4–32Â) and lossy TDF ($6–14Â) considerably with respect to compression ratios (Supplementary Material). In lossy mode, our method provides better signal reconstruction in low-coverage regions at the cost of less accuracy for higher coverage values when compared with lossy TDF. This is by design, as we consider exact reconstruction of low-coverage (especially uncovered) regions as more important because most thresholds (e.g. for SNP filtering) are also established in this range. CODOC also extracts/compresses data considerably faster (2–5Â for lossless modes) than TDF, being slower only for some lossy configurations. Our API answered random-access queries to uncached regions in $0.1–0.4 s and queries to cached BITs in the sub-millisecond range. API functions. In addition to good data compression, our Java API supports efficient random and sequential access to compressed DOC data and provides several useful access methods that distinguish it from other toolsets, thereby facilitating the development of novel data analysis tools. Notable functions include strand-specific DOC extraction, querying of regions above/ below given coverage thresholds, calculation of minimum/average coverage per region of interest, signal rescaling (normalization) or pairwise combination of multiple DOC signals (e.g. for calculating average, difference or minimum signals). CODOC also enables users to choose from various quantization strategies (which allows balancing of signal reconstruction accuracy and required storage space) or to implement own corridor functions. One special API feature is that it reports strict upper and lower error bounds for returned interpolated values based on the quantization corridor at this position. This enables applications to estimate potential errors and react respectively. Conclusions. DOC helps to answer central questions of many HTS data analyses: How abundant is a certain transcript in my data? What genomic regions are sufficiently covered/accessible for certain analyses? What sequenced genomic regions in my data are amplified/deleted? CODOC helps researchers to answer these questions in a fast and accurate way. It is, to the best of our knowledge, the first open data format and API that was specially designed for the computational analysis of such data; by this it complements current formats developed with highly specific application scenarios (e.g. display in a genomic browser) in mind. Besides its comprehensive API functionalities, it is particularly the efficient compression of CODOC that proves beneficial when dealing with the enormous amounts of data produced by current HTS technologies.</p></div>
			<note place="foot">Consequently, they are not optimized for strong data compression or result in very lossy signal reconstruction, which makes them unfavorable for storing, processing and sharing large amounts of such data. Therefore, we developed CODOC, a novel data format and application programming interface (API) especially focusing on good data compression and exact signal reconstruction. 2 COMPRESSION ALGORITHM CODOC supports lossy and lossless data compression. Generally, DOC data are compressed by run-length encoding (RLE), exploiting several HTS data characteristics to get longer and thus overall fewer run-lengths, which finally requires less storage space. This is achieved by using quantization intervals (&apos;corridors&apos;) that define an upper and a lower bound for coverage values. A corridor interval is calculated from an initial coverage value cov i at a genomic position pos i =ðchr i ; off i Þ, consisting of a chromosomal identifier and an offset value, and possibly additional parameters. Starting from pos i , CODOC iterates over subsequent positions pos i+1 ; pos i+2 ; ::: until it finds a value cov e at position pos e that lies outside of the corridor. Next, a new codeword consisting of this chromosomal position, the coverage right before the corridor was left (cov eÀ1 ) and the current coverage cov e is created. This latter value then acts as the initial coverage of the next corridor. By this, CODOC creates codewords only at positions showing a substantial signal change and stores exact coverage values and signal slopes at such &apos;key positions&apos;. Quantization corridors. CODOC supports several different and even user-defined quantization corridor schemas, including uniform and non-uniform alternatives (Supplementary Material). An example for the latter is our proposed default corridor schema (Fig. 1). C def ðp; cov i Þ = ½Roundðcov i Â ð1 À pÞÞ; Roundðcov i Â ð1+pÞÞ with p 2 ½0; 1 being a parameter to control compression &apos;lossyness&apos;. In this schema, the corridor height shrinks with low and grows with high initial coverage values. Consequently, the signal in low-coverage regions can be reconstructed more accurately, as already small changes will make it leave the (narrower) corridors, which results in more fine-grained sampling in such areas. Note that p = 0 actually results in lossless data compression, while larger values for p compress the data in an increasingly lossy manner. External information. Users may pass lists of genetic variants (e.g. VCF files) to our compressor, which considers these variant positions as &apos;key positions&apos; without storing corresponding codewords, as the respective information (including variant</note>

			<note place="foot">ß The Author 2014. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The author thanks Stefan Leitich for helpful discussions about audio compression methods and several CIBIV colleagues and the reviewers for critically reading the manuscript.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2677</head><p>CODOC</p></div>			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>