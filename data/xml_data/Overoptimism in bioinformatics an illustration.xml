
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gene expression Over-optimism in bioinformatics: an illustration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010">2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Monika</forename>
								<surname>Jelizarow</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics, Biometry and Epidemiology</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<addrLine>Marchioninistr. 15</addrLine>
									<postCode>81377</postCode>
									<settlement>Munich, Germany</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Vincent</forename>
								<surname>Guillemot</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics, Biometry and Epidemiology</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<addrLine>Marchioninistr. 15</addrLine>
									<postCode>81377</postCode>
									<settlement>Munich, Germany</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Signal Processing and Electronics Systems -3, rue Joliot Curie, Plateau de Moulon</orgName>
								<orgName type="laboratory">SUPELEC Sciences des Systèmes (E3S)</orgName>
								<address>
									<postCode>91192</postCode>
									<settlement>Gif-sur-Yvette Cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Arthur</forename>
								<surname>Tenenhaus</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Signal Processing and Electronics Systems -3, rue Joliot Curie, Plateau de Moulon</orgName>
								<orgName type="laboratory">SUPELEC Sciences des Systèmes (E3S)</orgName>
								<address>
									<postCode>91192</postCode>
									<settlement>Gif-sur-Yvette Cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Korbinian</forename>
								<surname>Strimmer</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Medical Informatics, Statistics and Epidemiology</orgName>
								<orgName type="institution">University of Leipzig</orgName>
								<address>
									<addrLine>Härtelstr. 16-18</addrLine>
									<postCode>04107</postCode>
									<settlement>Leipzig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName>
								<forename type="first">Anne-Laure</forename>
								<surname>Boulesteix</surname>
							</persName>
							<email>boulesteix@ibe.med.uni-muenchen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics, Biometry and Epidemiology</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<addrLine>Marchioninistr. 15</addrLine>
									<postCode>81377</postCode>
									<settlement>Munich, Germany</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gene expression Over-optimism in bioinformatics: an illustration</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">26</biblScope>
							<biblScope unit="issue">16</biblScope>
							<biblScope unit="page" from="1990" to="1998"/>
							<date type="published" when="2010">2010</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btq323</idno>
					<note>[12:16 19/7/2010 Bioinformatics-btq323.tex] BIOINFORMATICS ORIGINAL PAPER Associate Editor: John Quackenbush, such that the study is completely reproducible. Contact:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: In statistical bioinformatics research, different optimization mechanisms potentially lead to &apos;over-optimism&apos; in published papers. So far, however, a systematic critical study concerning the various sources underlying this over-optimism is lacking. Results: We present an empirical study on over-optimism using high-dimensional classification as example. Specifically, we consider a &apos;promising&apos; new classification algorithm, namely linear discriminant analysis incorporating prior knowledge on gene functional groups through an appropriate shrinkage of the within-group covariance matrix. While this approach yields poor results in terms of error rate, we quantitatively demonstrate that it can artificially seem superior to existing approaches if we &apos;fish for significance&apos;. The investigated sources of over-optimism include the optimization of datasets, of settings, of competing methods and, most importantly, of the method&apos;s characteristics. We conclude that, if the improvement of a quantitative criterion such as the error rate is the main contribution of a paper, the superiority of new algorithms should always be demonstrated on independent validation data. Availability: The R codes and relevant data can be downloaded from</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In statistical bioinformatics research, the reported results on the performance of new algorithms are known to be over-optimistic, as recently discussed in a letter to the editors of Bioinformatics (<ref type="bibr" target="#b9">Boulesteix, 2010</ref>). The current article aims at illustrating the different mechanisms leading to over-optimism through a concrete example from an active methodological research field. The first and perhaps most obvious reason for over-optimism is that researchers sometimes randomly search for a specific dataset such that their new method works better than existing approaches, yielding a so-called 'dataset bias'. While a method cannot reasonably be expected to yield 'universally better' results in all datasets, * To whom correspondence should be addressed. it would be wrong to report only favorable datasets without mentioning and/or discussing the other results. This strategy induces an optimistic bias. This aspect of over-optimism is quantitatively investigated in the study by<ref type="bibr" target="#b48">Yousefi et al. (2010</ref>) and termed as 'optimization of the dataset' in this article. The second source of over-optimism, which is related to the optimal choice of the dataset mentioned above, is the optimal choice of a particular setting in which the superiority of the new algorithm is more pronounced. For example, researchers could report the results obtained after a particular feature filtering which favors the new algorithm compared with existing benchmark approaches. This mechanism, which is strongly related to data overfitting, is termed as 'optimization of the settings' in this article. The third source of over-optimism is related to the choice of the existing benchmark methods applied for comparison purposes. Researchers are supposed to compare their new algorithm to state-of-the-art methods, but may consciously or subconsciously choose suboptimal existing methods and exclude the best competing methods from the comparison for any reason, e.g. because running the software demands very particular knowledge, because previous authors excluded these methods as well, because the methods induce high-computational expense or because they belong to a completely different family of approaches and thus do not fit in the considered framework. Then the new algorithm artificially seems better than competing approaches and over-optimistic results on the superiority of the new algorithm are reported—because the best competing approaches are disregarded. Since the definition of state-of-theart methods is often ambiguous, such problems may occur even when researchers are decided to perform a fair comparison. This mechanism, also known as 'straw-man phenomenon' is termed as 'optimization of the competing methods' in this article. Finally, researchers often tend to optimize their new algorithms to the datasets they consider during the development phase (<ref type="bibr" target="#b9">Boulesteix, 2010</ref>). This mechanism essentially affects all research fields related to data analysis such as statistics, machine learning or bioinformatics. Indeed, the trial-and-error process constitutes an important component of data analysis research. As most inventive ideas have to be improved sequentially before reaching an acceptable maturity, the development of a new method is per se an unpredictable search process. The problem is that, as stated by the Bioinformatics editorial team (<ref type="bibr" target="#b37">Rocke et al., 2009</ref>), this search process leads to an artificial optimization of the method's characteristics to the considered datasets. Hence, the superiority of the novel method over an existing method [as measured, e.g. through the difference between<ref type="bibr">[12:16 19/7/2010 Bioinformatics-btq323.tex]</ref>Page<ref type="bibr">: 1991 1990–1998</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Over-optimism in bioinformatics</head><p>the cross-validation (CV) error rates] is sometimes considerably overestimated. In a concrete medical prediction study, fitting a prediction model and estimating its error rate using the same training dataset yields a downwardly biased error estimate commonly termed as apparent error. In the same spirit, computing CV error rates with different classifiers and systematically selecting the classifier variant with the smallest error rate yields a substantial optimization bias (<ref type="bibr" target="#b7">Boulesteix and Strobl, 2009</ref>). Similarly, developing a new algorithm (i.e. selecting one of many variants) and evaluating it by comparison to existing methods using the same dataset may lead to optimistically biased results in the sense that the new algorithm's characteristics overfit the used dataset. This source of over-optimism is termed as 'optimization of the method's characteristics' in this article. The four mechanisms discussed above may lead to overoptimistic conclusions regarding the superiority of the new method compared with existing methods. The importance of validation with independent data has recently gained much attention in biomedical literature. For instance, we refer to the empirical study by<ref type="bibr" target="#b12">Daumer et al. (2008)</ref>which points out the usefulness of a pre-publication validation strategy based on data splitting. To our knowledge, no such study was performed in the context of methodological bioinformatics research and this issue has long been underconsidered in the literature. The present article aims at filling this gap. It reviews and illustrates the problem of validation and false research findings through a concrete example from a current research field: the incorporation of prior biological knowledge on gene functional groups into highdimensional microarray-based classification. The 'promising idea' we pursue here is to extend the shrinkage correlation estimator of<ref type="bibr" target="#b38">Schäfer and Strimmer (2005)</ref>to incorporate prior knowledge on gene functional groups with the aim to improve the performance of linear discriminant analysis (LDA). This approach combines a simple and well-established statistical method, regularized discriminant analysis (DA), with the incorporation of prior biological knowledge on gene functional groups, a popular concept that has attracted a lot of attention in the last few years (<ref type="bibr" target="#b6">Binder and Schumacher, 2009;</ref><ref type="bibr" target="#b19">Guillemot et al., 2008;</ref><ref type="bibr" target="#b21">Hall and Xue, 2010;</ref><ref type="bibr" target="#b26">Jacob et al., 2009;</ref><ref type="bibr" target="#b33">Li and Li, 2008;</ref><ref type="bibr" target="#b36">Rapaport et al., 2007;</ref><ref type="bibr" target="#b40">Slawski et al., 2010;</ref><ref type="bibr">Tai and Pan, 2007a, b;</ref><ref type="bibr" target="#b49">Yousef et al., 2009</ref>). Intriguingly, while this method does not yield any improvement in terms of prediction error rate, it is straightforward to produce overoptimistic results via any of the four mechanisms discussed above. Note that we could have used virtually any method to illustrate these mechanisms of over-optimism. However, classification with prior knowledge addresses a non-trivial and still unanswered research question within an evolving bioinformatics field. Based on this example, we demonstrate quantitatively that optimization of the dataset, optimization of the settings, optimization of the competing methods and, most importantly, optimization of the method's characteristics can lead to substantially biased results and overoptimistic conclusions on the superiority of the new method. Note that this study is deliberately of empirical nature. We neither model the different sources of over-optimism theoretically nor do we derive analytical expressions of the resulting bias for simplified situations, because we feel it would not reflect the complexity of the addressed mechanisms. Instead, we stick to concrete observations to illustrate what consciously or subconsciously happens in virtually all methodological projects—possibly including our own projects. We are convinced that most biased results are presented by mistake and that the involved researchers are disposed to make efforts toward better practice. It would be naive to believe that overoptimism in published research can be completely avoided, but we feel that a quantitative demonstration of the optimistic bias affecting methodological research may perhaps increase awareness on such problems and give many researchers food for thoughts. The remainder of this article is organized as follows. The promising idea is briefly sketched in Section 2.1 to make our considerations on validation more understandable. The design of the analysis is described in Section 2.2, while Section 3 presents the results of the new and existing methods on four real-life datasets and the different interpretations depending on whether one fishes for significance or not. Other potential sources of biases, possible explanations for the disappointing error rates of the promising idea and further perspectives are presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A 'promising idea'</head><p>In this section, we describe the technical details of the classification method which we later use to illustrate the diverse sources and pitfalls of overoptimism. Readers who are not interested in the methodological part may skip this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">DA and regularization</head><p>We consider a high-dimensional dataset with continuous predictors such as microarray gene expression data, with the aim to predict a categorical response variable of interest, e.g. the disease status or the long-term disease outcome. DA is a widely used classification method. DA is based on the assumption that the random vector x of predictors follows a multivariate normal distribution x|(Y = r) ∼ N(µ r , r ) within each class r (for r = 1,...,c). A new observation x new is then assigned to the class with maximal posterior probability. This decision rule can be formulated in terms of a simple decision function which is linear in x new if it is assumed that 1 = ··· = c , yielding the so-called LDA. Most importantly, the decision function involves the inverse −1 of the covariance matrix. In standard n &gt; p settings, −1 is simply estimated through the inverse˜Sinverse˜ inverse˜S −1 of the pooled estimator˜Sestimator˜ estimator˜S of the within-covariance matrix, which is itself defined as a weighted sum of the unbiased estimators of the within-class covariance matrices. More technical details on classical LDA are given in the Additional File 1 from our web site. In the high-dimensional setting considered here the pooled covariance estimator˜Sestimator˜ estimator˜S is singular and hence not invertible. In regularized LDA (RLDA), the singularity problem is resolved by employing a shrinkage (<ref type="bibr" target="#b16">Efron and Morris, 1977;</ref><ref type="bibr" target="#b42">Stein, 1955</ref>) rather than an empirical estimator of the covariance—see the seminal paper by<ref type="bibr" target="#b17">Friedman (1989)</ref>. More recently, variants of RLDA are considered, e.g., in<ref type="bibr" target="#b20">Guo et al. (2007)</ref>and Ahdesmäki and Strimmer (2010).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">RLDA with KEGG</head><p>An increasingly popular approach is to regularize the within-class covariance by incorporating external biological knowledge from databases such as the Kyoto Encyclopedia of Genes and Genomes (KEGG;<ref type="bibr" target="#b27">Kanehisa and Goto, 2000</ref>). The underlying motivation of this approach is to improve both the prediction accuracy and the results' interpretability. KEGG is a freely available database of biological systems consisting of multiple subdatabases. KEGG PATHWAY as one of these subdatabases contains a collection of pathway maps representing recent knowledge on molecular interaction and reaction networks for metabolism, various cellular processes and human diseases (<ref type="bibr" target="#b27">Kanehisa and Goto, 2000</ref>). More precisely, pathways are represented as graphs in which the edges stand<ref type="bibr">Page: 1992</ref><ref type="bibr" target="#b2">Page: 1990</ref><ref type="bibr" target="#b2">Page: –1998</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M.Jelizarow et al.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0">otherwise</head><p>The notation i ∼ j means that genes i and j are connected, i.e. genes i and j are in the same gene functional group. The term s ij denotes the entry of the unbiased covariance matrix in row i, column j.</p><p>for the chemical reactions or relations and the vertices stand for the genes involved. In the context of microarray-based classification, Tai and Pan (2007a) assume that a KEGG pathway forms a gene functional group. They postulate that genes from the same functional group tend to be more correlated than genes from different functional groups, and that information from KEGG can thus be used to improve the modeling of between-genes correlation in the context of classification. Starting from these attractive ideas, we propose an alternative simple approach to incorporate prior knowledge from KEGG into the estimation of the correlation, with applications to LDA. The promising idea can be seen as a further variant of RLDA incorporating biological knowledge on gene functional groups extracted from KEGG via a modified shrinkage estimator of the covariance matrix, as outlined in Sections 2.1.3 and 2.1.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">The shrinkage estimator SHIP incorporating prior knowledge To</head><p>address the methodological challenges arising from the n p data situation (the pooled estimate˜Sestimate˜ estimate˜S of the covariance matrix is not invertible), we now propose a covariance estimation procedure which we refer to as SHIP standing for SHrinking and Incorporating Prior knowledge. The resulting covariance estimator SHIP is based on the Stein-type shrinkage estimator discussed by<ref type="bibr">Wolf (2003, 2004</ref>) and applied to correlation by<ref type="bibr" target="#b38">Schäfer and Strimmer (2005)</ref>in the context of high-dimensional genomic data. Additionally, the new estimator incorporates prior biological knowledge on gene functional groups extracted from the KEGG database. In a few words, the shrinkage estimator originally proposed by Ledoit and Wolf is the asymptotically optimal convex linear combination * = λT+(1−λ)S, where λ ∈<ref type="bibr">[0,</ref><ref type="bibr">1]</ref>denotes the analytically determined optimal shrinkage intensity, T stands for a structured covariance target, and S is the unstructured standard unbiased empirical covariance matrix. The resulting 'shrinkage estimator' of the covariance matrix is then invertible (provided T is chosen adequately) and stabilized. The optimal shrinkage intensity λ is determined with respect to a quadratic loss function, which is common and intuitive in statistical decision theory, resulting in a simple analytical formula (<ref type="bibr" target="#b38">Schäfer and Strimmer, 2005</ref>). See Additional File 1 from our web site for more details on the computation of λ.</p><p>The covariance target T plays an essential role in the computation of the shrinkage estimator by Ledoit and Wolf. Its choice, however, turns out to be very complex. On the one hand, T is required to be positive definite and to involve only a small number of free parameters. On the other hand, it should reflect important characteristics of the covariance structure between the variables (genes). An overview of commonly used covariance targets A to F is given in<ref type="bibr" target="#b38">Schäfer and Strimmer (2005)</ref>. In this article, we consider targets D and F with constant correlation as reference methods (see<ref type="figure" target="#tab_1">Table 1</ref>, left and middle). In order to incorporate information from KEGG PATHWAY, we propose a modified version of target F where pairs of connected genes (i.e. genes from the same gene functional group) have non-zero common correlation ¯ r, as in<ref type="bibr" target="#b43">Tai and Pan (2007a)</ref>. This correlation is simply given as the mean correlation of all pairs of connected genes. In case a gene does not occur in any gene functional group, we assume this gene forming its own group with group size one as in Tai and Pan (2007a). The resulting target G is displayed in<ref type="figure" target="#tab_1">Table 1</ref>where T is defined according to target G and the optimal shrinkage intensity λ can be computed analytically (see Additional File 1 from our web site).</p><p>The shrinkage covariance estimator SHIP is implemented in the R package 'SHIP' which is publicly available from the companion web site and from the CRAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">LDA using</head><p>SHIP The resulting estimator SHIP of the covariance matrix can then simply be used in the context of LDA. In a nutshell, we compute the shrinkage estimatorsSHIP separately for each class r = 1,...,c and subsequently pool these within-class shrinkage estimators according to the standard procedure known from LDA. See Additional File 1 from our web site for more details. Note that the resulting pooled estimator is not necessarily positive definite because the target is not always positive definite. However, it is typically much better conditioned thañ S. To cope with this problem, we simply compute the well-known Moore–Penrose pseudoinverse (<ref type="bibr" target="#b35">Penrose, 1955</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Design of the study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Datasets</head><p>In this study, we successively consider four publicly available microarray datasets to illustrate the potential optimization of the dataset and demonstrate the importance of validation on different datasets. Golub's leukemia dataset (n = 72, p = 7129) is part of the R package 'golubEsets' (<ref type="bibr" target="#b18">Golub, 2010</ref>), while the CLL dataset (n = 22, p = 12 625) is available from the package 'CLL' (<ref type="bibr" target="#b47">Whalen, 2010</ref>). The prostate dataset by<ref type="bibr" target="#b39">Singh et al. (2002)</ref>(n = 102, p = 12 625) and the breast cancer dataset by<ref type="bibr" target="#b46">Wang et al. (2005)</ref>(n = 286, p = 22283) are available from gene expression omnibus. We normalized them using the GC Robust Multi-Array Average method. The resulting data matrices are available from the companion web site. All datasets include a binary outcome variable which has to be predicted based on gene expression data. A brief overview of the datasets is given in Additional File 1 from our web site.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Settings</head><p>Prediction accuracy is estimated using the well-established 10 times 5-fold CV evaluation scheme. The 5-fold CV is repeated 10 times in order to achieve more stable results (<ref type="bibr" target="#b8">Boulesteix et al., 2008;</ref><ref type="bibr" target="#b10">Braga-Neto and Dougherty, 2004</ref>). We focus on the average misclassification rate as a measure of prediction accuracy, i.e. the average test error obtained over all 10×5 = 50 test sets. In order to limit the computational effort and to reduce the influence of noise, we do not employ all available genes of a dataset, but perform variable selection (for each learning set successively, as commonly recommended). We use three variable selection criteria: the standard t-test, the Limma procedure by Smyth (2004) and the standard rank-based Wilcoxon test, each with four different numbers of selected genes (p * = 100,200,500,1000). Hence, we obtain 3×4 = 12 combinations of selection procedures and numbers of selected genes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Competing methods</head><p>For comparison purposes, we furthermore apply the diagonal LDA (DLDA), the nearest shrunken centroids (NSC) method by<ref type="bibr" target="#b45">Tibshirani et al. (2002)</ref>that is also called prediction<ref type="bibr">Page: 1993</ref><ref type="bibr" target="#b2">Page: 1990</ref><ref type="bibr" target="#b2">Page: –1998</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Over-optimism in bioinformatics</head><p>analysis with microarrays (PAM) and support vector machines (SVM) as competing approaches. We perform variable selection for DLDA with p * = 100,200,500,1000 and three selection methods successively, Following common practice, we skip the variable selection for NSC and SVM where the influence of irrelevant genes is reduced automatically. Tuning parameters for NSC (shrinkage parameter) and SVM (cost) are optimized via internal 3-fold CV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Method's characteristics</head><p>When developing a new algorithm, researchers often adapt their method sequentially depending on their experiences with example datasets and preliminary results. Many variants that are tried out at this stage finally turn out to yield bad results or fail for any other reason. In contrast to the aspects of the analysis design discussed above, this aspect often remains unmentioned when writing a paper, except perhaps a few remarks in the discussion. However, the variants that are tried out during the development of new algorithms are in a broad sense part of the design of the analysis. Indeed, they are often assessed using the same procedures as the final new algorithm that is eventually published. When assessing the promising idea described in Section 2.1, we also thought of possible variants of the proposed RLDA incorporating prior knowledge. In contrast to standard practice, we publicly mention all these variants in the present article and demonstrate what happens when one systematically tries to optimize the new algorithm with regard to its characteristics. Henceforth, the promising idea outlined in Section 2.1 is referred to as rlda.TG unless otherwise emphasized. More precisely, the term rlda.TG specifies the RLDA with the shrinkage estimators of the within-class covariance matrices being based on the knowledge-based covariance target G as introduced in Section 2.1.3. During the development phase, we successively considered the 10 following variants of rlda.TG termed as rlda.TG (1) , ..., rlda.TG (10). These 10 variants can be divided into two groups. The first group comprises rlda.TG (1) to rlda.TG (7) which differ in the assignment of 'problematic' genes. By problematic genes, we mean either genes that are in no functional group or genes that are in at least two different functional groups, thus making their assignment to a functional group impossible or arbitrary, respectively. In contrast to the original variant rlda.TG, variant rlda.TG (1) simply excludes genes that are not in any gene functional group (∼50% in each dataset) from the analysis. Variant rlda.TG (2) differs from rlda.TG in the treatment of genes occurring in multiple gene functional groups: they are simply eliminated from the dataset. In contrast, both rlda.TG (3) and rlda.TG (4) handle these genes similarly to Tai and Pan (2007a): if a gene occurs in multiple gene functional groups, it is considered as belonging to the gene functional group with the smallest or largest number of genes, respectively. If the smallest (respectively largest) gene functional group is not unique, rlda.TG (3) and rlda.TG (4) choose one of them by chance and consider it as the smallest (respectively largest). Note that, while these two variants may seem arbitrary, they have been applied in a previous relevant publication (<ref type="bibr" target="#b43">Tai and Pan, 2007a</ref>). Trying them out thus appears to be natural. The methods rlda.TG (5) to rlda.TG</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">General approach</head><p>This section presents different interpretations of the results of the new methods rlda.TG, rlda.TG (1) , ..., rlda.TG (10) and existing methods on four real-life datasets. While Section 3.2 presents the performance of the new algorithm(s) from an over-optimistic point of view (i.e. after fishing for significance), Section 3.3 follows a less biased approach based on validation with independent datasets. The four optimization mechanisms are introduced sequentially and independently of each other in Section 1. However, they are in fact tightly linked in practice, thus making a perfectly realistic study very difficult. In Section 3.2, we consider a simplified optimization process mimicking one of many possible optimization scenarios for illustration purposes. We are aware of the many other potential schemes, but an exhaustive study would go beyond the scope of this article. We feel that the chosen example reflects the influence of the four mechanisms reasonably well. In addition to the results provided in this section, a more extensive report of the results is given in Additional File 2 from our web site. In this study, all four datasets are first analyzed independently of each other in Section 3.2 to mimick what would happen if researchers did not try to validate their results on different datasets. It is then shown in Section 3.3 that a proper validation strategy, in which researchers do not use the same datasets to develop and evaluate their new algorithm, leads to much less favorable results. The whole analysis is completely reproducible using the R codes available from our web site.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">An (over-)optimistic view</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Optimization of the settings</head><p>We first consider the new promising method rlda.TG while ignoring its variants rlda.TG (1) , ..., rlda.TG (10). For a given dataset, someone 'fishing for significance' may look for the variable selection scheme and number p * of selected variables yielding the lowest error rate. In this spirit,<ref type="figure" target="#tab_2">Table 2</ref>the Singh data. Obviously, the classification error rates strongly depend on the variable selection settings. Moreover, there is no universally better setting performing best for all datasets, although settings with small p * tend to yield smaller error rates in general. Researchers who 'fish for significance' would select the setting yielding the minimal error rate for the dataset they consider, thus inducing an optimistic bias through 'optimization of the settings'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Optimization of the method's characteristics</head><p>Moreover, they would certainly try to further improve the new algorithm's performance by considering the additional variants rlda.TG (1) , ..., rlda.TG (10) .<ref type="figure" target="#fig_4">Figure 1</ref>displays the CV error rates of rlda.TG and its variants in the selected setting(s) for each dataset. Especially for the CLL and the Wang dataset, it can be clearly seen that some of the variants decrease the error rate substantially compared to rlda.TG. All in all, we achieve the error rates 0.025 for the Golub data (with rlda.TG (5) ), 0.129 for the CLL data (with rlda.TG (5) ), 0.342 for the Wang data (with rlda.TG (6) ) and 0.078 for the Singh data (with rlda.TG (8)</p><p>). This represents an improvement compared to the bold optimal error rates from<ref type="figure" target="#tab_2">Table 2</ref>, illustrating the mechanism denoted as 'optimization of the method's characteristics'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Optimization of the competing approaches</head><p>Another mechanism of the optimization process is the choice of the competing approaches that are compared with the new algorithm. For each of the four datasets,<ref type="figure" target="#tab_3">Table 3</ref>shows the difference between the error rate of the optimal method in the optimal setting and the error rate of rlda.TD (shrinkage covariance with the diagonal target D), rlda.TF (shrinkage covariance with target F) and DLDA (classical DLDA). These competing approaches are applied after variable selection following the optimal setting identified from<ref type="figure" target="#tab_2">Table 2</ref>. Further, results are shown for two good standard methods without preliminary variable selection: the NSC method and the SVM. Obviously, these competing approaches perform very differently. Hence, the new algorithm's performance appears more or less impressive depending on the competing methods shown in the comparison study. A possible (critical) strategy could be to select the competing approaches depending on the tested 'research hypothesis'. If the hypothesis was that the new algorithm generally improves the performance of state-of-the-art approaches, we would consider as many approaches as possible. If the hypothesis was that it performs better than other LDA approaches, we would consider all LDAbased competitors. If the hypothesis was that the incorporation of correlations is useful, we would consider rlda.TD. If the hypothesis was that the incorporation of correlations becomes better through KEGG pathways, we would consider rlda.TF. This strategy may seem good at first view, but yields some problems. First, the tested hypothesis should not be chosen a posteriori by the researcher based on the results. Indeed, it can be seen from<ref type="figure" target="#tab_3">Table 3</ref>that this also yields a kind of optimization. Second, it may also lead to spurious results. For example, one may conclude from the negative differences D(M opt ,rlda.TF) that KEGG is useful in this context. Another more realistic explanation is that rlda.TG is better than rlda.TF because the estimated correlation matrix is sparser—and not because of the KEGG pathways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Optimization of the dataset</head><p>Some researchers may also 'optimize the dataset' and choose to show only the results that are more favorable to their method. For an extensive study on this problem including theoretical considerations, see<ref type="bibr" target="#b48">Yousefi et al. (2010)</ref>. It can be clearly seen from<ref type="figure" target="#tab_3">Table 3</ref>that the results on the CLL data are much more favorable to our new method than the other three datasets. This is probably due to the very small size (n = 22) implying a high variability and thus<ref type="bibr">Page: 1995</ref><ref type="bibr" target="#b2">Page: 1990</ref><ref type="bibr" target="#b2">Page: –1998</ref>The figures outside the diagonal can be understood as 'validation error rates'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Over-optimism in bioinformatics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. Frequency of selection</head><p>of the 11 investigated variants of rlda.TG over the three variable selection methods (t-test, Limma, Wilcoxon test) and four numbers of genes (100, 200, 500, 1000), i.e. over 3×4 = 12 settings. By 'selection' we mean that the variant yields the smallest error rate over the 11 variants. For example, in the Wang dataset, the lowest error rate is reached by rlda.TG7 in 9 of the 12 considered settings and by rlda.TG6 in only three settings. Note that the 'best' variant is not necessarily unique, i.e. for a specific dataset (row) the frequencies' sum may be &gt;12. stronger optimization effects. The optimization of the dataset and the optimization of the settings may thus be tightly connected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">On the usefulness of validation with fresh data</head><p>Until now, the four datasets were analyzed independently of each other. For each dataset, we obtained an optimal variant combined with an optimal setting that seemingly performed better than existing approaches, see<ref type="figure" target="#tab_3">Table 3</ref>. As previously discussed, these figures are the result of different optimization processes. One of them— the optimization of the method's characteristics—is an inherent component of biostatistics/bioinformatics research and cannot be avoided. Up to a point, the optimization of the settings can also be considered as inherent to data analysis research: for example, nobody expects researchers to focus on settings in which all methods turn out to perform equally bad. So how should we evaluate new methods and report their performance? In this section, we show the importance of a proper validation using datasets that were not used for the algorithm's development.<ref type="figure" target="#tab_4">Table 4</ref>shows the CV error rates of the four combinations of optimal settings and optimal variant when applied on the four datasets. Whereas the error rates on the diagonal are the optimal error rates already mentioned in the previous section, the error rates outside the diagonal can be seen as 'validation error rates' computed on independent fresh datasets. They are obviously much higher than the optimal error rates, illustrating the consequences of the optimization processes. In the same vein,<ref type="figure">Figure 2</ref>displays the number of variable selection settings (out of 3×4 = 12) in which each of the variants rlda.TG, rlda.TG (1) ,..., rlda.TG (10) yields the lowest error rate, for each dataset separately. It can be seen that the 'optimal variant' strongly depends on the dataset (because the four rows are very different) and on the setting (because we have many intermediate values like 2, 3, 4, 5 &lt; 12). There is no clear winner, but readers may have the impression that there is a clear winner if they do not see all the results (i.e. not all datasets or/and not all settings). In conclusion, validation using fresh independent data that were not used in the development phase would have avoided overoptimistic conclusions on the new algorithm's superiority. This kind of validation automatically corrects the bias induced by the optimization of the settings and the optimization of the method's characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Other sources of bias</head><p>As illustrated in Section 3 based on the example of RLDA, the four investigated sources of over-optimism may yield substantially over-optimistic results. Beyond the four mechanisms outlined in this article, various other sources of over-optimism may also affect the reported results. For instance, one might optimize the evaluation criterion: the sensitivity and specificity may yield other results than the error rate, especially in case of strongly unequal class sizes. Both prediction measures are reported in Additional File 2 from our web site. The applied normalization technique may also affect the results (<ref type="bibr" target="#b32">Lim et al., 2007</ref>) and yield optimization potential. Another indirect source of over-optimism is related to technical problems: if an implementation problem occurs with the competing approaches and slightly worsens their results, researchers often tend to spontaneously accept these inferior results. Conversely, they would probably obstinately look for the error if such problems occur with their new algorithm. Note that the validation strategy recommended in this article would not help in this case, since the error in the competing methods would also affect the validation phase. To conclude this enumeration of sources of bias, we point out that the occasional publication of negative results in methodological journals may in the long run encourage researchers to be less biased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">On CV as a potential solution</head><p>Section 3 demonstrates that validation based on independent datasets avoids hasty over-optimistic conclusions and automatically corrects for the optimization of the settings and optimization of the methods' characteristics. A natural question is whether a CV procedure (or related approach) might be used in place of validation with independent validation data. CV is useful to choose the best number of genes and the best variable selection scheme for each method considered in the comparison study. Such a CV correctly addresses the 'optimization<ref type="bibr">Page: 1996</ref><ref type="bibr" target="#b2">1990</ref><ref type="bibr" target="#b2">–1998</ref>of the settings' mechanism and is sometimes used in methodological studies, as recommended in Ambroise and<ref type="bibr" target="#b4">McLachlan (2002)</ref>for the number of genes. From a theoretical point of view, CV could also be applied to select the methods' characteristics (i.e. to select among the variants rlda.TG, rlda.TG (1) , ..., rlda.TG (10) ). In this case, however, the application of a CV procedure is much more problematic in practice because the different variants of rlda.TG (rlda.TG (1) , ..., rlda.TG (10) in our example) are usually not investigated simultaneously. Researchers typically begin with the most intuitive variant. Having realized the latter's sub-optimality (e.g. in terms of error rates) they investigate a few alternative variants, which often requires up to several months. Moreover, presenting first results at conferences often leads to fruitful discussions with other researchers, resulting in further variants of the original method, and so on. While the 10 variants are considered simultaneously in the current article, this process typically drags on in practice, and the variants are investigated rather successively than simultaneously. Therefore, researchers cannot be expected to perform an internal CV to choose between variants they have explored (and rejected) at the beginning of their project. An advantage of validation with fresh data over CV is that it ensures a more stringent separation between data used for development and data used for evaluation. CV might be incomplete in practice, for instance if researchers forget some of the variants they have tried some time ago. In statistical learning terminology, we would say that they select a 'tuning parameter' (here: the methods' characteristics) using the whole training set instead of repeating the selection procedure in each iteration. Such human errors cannot occur if validation is performed with a fresh dataset after having developed a method. Moreover, validation based on other independent datasets has the considerable advantage that it takes the variability between datasets into account, a very important aspect discussed in Section 4.3. Finally, CV induces substantial computational expense. Using a complete embedded CV procedure involving three layers to (i) estimate the error rate; (ii) select the number of genes, the variable selection scheme and some additional tuning parameters of the method internally; and (iii) select the best variant of the method (among rlda.TG, rlda.TG (1) , ..., rlda.TG (10) ) internally rapidly becomes computationally intractable and, in general, cannot be recommended in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">On the difficulty of error rate estimation</head><p>Most importantly, over-optimism due to the various optimization mechanisms results from insufficient sample size. If sample sizes were in the hundreds of thousands, the problems would be solved because they result from imprecision of the error estimates (<ref type="bibr" target="#b23">Hanczar et al., 2010;</ref><ref type="bibr" target="#b48">Yousefi et al., 2010</ref>). Optimization biases occur because CV error estimates have large unknown variance (Braga-Neto and<ref type="bibr" target="#b10">Dougherty, 2004</ref>), and are even virtually uncorrelated with the actual error (<ref type="bibr" target="#b22">Hanczar et al., 2007</ref>) in small sample settings. Thus, the methods/variants/settings yielding the smallest error rates with a particular dataset do not necessarily have the smallest true error rates, hence the risk of over-optimization and the discrepancy between error rates obtained on training and validation datasets. This explains why optimization biases, which are relevant to all statistical research areas, particularly affect the analysis of small sample high-dimensional data.</p><p>The real problem is thus the absence of suitable means of error estimation based on a single dataset. When comparing prediction methods, we would like to reject the 'null hypothesis' that a newly proposed prediction algorithm has an error rate higher than or equal to the error rate of competing approaches. However, this possibility is killed at the outset by using CV on a single dataset because the internal variance (i.e. the variance within a single dataset) can be estimated but not the external variance (i.e. the variance between datasets). In a way, this external variance is taken into account when applying the algorithms to validation data. Note that the external variance could be potentially taken into account by using several training datasets. However, the estimation of external variability based on a small number of datasets is also a non-trivial issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">On simulations as a potential solution</head><p>Another way to take this 'between-datasets' variance into account is to perform simulation studies. However, while simulations are often extremely useful (<ref type="bibr" target="#b34">Mehta et al., 2004</ref>), some aspects of the developed methods can only be evaluated through real data studies. A general problem in high-dimensional data analysis is that it is very difficult to generate realistic datasets. Our example with KEGG-based RLDA can be seen as an extreme case, since it involves a complex cluster structure with clusters of different sizes that potentially overlap. An additional difficulty is that the performance of our promising idea essentially depends on two components: the incorporation of cluster structure through target G and the usefulness of KEGG in this context. While simulations may address the first aspect at the price of simplifying assumptions on the data structure, the second aspect can only be assessed through real data studies. Finally, we point out that simulation studies are potentially also affected by conscious or subconscious optimization mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Potential pitfalls of the promising idea</head><p>In our study, the optimistic results obtained with the selected variants of RLDA in the selected settings turn out to break down when validated based on 'fresh' validation datasets. This indicates that the seemingly favorable results were rather the consequence of intense optimization than the illustration of a real superiority of the new method. In a nutshell, let us point out possible reasons explaining the disappointing error rates of the initially promising idea. A general finding of<ref type="bibr" target="#b5">Bickel and Levina (2004)</ref>is that the DLDA highly outperforms the standard LDA in 'huge-dimensional' data. Assuming independence between the predictor variables hence does not impair the classification performance, but rather yields improvement when n p. This phenomenon has often been reported in the literature (<ref type="bibr" target="#b13">Domingos and Pazzani, 1997;</ref><ref type="bibr" target="#b15">Dudoit et al., 2002</ref>), and it is shown under broad conditions by Bickel and Levina (2004). Our results confirm this finding in the sense that incorporating between-genes correlations tends to yield higher error rates with increasing p * . Another aspect to be considered is whether the assumptions underlying the new approach do apply, i.e. whether these assumptions are consistent (at least not evidently inconsistent) with intrinsic properties of the investigated data. Our own method postulates that genes from the same pathway tend to be more correlated than genes from different pathways. From the current point of view, however, the assumption that the between-genes correlation structure is reflected in KEGG pathways and vice versa is<ref type="bibr">Page: 1997</ref><ref type="bibr" target="#b2">Page: 1990</ref><ref type="bibr" target="#b2">Page: –1998</ref>a widespread but vague assumption on the part of (bio)statisticians. Histograms of the Pearson's correlations between connected genes (i.e. genes sharing at least one pathway) and not connected genes (i.e. genes that do not have any pathway in common) are depicted in<ref type="figure" target="#fig_5">Figure 3</ref>, separately for each dataset. In this rather limited study based on only four datasets, genes belonging to the same pathway do not seem to be noticeably more correlated. To some extent, the vague assumption made by biostatisticians might be inappropriate because Pearson's correlation is a measure of linear association. For example, genes from the same pathway might correlate only in case the pathway is activated, hence inducing a complex dependence pattern that cannot be captured by linear correlation measures. Considering more complex association structures beyond Pearson's correlation might provide more insight into the interrelation between KEGG pathways and the between-genes association (<ref type="bibr" target="#b25">Hausser and Strimmer, 2009</ref>). Finally, let us point out that the 'disappointing results' reported in this article refer solely to the investigated combination between target G, KEGG and (R)LDA—neither to the individual components of the combination nor to further more sophisticated combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Over-optimism in bioinformatics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Epistemological considerations</head><p>More and more applications in bioinformatics and systems biology require systematic integration of data from different sources. Consequently, statistical approaches for incorporating prior biological knowledge are becoming more important. However, as yet it is unclear how to properly take account of this information and how to best handle the data from multiple sources. Furthermore, this raises questions on the consistency and also on the relevance of the available information. Facing the lack of complete biological knowledge, one must be aware that the employed information might not be specific enough for the phenotype considered. More generally, the need for a sound epistemological basis of statistical methods for high-dimensional biological data has to be addressed. Even though the relevance of this problem has been pointed out in the literature (<ref type="bibr" target="#b11">Braga-Neto, 2007;</ref><ref type="bibr" target="#b14">Dougherty, 2008;</ref><ref type="bibr" target="#b29">Keller, 2005;</ref><ref type="bibr" target="#b34">Mehta et al., 2004</ref>), it is not adequately considered yet in biostatistical and bioinformatical research. A key aspect of method development is that the apparent usefulness (as measured, e.g. by the error rate) of a method cannot be equated with the method's validity. In particular, the question of validity has to be resolved first, requiring a detailed exposition of the new method's characteristics and properties including the biological relevance of these characteristics and properties. The substantive context is indeed crucial in the context of methodological research (<ref type="bibr" target="#b28">Keiding, 2010</ref>). This is outlined by<ref type="bibr" target="#b34">Mehta et al. (2004)</ref>who propose a framework in which the epistemological foundation of statistical methods for high-dimensional data can be evaluated. Only once this has been done, the new method can be assessed objectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>[12:16 19/7/2010 Bioinformatics-btq323.tex]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><figDesc>(7) are obtained by combining rlda.TG (1) (i.e. eliminating genes that are in no functional group) with rlda.TG (2) , rlda.TG (3) and rlda.TG (4) (to handle genes occurring in more than one functional group). The second group comprises rlda.TG (8) , rlda.TG (9) and rlda.TG (10) which are based on a redefinition of the covariance target G. Variant rlda.TG (8) involves two parameters for the correlation (the average ¯ r + of the positive correlations and the average ¯ r − of the negative correlations) instead of the single parameter ¯ r, to take into account that genes from the same pathway might be negatively correlated. The variant rlda.TG (9) completely ignores negative correlations and computes the average correlation ¯ r based on positive correlations only. Finally, rlda.TG (10) tests the correlations (at the significance level 0.05) and sets the non-significant correlations to zero before the mean correlation ¯ r is computed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><figDesc>gives the classification error rates obtained with the 3×4 combinations of variable selection scheme and number p * of selected variables in each of the four investigated datasets. The bold numbers indicate the minimal error rate for each dataset. The standard errors of the error rates over the CV iterations range Page: 1994 1990–1998 M.Jelizarow et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.1.</head><figDesc>Fig. 1. Overview of the CV error rates of the different variants of rlda.TG, obtained for all datasets within the corresponding optimal settings s opt. The dashed line indicates the value obtained for rlda.TG within the data-specific s opt. Note that for both the Wang and the Singh data the optimal setting is not unique. The considered settings are those selected from Table 2: s opt = (200, Limma) for the Golub data, s opt = (200, Wilcoxon test) for the CLL data, s opt 1 = (200, t-test) (left bar) and s opt 2 = (200, Limma) (right bar) for the Wang data and s opt 1 = (100, t-test) (left bar) and s opt 2 = (100, Limma) (right bar) for the Singh data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.3.</head><figDesc>Fig. 3. Illustration of (i) correlations between connected genes and (ii) correlations between not connected genes by means of histograms. The illustration is given for the datasets Golub (n (i) = 255 441 versus n (ii) = 4 115 005), CLL (n (i) = 606 903 versus n (ii) = 9 901 917, Wang (n (i) = 1 096 193 versus n (ii) = 20 985 142) and Singh (n (i) = 606 903 versus n (ii) = 9 901 917), where n (i) and n (ii) denote the number of available correlations between connected genes and not connected genes, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>Table 1. Overview of targets D (diagonal, unequal variance), F (constant correlation) and G (where ¯ r is the average of sample correlations) Target D Target F Target G t ij = s ii if i = j 0 ifi = j t ij = s ii if i = j ¯ r √ s ii s jj if i = j t ij = ⎧ ⎪ ⎨ ⎪ ⎩ s ii if i = j ¯ r √ s ii s jj if i = j,i ∼ j</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 2. Overview of the CV errors obtained for rlda.TG where p * denotes the number of selected genes</figDesc><table>Selection procedure 
p  *  
Golub 
CLL 
Wang 
Singh 

t-test 
100 
0.029 
0.234 
0.382 
0.081 
200 
0.029 
0.269 
0.375 
0.133 
500 
0.032 
0.220 
0.383 
0.166 
1000 
0.049 
0.222 
0.380 
0.211 
Limma 
100 
0.031 
0.237 
0.383 
0.081 
200 
0.028 
0.274 
0.375 
0.125 
500 
0.039 
0.233 
0.384 
0.182 
1000 
0.060 
0.225 
0.376 
0.224 
Wilcoxon test 
100 
0.090 
0.192 
0.384 
0.135 
200 
0.170 
0.159 
0.379 
0.178 
500 
0.168 
0.185 
0.409 
0.158 
1000 
0.124 
0.221 
0.402 
0.197 

The bold values indicate the minimum values. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>Table 2: s opt = (200, Limma) for the Golub data, s opt = (200, Wilcoxon test) for the CLL data, s</figDesc><table>opt 
1 = (200, t-test) 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><figDesc>Table 3. Overview of the differences D between the error rates of the data-specific optimal variant M opt of rlda.TG and the methods rlda.TD, rlda.TF, DLDA, NSC and SVM within the data-specific optimal setting s opt M opt D(M opt ,rlda.TD) D(M opt ,rlda.TF) D(M opt ,DLDA) D(M opt ,NSC) D(M opt ,SVM)</figDesc><table>Golub 
rlda.TG (5) 
− 0.003 
− 0.003 
− 0.010 
0.004 
− 0.029 

CLL 
rlda.TG (5) 
− 0.017 
− 0.083 
− 0.055 
− 0.204 
− 0.269 

Wang 
rlda.TG (6) 
− 0.026 
− 0.026 
− 0.033 
− 0.034 
0.001 
Singh 
rlda.TG (8) 
− 0.008 
− 0.003 
− 0.048 
− 0.052 
− 0.022 

from 0.005 to 0.024 for the Golub data, from 0.022 to 0.031 for 
the CLL data, from 0.009 to 0.012 for the Wang data and from 
0.008 to 0.021 for </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><figDesc>Table 4. Performance of the optimal variants M opt of rlda.TG within the optimal settings s opt selected in each of the four datasets</figDesc><table>M opt 
s opt 
CVE Mopt Golub 
CVE Mopt CLL 
CVE Mopt Wang 
CVE Mopt Singh 

Golub 
rlda.TG (5) 
s opt = (200, Limma) 
0.025 
0.180 
0.345 
0.152 
CLL 
rlda.TG (5) 
s opt = (200, Wilcoxon test) 
0.079 
0.129 
0.363 
0.141 
Wang 
rlda.TG (6) 
s opt = (200, t-test) 
0.029 
0.221 
0.342 
0.115 
Singh 
rlda.TG (8) 
s opt = (100, Limma) 
0.033 
0.274 
0.384 
0.078 

</table></figure>

			<note place="foot">at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="5"> CONCLUSION In this article, we demonstrate quantitatively that a combination of various interrelated optimization mechanisms may yield substantially biased results and over-optimistic conclusions on the superiority of a new method. Over-optimism is widespread in statistical methods development (Hand, 2006) and also in bioinformatics and systems biology. Therefore, to properly evaluate a method other aspects need to be considered in addition to improvement of accuracy on real datasets, such as their conceptual simplicity, computational efficiency, interpretability, flexibility, ability to generalize or fit in a global framework, the absence of strong assumptions, the originality of the addressed research question or, most importantly, the validity of the underlying model. As noted by Mehta et al. (2004), &apos;illustration with single datasets of unknown nature, though interesting, is not a sound epistemological foundation for method development&apos;. Hence, when improvement of accuracy is presented as the major contribution, it should be validated using independent datasets that were not used during the development of the new method. There is, of course, no uniformly best method which can be shown to perform best on every real dataset. However, precisely because of that an adequate validation including a report of both positive and negative results is essential to substantiate and objectify the statements on a method&apos;s strength.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank the reviewers for the constructive comments, leading to a considerable improvement of the manuscript. We further thank Christoph Bernau for helpful remarks.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Feature selection in omics prediction problems using cat scores and false non-discovery rate control</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ahdesmäki</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Strimmer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="503" to="519" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="16" to="19" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>btq323. .tex]</note>
</biblStruct>

<biblStruct   xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName>
				<surname>Page</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Jelizarow</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Selection bias in gene extraction on the basis of microarray gene-expression data</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Ambroise</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">J</forename>
				<surname>Mclachlan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="6562" to="6566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Some theory for Fisher&apos;s linear discriminant function, &quot; naive Bayes &quot; , and some alternatives when there are many more variables than observations</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">J</forename>
				<surname>Bickel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Levina</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="989" to="1010" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Incorporating pathway information into boosting estimation of high-dimensional risk prediction models</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Binder</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Schumacher</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimal classifier selection and negative bias in error rate estimation: An empirical study on high-dimensional prediction</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">L</forename>
				<surname>Boulesteix</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Strobl</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med. Res. Methodol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Evaluating microarray-based classifiers: an overview</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">L</forename>
				<surname>Boulesteix</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Informat</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="77" to="97" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Over-optimism in bioinformatics research</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">L</forename>
				<surname>Boulesteix</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="437" to="439" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Is cross-validation valid for small-sample microarray classification?</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<forename type="middle">M</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="374" to="380" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Fads and fallacies in the name of small-sample microarray classification</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<forename type="middle">M</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sign. Process. Mag</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Reducing the probability of false positive research findings by pre-publication validation: Experience with a large multiple sclerosis database</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Daumer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med. Res. Methodol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">On the optimality of the simple Bayesian classifier under zero-one loss</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Domingos</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Pazzani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="103" to="130" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">On the epistemological crisis in genomics</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Genomics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="69" to="79" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Comparison of discrimination methods for the classification of tumors using gene expression data</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Dudoit</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Stein&apos;s paradox in statistics</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Efron</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Morris</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Am</title>
		<imprint>
			<biblScope unit="volume">236</biblScope>
			<biblScope unit="page" from="119" to="127" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Regularized discriminant analysis</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">H</forename>
				<surname>Friedman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="165" to="175" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<monogr>
		<title level="m" type="main">golubEsets. R package version 1.4.7</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Golub</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph-Constrained Discriminant Analysis of functional genomics data</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Guillemot</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Bioinformatics and Biomedicine Worshops</title>
		<meeting><address><addrLine>Philadelphia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="207" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Regularized linear discriminant analysis and its application in microarrays</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Guo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="86" to="100" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Incorporating prior probabilities into high-dimensional classifiers</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Hall</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J.-H</forename>
				<surname>Xue</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="31" to="48" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Decorrelation of the true and estimated classifier errors in high-dimensional settings</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Hanczar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Bioinformatics Syst. Biol</title>
		<imprint>
			<biblScope unit="volume">207</biblScope>
			<biblScope unit="page">38473</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Small-sample precision of roc-related estimates</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Hanczar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="822" to="830" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Classifier technology and the illusion of progress</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">J</forename>
				<surname>Hand</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Sci</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Hausser</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Strimmer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1469" to="1484" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Group Lasso with Overlap and Graph Lasso</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Jacob</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML 26</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">KEGG: Kyoto Encyclopedia of Genes and Genomes</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Kanehisa</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Goto</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="27" to="30" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Reproducible research and the substantive context</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Keiding</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="376" to="378" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Revisiting scale-free networks</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">F</forename>
				<surname>Keller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioEssays</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1060" to="1068" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Improved estimation of the covariance matrix of stock returns with an application to portfolio selection</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Ledoit</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Wolf</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Empir. Finan</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="603" to="621" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Honey, I shrunk the sample covariance matrix</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Ledoit</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Wolf</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Portf. Manag</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="110" to="119" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Comparative analysis of microarray normalization procedures: effects on reverse engineering gene networks</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">K</forename>
				<surname>Lim</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="282" to="288" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Network-constrained regularization and variable selection for analysis of genomic data</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Li</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1175" to="1182" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards sound epistemological foundations of statistical methods for high-dimensional biology</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Mehta</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="943" to="947" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">A generalized inverse for matrices</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Penrose</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Camb</title>
		<meeting>. Camb</meeting>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="page" from="406" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<analytic>
		<title level="a" type="main">Classification of microarray data using gene networks</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Rapaport</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b37">
	<analytic>
		<title level="a" type="main">Papers on normalization, variable selection, classification or clustering of microarray data</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">M</forename>
				<surname>Rocke</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="701" to="702" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b38">
	<analytic>
		<title level="a" type="main">A shrinkage approach to large-scale covariance matrix estimation and implications for functional genomics</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Schäfer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Strimmer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Appl. Genet. Mol. Biol</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b39">
	<analytic>
		<title level="a" type="main">Gene expression correlates of clinical prostate cancer behavior</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Singh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Cell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="203" to="209" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b40">
	<analytic>
		<title level="a" type="main">Feature selection guided by structural information</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Slawski</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>in. press</note>
</biblStruct>

<biblStruct   xml:id="b41">
	<analytic>
		<title level="a" type="main">Linear models and empirical Bayes methods for assessing differential expression in microarray experiments</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Smyth</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Appl. Genet. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b42">
	<analytic>
		<title level="a" type="main">Inadmissibility of the usual estimator for the mean of a multivariate normal distribution</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Stein</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Third Berkeley Symposium on Mathematical Statistics and Probability<address><addrLine>Berkeley, USA</addrLine></address></meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1955" />
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b43">
	<analytic>
		<title level="a" type="main">Incorporating prior knowledge of gene functional groups into regularized discriminant analysis of microarray data</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Tai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Pan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="3170" to="3177" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b44">
	<analytic>
		<title level="a" type="main">Incorporating prior knowledge of predictors into penalized classifiers with multiple penalty terms</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Tai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Pan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1775" to="1782" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b45">
	<analytic>
		<title level="a" type="main">Diagnosis of multiple cancer types by shrunken centroids of gene expression</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="6567" to="6572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b46">
	<analytic>
		<title level="a" type="main">Gene-expression profiles to predict distant metastasis of lymph-node-negative primary breast cancer</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Wang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet</title>
		<imprint>
			<biblScope unit="volume">365</biblScope>
			<biblScope unit="page" from="671" to="679" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b47">
	<monogr>
		<title level="m" type="main">CLL. R package version 1.2.8. Available at http://bioconductor</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Whalen</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>html</note>
</biblStruct>

<biblStruct   xml:id="b48">
	<analytic>
		<title level="a" type="main">Reporting bias when using real datasets to analyze classification performance</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">R</forename>
				<surname>Yousefi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="68" to="76" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b49">
	<analytic>
		<title level="a" type="main">Classification and biomarker identification using gene network modules and support vector machines</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Yousef</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">337</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>