
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequence analysis BioPig: a Hadoop-based analytic toolkit for large-scale sequence data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013">2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Henrik</forename>
								<surname>Nordberg</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Energy</orgName>
								<orgName type="institution">Joint Genome Institute</orgName>
								<address>
									<postCode>94598</postCode>
									<settlement>Walnut Creek</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Genomics Division</orgName>
								<orgName type="institution">Lawrence Berkeley National Laboratory</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Karan</forename>
								<surname>Bhatia</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Energy</orgName>
								<orgName type="institution">Joint Genome Institute</orgName>
								<address>
									<postCode>94598</postCode>
									<settlement>Walnut Creek</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Kai</forename>
								<surname>Wang</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Energy</orgName>
								<orgName type="institution">Joint Genome Institute</orgName>
								<address>
									<postCode>94598</postCode>
									<settlement>Walnut Creek</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Zhong</forename>
								<surname>Wang</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Energy</orgName>
								<orgName type="institution">Joint Genome Institute</orgName>
								<address>
									<postCode>94598</postCode>
									<settlement>Walnut Creek</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Genomics Division</orgName>
								<orgName type="institution">Lawrence Berkeley National Laboratory</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sequence analysis BioPig: a Hadoop-based analytic toolkit for large-scale sequence data</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">29</biblScope>
							<biblScope unit="issue">23</biblScope>
							<biblScope unit="page" from="3014" to="3019"/>
							<date type="published" when="2013">2013</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btt528</idno>
					<note type="submission">Received on February 18, 2013; revised on September 3, 2013; accepted on September 4, 2013</note>
					<note>BIOINFORMATICS ORIGINAL PAPER Associate Editor: Michael Brudno</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: The recent revolution in sequencing technologies has led to an exponential growth of sequence data. As a result, most of the current bioinformatics tools become obsolete as they fail to scale with data. To tackle this &apos;data deluge&apos;, here we introduce the BioPig sequence analysis toolkit as one of the solutions that scale to data and computation. Results: We built BioPig on the Apache&apos;s Hadoop MapReduce system and the Pig data flow language. Compared with traditional serial and MPI-based algorithms, BioPig has three major advantages: first, BioPig&apos;s programmability greatly reduces development time for parallel bioinformatics applications; second, testing BioPig with up to 500 Gb sequences demonstrates that it scales automatically with size of data; and finally, BioPig can be ported without modification on many Hadoop infrastructures, as tested with Magellan system at National Energy Research Scientific Computing Center and the Amazon Elastic Compute Cloud. In summary, BioPig represents a novel program framework with the potential to greatly accelerate data-intensive bioinformatics analysis. Availability and implementation: BioPig is released as open-source software under the BSD license at https://sites.google.com/a/lbl.gov/ biopig/ Contact: ZhongWang@lbl.gov</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Advances in DNA sequencing technologies are enabling new applications ranging from personalized medicine to biofuel development to environmental sampling. Historically, the bottleneck for such applications has been the cost of sequencing—the estimated cost of sequencing the first human genome (3 GB) completed a decade ago is estimated at $2.7 billion (http:// www.genome.gov/11006943). Current next-generation sequencing technologies (<ref type="bibr" target="#b11">Metzker, 2010</ref>) have extraordinary throughput at a much lower cost, thereby greatly reducing the cost of sequencing a human genome to 5$10 000 (http://www.genome. gov/sequencingcosts/, accessed December 2012). The price hit $5495 by the end of 2012 (http://dnadtc.com/products.aspx, accessed December 2012). With the cost of sequencing rapidly dropping, extremely large-scale sequencing projects are emerging, such as the 1000 Genomes Project that aims to study the genomic variation among large populations of humans (1000 Genomes<ref type="bibr" target="#b1">Project Consortium et al., 2010</ref>), and the cow rumen deep metagenomes project that aims to discover new biomass degrading enzymes encoded by complex microbial community (<ref type="bibr" target="#b3">Hess et al., 2011</ref>). As a result, the rate of growth of sequence data is now outpacing the underlying advances in storage technologies and compute technologies (Moore's law). Taking metagenomic studies as an example, data have grown from 70 Mb (million bases) from termite hindgut (<ref type="bibr" target="#b19">Warnecke et al., 2007</ref>) and 800 Mb Tamar wallaby (<ref type="bibr" target="#b15">Pope et al., 2010</ref>), to 268 Gb (billion bases) from cow rumen microbiome (<ref type="bibr" target="#b3">Hess et al., 2011</ref>) within the past 4 years. The recently published DOE Joint Genome Institute's (JGI) sequencing productivity showed that 30 Tb (trillion bases) of sequences were generated in 2011 alone (http://1.usa.gov/JGI-Progress-2011). Data analysis at terabase scales requires state-of-the-art parallel computing strategies that are capable of distributing the analysis across thousands of computing elements to achieve scalability and performance. Most of the current bioinformatics analysis tools, however, do not support parallelization. As a result, this exponential data growth has made most of the current bioinformatics analytic tools obsolete because they fail to scale with data, either by taking too much time or too much memory. For example, it would take 80 CPU years to BLAST the 268 Gb cow rumen metagenome data (<ref type="bibr" target="#b3">Hess et al., 2011</ref>) against NCBI non-redundant database. De novo assembly of the full dataset by a short read assembler, such as Velvet (<ref type="bibr" target="#b21">Zerbino and Birney, 2008</ref>), would require computers with 41 TB RAM and take several weeks to complete. Re-engineering the current bioinformatics tools to fit into parallel programming models requires software engineers with expertise in high-performance computing, and parallel algorithms take significantly longer time to develop than serial algorithms (<ref type="figure" target="#fig_1">Fig. 1</ref>). In addition, at this scale of computing hardware failures become more frequent, and most bioinformatics software lack robustness so that once they fail they have to be restarted manually. All these challenges contribute to the bottleneck in large-scale sequencing analysis. Cloud computing has emerged recently as an effective technology to process petabytes of data per day at large Internet companies. MapReduce is a data-parallel framework popularized by Google Inc. to process petabytes of data using large numbers of commodity servers and disks (<ref type="bibr" target="#b0">Dean and Ghemawat, 2008</ref>). Hadoop is an open-source implementation of the MapReduce framework and is available through the Apache Software Foundation (http://wiki.apache.org/hadoop). Hadoop uses a *To whom correspondence should be addressed. y Present address: Amazon Web Services, New York, NY 10019, USA distributed file system (HDFS) that brings computation to the data as opposed to moving the data to the computation as is done in traditional computing paradigms. In Hadoop, node-tonode data transfers are minimized, as Hadoop tries its best to perform automatic co-location of data and program on the same node. Furthermore, Hadoop provides robustness through a job handling system that can automatically restart failed jobs. Because of these advantages, Hadoop has been explored by the bioinformatics community (<ref type="bibr" target="#b4">Jourdren et al., 2012;</ref><ref type="bibr" target="#b18">Taylor, 2010</ref>) in several areas including BLAST (<ref type="bibr" target="#b5">Kolker et al., 2011</ref>), SNP discovery (<ref type="bibr" target="#b6">Langmead et al., 2009;</ref><ref type="bibr" target="#b10">McKenna et al., 2010</ref>), short read alignment (<ref type="bibr" target="#b13">Nguyen et al., 2011;</ref><ref type="bibr" target="#b16">Schatz, 2009</ref>) and transcriptome analysis (<ref type="bibr" target="#b7">Langmead et al., 2010</ref>). In addition, there are solutions that reduce the hurdle to run Hadoop-based sequence analysis applications (<ref type="bibr" target="#b4">Jourdren et al., 2012</ref>). Using Hadoop requires a good understanding of this framework; however, to break up programs into map and reduce steps, skills in programming languages, such as Java or Cþþ, are also required. To overcome the programmability limitations of Hadoop, several strategies have been developed. For example, Cascading is an application framework for Java developers by Concurrent Inc. (http://www.cascading.org/), which simplifies the processes to build data flows on Hadoop. Apache's Pig data flow language was developed to enable non-programmer data analysts to develop and run Hadoop programs (http://pig.apache.org/). Somewhat similar in nature to SQL, the Pig language provides primitives for loading, filtering and performing basic calculations over datasets. Pig's infrastructure layer consists of a compiler on the user's client machine that turns the user's Pig Latin programs into sequences of MapReduce programs that run in parallel on the nodes of the Hadoop cluster in a similar fashion to how a database engine generates a logical and physical execution plan for SQL queries. In this article, we describe BioPig, a set of extensions to the Pig language to support large sequence analysis tasks. We will discuss the design principles, give examples on use of this toolkit for specific sequence analysis tasks and compare its performance with alternative solutions on different platforms. There is a similar framework, SeqPig (http://seqpig.sourceforge.net/), developed in parallel to BioPig, which is also based on Hadoop and Pig. We provide a detailed comparison of the two in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head><p>Using the BioPig modules, we provide a set of scripts that showcase the functionality provided by the framework, as well being useful bioinformatics tools in their own right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">pigKmer</head><p>Given a set of sequences, the pigKmer module computes the frequencies of each kmer and outputs a histogram of the kmer counts. The kmer histogram task is ideally suited for MapReduce, as each kmer count can be generated in parallel without global knowledge of all kmers. The data file is partitioned into a number of blocks and each map task operates on a single block at a time. The grouping operation is naturally implemented in the reduce step where the kmers are sorted and partitioned across the available reducers. Each reducer then counts the number of items in its group. The histogram of the counts is generated in a second MapReduce iteration. Code Listing 1 shows the BioPig script for this calculation. The script first registers the BioPig functions defined in the jar file on line 1. On line 2, the dataset is read and identified as table named A with four columns: id (sequence id from the header of the FASTA), direction (0 if combined or not paired, 1 or 2 otherwise), sequence (the characters of the read sequence itself) and header (any additional data on theA number of variations of kmer counting are available: count only the number of unique reads that contain the kmer or group kmers within one or two hamming distance (<ref type="bibr" target="#b2">Hamming, 1950</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">pigDuster</head><p>pigDuster searches a set of query sequences against a database of known sequences for near exact matches. This application is useful to screen sequence datasets for contaminants or to perform pathogen detection from human sequence data. The application uses shared kmers to find matches to sequences in the dataset. The kmer indices of the known sequences and query sequences are compared by the join function with detect matches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">pigDereplicator</head><p>Raw sequence data often contain reads derived from PCR (<ref type="bibr" target="#b12">Mullis and Faloona, 1987</ref>) amplification. But due to small error rates introduced in the sequencing process, the same DNA sequence may not produce identical reads. To remove these artifacts, the dereplication application builds a kmer index by joining the beginning 16 bases of each read of a mate pair. Because the error rate increases as a function of sequencing length in many short read technologies, the first 16 bases typically contain fewer errors. The 32mer is then used to identify near identical read pairs. Code Listing 2 shows the BioPig code for dereplicating a dataset of reads. For brevity, the comments and headers have been left out. Lines 10 and 11 generate the hash from the first 16 and last 16 characters. Finally, the hash is grouped and all the sequences with a given hash are combined using the CONSENSUS function (for each base position in the sequence, the majority value determined).load the target sequences 9 READS = load '$reads' using gov.jgi.meta.pig.storage.FastaStorage as (id: chararray, d: int, seq: bytearray, header: chararray);-group the read pairs together by id and filter out any reads that-do not have a matching pair.-then combine the mate pairs into a single sequence10 GROUPEDREADS = group READS by id parallel $p; 11 MERGEDREADS = foreach GROUPEDREADS generate flatten(PAIRMERGE(READS)) as (id: chararray, d: int, seq: bytearray);-generate the hash 12 HASH = foreach MERGEDREADS generate IDENTITYHASH(UNPACK(seq)) as hash, UNPACK(seq) as seq; 13 HASHNEIGHBORS = foreach HASH generate flatten(HAMMINGDISTANCE($0, $distance)) as hash, '0', $1 as seq;-now merge all similar reads together 15 E = group HASHNEIGHBORS by $0 parallel $p; 16 F = foreach E generate $0, count($1), CONSENSUS($1);-return output 17 store E into '$output'; Most of the work in the script is done in the various functions defined in the BioPig library. PAIRMERGE() takes a set of sequences and merges them together into a single sequence; IDENTITYHASH() takes a sequence and returns the hash from the first 16 bases and the last 16 bases; and CONSENSUS() takes a set of sequences and calculates the majority base at each position and returns a new sequence. The sequence data loader used in line 2 also is defined in the BioPig library and supports reading and parsing FASTA-formatted sequence files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The BioPig framework and its design principles</head><p>The ideal analysis solution should address the three main challenges in data-intensive sequence analysis: (i) scalability: it should scale with data size; (ii) programmability: it should leverage a high-level data flow language that provides abstraction of parallelism details, which enables bioinformatics analysts to focus on analysis over large data sizes without concerns as to parallelization, synchronization or low-level message passing; and (iii) portability: it should be portable without extensive modification to various IT infrastructure. With these design principles in mind, we developed the BioPig data analytic toolkit on top of Apache Hadoop and Pig (<ref type="figure" target="#fig_1">Fig. 1</ref>). The resulting BioPig toolkit has both the scalability and robustness offered by Apache Hadoop, which uses the data-parallel methodology of MapReduce to parallelize analysis over many computing cores without significant loss in computing performance, and the programmability and parallel data flow control offered by Pig. In addition, as both Hadoop and Pig are implemented in Java, BioPig inherits their portability. The BioPig modular implementation consists of a set of libraries, which add an abstraction layer for processing sequence data. Within BioPig's open extensible framework, functions and libraries can be updated and added easily. The first release of BioPig contains three core functional modules. The BioPigIO module reads and writes sequence files in FASTA or FASTQ format. It enables Hadoop to split sequence files automatically and distribute them across many compute nodes. The BioPigAggregation module is a wrapper for common bioinformatics programs, such as BLAST, CAP3, kmerMatch and Velvet. Finally, there is a set of utilities that make working with nucleotide sequences more efficient by using compression and encoding.<ref type="figure" target="#tab_1">Table 1</ref>lists the functions provided by the current version of BioPig.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A simple BioPig application demonstrates scalability, programmability and portability</head><p>To systematically evaluate the scalability, programmability and portability of the BioPig framework, we implemented a kmerThe size of the dataset we used ranges from the 100 Mb to 500 Gb from the cow rumen metagenomic data (<ref type="bibr" target="#b3">Hess et al., 2011</ref>). Kmer counting is an essential component of many bioinformatics methods, such as genome and transcriptome assembly. Even though it is simple to compute, serial programs using kmer counting quickly run out of memory as the size of sequence data increases (<ref type="figure" target="#fig_2">Fig. 2</ref>). For simplicity, we fixed the size of kmer to 20 (K ¼ 20) and compared the scalability of the BioPig kmer counting program, KmerGenerator, to a serial version, Tallymer (<ref type="bibr">Stefan et al.</ref>) and an open-source MPI-version (https://github. com/JGI-Bioinformatics/Kmernator). As shown in<ref type="figure" target="#fig_2">Figure 2A</ref>, when run on the Magellan Hadoop cluster (NERSC) using 1000 mappers and reducers, respectively, BioPig KmerGenerator scales well from 1 to 500 Gb of input sequences. In contrast, Tallymer ran out of memory on a single node with 48 GB RAM with 1 Gb dataset, and our straightforward implementation of the MPI-version ran out of memory at 50 GB on the same machines tested. We noticed that when the size of the datasets is small (1, 5 and 10 Gb), the KmerGenerator takes about the same time to finish, possibly due to the overhead of the MapReduce framework that BioPig based on. As data sizes reach 10 Gb and higher, however, KmerGenerator scales well with data. To investigate whether or not other applications based on BioPig also scale with data, we tested ContigExtention and GenerateContig on increasing data sizes (10, 50 and 100 Gb). ContigExtention script (Code Listing 3) iteratively searches for short reads that map to the ends of a long sequence (contig) based on kmer matching and assembles the matching reads with the long sequence to extend it. The algorithm is linear, and we observed near linear performance with data (<ref type="figure" target="#fig_2">Fig. 2C</ref>). GenerateContig script consists of two steps: it starts by running BLAST to find short reads matching a long sequence, and subsequently assembles them into contigs by calling Velvet. The first step is linear in relation to input size, whereas the second assembly step is not. As a result, the overall performance of the script over data displays a nonlinear scaling pattern (<ref type="figure" target="#fig_2">Fig. 2D</ref>).KmerGenerator does kmer counting with just eight lines of code (Code Listing 1). To achieve the same task, Tallymer and the MPI-version use thousands of lines of code. This suggests BioPig has excellent programmability and can greatly reduce the amount of software development time. This programmability feature is particularly attractive to next-generation sequencing data analysis, as the sequencing technology rapidly changes and software constantly needs to be updated. To test the portability of BioPig, we evaluated KmerGenerator function on Amazon Elastic Compute Cloud (Amazon EC2). Porting BioPig to Amazon was simply a matter of uploading the BioPig core JAR (Java ARchive) file, along with the bioinformatics tools called from BioPig code. Without any change to the underlying code, KmerGenerator was successfully run on both platforms, demonstrating the portability of the BioPig framework. Furthermore, we observed scalability on EC2 similarly as on the Magellan system with various sizes of sequence datasets (<ref type="figure" target="#fig_2">Fig. 2B</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Embedding BioPig into other programming languages</head><p>BioPig has the flexibility to be embedded into other languages, such as Python or JavaScript, to achieve the types of control flows such as loops and branches that are not currently available in the Pig language. This flexibility greatly extends the usefulness of the BioPig toolkit. Code Listing 3 illustrates a Python script with BioPig embedded. This contig extension algorithm greedily extends by iteratively searching for reads that can extend the contig followed by extending the contigs via assembling the reads at each end. In each iteration, the Cap3 assembler is used to extend the contigs, of which only those contigs being extended go into the next iteration. The loop will stop when there are no contigs left to be extended or when it reaches a predefined number of steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">BioPig and related MapReduce-based frameworks</head><p>Several computational frameworks have been recently developed for bioinformatics. Despite that they all aim to scale analysis to big data by implementing existing algorithms onto MapReduce/ Hadoop, they differ significantly on the specific algorithms implemented (<ref type="figure">Table 2</ref>). Therefore, it is not straightforward to compare the performance of BioPig with these frameworks. The only exception is SeqPig (http://seqpig.sourceforge.net/). SeqPig and BioPig are two independent projects, and they share some similarities. First, they both extend Pig, and therefore have the same programming syntax. Second, there are a few similar functions (such as sequence import and export). Because they are based on the same framework (Hadoop and Pig), when run in the same hardware environment, they are expected to have similar run time performance. The difference between the two projects lies in their user-defined functions. BioPig includes several kmerbased applications that are not available in SeqPig (<ref type="figure">Table 2</ref>). It also provides wrappers to run many frequently used bioinformatics applications such as BLAST, Velvet and CAP3. In contrast, SeqPig mainly implements functions of Picard (http:// picard.sourceforge.net) and SamTools (<ref type="bibr" target="#b9">Li et al., 2009</ref>). SeqPig and BioPig can be installed side-by-side on the same Hadoop and Pig cluster. From the same Pig script, it is straightforward to call both BioPig and SeqPig functions.<ref type="figure">Table 2</ref>provides a comparison of BioPig with these related frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head><p>In this work, we present a solution for improved processing of large sequence datasets in many bioinformatics applications. Using only a few core modules, we believe we have demonstrated the usefulness of this toolkit, while its modular design should enable many similar applications that fit in the MapReduce framework. BioPig has several advantages over alternative parallel programming paradigm: it is easy to program; it is scalable to process large datasets (with 500 Gb being the largest one tested); and it is generically portable to several examples of Hadoop infrastructure, including Amazon EC2, without modification. We also noticed several limitations of BioPig, most of which likely derived from Hadoop itself. For example, it is slower than handcrafted MPI solutions. This is due to both the latency of Hadoop's initialization and the fact that generic MapReduce algorithms are not optimized for specific problems. For big datasets this limitation may not be a problem, as the time spent on data analysis far exceeds the cost for the start-up latency. Recently, the Hadoop community has started to address this problem. Certain commercial implementations of MapReduce, such as IBM's Symphony product, have been developed to reduce Hadoop's start-up latency. Another promising solution is SPARK, which can speed up Hadoop applications 100 times by using a low latency, in memory cluster computing (<ref type="bibr" target="#b20">Zaharia et al., 2012</ref>). Another issue is computing resource demand. When dealing with huge datasets, BioPig shifts the need from expensive resources such as large memory (1TB RAM) machines and/or parallel programming expertise, to large disk space on commodity hardware. For example, a kmer/read index in BioPig needs</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>FASTA header). Line 3 generates kmers (20mers in this example) for each read. Lines 4 and 5 group identical kmers and generate a count of each group. Lines 6 and 7 generate bins with counts of groups with the same number of kmers. Code Listing 1-a simple example of pig script to count kmers 1 register /.../biopig-core-0.3.0-job-pig.jar 2 A = load '$input' using gov.jgi.meta.pig.storage.FastaStorage as (id: chararray, d: int, seq: bytearray, header: chararray); 3 B = foreach A generate flatten(gov.jgi.meta.pig.eval.KmerGenerator(seq, 20)) as (kmer:bytearray); 4 C = group B by kmer parallel $p; 5 D = foreach C generate group, count(B); 6 E = group D by $1 parallel $p; 7 F = foreach E generate group, count(D); 8 store F into '$output';</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.1.</head><figDesc>Fig. 1. A comparison between algorithms with serial, MPI and BioPig implementations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.2.</head><figDesc>Fig. 2. Scalability of BioPig over increasing size of input sequencing data. (A) A comparison of the scalability of kmer counting program implemented with BioPig, serial programming or MPI programming models. All three programs were run on the same machines. (B) Kmer counting using Amazon EC2. (C and D) The scalability of ContigExtension (C) and GenerateContig on NERSC (D), the dotted lines are generated by linear regression</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><figDesc>Table 1. BioPig functions and programs callable from Pig</figDesc><table>Name 
Function 

BLAST/BLAT 
Wrappers for BLAST/BLAT 
Cap3, Minimus, Newbler, Velvet 
Assembler wrappers 
FASTA/FASTQ I/O 
FASTA/FASTQ format readers and writers compatible with Hadoop's block I/O 
KmerGenerator 
Generate Kmers from a sequence 
N50 
Calculate N50 value for a set of sequences 
PackSequence 
Store bases in a compact, but still printable, form. Useful to save space and also for debugging 
HammingDistance 
Implementation of Hamming distance 
SequencePairMerge 
Merge pairs with the same sequence ID and marked as 0 and 1 
SubSequence 
Get part of a sequence 
GenerateConsensus 
Calculate base consensus </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><figDesc>Table 2. BioPig compared with other related frameworks</figDesc><table>Framework 
Programming models 
Bioinformatics applications 

BioPig 
Pig/Java/Hadoop 
FASTA I/O; several kmer-based algorithms; wrapper for BLAST and short read assemblers 
Biodoop 
Python/Hadoop 
FASTA I/O; sequence alignment and alignment manipulation; wrapper for BLAST (Leo et al., 2009) 
GATK 
Java/MapReduce 
Short read alignment, variant calling 
Hadoop-BAM 
Java/Hadoop 
BAM (Binary Alignment/Map) I/O; alignment manipulation functions based on Picard API 
(Niemenmaa et al., 2012) 
SeqPig 
Pig/Java/Hadoop 
FASTA, FASTQ, BAM I/O; alignment-related functions; sequence statistics </table></figure>

			<note place="foot">Published by Oxford University Press. 2013. This work is written by US Government employees and is in the public domain in the US. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">BioPig at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">H.Nordberg et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">H.Nordberg et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from lots of disk space, with the largest index taking up 15 TB of disk space for the 500 Gb dataset. This trade-off is usually a favored option, as RAM is more expensive than hard disk. Therefore, processing large datasets with BioPig requires a stable Hadoop environment with fast interconnects and plentiful disk space. One might reduce the requirement by keeping fewer copies of the files in the HDFS system, which is a trade-off for redundancy. BioPig is built on MapReduce and Hadoop, so algorithms that do not run well on MapReduce will not run well with BioPig either. For example, large short read assembly algorithms involve large graph processing. Currently, they require messagepassing and have not yet been implemented on MapReduce.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors would like to thank Shane Cannon for providing technical support for Hadoop clusters, Rob Egan for providing MPI-based kmer counting program and Nicole Johnson for helpful edits and suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code Listing 3</head><p>1 #!/usr/bin/python 2 from org.apache.pig.scripting import * 3 indexFile = 'index.data' 4 contigFile = 'contigs.data' 5 p = 100 6 Pig.fs("rmr output") 7 PP = Pig.compile(""" 8 register /…/biopig-core-1.0.0-job.jar;-load the target sequences 9 readindex = load '$indexFile' using PigStorage as (seq: bytearray, kmer: bytearray);-load the contigs, create the index 10 contigs = load '$contigFile' using PigStorage as (geneid: chararray, seq: chararray); 11 contigindex = foreach contigs generate geneid, FLATTEN(gov.jgi.meta.pig.eval.KmerGenerator(seq, 20)) as (kmer:bytearray);-join reads with the contigs database 12 j = join readindex by kmer, contigindex by kmer PARALLEL $p; 13 k = foreach j generate contigindex::geneid as contigid, gov.jgi.meta.pig.eval.UnpackSequence(readindex::seq) as readseq; 14 kk = distinct k PARALLEL $p; 15 l = group kk by contigid PARALLEL $p; 16 m = foreach l { 17 a = $1.$1; 18 generate $0, a; 19 }-join the contigid back with the contigs 20 n = join contigs by geneid, m by $0 PARALLEL $p; 21 contigs = foreach n generate $0 as geneid, gov.jgi.meta.pig.aggregate.ExtendContigWithCap3($1, $3) as res:(seq:chararray, val:int); 22 split contigs into notextended if res.val==0, contigs if res.val==1; 23 contigs = foreach contigs generate $0 as geneid, res.seq as seq; 24 store contigs into 'output/step-$i'; 25 store notextended into 'output/data-$i'; 26 """) 27 for i in range(50): 28 stats = PP.bind().runSingle() 29 if not stats.isSuccessful(): 30 break; 31 if ( stats.getNumberRecords('output/step-'+str(i)) &lt;= 0): 32 break; 33 else: 34 contigFile = 'output/step-'+str(i)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">MapReduce: simplified data processing on large clusters</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Dean</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Ghemawat</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">A map of human genome variation from population-scale sequencing</title>
		<author>
			<persName>
				<forename type="first">Project</forename>
				<surname>1000 Genomes</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Consortium</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">467</biblScope>
			<biblScope unit="page" from="1061" to="1073" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Error detecting and error correcting codes</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">W</forename>
				<surname>Hamming</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AT&amp;T Tech. J</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="147" to="160" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Metagenomic discovery of biomass-degrading genes and genomes from cow rumen</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Hess</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">331</biblScope>
			<biblScope unit="page" from="463" to="467" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Eoulsan: a cloud computing-based framework facilitating high throughput sequencing analyses</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Jourdren</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1542" to="1543" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Classifying proteins into functional groups based on all-versus-all BLAST of 10 million proteins</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Kolker</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Omics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="513" to="521" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Searching for SNPs with cloud computing</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Langmead</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">134</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Cloud-scale RNA-sequencing differential expression analysis with Myrna</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Langmead</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">83</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Biodoop: bioinformatics on hadoop In: Parallel Processing Workshops</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Leo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPPW&apos;09. International Conference on IEEE</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="415" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">The sequence alignment/map format and SAMtools</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Li</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2078" to="2079" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">The Genome Analysis Toolkit: a MapReduce framework for analyzing next-generation DNA sequencing data</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Mckenna</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1297" to="1303" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Sequencing technologies-the next generation</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">L</forename>
				<surname>Metzker</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Genet</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="31" to="46" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Specific synthesis of DNA in vitro via a polymerase-catalyzed chain-reaction</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">B</forename>
				<surname>Mullis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">A</forename>
				<surname>Faloona</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Method Enzymol</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="335" to="350" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">CloudAligner: a fast and full-featured MapReduce based tool for sequence mapping</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Nguyen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Res. Notes</title>
		<imprint>
			<biblScope unit="page">171</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Hadoop-BAM: directly manipulating next generation sequencing data in the cloud</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Niemenmaa</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="876" to="877" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptation to herbivory by the Tammar wallaby includes bacterial and glycoside hydrolase profiles different from other herbivores</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">B</forename>
				<surname>Pope</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="14793" to="14798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">CloudBurst: highly sensitive read mapping with MapReduce</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">C</forename>
				<surname>Schatz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1363" to="1369" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">A new method to compute K-mer frequencies and its application to annotate large repetitive plant genomes</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Stefan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Genomics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1471" to="2164" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">An overview of the Hadoop/MapReduce/HBase framework and its current applications in bioinformatics</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">C</forename>
				<surname>Taylor</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Suppl. . 12</note>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Metagenomic and functional analysis of hindgut microbiota of a wood-feeding higher termite</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Warnecke</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">450</biblScope>
			<biblScope unit="page" from="560" to="565" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Zaharia</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation. USENIX Association</title>
		<meeting>the 9th USENIX Conference on Networked Systems Design and Implementation. USENIX Association<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Velvet: algorithms for de novo short read assembly using de Bruijn graphs</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">R</forename>
				<surname>Zerbino</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Birney</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="821" to="829" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>