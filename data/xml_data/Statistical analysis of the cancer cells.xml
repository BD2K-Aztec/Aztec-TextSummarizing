
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Statistical analysis of the cancer cell&apos;s molecular entropy using high-throughput data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011">2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Wessel</forename>
								<forename type="middle">N</forename>
								<surname>Van Wieringen</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Epidemiology and Biostatistics</orgName>
								<orgName type="institution">VU University Medical Center</orgName>
								<address>
									<postBox>PO Box 7075</postBox>
									<postCode>1007 MB</postCode>
									<settlement>Amsterdam</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">VU University Amsterdam</orgName>
								<address>
									<addrLine>De Boelelaan 1081a</addrLine>
									<postCode>1081 HV</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Aad</forename>
								<forename type="middle">W</forename>
								<surname>Van Der Vaart</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">VU University Amsterdam</orgName>
								<address>
									<addrLine>De Boelelaan 1081a</addrLine>
									<postCode>1081 HV</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Statistical analysis of the cancer cell&apos;s molecular entropy using high-throughput data</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">27</biblScope>
							<biblScope unit="issue">4</biblScope>
							<biblScope unit="page" from="556" to="563"/>
							<date type="published" when="2011">2011</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btq704</idno>
					<note type="submission">Systems biology Advance Access publication December 20, 2010 Received on September 14, 2010; revised on December 2, 2010; accepted on December 15, 2010</note>
					<note>[10:01 2/2/2011 Bioinformatics-btq704.tex] Page: 556 556–563 Associate Editor: Trey Ideker Contact: wvanwie@few.vu.nl Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: As cancer progresses, DNA copy number aberrations accumulate and the genomic entropy (chromosomal disorganization) increases. For this surge to have any oncogenetic effect, it should (to some extent) be reflected at other molecular levels of the cancer cell, in particular that of the transcriptome. Such a coincidence of cancer progression and the propagation of an entropy increase through the molecular levels of the cancer cell would enhance the understanding of cancer evolution. Results: A statistical argument reveals that (under some assumptions) an entropy increase in one random variable (DNA copy number) leads to an entropy increase in another (gene expression). Statistical methodology is provided to investigate the relation between the genomic and transcriptomic entropy using high-throughput data. Analyses of multiple high-throughput datasets using this methodology show a close, concordant relation among the genomic and transcriptomic entropy. Hence, as cancer evolves, and the genomic entropy increases, the transcriptomic entropy is also expected to surge.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>DNA copy number aberrations are one of the key characteristics of cancer (<ref type="bibr" target="#b21">Stratton et al., 2009</ref>). In fact, the accumulation of DNA copy number aberrations is the most consistent feature of cancer progression (<ref type="bibr" target="#b3">Fearon and Vogelstein, 1990</ref>). The entropy at the genomic level (chromosomal disorganization) of cancer cells thus exceeds that of healthy cells and tends to surge as cancer progresses. DNA copy number aberrations affect mRNA expression levels as the central dogma of molecular biology suggests and numerous high-throughput studies have shown (e.g.<ref type="bibr" target="#b17">Pollack et al., 2002</ref>). Aberrations need not only affect the expression of its driver gene, but may also alter expression levels of the other genes that map to the aberrated segment (<ref type="bibr" target="#b11">Kauraniemi and Kallioniemi, 2006;</ref><ref type="bibr" target="#b26">Van Wieringen et al., 2010</ref>). In fact, genomic aberrations also affect expression levels of other transcripts like microRNAs (e.g.<ref type="bibr" target="#b29">Zhang et al., 2006</ref>). The close relation between the genome * To whom correspondence should be addressed. and transcriptome suggests that the entropy increase spreads to other molecular levels of the cell's regulatory system, and is expected to manifest itself most prominently in the transcriptome. Indeed, for this surge in genomic entropy to have any phenotypic (oncogenetic) effect, it needs (to some extent) to propagate to the transcriptomic level and beyond. Cancer may be considered an evolutionary process, driven by random variation and natural selection (<ref type="bibr" target="#b14">Merlo et al., 2006;</ref><ref type="bibr" target="#b21">Stratton et al., 2009</ref>). During its life a cell may undergo heritable genetic alterations (e.g. DNA copy number aberrations). Such alterations may be neutral but may also affect the cell's phenotype. Irrespective of the type of alteration, any cell is subject to natural selection: it has to survive in the environment of the organism's tissue. Within this framework, a cancer cell can be thought of as having acquired alterations that resulted in beneficial traits to survive, proliferate and metastasize (<ref type="bibr" target="#b5">Hanahan and Weinberg, 2000</ref>). Evolution explores different paths via random variation, and the path that leads to a faster entropy increase is naturally selected (<ref type="bibr" target="#b10">Kaila and Annila, 2008</ref>). As cancer evolves/progresses, the entropy at the genomic level increases (<ref type="bibr" target="#b1">Castro et al., 2006;</ref><ref type="bibr" target="#b6">Höglund et al., 2005</ref>). Here we investigate, using high-throughput studies, whether this is reflected at the transcriptomic level (as suggested by<ref type="bibr" target="#b22">Tsafrir et al., 2006</ref>). If so, this may shed light on the path of evolution of the cancer cell. Before we facilitate the study of the propagation of increased entropy of genome to transcriptome in the cancer cell, we first provide a statistical argument that suggests this indeed seems plausible. We then present statistical methodology to analyze highthroughput genomic experiments in order to answer the following related questions: @BULLET Is the entropy of a cancer sample's transcriptome higher than that of a normal sample?</p><p>@BULLET Is a cancer sample's genomic entropy associated to that of its transcriptome?</p><p>These questions are portrayed schematically in<ref type="figure">Figure 1</ref>. We illustrate how the discussed methodology may be utilized by application to multiple datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivation</head><p>We provide statistical motivations for the hypothesis that an increase of the entropy at the genomic level leads to an increase of the entropy at the<ref type="bibr">[</ref><ref type="bibr">, 2006</ref>):</p><formula>H(X) := − ∞ −∞ f X (x) log[f X (x)]dx.</formula><p>For our first motivation, we assume that DNA copy number and gene expression are both measured without error and their relation at any loci may be described by a strict monotone increasing function g(·):</p><formula>Y = g(X).</formula><p>In addition, we note that, according to Theorem 1 of Ramsay (1998), any smooth monotone function g(x) may be represented as:</p><formula>g(x) = c 0 +c 1 x −∞ exp t −∞ w(s)ds dt,</formula><formula>(1)</formula><p>where c 0 and c 1 are constants and w is a coefficient function that is</p><formula>, H(X 1 ) ≤ H(X 2 ) implies H(g(X 1 )) ≤ H(g(X 2 ))</formula><p>To show that H(g(</p><formula>X 1 )) ≤ H(g(X 2 )</formula><p>), we first note that the entropies of X 1 and g(X 1 ) are linked via:</p><formula>H(g(X 1 )) = H(X 1 )+ ∞ −∞ f X 1 (x) log dg(x) dx dx.</formula><p>It thus suffices to show that:</p><formula>∞ −∞ f X 1 (x) log dg(x) dx dx ≤ ∞ −∞ f X 2 (x) log dg(x) dx dx.</formula><p>In case g(·) is a linear map, g(x) = ax, this is immediate. To prove the desired inequality for other choices of g(·), we note that, appealing to Representation (1), the original inequality follows if we prove:</p><formula>∞ −∞ f X 1 (x) x −∞ w(s)dsdx ≤ ∞ −∞ f X 2 (x) x −∞ w(s)dsdx.</formula><formula>(2)</formula><p>Integration by parts yields:the expression levels of two genes that map to these loci. Furthermore, assume that the relation between DNA copy number changes and gene expression at both loci may be described by Y = βX +ε with X and ε independent, E(Y |X) = βX, and β ≥ 0 to reflect the empirical observation that a(n) increase/decrease in DNA copy number leads to a(n) increase/decrease in gene expression. Theorem 1 then tells us that if locus 2 is more prone to be aberrated at the genomic level than locus 1 (statistically operationalized as X 1 ≤ disp X 2 ), this is reflected in the transcriptome (under the assumption of a simple linear model) and locus 2 will exhibit more abnormal expression levels than locus 1. Corollary 1 is formulated in terms of the dispersive ordering, whereas our interest is in the entropy ordering. The relevance of Theorem 1 stems from the fact that dispersive ordering implies entropy ordering, i.e. X 1 ≤ disp X 2 ⇒ H(X 1 ) ≤ H(X 2 ) (<ref type="bibr" target="#b16">Oja, 1981</ref>). Corollary 1 is illustrated for the entropy ordering by two examples. In the first, let</p><formula>∞ −∞ f X 1 (x) x −∞ w(s)dsdx = ∞ −∞ w(s)ds− ∞ −∞ F X 1 (x)w</formula><formula>X 1 ∼ N (0,σ 2 X 1 ), X 2 ∼ N (0,σ 2 X 2</formula><p>), ε 1 ∼ N (0,σ 2 ε ) and 2 ∼ N (0,σ 2 ε ), all are independent. Then:</p><formula>H(βX 2 +ε 2 )−H(βX 1 +ε 1 ) = 1 2 log(β 2 σ 2 X 2 +σ 2 ε )− 1 2 log(β 2 σ 2 X 1 +σ 2 ε ). In case H(X 2 ) ≥ H(X 1 ), and thus σ 2 X 2 ≥ σ 2 X 1</formula><p>, this difference is non-negative. In the second example, let</p><formula>X 1 ∼ Cauchy(0,γ X 1 ), X 2 ∼ Cauchy(0,γ X 2 ), ε 1 ∼</formula><p>Cauchy(0,γ ε ) and ε 2 ∼ Cauchy(0,γ ε ), all are independent. Then, using a result from Blyth (1986):</p><formula>H(βX 2 +ε 2 )−H(βX 1 +ε 1 ) = log(βγ X 2 +γ ε )− log(βγ X 1 +γ ε ). In case H(X 2 ) ≥ H(X 1 ), and thus γ X 2 ≥ γ X 1</formula><p>, this difference is non-negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Experiments</head><p>Two types of experiments are considered. The first (referred to as experiment of Type I) involves n C samples from cancerous tissue of a particular same type. For all samples, both a DNA copy number and a gene expression profile are assumed to be available. The random variables X ij and Y ij represent the DNA copy number and the expression level of gene j, j = 1,...,p, of sample i, i = 1,...,n C , respectively. Together the DNA copy number profiles of all samples make up X = (X ij ) i=1,...,n C ;j=1,...,p , the n×p genomic aberration matrix, which we assume to contain no missing values. The gene expression matrix Y is defined similarly, also without missing values. The DNA copy number and gene expressions profiles of sample i are thus the i-th rows of matrices X and Y, respectively, and will be denoted X i and Y i. The second type of experiment (Type II) involves n = n N +n C samples, of which n N originate from normal, healthy tissue and n C from cancerous tissue of the same type. For all samples, only a gene expression profile is available. The gene expression data of experiments of Type II is denoted as above by Y = (Y ij ) i=1,...,n;j=1,...,p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Entropy</head><p>In the present context, entropy measures the diversity (at the molecular level) of the samples under study. Here diversity (entropy) at the transcriptomic Page: 558 556–563</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W.N.van Wieringen and A.W.van der Vaart</head><p>level is compared between normal and cancer samples. At the genomic level, the entropy of normal samples reaches a minimum (the DNA copy number of its autosomale genome equals two), whereas that of cancer samples can be anything as DNA copy number aberrations may be abound. From Section 2.1 we thus expect that this propagates to the expression levels, resulting in a higher entropy of the cancer transcriptome. To test this, we use the entropy difference between the normal and cancer samples as test statistic, and generate its null distribution by resampling. In order to calculate the test statistic, we first point out how the entropy of a set of samples may be calculated from high-dimensional data (p &gt; n), then discuss the test. The entropy of a multivariate normally distributed random variable, Y T i ∼ N (µ,), is given by 1 2 log<ref type="bibr">[det()]</ref>+ 1 2 p<ref type="bibr">[1+log(2π)]</ref>. For low-dimensional situations (p &lt; n) the determinant of , and thus the entropy of Y i , is straightforwardly obtained by estimating by S = 1 n (Y−ˆµY−ˆ Y−ˆµ) T (Y−ˆµY−ˆ Y−ˆµ),</p><formula>wherê µ = ( 1 n n i=1 Y i1 ,..., 1 n n i=1 Y ip )</formula><p>, and computing the product of its eigenvalues. However, in case p &gt; n the estimate S is singular and consequently det(S) = 0. This problem can be overcome when using a shrunken estimate of (<ref type="bibr" target="#b19">Schäfer and Strimmer, 2005</ref>). The shrunken estimate of the covariance matrix is given by: ˆ = (1−λ)S+λT, where S as before, T a diagonal matrix with diag(T) = diag(S) and the optimal (in some sense) λ equals [</p><formula>i =j Var(r ij )]/[ i =j r 2 ij ] with r ij = (S) ij / (S) ii (S)</formula><formula>= p j=1 s 2 jj p j=1 (1−λ)ν j +λ ,</formula><formula>where ν p ≥ ... ≥ ν p−n ≥ 0 = ν p−n−1 = ... = ν 1</formula><p>are the eigenvalues of˜Sof˜ of˜S = T −1/2 ST −1/2. The non-zero νs coincide with the square of the singular values of (Y−ˆµY−ˆ Y−ˆµ)T −1/2 / √ n. Hence, we are able to estimate the entropy when p &gt; n. To contrast the multivariate normality-based entropy, we also use a nonparametric motivated entropy estimate (<ref type="bibr" target="#b12">Kozachenko and Leonenko, 1987;</ref><ref type="bibr" target="#b13">Leonenko et al., 2008</ref>). Hereto we need the k-th nearest neighbor probability density estimate of f (·) given by:</p><formula>ˆ f k (Y i ) = k n−1 (p/2+1) π p/2 1 [d k (Y i )] p ,</formula><formula>where π p/2 / /(p/2+1)</formula><p>the volume of the unit ball in R p , and d k (Y i ) the Euclidean distance between Y i and its k nearest neighbor. The entropy is then estimated by:</p><formula>ˆ H k (Y) = − 1 n n i=1 logˆf logˆ logˆf k (Y i ) ,</formula><formula>(3)</formula><p>the average entropy of the observations (as determined within the sample). To test for a difference in entropy, we use the following test statistics:</p><formula>T = ˆ H (C) − ˆ H (N) , wherê H (C) andˆHandˆ andˆH (N)</formula><p>are the estimated entropy in cancer and normal sample, respectively. To obtain the null distribution of this test statistic, we permute 1 the sample labels and recalculate the test statistic. This process is repeated L times. The significance level of the tests is now calculated by {#|T obs ≤ T for = 1,...,L}/L, the proportion of the null distribution that exceeds the observed test statistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Mutual information</head><p>To investigate the genomic–transcriptomic entropy association, we use the concept of mutual information, a general measure of dependence between two random variables. Here, it measures the amount of information shared by genome and transcriptome. Or, loosely, how much knowledge of the genomic entropy tells us about the transcriptomic entropy. Formally, mutual information is defined as (<ref type="bibr" target="#b2">Cover and Thomas, 2006</ref>):</p><formula>I(Y ;X) := f (Y ,X) (y,x) log f (Y ,X) (y,x) f Y (y)f X (x)</formula><p>dydx.</p><formula>(4)</formula><p>Mutual information and entropy are related via I(</p><formula>Y ;X) = H(Y )−H(Y |X) = H(Y )+H(X)−H(Y ,X), where H(Y |X)</formula><p>is the conditional entropy of Y given X and H(Y ,X) the joint entropy of Y and X. Hence, by studying the mutual information between Y and X, we compare the unconditional entropy of the gene expression to its conditional counterpart, conditional on DNA copy number. In case, gene expression is independent of DNA copy number: H(Y|X) = H(Y) and I(Y;X) = 0. Mutual information can thus be used to test whether the transcriptomic entropy is associated with the genomic entropy, using resampling to assess whether mutual information deviates significantly from zero. We describe how to estimate I(Y;X) for the distributions considered in Section 2.3. In case of multivariate normality,</p><formula>Z := (X,Y) T : Z ∼ N µ X µ Y , XX YX XY YY =: N (µ Z , ZZ ),</formula><p>the mutual information in the high-dimensional setting of p &gt; n is estimated by:</p><formula>1 2 p j=1 log (1−λ Z )ν Y j +λ Z + 1 2 p j=1 log (1−λ Z )ν X j +λ Z − 1 2 2p j=1 log (1−λ Z )ν Z j +λ Z ,</formula><p>where λ Z the shrinkage parameter of shrunken estimate of ZZ , ν Z j the eigenvalues of T −1/2</p><formula>ZZ S ZZ T −1/2 ZZ with S ZZ = 1 n (Z−ˆµZ−ˆ Z−ˆµ Z )(Z−ˆµZ−ˆ Z−ˆµ Z ) T and T ZZ a</formula><p>diagonal matrix with diag(T ZZ ) = diag(S ZZ ), and ν Y j and ν X j defined similarly. Again to contrast the multivariate normality assumption, we also estimate the mutual information under the assumption of the k-th nearest neighbor distribution. We follow the approach described in<ref type="bibr" target="#b13">Kraskov et al. (2004)</ref>. The joint entropy, the entropy of Z = (X,Y) T , can be estimated using (3). To obtain the mutual information we need to subtract if from the entropies of X and Y, using the same k. However, this results in a biased mutual information estimate (<ref type="bibr" target="#b13">Kraskov et al., 2004</ref>). To resolve this<ref type="bibr" target="#b13">Kraskov et al. (2004)</ref>propose, in the estimation of the marginal entropies, to fix the nearest neighbor distance instead of k. Let d k (Z i ) be the distance (with respect to the uniform norm) of Z i to its k-th nearest neighbor, and k x (i) and k y (i) the number of samples that fall marginally in the balls B(</p><formula>X i ,d k (Z i )) and B(Y i ,d k (Z i )</formula><p>). Now, following<ref type="bibr" target="#b27">Wang et al. (2009)</ref>, the marginal entropies are estimated, by, e.g.:</p><formula>˜ H kz (X) = − 1 n n i=1 ψ[k x (i)]+log(n−1) (5) +log π p/2 (p/2+1) + 1 n n i=1 log[d kx (i) (X i )] p ],</formula><p>where ψ is the digamma function and k Z refers to the k chosen for Z. The mutual information is then estimated by˜Hby˜ by˜H kz</p><formula>(X)+ ˜ H kz (Y)− ˆ H k (Z): ˆ I(Y;X) = ψ(k z )− 1 n n i=1 [ψ(k x (i))+ψ(k y (i))]+ψ(n),</formula><formula>(6)</formula><p>where last term of entropy estimates (5) cancel out as the same nearest neighbor distances are used in the calculation of joint and marginal entropies. Estimator (6) is less biased than a mutual information estimate using the same k for the estimation of H(X),H(Y) and H(Z) (<ref type="bibr" target="#b13">Kraskov et al., 2004;</ref><ref type="bibr" target="#b27">Wang et al., 2009</ref>). Note that the estimator<ref type="bibr" target="#b24">de Wiel, 2009</ref>). Under the null hypothesis, there is no association between copy number and expression, consequently the permutation only yields the random behavior of the test statistic. For each permutation, the test statistic is recalculated. After L permutations, the P-value is calculated as in Section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SIMULATION</head><p>Here we report a simulation study that serves three ends: (i) to make an informed choice on the nearest neighbor parameter k for entropy and mutual information estimation, (ii) to investigate the behavior of the entropy and mutual information estimators under increased high dimensionality and (iii) to study the relation between the two entropy estimators (similarly for the two mutual information estimators). The simulation study involves artificial data, which facilitates insight on all three ends, as they are sampled from a known distribution with known entropy and mutual information. The setup is described next. Artificial datasets involve n = 20 samples from a p-variate, p = αn with α varying from 0.5 to 50, normal distribution N (0,). Prior to the generation of each dataset, the covariance matrix is itself randomly drawn from an inverse Wishart distribution</p><formula>= p+2, E() = ,</formula><p>the sampled covariance structure mimics that found in real datasets. For each α, multiple datasets are drawn, for which the entropy is estimated (assuming normality and k-NN) and the true entropy using the corresponding drawn is calculated. For the investigation of mutual information datasets with twice the number of genes are drawn, which is randomly split in two equal sized parts. In the estimation of the k-nearest neighbor entropy and mutual information k = βn , where β ranges from 0.05 to 0.95. The simulation results are first used to find an optimal β for entropy and mutual information estimation (when assuming k-NN). Hereto the correlation betweenˆHbetweenˆ betweenˆH knn and H true and betweenˆIbetweenˆ betweenˆI knn and I true are calculated for all combinations of α and β. Ideally, these correlations are high, and an optimal choice of β results in the highest correlations over the whole range of α. The correlations are displayed as a contour plot in<ref type="figure" target="#fig_1">Figure 2</ref>. From<ref type="figure" target="#fig_1">Figure 2</ref>, it is clear that both entropy and mutual information estimation are served best by choosing a small β (irrespective of the high-dimensionality parameter α). To investigate the effect of increased high dimensionality on entropy and mutual information, we again calculate the correlations betweenˆHbetweenˆ betweenˆH and H true and betweenˆIbetweenˆ betweenˆI and I true (assuming both normality and k-NN), which are denoted ρ normˆH</p><formula>normˆ normˆH,H , ρ knnˆH knnˆ knnˆH,H , ρ normˆI normˆ normˆI,I and ρ normˆI normˆ normˆI,I</formula><p>. These correlations are plotted against α (plots not shown, but can be deduced from<ref type="figure" target="#fig_1">Figure 2</ref>for the k-NN assumption). This reveals that the correlations at first decay rapidly as α is increased, but thatitself is not surprising, but the leveling off is encouraging, as the proposed estimators, although noisy, do provide information on the quantities of interest. Finally, we investigate whether the estimates of the different operationalizations of entropy and mutual information induced by the different distributional assumptions yield concordant results. We now correlatê H norm withˆHwithˆ withˆH knn andˆIandˆ andˆI norm withˆIwithˆ withˆI knn. For the simulated data, Spearman's correlation between the entropy estimates is high (varying from 0.80 to 0.95, with highest values assumed for small k) and between the mutual information estimates is substantial (varying from 0.40 to 0.75, with highest values assumed for small k). The concordance of the results from both entropy and mutual information operationalizations indicates that they indeed measure the same quantity. By lack of information on the true entropy and mutual information, which is the case when analyzing real data, corroboration between the two operationalizations enhances the confidence in the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPLICATION TO CANCER DATA</head><p>The aforementioned methodology is applied to publicly available datasets representing both experiment types described in Section 2.2. The first five columns of<ref type="figure" target="#tab_1">Table 1</ref>give an overview of the datasets analyzed. Details on array platforms, preprocessing, etc. are provided in the Supplementary Material. Here we only point out that we have used normalized instead of segmented or called copy number data (refer to<ref type="bibr" target="#b25">Van Wieringen et al., 2007</ref>, for details on the differences between these data) in the analysis of Type I experiments, as beforehand the multivariate normality assumption is unlikely to hold for the latter two. See Section 4.4 for a discussion on normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Analyses of Type II experiments</head><p>We analyzed the Type II experiments listed in<ref type="figure" target="#tab_1">Table 1</ref>this figure, it is clear that the cancer samples are much more spread out, indeed indicating a higher entropy. The Kim and Mougeot datasets are of particular interest as they comprise, next to the analyzed normal and cancer samples, samples from other cancer stages. The former also yields samples from an intermediate and a more advanced stage: 'pin' (13 samples) and 'metastatic' (20 samples), whereas the latter contains two intermediate categories: 'benign' (18 samples) and 'borderline malignant' (3 samples). This allows further investigation of the transcriptomic entropy increase with advanced cancer stage. All possible stage pairs are compared, testing for larger entropy in the more advanced cancer stage. Tables 2 and 3 contain the results. In the Mougeot dataset, not all entropy comparisons are significant, which is (to a large extent) due to the low sample size of the 'borderline malignant' group (consisting of only three samples). Nonetheless, the general picture that emerges from the pairwise analyses in both datasets is that the entropy of a cancer stage exceeds that of preceding stages.The entropy surge may be interpreted as an increased diversity among the cancer samples. This may suggest that cancers develop along many different routes, though all leading to unproliferated growth. Consequently, it may prove difficult to develop one therapy that will benefit all individuals with the same cancer. A large (genomic and transcriptomic) diversity increases chances of individuals being and becoming resistant against the therapy, as has been observed in many cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W.N.van Wieringen and A.W.van der Vaart</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 561 556–563</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of the cancer cell's molecular entropy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analyses of Type I experiments</head><p>For all Type I experiments listed in<ref type="figure" target="#tab_1">Table 1</ref>, the mutual information I(Y;X) between gene expression and DNA copy number is calculated. To evaluate whether I(Y;X) deviates significantly from zero, a permutation test as described in Section 2.4 with L = 1000 permutations is conducted. The results are presented in<ref type="figure" target="#tab_1">Table 1</ref>. Due to very different (and in these high-dimensional situations hard to verify) distributional assumptions, the P-values may differ between the two presented tests. Nonetheless, in almost all Type I datasets the mutual information deviates significantly from zero, under both distributional assumptions. Hence, the unconditional transcriptomic entropy H(Y) significantly exceeds the conditional transcriptomic entropy H(Y|X), conditional on the DNA copy number. The Kim dataset, however, shows deviating behavior with clear non-significant P-values. This seems due to the fact that these cancer samples contain relatively few (compared to, e.g. the Bergamaschi dataset) genomic aberrations (as determined by the calling method CGHcall, Van<ref type="bibr" target="#b23">de Wiel et al., 2007</ref>). When analyzing the metastatic samples of the Kim dataset, that contain more genomic aberrations, the permutation test for H 0 : I(Y;X) = 0 yields P-values 0.076 (normality) and 0.064 (k-NN). This suggests that also in prostate cancer the genomic entropy surge is propagated to the transcriptome, although perhaps at a later stage in the progression of the disease. To accompany the above test results, we propose a visualization. The k-NN entropy statistic (3) is composed of the entropies at each observation. Each sample's contribution to the k-th nearest neighbor genomic entropy estimate may then be plotted against its contribution to the k-th nearest neighbor transcriptomic entropy estimate. If indeed the entropies of the two molecular levels are closely related, we expect the 'marginal' entropies at eachobservation, log<ref type="bibr">[</ref></p><formula>ˆ f k (Y i )] and log[ ˆ f k (X i )]</formula><p>, to be positively associated. This is visualized for two datasets in the upper panels of<ref type="figure" target="#fig_4">Figure 4</ref>. Indeed, as the test results reported in<ref type="figure" target="#tab_1">Table 1</ref>suggest, the 'marginal' entropies of the two molecular levels reveal a positive association. The concordant surge in entropy of the cancer cell's molecular levels may be interpreted as follows. Hereto we exploit the reciprocal link between entropy and information (<ref type="bibr" target="#b4">Gatenby and Frieden, 2004</ref>). As cancer progresses, the information content of a cancer cell's genome declines until some minimum necessary for the cell to function and proliferate is reached. The above shows that this information decline is reflected at the transcriptomic level, which is a prerequisite for it to have any phenotypic/oncogenic effect. Together these observations suggest that in the evolution (which naturally selects the path of steepest entropy ascent, Kaila and Annila 2008) of the cancer cell, natural selection acts on those parts of the genome and transcriptome that contribute to the cancer cell's minimum information content that enable it to realize its small but focused agenda, making more copies of itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Potential</head><p>Besides providing insight into cancer progression, entropy may be used in clinical cancer research. We illustrate this by two examples, which can be the basis for further research. We shall refer to the observation that molecular entropy increases with cancer progression as the Entropy Assumption. In the first example, we aim to reconstruct (unsupervisedly) the order of the samples' cancer advancement using the Entropy Assumption. A simple unsupervised procedure to achieve this would be to start with all samples together, and remove them one by one in accordance with the largest entropy decrease of the expression levels caused by a sample's removal. If the Entropy Assumption has some value, the order of removal is, negatively related to the samples' cancer advancement: Page: 562 556–563</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W.N.van Wieringen and A.W.van der Vaart</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>W −1 (d,), with d = p+2 (allowing to vary around ) and = V T V. The p×p dimensional matrix V is diagonal with diagonal elements sampled randomly from the empirical distribution of MADs of gene expression data from individual genes of the Chitale dataset. The p×p dimensional matrix is the correlation matrix estimated from the expression data of the same p genes of the Chitale dataset (see Section 4), with correlations estimated by Kendall's τ, which ensures the positive definiteness of. As, for d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.2.</head><figDesc>Fig. 2. Contour plots of the correlations betweenˆHbetweenˆ betweenˆH knn and H true and betweenˆIbetweenˆ betweenˆI knn and I true in relation to high dimensionality (α = p/n) and the number of nearest neighbors (β = k/n). this decay levels off to settle at ρ normˆH normˆ normˆH,H ≈ 0.30 and ρ normˆI normˆ normˆI,I ≈ 0.40 after</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><figDesc>by calculating the entropy difference between cancer and normal groups under the normality and k-th nearest neighbor distributional assumptions discussed in Section 2.3. Permutation tests with L = 1000 were used to evaluate the null hypothesis of no entropy difference between the groups. The results are displayed in Table 1. All Type II datasets show a significant (P &lt; 0.10 for all analyses, but more often than not Page: 560 556–563</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.3.</head><figDesc>Fig. 3. The dotted and dashed lines result from convex hull peeling per group. For clarity, the groups have been artificially separated by adding a constant to the first principal component of the normal samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.4.</head><figDesc>Fig. 4. In all panels, the marginal genomic and transcriptomic entropies are plotted against each other. Each graph contains the isotonic regression fit in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.5.</head><figDesc>Fig. 5. Order in which samples are removed (by the unsupervised procedure) against cancer stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>with density f X as (Cover and Thomas</figDesc><table>10:01 2/2/2011 Bioinformatics-btq704.tex] 

Page: 557 556–563 

Analysis of the cancer cell's molecular entropy 

Fig. 1. Schemata of entropy relations of genome and transcriptome within 
and between normal and cancer samples. 

transcriptomic level. To do so, we need the definition of the (differential) 
entropy of a continuous random variable X </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>Lebesque square integrable. By construction, w satisfies the second-order differential equation d 2 g(x)/dx 2 = wdg(x)/dx. For w(s) = 0, g(·) is linear; w(s) = α implies g(·) is an exponential function. Proposition 1. Let X 1 and X 2 be random variables with symmetric and zero-centered densities f X 1 and f X 2</figDesc><table>, respectively, and g(·) a strict monotone 
function generated by Representation (1) using an even coefficient function 
w(s), i.e. w(s) = w(−s). Then</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>(x)dx. From the assumption that X 1 and X 2 are symmetrically distributed around zero and the symmetry of w(s), w(s) = w(−s), the second term on the righthand side equals ∞ 0 w(s)ds and equality in Equation (2) follows. Hence, under the assumptions of Proposition 1, we have shown that H(X 1 ) ≤ H(X 2 ) implies H(g(X 1 )) ≤ H(g(X 2 )). If one is not willing to assume that DNA copy number and gene expression are known without error, we provide an alternative motivation. Hereto we introduce the concept of dispersive ordering between two random variables (Shaked and Shanthikumar, 2007). Let X 1 and X 2 be two random variables with distributions F X 1 and F X 2 , respectively. Let F −1 X 1 and F −1 X 2 be the right continuous inverses of F X 1 and F X 2 , and assume that: F −1 X 1 (b)−F −1 X 1 (a) ≤ F −1 X 2 (b)−F −1 X 2 (a) for all 0 ≤ a ≤ b ≤ 1. Then X 1 is said to be smaller than X 2 in the dispersive order, denoted by X 1 ≤ disp X 2. Corollary 1. Let X 1 ,X 2 and ε be independent random variables with logconcave densities f X 1 ,f X 2 and f ε , respectively. Then, if X 1 ≤ disp X 2 and β ≥ 0: βX 1 +ε ≤ disp βX 2 +ε. 1 and X 2 be random variables representing the DNA copy number changes at two different loci and Y 1 and Y 2</figDesc><table>Corollary 1 follows directly from Theorems 3.B.4 and 3.B.9 of Shaked and 
Shanthikumar (2007). 
To interpret Corollary 1, let X </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>jj (confer Schäfer and Strimmer, 2005). The determinant of the shrunken covariance estimate is given by:</figDesc><table>det( ˆ 
) = det(T 1/2 T −1/2 ˆ 
T −1/2 T 1/2 ) 

= det(T 1/2 ) 2 det[(1−λ)T −1/2 ST −1/2 +λI] 

= det(T) det[(1−λ) ˜ 
S+λI] 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>to scale X and Y to comparable scales before the estimation of mutual information. The null distribution of the test statistic is estimated by permuting the columns of the gene expression matrix Y, while the DNA copy number matrix X is left unchanged (as is done in Van Wieringen and Van</figDesc><table>(6) is not scale invariant, and Kraskov [10:01 2/2/2011 Bioinformatics-btq704.tex] 

Page: 559 556–563 

Analysis of the cancer cell's molecular entropy 

et al. (2004) recommend </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><figDesc>Table 1. Datasets used (more details are provided in the Supplementary Material) and their analysis results</figDesc><table>Dataset 
Cancer 
Experiment 
n N 
n C 
Entropy 
Entropy 
Mutual information 
Mutual information 
name 
type 
type 
P-value 
P-value 
P-value 
P-value 
(normality) 
(knn, k = 1) 
(normality) 
(knn, k = 1) 

Chandran 
Prostate 
II 
54 
54 
0.012 
0.026 
– 
– 
D'Errico 
Gastric 
II 
31 
38 
0.000 
0.000 
– 
– 
Kim 
Prostate 
II 
15 
32 
0.008 
0.002 
– 
– 
Landi 
Lung 
II 
49 
58 
0.000 
0.000 
– 
– 
Mougeot 
Ovary 
II 
14 
32 
0.000 
0.000 
– 
– 
Scotto 
Cervix 
II 
25 
41 
0.070 
0.000 
– 
– 
Singh 
Prostate 
II 
50 
52 
0.000 
0.046 
– 
– 
Stirewalt 
AML 
II 
18 
26 
0.000 
0.000 
– 
– 

Bergamaschi 
Breast 
I 
– 
85 
– 
– 
0.000 
0.000 
Carvalho 
Colon 
I 
– 
62 
– 
– 
0.000 
0.002 
Chitale 
Lung 
I 
– 
88 
– 
– 
0.000 
0.000 
Kim 
Prostate 
I 
– 
17 
– 
– 
0.664 
0.184 
Lenz 
DLBCL 
I 
– 
203 
– 
– 
0.000 
0.106 
Oudejans 
DLBCL 
I 
– 
42 
– 
– 
0.000 
0.002 
Pollack 
Breast 
I 
– 
41 
– 
– 
0.000 
0.020 
Zhang 
Breast 
I 
– 
263 
– 
– 
0.000 
0.000 

Note that the Kim dataset is a combination of a Type I and a Type II experiment and therefore appears twice in the table below. Its n C differs, for the analysis of its Type I version 
the cancer samples are limited to those that have both DNA copy number and gene expression profiles available. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><figDesc>Table 2. Additional results for the Kim dataset: P-values under both distributional assumptions</figDesc><table>Cancer 
stage 

Distributional 
assumption 

Cancer stage 

PIN 
Cancer 
Metastasis 

Normal 
Normal 
0.468 
0.012 
0.000 
Normal 
knn, k = 1 
0.356 
0.004 
0.000 

PIN 
Normal 
0.016 
0.000 
PIN 
knn, k = 1 
0.014 
0.000 

Cancer 
Normal 
0.008 
Cancer 
knn, k = 1 
0.008 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><figDesc>Table 3. Additional results for the Mougeot dataset: P-values under both distributional assumptions</figDesc><table>Cancer stage 
Distributional 
assumption 

Cancer stage 

Benign 
Borderline 
Malignant 
malignant 

Normal 
Normal 
0.104 
0.019 
0.000 
Normal 
knn, k = 1 
0.450 
0.048 
0.000 

Benign 
Normal 
0.004 
0.000 
Benign 
knn, k = 1 
0.082 
0.004 

Borderline 
Normal 
0.446 
malignant 
Borderline 
knn, k = 1 
0.340 
malignant 

</table></figure>

			<note place="foot">© The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="1"> The non-parametric bootstrap seems inappropriate here (and in Section 2.4) as it draws datasets of the same dimensions with identical replicate samples, which inflates the compactness (decreases entropy) of the dataset.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion</head><p>Analysis results depend to some extent on the normalization method applied. It may therefore be argued that we should have preprocessed the data with more than one normalization method. In fact, we analyzed two different preprocessed version of the Singh dataset (Type II), one preprocessed by the RMA approach of<ref type="bibr" target="#b9">Irizarry et al. (2003)</ref>and the other by the MAS approach of Affymetrix. This yielded comparable significant P-values. In addition, we re-analyzed the Chitale dataset (Type I) with segmented (instead of normalized) DNA copy number data, also leading to identical results. For other datasets (when preprocessed differently, we expect similarly consistent analysis results. Perhaps more convincing, is the fact that, e.g. an entropy difference among normal and cancer is observed over many datasets, involving many different tissue types, generated on various platforms and preprocessed with a diverse array of normalization methods. This despite the fact that normalization aims to remove differences between hybridizations, and is thus likely to obscure (at least partially) the entropy signal. In addition, results are, within tissue type, consistent over datasets (for Type II experiments, the prostate cancer datasets Chandran, Kim and Singh reveal concordant results; for Type I experiments, the results of the breast cancer datasets Bergamaschi, Pollack and Zhang all agree). In future, the data analyzed here (DNA copy number and gene expression data) will be generated by means of massive parallel sequencing (MPS). Apart from the improved resolution, such data will be less noisy and normalization is expected to be less of a nuisance for entropy comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We provided a motivating statistical argument which suggests that an increase in genomic entropy is reflected in the transcriptome. Statistical methodology that facilitates the investigation of this hypothesis through analysis of high-throughput DNA copy number and gene expression data is presented. The methodology is illustrated on a multitude of datasets from cancers of various tissues. In addition, the results from analyses of these data suggest that the hypothesis of a related genomic and transcriptomic entropy in cancer has more than only face value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict</head><p>of Interest: none declared.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Convolutions of Cauchy distributions</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">R</forename>
				<surname>Blyth</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. Math. Mon</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="645" to="647" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Chromosome aberrations in solid tumors have a stochastic nature</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A A</forename>
				<surname>Castro</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mutat. Res</title>
		<imprint>
			<biblScope unit="volume">600</biblScope>
			<biblScope unit="page" from="150" to="164" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">M</forename>
				<surname>Cover</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">A</forename>
				<surname>Thomas</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>John Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>2nd. edn</note>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">A genetic model for colorectal tumorigenesis</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Fearon</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Vogelstein</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="759" to="767" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Information dynamics in carcinogenesis and tumor growth</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">A</forename>
				<surname>Gatenby</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">R</forename>
				<surname>Frieden</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mutat. Res</title>
		<imprint>
			<biblScope unit="volume">568</biblScope>
			<biblScope unit="page" from="259" to="273" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">The hallmarks of cancer</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Hanahan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Weinberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="57" to="70" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Statistical behavior of complex cancer karyotypes</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Höglund</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genes Chromosomes Cancer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="327" to="341" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="563" to="556" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<monogr>
		<title level="m" type="main">Analysis of the cancer cell&apos;s molecular entropy</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploration, normalization, and summaries of high density oligonucleotide array probe level data</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">A</forename>
				<surname>Irizarry</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="249" to="264" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Natural selection for least action</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<forename type="middle">R I</forename>
				<surname>Kaila</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Annila</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. R. Soc. A</title>
		<meeting>. R. Soc. A</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3055" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Activation of multiple cancer-associated genes at the ERBB2 amplicon in breast cancer</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Kauraniemi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kallioniemi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Endocr. Relat. Cancer</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="39" to="49" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">On statistical estimation of entropy of a random vector</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">F</forename>
				<surname>Kozachenko</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<forename type="middle">N</forename>
				<surname>Leonenko</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Problems Inform. Transm</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="95" to="101" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimating mutual information Estimation of entropies and divergences via nearest neighbors</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kraskov</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<publisher>Tatra Mountains Math. Publications</publisher>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="265" to="273" />
			<date type="published" when="2004" />
			<publisher>Tatra Mountains Math. Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Cancer as an evolutionary and ecological process</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">M F</forename>
				<surname>Merlo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Cancer</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="924" to="935" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">KEGG: Kyoto Encyclopedia of Genes and Genomes</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Ogata</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="29" to="34" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">On location, scale, skewness and kurtosis of univariate distributions. Scand</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Oja</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="154" to="168" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Microarray analysis reveals a major direct role of DNA copy number alteration in the transcriptional program of human breast tumors</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">R</forename>
				<surname>Pollack</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci</title>
		<meeting>. Natl Acad. Sci</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="12963" to="12968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimating smooth monotone functions</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">O</forename>
				<surname>Ramsay</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="365" to="375" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">A shrinkage approach to large-scale covariance matrix estimation and implications for functional genomics</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Schäfer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Strimmer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Appl. Genet. Mol. Biol</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<monogr>
		<title level="m" type="main">Stochastic Orders and Their Applications</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Shaked</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">G</forename>
				<surname>Shanthikumar</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">The cancer genome</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">R</forename>
				<surname>Stratton</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">458</biblScope>
			<biblScope unit="page" from="719" to="724" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Relationship of gene expression and chromosomal abnormalities in colorectal cancer</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Tsafrir</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Res</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="2129" to="2137" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">CGHcall: calling aberrations for array CGH tumor profiles</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Van De Wiel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="892" to="894" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Nonparametric testing for DNA copy number induced differential mRNA gene expression</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">N</forename>
				<surname>Van Wieringen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Van De Wiel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="19" to="29" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<monogr>
		<title level="m" type="main">Normalized, segmented or called aCGH data? Cancer Inform</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">N</forename>
				<surname>Van Wieringen</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="331" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">A random coefficients model for regional coexpression associated with DNA copy number aberrations</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">N</forename>
				<surname>Van Wieringen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Appl. Genet. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Divergence estimation for multidimensional densities via knearest-neighbor distances</title>
		<author>
			<persName>
				<forename type="first">Q</forename>
				<surname>Wang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="2392" to="2405" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">The Biology of Cancer</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">A</forename>
				<surname>Weinberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Garland Science</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">MicroRNAs exhibit high frequency genomic alterations in human cancer</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="9136" to="9141" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>