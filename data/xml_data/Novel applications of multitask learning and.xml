
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Novel applications of multitask learning and multiple output regression to multiple genetic trait prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Dan</forename>
								<surname>He</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM T.J. Watson Research</orgName>
								<orgName type="institution" key="instit2">Yorktown Heights</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">David</forename>
								<surname>Kuhn</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="institution">USDA-ARS Subtropical Horticultural Research Station</orgName>
								<address>
									<settlement>Miami</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Laxmi</forename>
								<surname>Parida</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM T.J. Watson Research</orgName>
								<orgName type="institution" key="instit2">Yorktown Heights</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Novel applications of multitask learning and multiple output regression to multiple genetic trait prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btw249</idno>
					<note>Availability and implementation: The programs we used are either public or directly from the referred authors, such as MALSAR (http://www.public.asu.edu/~jye02/Software/MALSAR/) package. The Avocado data set has not been published yet and is available upon request. Contact: dhe@us.ibm.com</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Given a set of biallelic molecular markers, such as SNPs, with genotype values encoded numerically on a collection of plant, animal or human samples, the goal of genetic trait prediction is to predict the quantitative trait values by simultaneously modeling all marker effects. Genetic trait prediction is usually represented as linear regression models. In many cases, for the same set of samples and markers, multiple traits are observed. Some of these traits might be correlated with each other. Therefore, modeling all the multiple traits together may improve the prediction accuracy. In this work, we view the multitrait prediction problem from a machine learning angle: as either a multitask learning problem or a multiple output regression problem, depending on whether different traits share the same genotype matrix or not. We then adapted multitask learning algorithms and multiple output regression algorithms to solve the multitrait prediction problem. We proposed a few strategies to improve the least square error of the prediction from these algorithms. Our experiments show that mod-eling multiple traits together could improve the prediction accuracy for correlated traits.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Whole genome prediction of complex phenotypic traits using highdensity genotyping arrays has attracted lots of attention, as it is relevant to the fields of plant and animal breeding and genetic epidemiology (<ref type="bibr" target="#b8">Cleveland et al., 2012;</ref><ref type="bibr" target="#b12">Hayes et al., 2009;</ref><ref type="bibr" target="#b13">Heffner et al., 2009;</ref><ref type="bibr" target="#b15">Jannink et al., 2010;</ref><ref type="bibr" target="#b19">Lande and Thompson, 1990;</ref><ref type="bibr" target="#b22">Meuwissen et al., 2001;</ref><ref type="bibr" target="#b25">Rincent et al., 2012;</ref><ref type="bibr" target="#b31">Xu and Crouch, 2008</ref>). Given a set of biallelic molecular markers, such as SNPs, with genotype values typically encoded as {0, 1, 2} on a collection of plant, animal or human samples, the goal is to predict the quantitative trait values by simultaneously modeling all marker effects. The problem is called genetic trait prediction or genomic selection. More specifically, the genetic trait prediction problem is defined as follows. Given n training samples, each with m ) n genotype values (we use 'feature', 'marker', 'genotype', 'SNP' interchangeably) and a trait value, and a set of n 0 test samples each with the same set of genotype values but without trait value, the task is to train a predictive model from the training samples to predict the trait value, or phenotype of each test sample based on their genotype values. Let Y be the trait value of the training samples. The problem is usually represented as the following linear regression model:</p><formula>Y ¼ b 0 þ X m i¼1 b i X i þ e l (1)</formula><p>where X i is the ith genotype value, m is the total number of genotypes and b i is the regression coefficient for the ith genotype, e l is the error term. There have been lots of work on predicting genetic trait values from genotype data, such as rrBLUP (Ridge regression BLUP) (<ref type="bibr" target="#b22">Meuwissen et al., 2001</ref>), Elastic-Net, Lasso, Ridge Regression (<ref type="bibr" target="#b28">Tibshirani, 1994;</ref><ref type="bibr" target="#b27">Shaobing Chen et al., 1998</ref>), Bayes A, Bayes B (<ref type="bibr" target="#b22">Meuwissen et al., 2001</ref>), Bayes C p (<ref type="bibr" target="#b18">Kizilkaya et al., 2010</ref>) and Bayesian Lasso (<ref type="bibr" target="#b20">Legarra et al., 2011;</ref><ref type="bibr" target="#b23">Park and Casella, 2008</ref>), as well as other machine learning methods. Most of the work assumes that for each set of samples there is only one trait, and therefore, a single regression is conducted to predict the trait value. However, in reality, it is quite often the case that we could observe and measure multiple traits rather than one, especially for crops and animals. For example, for plant dataset, once we obtain a fruit, we could measure its weight, size, etc. This will give us multiple traits. Obviously some V C The Author 2016. Published by Oxford University Press.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>i37</head><p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com<ref type="bibr">Bioinformatics, 32, 2016</ref><ref type="bibr" target="#b16">Jia and Jannink, 2012</ref>), Bayesian multivariate antedependence model (<ref type="bibr" target="#b17">Jiang et al., 2015</ref>). GBLUP and multitrait BayesA are mainly based on the framework of linear regression. The Bayesian multivariate antedependence model considers nonstationary correlations between SNP markers through assuming a linear relationship between the effects of adjacent markers. These methods are shown to have superior performance compared with single trait prediction methods. In this work, we study the multitrait prediction problem from a machine learning angle. We consider the multitrait prediction problem as a multitask learning problem or a multiple output regression problem. When there are multiple sets of samples, each having a separate set of genotypes on the same set of markers as well as a corresponding trait, the multitrait prediction problem can be converted into a multitask learning problem. This is shown in<ref type="figure" target="#fig_1">Figure 1</ref>. We can see that there are three sample sets and three different genotype matrices. These genotype matrices, however, share the same set of markers. Each sample set has a different trait. When there is only set of samples and one set of genotypes but multiple traits, the problem can be converted into a multiple output regression problem, as shown in<ref type="figure" target="#fig_2">Figure 2</ref>. There is only one sample set and one genotype matrix. There are three different traits. Although lots of work have been done for the multitrait prediction problem, this is indeed the first time the problem is modeled as a multitask learning and a multiple output regression problem. We can see that for the multitask learning problem, if we learn each task independently, we only use a small portion of the samples.</p><p>If we model all the tasks at the same time, we leveraged the information from the complete set of samples and the improvement could be significant when the number of tasks is large. In the multiple output regression problem, although all the tasks share the same set of samples and there is no advantage on the sample size, the prediction performance could still be improved by the modeling the correlations among the tasks. In this work, we adapt the state-of-the-art multitask learning algorithms and multiple output regression problems to solve the multiple trait prediction problem. For the multiple output regression problem, we conduct an iterative algorithm to learning the variable one at a time with others fixed. The objective function is convex when we only optimize one variable with others fixed, and therefore, efficient optimization is allowed. We observed that a direct application of these algorithms to the multiple trait prediction problem usually leads to poor least square error. We applied strategies such as centering the genotype matrix to improve the prediction performance. We showed that modeling all the traits together could improve the prediction compared with predicting each trait independently, especially for the correlated traits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Given the traditional encoding of genotypes as {0, 1, 2}, lots of techniques have been applied to the genetic trait prediction problem defined in Equation (1). Consider the typical situation for linear regression, where we have the training set y 2 R l ; x 2 R lÂn , in a standard linear regression, we wish to find parameters b 0 ; b such that the sum of square residuals, P l i¼1 ðy i À b 0 À x &gt; i;Á bÞ 2 , is minimized. Many machine learning methods have been applied to the genetic single trait prediction problem, such as Elastic-Net, Lasso, Ridge Regression (<ref type="bibr" target="#b27">Shaobing Chen et al., 1998;</ref><ref type="bibr" target="#b28">Tibshirani, 1994</ref>), Bayes A, Bayes B (<ref type="bibr" target="#b22">Meuwissen et al., 2001</ref>), Bayes C p (<ref type="bibr" target="#b18">Kizilkaya et al., 2010</ref>) and Bayesian Lasso (<ref type="bibr" target="#b20">Legarra et al., 2011;</ref><ref type="bibr" target="#b23">Park and Casella, 2008</ref>). They could be applied to predict the multiple traits where each trait is predicted independently. In this work, we applied ridge regression (<ref type="bibr" target="#b14">Hoerl and Kennard, 1970</ref>) for single trait prediction, which aims to minimize the following objective function.</p><formula>min X l i¼1 ðy i À x T i bÞ 2 þ k X n j¼1 b 2 j ; " (2)</formula><p>The solution of ridge regression is given by:</p><p>b ¼ ðX T X þ kIÞ À1 X T y</p><formula>(3)</formula><p>which is similar to the ordinal least square solution, but with the addition of a 'ridge' down the diagonal. Ridge regression has been shown to have certain bias as ÀkðX T X þ kIÞ À1 b. The unbiased version of rrBLUP (Ridge regression BLUP) (<ref type="bibr" target="#b22">Meuwissen et al., 2001;</ref><ref type="bibr" target="#b30">Whittaker et al., 2000</ref>) is one of the most popular methods for genetic trait prediction. rrBLUP simply is ridge regression with a specific choice of k in (2). Specifically, Meuwissen et al.<ref type="bibr">, 2012</ref>), Bayesian multivariate antedependence model (Jiang<ref type="bibr">et al., 2015</ref>). In multivariate models with m traits, marker effects on phenotypic traits were estimated from the mixed linear model below:</p><formula>y ¼ u þ X p j¼1 X j a j d j þ e (4)</formula><p>where y is a n Â m matrix with n samples and m traits, a j is a 1 Â m vector for the effects of the jth marker on all m traits which is assumed to be normally distributed a j $ Nð0; R aj Þ; R aj is m Â m variance-covariance matrix for the jth marker, e is a n Â m matrix for residual error that follows a normal distribution. In multitrait GBLUP (<ref type="bibr" target="#b8">Cleveland et al., 2012</ref>), unstructured covariance matrix among traits was assumed and the relationship matrix derived from SNPs were fit in ASReml (<ref type="bibr" target="#b10">Gilmour et al., 2009</ref>). The multitrait BayesA model (<ref type="bibr" target="#b16">Jia and Jannink, 2012</ref>) assumes the prior of R aj follows a scaled inverse-Wishart distribution, which were given a flat prior and estimated from the data using the Metropolis algorithm to sample from the joint posterior distribution. Gibbs sampling and MCMC are applied to estimate the parameters. In the Bayesian multivariate antedependence model (<ref type="bibr" target="#b17">Jiang et al., 2015</ref>), it is assumed that the adjacent markers are correlated as below:</p><formula>a j ¼ d j j ¼ 1 t j;jÀ1 a jÀ1 þ d j j ¼ 2;. .. ; p ( (5)</formula><p>where t j;jÀ1 is the scalar antedependence parameter of a j on a jÀ1. Again, the parameters are estimated via Gibbs sampling and MCMC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multitrait prediction</head><p>As we have discussed before, there are two types of multitrait prediction problem:</p><p>@BULLET For each trait, the genotype matrix is different: the problem can be formalized as a multitask learning problem. @BULLET For each trait, the genotype matrix is the same: the problem can be formalized as a multiple output regression problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multitask learning</head><p>Many algorithms have been proposed (<ref type="bibr" target="#b0">Abernethy et al., 2006</ref><ref type="bibr" target="#b1">Abernethy et al., , 2009</ref><ref type="bibr" target="#b2">Agarwal et al., 2010;</ref><ref type="bibr" target="#b3">Argyriou et al., 2007;</ref><ref type="bibr" target="#b6">Chen et al., 2012;</ref><ref type="bibr" target="#b9">Evgeniou and Pontil, 2004;</ref><ref type="bibr" target="#b21">Liu et al., 2009;</ref><ref type="bibr" target="#b32">Zhou et al., 2011</ref>) for the multitask learning problem. Here we mainly focused on four algorithms: Cluster-based MTL (CMTL) (<ref type="bibr" target="#b32">Zhou et al., 2011</ref>), ' 1-norm regularized MTL, ' 2;1-norm regularized MTL (<ref type="bibr" target="#b21">Liu et al., 2009</ref>) and Trace-norm Regularized MTL (<ref type="bibr" target="#b1">Abernethy et al., 2009</ref>). Lasso (<ref type="bibr" target="#b29">Tibshirani, 1996</ref>) is a well-known method that uses the ' 1-norm (or Lasso) regularizer to reduce model complexity and learn features. It can be easily extended for single task learning to multitask learning. The objective function for ' 1-norm regularized MTL is based on least square lasso:</p><formula>min W X t i¼1 jjW T i X i À Y i jj 2 F þ q 1 jjWjj 1 þ q L2 jjWjj 2 F (6)</formula><p>where X i denotes the input matrix of the ith task, Y i denotes the ith trait, W i is the coefficient matrix for task i, the regularization parameter q 1 controls sparsity and the optional q L2 regularization parameter controls the ' 2-norm penalty. Note that both ' 1-norm and ' 2-norm penalties are used in Elastic Net. Besides a simple ' 1-norm regularizer, we could constrain all coefficient matrices to share a common set of features. This motivates the group sparsity, i.e. the ' 1 =' 2-norm, or ' 2;1-norm, regularized learning (<ref type="bibr" target="#b21">Liu et al., 2009</ref>). The objective function for ' 2;1-norm regularized MTL is also based on least square lasso:</p><formula>min W X t i¼1 jjW T i X i À Y i jj 2 F þ q 1 jjWjj 2;1 þ q L2 jjWjj 2 F (7)</formula><p>where X i denotes the input matrix of the ith task, Y i denotes the ith trait, W i is the coefficient matrix for task i, the regularization parameter q 1 controls sparsity and the optional q L2 regularization parameter controls the ' 2-norm penalty. Notice the difference of the objective functions in Equations (6) (jjWjj 1 ) and</p><p>(7) (jjWjj 2;1 ). We could also constrain the coefficient matrices from different tasks to share a low-dimensional subspace, i.e. W is of low rank. By replacing the rank of W with trace norm jjWjj Ã ¼ P i d i ðWÞ, the objective function becomes:</p><formula>min W LðWÞ þ cjjPjj 1 subject to : W ¼ P þ Q; jjQjj Ã &lt; ¼ s (8)</formula><p>where the task coefficient matrices W is decomposed into two components: a sparse part P and a low-rank part Q. The advantage of CMTL is that prior knowledge on the cluster structure of the traits can be embedded into the objective function. For our multitrait problem setting, we always know what are the traits and what they measured. Thus it is very often we know which traits are more correlated with each other. For example, fruit weight and fruit size are known to be highly correlated. These more correlated traits should be in one cluster. By leveraging such cluster information, we could improve the multitrait prediction algorithm. The objective function for CMTL is based on the spectral relaxed kmeans clustering: min W;F:F T F¼I k LðWÞ þ aðtrðW T WÞ À trðF T W T WFÞÞ þ btrðW T WÞ</p><formula>(9)</formula><p>where k is the number of clusters and F captures the relaxed cluster assignment information. As the above objective function is not convex, a convex relaxation cCMTL is also proposed as below:</p><formula>min W LðWÞ þ q 1 gð1 þ gÞtrðWðgI þ MÞ À1 W T Þ subject to : trðMÞ ¼ k; M I; M 2 S t þ ; g ¼ q 2 q 1 (10)</formula><p>Accelerated Projected Gradient (<ref type="bibr" target="#b32">Zhou et al., 2011</ref>) is applied to optimize the above objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multiple output regression</head><p>Given a set of training data consisting of N samples, each sample is associated with a genotype matrix X of D-dimension and a trait matrix Y of C-dimension, the multioutput regression model is shown below:</p><formula>Y ¼ XB þ E (11)</formula><p>where B ¼ ½B 1 ;. .. ; B c  is a D Â C regression coefficient matrix, each element B j is the vector of the regression coefficient for the jth trait. E ¼ ½ 1 ;. .. ; N  T is an N Â C matrix, where i ¼ ð i1 ;. .. ; iC Þ 2 R C denotes the residual errors on each trait prediction introduced by the ith sample. Multiple output regression has been widely used in a variety of domains such as stock prices prediction, pollution prediction, etc. It was first noticed by<ref type="bibr" target="#b4">Breiman (2000)</ref>and Friedman that through utilizing correlations between outputs the regression accuracy can be improved. In general there are two types of correlations: the task Novel applications of multitask learning i39 correlation and the noise correlation. Most of the work focus on modeling only one type of correlation, either task correlation or noise correlation. Some recent works (<ref type="bibr" target="#b5">Cai et al., 2014;</ref><ref type="bibr" target="#b24">Rai et al., 2012</ref>where j:j denotes the determinant of a matrix. The inverse covariance matrix X À1 couples the correlated noise across tags and similarly, R À1 obtained relationships among the multiple tasks' regression coefficients. Apparently, both X À1 and R À1 are learnt from the training data rather than pre-defined prior knowledge. The last two terms trðX À1 Þ and trðR À1 Þ are the regularizers, which impose the matrix variate Gaussian priors on both X À1=2 and R À1=2 to solve the overfitting issue. The objective function in Equation (12) of the multiple output regression model is not jointly convex in all variables but individually convex in each variable while others are fixed. Therefore, in order to optimize the objective function, an iterative algorithm is applied: (i) Fix X À1 and R À1 and estimate B, (ii) Fix X À1 and B and estimate R À1 and (iii) Fix R À1 and B and estimate X À1. The process iterates and stops when the value of the objective function does not change or when the number of iterations exceeds a pre-defined threshold. As the multiple output regression model is convex in each variable while others are fixed, when we optimize a variable, we can take the derivative of the variable to estimate its optimal value. To estimate B with fixed X À1 and R À1 , we set the derivative of B over the objective function as 0 and we obtain: 2X T XBX À1 þ 2k 1 B þ 2k 2 BR À1 ¼ 2X T YX À1</p><p>) X T XB þ k 1 BX þ k 2 BR À1 X ¼ X T Y</p><p>Both works (<ref type="bibr" target="#b5">Cai et al., 2014;</ref><ref type="bibr" target="#b24">Rai et al., 2012</ref>) applied the above algorithm. However, when optimizing B, the work (<ref type="bibr" target="#b24">Rai et al., 2012</ref>) applies Kronecker product which generates a DC Â DC matrix whose complexity might be high for large D and C. Then work<ref type="bibr" target="#b5">Cai et al. (2014)</ref>MOR improved the complexity by applying Cholesky factorization and singular value decomposition and they showed that the efficiency of the optimization process can be significantly improved. Please refer to<ref type="bibr" target="#b24">Rai et al. (2012) and</ref><ref type="bibr" target="#b5">Cai et al. (2014)</ref>for the details of the two approaches. As k 1 X þ k 2 R À1 X is systemic and positive-definite, the Cholesky factorization is performed on it to produce lower triangular matrix P:</p><formula>k 1 X þ k 2 R À1 X ¼ PP T By setting X ¼ U 1 R 1 V T 1 and P ¼ U 2 R 2 V T 2</formula><p>be the SVD of X and P, respectively, we obtain the following:</p><formula>V 1 R 1 U T 1 U 1 R 1 V T 1 B þ BU 2 R 2 V T 2 V 2 R 2 U T 2 ¼ X T Y</formula><p>By setting ~ B ¼ V T 1 BU 2 and S ¼ V T 1 X T YU 2 , we could obtain B as:</p><formula>B ¼ V 1 ~ BU T 2</formula><p>When optimize R À1 with fixed X À1 and B, we set the derivative of R À1 over the objective function as 0 and we obtain:where I C is an C Â C identity matrix and M À1 denotes the inverse matrix of the matrix M. The k 1 ; k 2 ; k 3 are selected by cross-validation. In<ref type="bibr" target="#b5">Cai et al. (2014)</ref>, dimensionality reduction is applied on both feature space and target (trait) space. Feature space is the space for all the features, which are the genotypes in our setting. Target space is the space for all the target variables, which are the multiple genetic traits in our setting. On feature space, PCA is applied to reduce the dimensionality. On target space, a regularizer is applied to reduce the dimensionality. In this work, we did not conduct dimensionality reduction as in the dataset we studied, the number of features (in thousands) and the number of traits (eight) are not very big. Notice the MOR method without the task correlations and noise correlations can be reduced to a standard ridge regression. We observed that a direct application of the MOR method usually leads to poor accuracy, as the predicted values are usually far off the true values. In order to address this issue, we centered the input data matrix X as XÀMeanðXÞ StdðXÞ , where Mean(X) computes the column-wise mean of X, Std(X) computes the column-wise standard deviation of X. We call it Centered MOR. It turns out that the centering strategy significantly improved the least square error of MOR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simulated data</head><p>We first simulate the data using the Equation (11). Recall that in this equation, B ¼ ½B 1 ;. .. ; B c  is a D Â C regression coefficient matrix, each element B j is the vector of the regression coefficient for the j-th trait. In order to add task correlation among all the B j 's, we sample B j 's from a standard normal distribution. Similarly, to add noise correlation, we sample the residual errors E from a standard normal distribution. The genotype matrix X is randomly sampled from the values<ref type="bibr">[0,</ref><ref type="bibr">1,</ref><ref type="bibr">2]</ref>. The traits Y are then computed as Y ¼ XB þ E. We simulated four traits for 200 samples, each with 2000 markers. We repeat all the experiments 10 times and computed the average performance. To evaluate the performance of the prediction methods, we used r 2 (r-square, the square of the person's correlation coefficient between the predicted trait values and the real trait values, a popular metric for genomic selection. For genomic selection, the r 2 is almost consistent with least square error). For r 2 , the larger the better. Notice we do not compare multitask learning and multiple output regression methods directly as they will be used in different scenarios. Multitask learning is used when we have unique set of samples for different traits. Multiple output regression is used when we have the same set of samples for all the traits.trait, we train ridge regression only on one subset of 50 samples. For CMTL, as we do not have specific clusters, we just randomly group two of the traits in one cluster and the other two in another cluster. We show the results in<ref type="figure" target="#fig_4">Figure 3</ref>. We can see that the four multitask learning algorithms in general achieved much better results than that of the single trait ridge regression. This is reasonable as the single trait ridge regression only uses 50 samples for the prediction. The CMTL does not have any advantage over other methods as the traits are indeed not in clusters. The trace-norm MTL achieved the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Multitask learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Multiple output regression</head><p>Here we conducted 10-fold cross validation for each trait and we compare the performance of the the single trait prediction method: single trait ridge regression, the multiple output regression methods: the centered MOR method and the state-of-the-art multitrait prediction methods: the multitrait BayesA algorithm, the Bayesian multivariate antedependence model. We did not show the performance of MOR here as it in general has poor performance. The results are shown in<ref type="figure">Figure 4</ref>. We can see that the multitrait algorithms have better performance than the single trait ridge regression does. The Bayesian multivariate antedependence model does not outperform BayesA in that we do not insert LD in our dataset. The centered MOR has the best performance. We also see that the multitrait prediction methods and multiple output regression methods made improvements on all four traits over the single trait prediction method. This is because all the four traits are correlated as they are sampled from the same standard normal distribution. As we will show later in the experiments on real data, when the traits are not correlated, multitrait prediction does not make obvious improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Real data</head><p>Next we evaluate the performance of the multitrait prediction methods on a real plant dataset, the avocado dataset, which contains 8 traits, 160 samples and 2663 markers. The eight traits are: fruit weight, seed weight, fruit length, fruit width, fruit diameter, number of fruit (log), mesocarp weight and water loss percentage. From the name of the traits, we know which traits are more correlated with each other. We show the heat map of the correlation of these traits in<ref type="figure" target="#fig_5">Figure 5</ref>. Notice in the Figure there are two more traits 'seed width' and 'seed length'. They are not included in the experiments. From the heat map, we can see that the first six traits are more correlated with each other and the last two traits are less correlated with the remaining traits.<ref type="figure">Fig. 4</ref>. The r 2 of the centered MOR method, the multitrait BayesA algorithm, the Bayesian multivariate antedependence model versus the single trait ridge regression algorithm on the simulated dataNovel applications of multitask learning i41</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Multitask learning</head><p>We randomly divide the genotype matrix into eight subsets, each with 20 samples. Notice the eight genotype matrices share the same set of markers. For each subset, we keep only one trait for the corresponding samples in the subset. Therefore, we ended up with eight datasets, each with a single trait. For single trait prediction, to predict the j-trait, we first train a predictive model on the jth dataset. Then we take all the other datasets as input and apply the predictive model on the other datasets to predict the jth trait for them. The predictive model we used here is ridge regression. For the multitrait prediction, we used four algorithms: Cluster-based MTL (CMTL), L1-norm regularized MTL, L2,1-norm regularized MTL and Trace-norm Regularized MTL. For CMTL, we applied our prior knowledge on the structure of the clusters, namely the traits fruit weight, fruit length, fruit width, fruit diameter are highly correlated and they should be in one cluster. We compare the performance of the four multitask learning algorithms with the single trait prediction. We show the results in<ref type="figure" target="#fig_6">Figure 6</ref>. We can see the obviously the multitrait prediction significantly outperforms the single trait prediction, as the single traitprediction used only one-eighth of the complete data to predict each trait. We also observed that CMTL and the TraceNormMTL achieved better results than the other two MTL methods, as they conducted more complicated strategies rather than simple regularization. The TraceNormMTL achieved slightly better results than CMTL, indicating that reducing the original problem into a lower dimensional subspace is indeed an effective strategy when the dimensionality of the original problem is high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Multiple output regression</head><p>For the multiple output regression methods, as there is only one dataset, we conducted 10-fold cross validation. We evaluated the performance of the single trait prediction method: single trait ridge regression, the multiple output regression methods: the centered MOR method and the state-of-the-art multitrait prediction methods: the multitrait BayesA algorithm, the Bayesian multivariate antedependence model. Again we do not include MOR here as it has poor performance. Notice we do not include the multitrait GBLUP algorithm here as the two multitrait prediction methods have been shown to have superior performance over the multitrait GBLUP algorithm. The performance is again evaluated by the two metrics: r 2 and the least square error. As we can see in<ref type="figure" target="#fig_7">Figure 7</ref>, both the multitrait and multiple output regression methods outperform single trait ridge regression. The centered MOR method achieved better performance than the ridge regression does, indicating that the centering strategy is critical for the genetic trait prediction problem. The centered MOR method also shows competitive performance compared with the multitrait prediction methods on most of the traits. Also we can observe that the improvement are mainly made on the first six traits, which are highly correlated with each other. For the last two traits, the multitrait prediction does not show advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and future work</head><p>In this work, we studied the multitrait prediction problem where the multiple quantitative trait values of a set of samples are predicted from their corresponding genotypes. We modeled the problem from a machine learning perspective. We considered the problem as either a multitask learning problem or a multiple output regression problem. By adapting the state-of-the-art machine learning algorithms, we showed that the prediction accuracy can be improved by modeling all the traits together and we also showed that the machine learning methods are indeed very competitive with the existing statistical methods. We also observed that the MOR method without the task correlations and noise correlations can be reduced into a standard ridge regression. From our previous study on single genetic trait prediction (<ref type="bibr" target="#b11">Haws et al., 2015</ref>), we observed that rrBLUP (the unbiased version of ridge regression) achieves better performance than ridge regression does. In our future work, we would like to extend the MOR method to take the form of rrBLUP rather than ridge regression, which might improve its performance. Conflict of Interest: none declared.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>(2001) assumes that the b coefficients are iid from a normal distribution such that b i $ Nð0; r 2 b Þ. Then the choice of k ¼ r 2 e =r 2 b where r 2 e is the residual error. In this case, the ridge regression penalized estimator is equivalent to best linear unbiased predictor (BLUP) (Ruppert et al., 2003). Many methods for multitrait prediction where all the traits are modeled together have also been proposed, such as multitrait GBLUP (Clark and van der Werf, 2013), Multitrait BayesA (Jia and Jannink</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.1.</head><figDesc>Fig. 1. An example of multitasking learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.2.</head><figDesc>Fig. 2. An example of multiple output regression</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.3.</head><figDesc>Fig. 3. The r 2 of the four multitask learning algorithms versus the single trait ridge regression algorithm on the simulated data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.5.</head><figDesc>Fig. 5. The heat map of the correlation of the eight traits</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig.6.</head><figDesc>Fig. 6. The r 2 of the four multitask learning algorithms versus the single trait ridge regression algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig.7.</head><figDesc>Fig. 7. The r 2 of the centered MOR method, the multitrait BayesA algorithm, the Bayesian multivariate antedependence model versus the single trait ridge regression algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>, i37–i43 doi: 10.1093/bioinformatics/btw249 ISMB 2016 of the traits are correlated, such as weight and size. Leveraging such correlations in the predictive model might improve the prediction accuracy. Therefore, we call the problem of modeling multiple traits at once as multitrait prediction problem. Lots of work have been proposed for the multitrait prediction problem from genotype data, such as multitrait GBLUP (Clark and van der Werf, 2013), Multitrait BayesA (</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>k 2 B T B À DR þ k 4 I C ¼ 0 ) R À1 ¼ ð k 2 B T B þ k 4 I C D Þ When optimize X À1 with fixed R À1 and B, we set the derivative of R À1 over the objective function as 0 and we obtain: ðY À XBÞ T ðY À XBÞ À NX þ k 3 I C ¼ 0</figDesc><table>) X À1 ¼ ð 
ðY À XBÞ T ðY À XBÞ þ k 3 I C 
N 
Þ À1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>We randomly split the 200 samples into 4 subsets, each with 50 samples and one corresponding trait. All of these subsets share the</figDesc><table>i40 
D.He et al. same 2000 set of markers, with their corresponding genotype values. 
We tested the performance of the four multitask learning algorithms 
[Cluster-based MTL (CMTL), L1-norm regularized MTL, L2,1-
norm regularized MTL, Trace-norm Regularized MTL] against the 
single trait ridge regression algorithm. Notice for the single trait al-
gorithm, for each </table></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<monogr>
		<title level="m" type="main">Low-rank matrix factorization with attributes. arXiv Preprint Cs</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Abernethy</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">A new approach to collaborative filtering: operator estimation with spectral regularization</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Abernethy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="803" to="826" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning multiple tasks using manifold regularization</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Agarwal</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="46" to="54" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">A spectral regularization framework for multi-task structure learning</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Argyriou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="25" to="32" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Randomizing outputs to increase prediction accuracy</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Breiman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="229" to="242" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-output regression with tag correlation analysis for effective image tagging</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Cai</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="volume">8422</biblScope>
			<biblScope unit="page" from="31" to="46" />
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning incoherent sparse and low-rank patterns from multiple tasks</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Chen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowl. Discov. Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Genomic best linear unbiased prediction (gblup) for the estimation of genomic breeding values</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">A</forename>
				<surname>Clark</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Van Der Werf</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Genome-Wide Association Studies and Genomic Prediction</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="321" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">A common dataset for genomic analysis of livestock populations</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Cleveland</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">G3: Genes— Genomes— Genetics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="429" to="435" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Regularized multi–task learning</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Evgeniou</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Pontil</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<monogr>
		<title level="m" type="main">ASReml user guide release 3.0. VSN International Ltd</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">R</forename>
				<surname>Gilmour</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Hemel Hempstead, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Variable-selection emerges on top in empirical comparison of whole-genome complex-trait prediction methods</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">C</forename>
				<surname>Haws</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">138903</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Genomic selection in dairy cattle: Progress and challenges</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">J</forename>
				<surname>Hayes</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Dairy Sci</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="433" to="443" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Genomic selection for crop improvement</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">L</forename>
				<surname>Heffner</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Crop Sci</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Ridge regression: biased estimation for nonorthogonal problems</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">E</forename>
				<surname>Hoerl</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">W</forename>
				<surname>Kennard</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Genomic selection in plant breeding: from theory to practice</title>
		<author>
			<persName>
				<forename type="first">J.-L</forename>
				<surname>Jannink</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brief. Funct. Genomics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="166" to="177" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiple-trait genomic selection methods increase genetic value prediction accuracy</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Jia</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J.-L</forename>
				<surname>Jannink</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetics</title>
		<imprint>
			<biblScope unit="volume">192</biblScope>
			<biblScope unit="page" from="1513" to="1522" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint prediction of multiple quantitative traits using a bayesian multivariate antedependence model</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Jiang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Heredity</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="29" to="36" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Genomic prediction of simulated multibreed and purebred performance using observed fifty thousand single nucleotide polymorphism genotypes</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Kizilkaya</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Anim. Sci</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="544" to="551" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficiency of marker-assisted selection in the improvement of quantitative traits</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Lande</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Thompson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetics</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="743" to="756" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved lasso for genomic selection</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Legarra</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genet. Res</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page">77</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-task feature learning via efficient l 2, 1-norm minimization</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Liu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence</title>
		<meeting>the twenty-fifth conference on uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="339" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Prediction of total genetic value using genome-wide dense marker maps</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">H E</forename>
				<surname>Meuwissen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetics</title>
		<imprint>
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="page" from="1819" to="1829" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">The bayesian lasso</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Park</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Casella</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="681" to="686" />
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Simultaneously leveraging output and task structures for multiple-output regression</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Rai</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="3185" to="3193" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Maximizing the reliability of genomic selection by optimizing the calibration set of reference individuals: Comparison of methods in two diverse groups of maize inbreds</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Rincent</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetics</title>
		<imprint>
			<biblScope unit="volume">192</biblScope>
			<biblScope unit="page" from="715" to="728" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Semiparametric Regression</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Ruppert</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cambridge Series in Statistical and Probabilistic Mathematics</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Atomic decomposition by basis pursuit</title>
		<author>
			<persName>
				<forename type="first">Shaobing</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="33" to="61" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Marker-assisted selection using ridge regression</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">C</forename>
				<surname>Whittaker</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genet. Res</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="249" to="252" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Marker-assisted selection in plant breeding: from publications to practice</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Xu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">H</forename>
				<surname>Crouch</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Crop Sci</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Clustered multi-task learning via alternating structure optimization</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Zhou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="702" to="710" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<monogr>
		<title level="m" type="main">Novel applications of multitask learning i43</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>