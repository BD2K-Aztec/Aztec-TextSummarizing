
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Genome analysis Crowdsourcing for bioinformatics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013">2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Benjamin</forename>
								<forename type="middle">M</forename>
								<surname>Good</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Molecular and Experimental Medicine</orgName>
								<orgName type="institution">The Scripps Research Institute</orgName>
								<address>
									<addrLine>La Jolla</addrLine>
									<postCode>92037</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Andrew</forename>
								<forename type="middle">I</forename>
								<surname>Su</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Molecular and Experimental Medicine</orgName>
								<orgName type="institution">The Scripps Research Institute</orgName>
								<address>
									<addrLine>La Jolla</addrLine>
									<postCode>92037</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Genome analysis Crowdsourcing for bioinformatics</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">29</biblScope>
							<biblScope unit="issue">16</biblScope>
							<biblScope unit="page" from="1925" to="1933"/>
							<date type="published" when="2013">2013</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btt333</idno>
					<note type="submission">Received on February 15, 2013; revised on April 30, 2013; accepted on June 5, 2013</note>
					<note>Associate Editor: Jonathan Wren</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Bioinformatics is faced with a variety of problems that require human involvement. Tasks like genome annotation, image analysis, knowledge-base population and protein structure determination all benefit from human input. In some cases, people are needed in vast quantities, whereas in others, we need just a few with rare abilities. Crowdsourcing encompasses an emerging collection of approaches for harnessing such distributed human intelligence. Recently, the bioinformatics community has begun to apply crowd-sourcing in a variety of contexts, yet few resources are available that describe how these human-powered systems work and how to use them effectively in scientific domains. Results: Here, we provide a framework for understanding and applying several different types of crowdsourcing. The framework considers two broad classes: systems for solving large-volume &apos;microtasks&apos; and systems for solving high-difficulty &apos;megatasks&apos;. Within these classes, we discuss system types, including volunteer labor, games with a purpose, microtask markets and open innovation contests. We illustrate each system type with successful examples in bioinformatics and conclude with a guide for matching problems to crowdsourcing solutions that highlights the positives and negatives of different approaches.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Imagine having easy, inexpensive access to a willing team of millions of intelligent workers. What could you accomplish? Lakhani and colleagues produced 30 new sequence alignment algorithms that each improved on the state-of-the-art, in 2 weeks, for $6000 (<ref type="bibr" target="#b28">Lakhani et al., 2013</ref>). Others improved a 44-species multiple alignment (<ref type="bibr" target="#b20">Kawrykow et al., 2012</ref>), developed a new protein folding algorithm (<ref type="bibr" target="#b21">Khatib et al., 2011a</ref>), produced accurate parasite counts for tens of thousands of images of infected blood cells (<ref type="bibr" target="#b31">Luengo-Oroz et al., 2012</ref>), and still others are attempting to translate the entire web into every major language (http://duolingo.com). Crowdsourcing systems make these and many other monumental tasks approachable. Here, we explore what these systems are and how they are being applied in bioinformatics. The term 'crowdsourcing' was coined in 2006 to describe 'the act of taking a job traditionally performed by a designated agent</p><p>(usually an employee) and outsourcing it to an undefined, generally large group of people in the form of an open call' (<ref type="bibr" target="#b19">Howe, 2006</ref>). Now, it is used to describe a range of activities that span the gamut from volunteers editing wiki pages or tagging astronomical images to experienced professionals tackling complex algorithm development challenges. Here, we will focus specifically on systems for accomplishing directed work that requires human intelligence. These human-powered systems are built to solve discrete tasks with clear end points. They are distinct from other common, community-driven branches of crowdsourcing, such as wikis, in that they allow for top-down control over the work that is conducted. (For an extensive introduction to wikis in biology, see<ref type="bibr" target="#b14">Galperin and Fernandez-Suarez, 2012</ref>). The tasks discussed here have been historically approached from an artificial intelligence perspective—where algorithms attempt to mimic human abilities (<ref type="bibr" target="#b40">Sabou et al., 2012</ref>). Now, crowdsourcing gives us access to a new methodology: 'artificial artificial intelligence' (https://www.mturk.com/). The objective of this review is to give insights into how, from a practical perspective based on recent successes, to use this new force to tackle difficult problems in biology. We divide crowdsourcing systems into two major groups: those for solving 'microtasks' that are large in number but low in difficulty, and those for solving individually challenging 'megatasks'. In Section 2, we present an overview of microtask solutions with subsections on volunteer systems, casual games, microtask markets, forced labor (workflow sequestration) and education. Section 3 describes crowdsourcing approaches to megatasks with subsections on innovation challenges and hard games. Section 4 concludes the article with a guide for matching problems to potential crowdsourcing solutions, pointers to information about forms of crowdsourcing not covered here and a brief exploration of the potential consequences of crowdsourcing on society.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CROWDSOURCING MICROTASKS</head><p>Microtasks can be solved in a short amount of time (typically a few seconds) by any human who is capable of following a short series of instructions. In bioinformatics, microtasks often orient around image or text annotation. In these cases, crowdsourcing systems provide system designers with access to vast numbers of workers who, working in parallel, can collectively label enormous volumes of data in a short time. These systems achieve high quality, typically as good as or better than expert annotators, through extensive use of redundancy and aggregation. Annotation tasks are presented to multiple workers, and their contributions are integrated, e.g. through voting, to arrive at the final solution. *To whom correspondence should be addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Volunteer (citizen science)</head><p>Perhaps the most surprisingly effective strategy for incentivizing large-scale labor in support of scientific objectives is simply to ask for volunteers. This pattern, often referred to as 'citizen science', dates back at least to the year 1900, when the annual Christmas bird counts were first organized by the National Audubon Society (<ref type="bibr" target="#b7">Cohn, 2008</ref>). Now, it is exemplified by the Zooniverse project and its initial product Galaxy Zoo (<ref type="bibr" target="#b29">Lintott et al., 2008</ref>). Galaxy Zoo has successfully used the web to tap into a willing community of contributors of previously unimaginable scale. Within the first 10 days of its launch in July 2007, the Galaxy Zoo web site had captured 8 million morphological classifications of images of distant galaxies (<ref type="bibr" target="#b6">Clery, 2011</ref>). After 9 months, 4100 000 people had contributed to the classification of 41 million images—with an average of 38 volunteers viewing each image. Now, the Zooniverse project, in collaboration with Cancer Research UK, is moving into the biomedical domain with a project called CellSlider (http://www.cellslider.net). In CellSlider, volunteers are presented with images of stained cell populations from cancer patient biopsies and asked to label the kinds and quantities of different cell types. In particular, volunteers seek out irregularly shaped cells that have been stained yellow based on the level of estrogen receptor expressed by the cell. Quantifying the amount of these 'cancer core' cells in a particular patient can help to ascertain the extent to which a treatment is helping the patient, and thus can be used to help personalize and improve therapy. Launched on October 24, 2012, the initiative has not published its finding yet, but it claimed to have analyzed 550 000 images in its first 3 months of operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Casual games</head><p>Aside from simply relying on the altruistic urges of the audience, a growing number of crowdsourcing initiatives attempt to reward participation with fun. In these 'games with a purpose', microtasks are presented in the context of simple, typically webbased games (<ref type="bibr" target="#b1">Ahn and Dabbish, 2008</ref>). (We distinguish these microtask games from other closely related games designed to solve difficult problems in Section 3.1.) In these 'gamified' crowdsourcing systems, the participants earn points and advance through levels just like other games, but the objectives in each game are closely aligned with its higher-level purpose. To win, game players have to solve real-world problems with high quality and in large quantities. Casual crowdsourcing games have been actively developed by the computer science community since the ESP Game emerged with great success for general-purpose image labeling in 2003 (<ref type="bibr" target="#b0">Ahn and Dabbish, 2004</ref>). The first casual games within the realm of bioinformatics address the tasks of multiple sequence alignment and image annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Multiple sequence alignment Phylo</head><p>is a game in which players help to improve large multiple sequence alignments by completing a series of puzzles representing dubious sections from precomputed alignments (<ref type="bibr" target="#b20">Kawrykow et al., 2012</ref>). To complete a puzzle, players move Tetris-like, color-coded blocks representing nucleotides around until the computed alignment score reaches at least a predetermined level, with more points awarded for better alignments. These human-generated alignment sections are then integrated back into the full computationally generated alignments. In the first 7 months of game-play, Phylo recruited 412 000 players who collectively completed 4254 000 puzzles. When the alignment blocks from game players were reassembled, they resulted in improvements to 470% of the original alignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Image annotation</head><p>Following shortly after Phylo, two research groups independently developed games focused on the classification of images related to malaria infection. Mavandadi and colleagues describe a web-based game called MOLT that challenges players to label red blood cells from thin blood smears as either infected or uninfected (Mavandadi et al., 2012a, b). Luengo-Oroz and colleagues present a game called MalariaSpot for counting malaria parasites in thick blood smears (<ref type="bibr" target="#b31">Luengo-Oroz et al., 2012</ref>). The similar approaches taken by both of these systems reflect consistent themes for microtask platforms; both systems aggregate the responses of multiple players (sometimes 420) to produce the annotation for each image and use images with known annotations to benchmark player performance. Using these techniques, both MOLT and MalariaSpot achieved expert-level performance on their respective tasks. Both systems share a vision of using their crowdsourcing approach to enable the rapid, accurate and inexpensive annotation of medical images from regions without access to local pathologists in a process known as 'tele-pathology'. These systems are also envisioned to play a role in training both human pathologists and automated computer vision algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Microtask markets</head><p>Microtask markets are probably the most well-known and thoroughly used variety of crowdsourcing. Rather than attempting to use fun or altruism as incentives, these systems simply use cash rewards. Where a game like MalariaSpot provides points for each labeled image, a microtask market would allow contributors to earn a small amount of money for each unit of work. Within bioinformatics, microtask markets have so far been used for image and text annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Image annotation</head><p>Although microtask markets have enjoyed widespread use for general image annotation tasks since their inception, there are few published examples of applications in bioinformatics—though many are in progress. Nguyen and colleagues provide a prototypical example (<ref type="bibr" target="#b37">Nguyen et al., 2012</ref>). They describe the application of the Amazon Mechanical Turk (AMT) crowdsourcing service to detect polyps associated with colorectal cancer in images generated through computed– tomographic colonography. Using the AMT, they paid crowd workers to label images of polyp candidates as either true or false. For each task (known as a 'HIT' for 'human intelligence task'), the workers were presented with 11 labeled training images to use to make their judgment on the test image. Workers were paid $0.01 for each image that they labeled. In the first of two replicate trials with nearly identical results, 150 workers collectively completed 5360 tasks resulting in 20 independent assessments of each of 268 polyp candidates. This work was completed in 3.5 days at a total cost of $53.60 (plus some small overhead fees paid to Amazon). A straightforward voting strategy was used to combine the classifications made by multiple workers for each polyp candidate. The classifications generated by this system were then assessed based on agreement with expert classifications and compared with results from a machine learning algorithm. The results of the crowd-powered system and the machine learning system were not significantly different. Both systems produced an area under the receiver operating characteristic curve close to 0.85. Although this system did not improve on the automated system, it demonstrated that minimally trained AMT workers could perform this expert-level task rapidly and with high quality. In subsequent work, the same research group reported significant improvements with a new system that integrated the automated predictions with those derived from crowdsourcing to produce an area under the receiver operating characteristic curve of 0.91 on the same data (<ref type="bibr" target="#b46">Wang et al., 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Text annotation</head><p>With much of the world's biological and medical knowledge represented in text, natural language processing (NLP) is a core component of research in bioinformatics. Many tasks in NLP require extensive amounts of expensive linguistic annotation. For example, NLP systems that detect concepts and relationships often need large corpuses of semantically tagged text for training (<ref type="bibr" target="#b23">Kim et al., 2003</ref>). In seeking a faster, less-expensive method for acquiring these data, the NLP community was among the first to explore the use of crowdsourcing for research purposes (<ref type="bibr" target="#b40">Sabou et al., 2012</ref>). Early work by Snow and colleagues demonstrated that expert-level text annotations could be collected 'cheap and fast' using the AMT platform and also provided a pattern for correcting biases common to crowdsourcing systems (<ref type="bibr" target="#b42">Snow et al., 2008</ref>). Although this and related work has achieved good results with common language tasks, biomedical text (with its more challenging vocabulary) is just beginning to be approached through crowdsourcing. Yetisgen-Yildiz and colleagues demonstrated that AMT workers could produce effective annotations of medical conditions, medications and laboratory tests within the text of clinical trial descriptions (<ref type="bibr" target="#b48">Yetisgen-Yildiz et al., 2010</ref>). Burger and colleagues also used the AMT to validate predicted gene mutation relations in MEDLINE abstracts (<ref type="bibr" target="#b5">Burger et al., 2012</ref>). They found that the workers (paid $0.07/task) were easily recruited, responded quickly and (as is typical of all crowdsourcing systems) displayed a wide range of abilities and response rates with the best-scoring worker producing an accuracy of 90.5% with respect to a gold standard on41000 HITs. Using majority voting to aggregate the responses from each worker, they achieved an overall accuracy of 83.8% across all 1733 candidate gene mutation relationships presented for verification. Finally, Zhai and colleagues recently showed that crowdsourcing could be used for detailed processing of the text from clinical trial announcements including the following: annotating named entities, validating annotations from other workers and identifying linked attributes, such as side effects of medications (<ref type="bibr" target="#b49">Zhai et al., 2012</ref>).projects have incorporated the annotation of new sequences directly into the curriculum of undergraduate courses (<ref type="bibr" target="#b18">Hingamp et al., 2008</ref>). Using standard crowdsourcing mechanisms, redundancy and aggregation, as well as review by expert curators, these initiatives have generated thousands of high-quality annotations (<ref type="bibr" target="#b4">Brister et al., 2012</ref>). From both a social and an economic perspective, this approach has the elegant property of simultaneously accomplishing the desired work and generating the capital needed to pay the workers. In this case, the capital is the knowledge that they are acquiring by interacting with the system. In contrast to other approaches such as the forced labor of ReCAPTCHA, which may be considered a nuisance or even an exploitation, offering education on a topic of interest appears to be a much more fair exchange. The startup company DuoLingo (founded by the creator of ReCAPTCHA) now uses this pattern on a massive scale by helping millions of students learn foreign languages while simultaneously harvesting their efforts to translate web documents (http://duolingo.com).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CROWDSOURCING MEGATASKS</head><p>In addition to rapidly completing large volumes of simple tasks, different incarnations of the crowdsourcing paradigm can be applied to solve individual tasks that might take weeks or even months of expert-level effort to complete. In these cases, the goal is to use crowdsourcing to seek out and enable the few talented individuals from a large candidate population that might, through the heterogeneous skills and perspectives that they provide, be able to solve problems that continue to stymie traditional research organizations. This shift from high-volume tasks to high-difficulty tasks affords different requirements for successful crowdsourcing. Two approaches that have generated impressive successes in bioinformatics are hard games and innovation contests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hard games</head><p>In contrast to casual games like MalariaSpot that are designed to complete large volumes of microtasks, the games discussed here provide players with access to small numbers of extremely challenging individual problems. Although casual games tend toward what the gaming community describes as 'grinding', where the players perform highly repetitive actions, hard games provide rich interactive environments that promote long-term exploration and engagement with a challenge. Thus far, two such games have been successful in bioinformatics, Foldit and EteRNA. In Foldit, the goal of most games (or puzzles) is typically to find the 3D conformation of a given protein structure that results in the lowest calculated free energy (<ref type="bibr" target="#b9">Cooper et al., 2010</ref>). To achieve this goal, players interact with a rich desktop game environment that builds on the Rosetta structure prediction tool suite (<ref type="bibr" target="#b39">Rohl et al., 2004</ref>). In contrast to casual games in which players can play (and contribute solutions) within minutes, Foldit players must first advance through an extensive series of training levels that can take several hours to complete. These introductory levels systematically introduce increasingly complex game features that allow players to manipulate protein structures via both direct manipulation (dragging and twisting pieces of the protein) and through the execution of small optimization algorithms like 'wiggle'. Importantly, these training levels abstract the complex mechanics of protein folding into concepts that are accessible to lay game players. Since its inception in 2008, Foldit has captured the attention of hundreds of thousands of players, some of whom have achieved remarkable scientific successes. Foldit players have outperformed some of the world's best automated structure prediction systems and aided in the solution of an important retroviral structure that had eluded specialists for decades (<ref type="bibr" target="#b22">Khatib et al., 2011b</ref>). In addition to solving naturally occurring protein structures, players have recently succeeded in optimizing the design of engineered enzymes to achieve specific physicochemical goals (<ref type="bibr" target="#b12">Eiben et al., 2012</ref>). Although these individual successes are impressive, the greater challenge remains to devise algorithms that fold proteins automatically. In addition to the visually oriented puzzle interface, Foldit introduced a scripting system that allows players to compose automated workflows. These scripts string together multiple optimization widgets and may be used in combination with direct manipulation. In one of the most intriguing developments from this initiative, Foldit players used the provided scripting interface to collaboratively write folding algorithms that rival professionally designed solutions (<ref type="bibr" target="#b21">Khatib et al., 2011a</ref>). Following directly from Foldit's success, some of Foldit's creators have released a new game called EteRNA (http://eterna. cmu.edu). In EteRNA, the goal is to design an RNA molecule that will fold into a particular predefined shape. Design contests are run every week, and the best designs are evaluated in the laboratory providing real-world feedback. This connection between the gamer community and the scientists behind the game has proven effective in recruiting tens of thousands of players— including a few star players that are not only producing valuable new designs but are also identifying new rules of RNA behavior (<ref type="bibr" target="#b27">Koerner, 2012</ref>). Although much is made of the numbers of players to access these games, it is important to realize that only a small fraction of these players contribute directly to any important advance. These games are portals for recruiting, engaging and enabling a small number of people with exceptional skills who would never normally have the opportunity to help solve these problems. In essence, these games are as much about discovering latent scientists as they are about making scientific discoveries (<ref type="bibr" target="#b15">Good and Su, 2011</ref>). Most of the players are not active scientists by trade and typically have little to no formal training. Although most do not contribute directly to solutions, a few bring a different perspective that opens up an entirely new way of looking at and solving the problem. Such a diversity of human intelligence, if filtered and aggregated effectively, is a powerful and much sought-after force.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Open innovation contests</head><p>Open innovation contests define particular challenges and invite anyone in the general public to submit candidate solutions. The solutions are evaluated and if they meet the defined criteria, the best solutions are rewarded with cash prizes. The prizes and the social prestige garnered by winning a large public contest provide the key incentives driving participation in these initiatives. First pioneered by Innocentive, a 2001 spinoff of Eli Lilly meant to improve its research pipeline, a variety of platforms for operating these contests have recently emerged. Within bioinformatics, key open innovation platforms include Innocentive (which is used on a wide variety of tasks), TopCoder (for software development and algorithm design) and Kaggle (for data analysis). As with games, contests make it possible to let enormous numbers of potential 'solvers' try out their unique abilities on the specified problem. In contrast to games, which require extensive, costly development time before any possible reward from the community might be attained, the up-front cost of running an innovation contest is comparatively small. If no one solves the posted problem, little is lost by the problem poster. Further, financial incentives are far easier to tune than game mechanics. The harder and more important the problem is, the bigger the offered bounty for its solution. Common prizes range from a few thousand dollars for small coding challenges that can be accomplished by individuals in their spare time to million-dollar contests that can require large project teams and/or long-term time commitments. Many successes in bioinformatics have already been attained at the lower end of the prize spectrum. As an example, Lakhani and colleagues recently assessed the potential of the TopCoder platform on a difficult sequence alignment problem (<ref type="bibr" target="#b28">Lakhani et al., 2013</ref>). To test the hypothesis that 'big data biology is amenable to prize-based contests', they posted a challenge related to immune repertoire profiling on TopCoder with a prize pool of just $6000. In the 2 weeks that the contest was run, 733 people participated and 122 submitted candidate solutions. In comparison with one prior 'industry standard' (NCBI's MegaBlast), 30 of the submitted solutions produced more accurate alignments and all ran substantially faster. None of the participants in the competition was a professional computational biologist, with most describing themselves as software developers. In addition to this academic study, industry representatives report extensive use of these small-scale coding competitions as part of their bioinformatics research and development pipelines (<ref type="bibr" target="#b36">Merriman et al., 2012</ref>). At the upper end of the prize spectrum, one of the first successful million-dollar contests led to the discovery of a novel biomarker for amyotrophic lateral sclerosis (<ref type="bibr" target="#b44">Talan, 2011</ref>). Currently, groups such as Life Technologies and the U.S. Government's Defense Threat Reduction Agency (DTRA) are running million-dollar contests for development of novel sequencing technologies and organism detection from complex mixtures of DNA sequence, respectively. These examples highlight the potential of open innovation contests to focus the attention of large numbers of talented people on solving particular challenging problems. These systems offer solution seekers with an approach that can be highly cost effective in recruiting such talent. As an example, Lakhani and colleagues estimate that contest participants spent 2684 hours working on their problem. Given a 2-week time period and a total cost of $6000, this is a remarkable amount of skilled labor and an incredibly short amount of time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Choosing a crowdsourcing approach</head><p>Although diverse in their implementations and goals, the crowdsourcing systems described in this review each attempt to advance science by enabling the overwhelming majority of people who reside outside of the ivory tower to participate in the process (<ref type="bibr" target="#b8">Cooper, 2013</ref>). How this process unfolds—how well it solves the problems at hand and how it influences the participants—depends deeply on the nature of each problem and the approach taken by system architects. Although the diversity of potential tasks in bioinformatics renders a global rubric for composing crowdsourcing solutions unlikely, the examples presented in this review and organized in<ref type="figure">Table 1</ref>suggest some general guidelines (<ref type="figure" target="#fig_0">Fig. 1</ref>). Crowdsourcing generally begins where automation fails. Tasks that can be automated generally should be, and workers should be focused on tasks that extend the reach of current computational approaches (<ref type="bibr" target="#b20">Kawrykow et al., 2012</ref>). As such, the first question to answer when deciding how or if crowdsourcing may be useful is 'what tasks (or subtasks) of the larger problem can currently be solved computationally and which cannot'? Once the tasks that require human abilities are defined, use the following (summarized in<ref type="figure" target="#fig_0">Fig. 1</ref>) to identify crowdsourcing systems that may be suitable. Highly granular, repetitive tasks such as image classification can be approached via volunteer initiatives, casual games, workflow sequestration and microtask markets. Games and direct volunteer labor are of most value when the number of required tasks is exceedingly large—too large to pay workers even small amounts per unit of work. The downsides of depending on volunteers or game players are that there is no guarantee that they will generate the required amount of labor, and nearly all of the potentially substantial cost of building the crowdsourcing solution (the game, the web site) must be paid up-front before any possible benefit is attained. Depending on the task, workflow sequestration can be a powerful approach, as it not only effectively forces the required labor but can also be used to target specific populations of workers. The downside is that the alignment of workflows with microtasks will likely not be possible in many cases. Finally, microtask markets have the benefit of offering system designers with an immediate workforce of massive scale and precise control of the nature and volume of their activities. The main negative aspect of microtask markets is that, because of the per-unit cost of the work, they do not have the capacity to scale up in the way that the other forms do. When it comes to megatasks involving extended work and specialized skills, innovation contests and hard games can be considered. Among these, innovation contests are by far the most popular and generalizable framework. These systems have repeatedly produced solutions to difficult problems in a variety of domains at comparatively tiny costs, and we expect their use to continue to expand. Hard games, like Foldit, are fascinating for the potential scale, diversity and collaborative capacity of the gamer/solver population; however, these benefits are not guaranteed and come at a high up-front cost in development time. Furthermore, it simply may not be possible to gamify many important tasks. The tasks most suited to approaches with hard games are those that have scoring functions, such as Foldit's free energy calculation, that can link performance in the game directly to the problem under study. Without such mapping, it will be difficult to provide the players with the feedback they need to learn the problem space and thus become effective solvers. Looking forward, the didactic division used here between systems for completing microtasks and those for solving megatasks will likely be blurred as new integrated systems arise that take advantage of key aspects of multiple forms of crowdsourcing (<ref type="bibr" target="#b3">Bernstein, 2012</ref>). The emergent community-driven processes that gave rise to Wikipedia offer some hints at what such future systems might look like (<ref type="bibr" target="#b24">Kittur and Kraut, 2008</ref>). Such systems will have to promote the rapid formation of extended communities of participants that display a wide variety of skills and proclivities who come together to achieve a common highlevel goal. For the moment, such problem-solving communities remain difficult to generate and to sustain. But, as the science of crowdsourcing advances, it will be increasingly possible for system architects to guide these collective intelligences into existence (<ref type="bibr" target="#b25">Kittur et al., 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Related systems</head><p>Here, we focused only on crowdsourcing approaches that are specifically relevant to common problems in bioinformatics. For broader reviews, see 'Crowdsourcing systems on the world wide web' (<ref type="bibr" target="#b11">Doan et al., 2011</ref>), 'Human computation: a survey and taxonomy of a growing field' (<ref type="bibr" target="#b38">Quinn and Bederson, 2011</ref>) and 'Crowd-powered systems' (<ref type="bibr" target="#b3">Bernstein, 2012</ref>). Within bioinformatics, two other important emerging approaches that depend on the crowd, but not the crowd's intelligence, are distributed computing and online health research. Systems like Rosetta@home and the more-general purpose Berkeley Open Infrastructure for Network Computing (BOINC) use the spare cycles of thousands of personal computers to advance research in bioinformatics, particularly protein folding and docking simulations (<ref type="bibr" target="#b41">Sansom, 2011</ref>). In the medicalthe tree from the top left to identify approaches that may suit your particular challenge. In many cases there might not be a known crowdsourcing approach that is suitable domain, the term crowdsourcing is often used to describe largescale patient data collection through online surveys. Personal genomics companies, such as 23andme, have surveyed their genotyped 'crowd' to enable many new discoveries in genetics (<ref type="bibr" target="#b10">Do et al., 2011;</ref><ref type="bibr" target="#b45">Tung et al., 2011</ref>). In addition, a variety of initiatives have begun exploring the crowdsourcing of both patient-initiated and researcher-initiated (non-clinical) patient trials. Such 'crowdsourced health research' is an important and growing area, but conceptually distinct from the crowdsourcing applications considered here. For a recent survey of the literature on this topic, see Swan (2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Social impact</head><p>While we have focused primarily on the economic aspects of crowdsourcing, kinds of work and cost, there is another aspect that is important to consider. Crowdsourcing is not just a new way of performing difficult computations rapidly and inexpensively; it represents a fundamental change in the way that scientific work is distributed within society. Recalling the original definition, crowdsourcing is a shift from work done in-house to work done in the open by anyone that is able. This means not only that we can often solve more problems more efficiently, but also that different people are solving them. As a result, there are both ethical concerns about worker exploitation that must be addressed and novel opportunities for societal side benefits that are important to explore. Some have expressed concern for the well-being of players of scientific crowdsourcing games (<ref type="bibr" target="#b16">Graber and Graber, 2013</ref>), and it is reasonable to ask about the morality of forcing hundreds of millions of people to solve ReCAPTCHAs to go about their daily work. However, the majority of worry about real exploitation is related to the workers in microtask markets. In some cases, people spend significant amounts of time earning wages that amount to 5$2/hour (<ref type="bibr" target="#b13">Fort et al., 2011</ref>). Although problemfocused, resource-strapped researchers may rejoice at the opportunity to address the new scientific questions that this workforce makes possible, it is both socially responsible and vital for longterm success to remain aware that there are people at the other end of the line completing these tasks. In fact many of the newer crowdsourcing companies, e.g. MobileWorks, now make worker conditions a top priority with guaranteed minimum wages and opportunity for advancement within their framework. Keeping worker satisfaction in mind should not only help encourage fair treatment but will also help designers come up with more effective crowdsourcing solutions. Paying workers well, building up long-term relationships with them and providing tasks that may provide them with benefits aside from any direct per-task reward in fun or money not only makes for a happier workforce but also makes for a far more powerful one (<ref type="bibr" target="#b26">Kochhar et al., 2010</ref>). While much is made of the power of our visual system in the context of crowdsourcing, our ability to learn is what separates us from the rest of the animal kingdom. Tapping into this innate ability and our strong desire to use it will produce crowdsourcing systems that not only solve scientific problems more effectively but, in the process, will end up producing many more scientifically literate citizens. Before crowdsourcing models started to appear, only a small fraction of society had a direct input into the advance of science. Consider protein folding. Foldit changed the number of people thinking about and working on protein-folding problems from perhaps a few thousand to hundreds of thousands. Consider also the new phenomenon of 'crowdfunding' (<ref type="bibr" target="#b47">Wheat et al., 2013</ref>). Now members of the public, not just members of government grant review panels, have a vote in what science is funded. The majority of Foldit players will not directly contribute to an important advance, but some will. Perhaps, more importantly, Foldit players and contributors to the various other crowdsourcing initiatives discussed here are much more cognizant of these scientific problems than they ever were before. If fostered effectively by system architects, a new crowdsourcinggenerated awareness will improve how the general public perceives science and will affect how they vote and how they encourage future generations. Taken together, the different manifestations of the crowdsourcing paradigm open up many new avenues for scientific exploration. From the high-throughput annotation of millions of images, to the one-off introduction of a novel twist on RNA structure design by a librarian, these new systems are expanding scientific problem-solving capacity in unpredictable ways. To take advantage of these new ways of accomplishing work takes both openness and, in some cases, some amount of humility. The scientific community must be willing to share our greatest problems and step aside to let others help us solve them.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. Crowdsourcing decision tree. When considering a crowdsourcing approach, work through the tree from the top left to identify approaches that may suit your particular challenge. In many cases there might not be a known crowdsourcing approach that is suitable</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><figDesc>Table 1. Crowdsourcing systems</figDesc><table>Task class System type 

Conditions where appropriate 

Examples 

Primary explicit 
incentive 

Primary quality 
control 

Tools/platforms 

Micro 

Volunteer 
Tasks of interest to general public, 
high-task volume 

Image classification (e.g. CellSlider, 
Galaxy Zoo) 

None 

R&amp;A 

Bossa, PyBossa 

Micro 

Casual game 
Access to game developers, high-task 
volume 

Multiple sequence alignment, image 
classification (e.g. Phylo, MOLT) 

Fun 

R&amp;A 

None 

Micro 

Microtask 
market 
Access to sufficient funds for required 
volume of work 

Image classification, text annotation 
(e.g. polyp classification for colon 
cancer detection) 

Money 

R&amp;A 
Platforms: Mechanical Turk, 
Clickworker, Microworkers, 
MobileWorks Meta services: 
Crowdflower, Crowdsource 

Tools: Turkit, Crowdforge 

Micro 

Forced labor 
Control over a workflow that your 
target population needs to execute 

Character recognition, linking drugs 

to clinical problems (e.g. 
ReCAPTCHA) 

Completing another 

task of personal 
importance 

R&amp;A 

None 

Micro 

Educational 
Twin goals of education and task 
completion 

Genome annotation, document 
translation (e.g. DuoLingo) 

Education 

R&amp;A 

annotathon.org 

Mega 

Hard game 
Access to game developers, problem 
with solution quality function that 
can be tied to a game score 

Protein folding, RNA structure 
design (e.g. Foldit, EteRNA) 

Fun 

Automatic scoring 
function 

None 

Mega 

Innovation 
contest 
Access to sufficient resources to provide 
desirable reward for solution. 

Algorithm development (e.g. 
DTRA) 

Money 

Manual review by 
contest creators 

Innocentive, TopCoder, 
Kaggle 

Note: 
R&amp;A ¼ 
Redundancy and aggregation. Types of crowdsourcing systems are displayed, from the top down, in roughly increasing order of difficulty per task and decreasing number of individual tasks that the system must 

solve. The easiest and most prolific are the character recognition microtasks of ReCAPTCHA, whereas the most difficult are the innovation contests for megatasks like algorithm development. The table describes the dominant 
characteristics of most systems in each class, but cannot be exhaustive. For example, automatic scoring functions could certainly be applied in some innovation contests, but are not listed explicitly because they are not a 
fundamental requirement—as they are for all of the hard games identified thus far. </table></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">B.M.Good and A.I.Su at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="2">.3.3 Microtask platforms The AMT was the first and remains the leading microtask market, but there are a variety of other platforms emerging (Table 1). In addition, meta-services like Crowdflower help to address standard problems in microtask markets, such as spammer identification, worker rating and response aggregation. From the task-requestor perspective, the meta-services generally offer less control over the operation of the system but solve many common problems effectively. Aside from these services, a small but growing number of open source projects for working with crowdsourcing systems are now available. For example, see Turkit (Little et al., 2010) and CrowdForge (Kittur et al., 2011). 2.4 Forced labor (workflow sequestration) If altruism, fun or money is not sufficient to motivate workers, it is sometimes possible to force them to work for free. This strategy has been used most effectively in the omnipresent ReCAPTCHA (Ahn et al., 2008). ReCAPTCHA is a security system for web sites that requires users to type in two words that they see in a distorted image. One word is known and thus used for verification that the user is a human (not a program), and the other is a scanned image of text that needs to be digitized. Because this task is difficult to accomplish computationally, it provides organizations with a way to defend against automated spammers, thus saving them large amounts of work. At the same time, the decision by web site owners to use the system effectively forces hundreds of millions of web users to work on large-scale optical character recognition tasks for free. ReCAPTCHA uses the incentive to complete a task that is important to the users/workers (logging in to a web site) to motivate them to complete a task that is important to the system designer (digitize books). McCoy and colleagues recently applied this pattern for clinical knowledge base construction (McCoy et al., 2012). In this study, the &apos;crowd&apos; consisted of the physicians in a large medical community; the incentive was to use an electronic health record system to prescribe medications, and the task was to capture links between medications and patient problems. To prescribe a medication, the physicians were required to link it to the associated clinical problem. Using this pattern, 867 clinicians created 239 469 problem-medication links in 1 year, including 41 203 distinct links. To identify problem-medication links with high precision, the authors implemented a filtering system that incorporated both the number of patients for which a pair was asserted (voting) and the baseline probability of each pair (penalizing pairs likely to co-occur by chance). Using a manually defined threshold intended to minimize false-positive findings, this filter yielded 11 166 distinct problem-medication pairs. Compared with expert review of the associated records, these links had a specificity of 99.6% and a sensitivity of 42.8%. The success of this early study, conceptual articles that describe similar patterns (Herna´ndezHerna´ndez-Chan et al., 2012) and the continued increase in adoption of electronic health record systems suggest that this approach will enjoy widespread application in the biomedical domain. Within bioinformatics, this kind of workflow sequestration is, so far, most commonly seen in educational settings as described in the next section. 2.5 Crowdsourcing and education Genome annotation is a crucial activity in bioinformatics and is one that requires extensive human labor. With an ever-increasing supply of genomes to annotate, there is an effectively infinite amount of work to accomplish and, as this work is non-trivial, a need to train large numbers of students to accomplish it. Killing two birds with one stone, a number of annotation</note>

			<note place="foot">Crowdsourcing for bioinformatics at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">A variety of contests exist in the academic sphere, such as the long-running Critical Assessment of Protein Structure Prediction (CASP) for protein structure prediction and the recent series of challenges in systems biology operated by the Dialogue for Reverse Engineering Assessments and Methods (DREAM) initiative (Marbach et al., 2012). For the most part, these contests remain distinct from other innovation contests in that they focus on recruiting submissions specifically from academics, using scientific publications as one form of incentive. 4 DISCUSSION Here, we presented a series of success stories where different forms of crowdsourcing were successfully applied to address key problems in bioinformatics. It is worth noting that crowdsourcing is not a panacea. Although it is difficult to find published examples of crowdsourcing failures in science, clearly not all attempts will succeed. For example, only 57% of Innocentive challenges were successfully solved in 2011 (up from 34% in 2006) (Spradlin 2012), many attempts to draw in volunteer crowds fail (notably among scientific wikis) and attempts to use the Mechanical Turk often face challenges associated with spammers or poorly performing workers. In our own unpublished research, we have struggled to find ways to map problems in bioinformatics to representations that are suitable for gamification. The challenge of successfully orchestrating a scientific crowdsourcing initiative should not be underestimated. Yet the successes described above provide ample evidence that, in many cases, these approaches are worth consideration. As noted by Innocentive president Dwayne Spradlin, the primary challenge to successfully applying crowdsourcing is really in choosing the right problem for the crowd to solve (Spradlin 2012). In the next section, we provide a guide for matching problems to potential crowdsourcing-driven solutions, noting both plusses and minuses associated with each system.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Labeling images with a computer game</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">V</forename>
				<surname>Ahn</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Dabbish</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2004 SIGCHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="319" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Designing games with a purpose</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">V</forename>
				<surname>Ahn</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Dabbish</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="58" to="67" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">reCAPTCHA: Human-Based Character Recognition via Web Security Measures</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">V</forename>
				<surname>Ahn</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">321</biblScope>
			<biblScope unit="page" from="1465" to="1468" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Crowd-powered systems</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">S</forename>
				<surname>Bernstein</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: Electrical Engineering and Computer Science. Massachusetts Institute of Technology</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Microbial virus genome annotation-Mustering the troops to fight the sequence onslaught</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">R</forename>
				<surname>Brister</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Virology</title>
		<imprint>
			<biblScope unit="volume">434</biblScope>
			<biblScope unit="page" from="175" to="180" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<monogr>
		<title level="m" type="main">Validating candidate gene-mutation relations in MEDLINE abstracts via crowdsourcing (eds) Data Integration in the Life Sciences</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Burger</surname>
			</persName>
		</author>
		<editor>Bodenreider,O. and Rance,B.</editor>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="83" to="91" />
			<pubPlace>Berlin ; Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Galaxy evolution. Galaxy zoo volunteers share pain and glory of research</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Clery</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="page" from="173" to="175" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Citizen science: can volunteers do real research?</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">P</forename>
				<surname>Cohn</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioScience</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<date type="published" when="0192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">The most stressful science problem</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Cooper</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American Blog</title>
		<imprint>
			<date type="published" when="2013-06-30" />
		</imprint>
	</monogr>
	<note>date. last accessed</note>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting protein structures with a multiplayer online game</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Cooper</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">466</biblScope>
			<biblScope unit="page" from="756" to="760" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Web-based genome-wide association study identifies two novel loci and a substantial genetic component for Parkinson&apos;s disease</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">B</forename>
				<surname>Do</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Genet</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1002141</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Crowdsourcing systems on the world-wide web</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Doan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page">86</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Increased Diels-Alderase activity through backbone remodeling guided by Foldit players</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">B</forename>
				<surname>Eiben</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="190" to="192" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Amazon mechanical turk: gold mine or coal mine?</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Fort</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Ling</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="413" to="420" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">The 2012 nucleic acids research database issue and the online molecular biology database collection</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">Y</forename>
				<surname>Galperin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">X</forename>
				<forename type="middle">M</forename>
				<surname>Fernandez-Suarez</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Games with a scientific purpose</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Good</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Su</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">135</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Internet-based crowdsourcing and research ethics: the case for IRB review</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Graber</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Graber</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Ethics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="115" to="118" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge acquisition for medical diagnosis using collective intelligence</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Herna´ndezherna´ndez-Chan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Syst</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="5" to="9" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Metagenome annotation using a distributed grid of undergraduate students</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Hingamp</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Biol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">296</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<monogr>
		<title level="m" type="main">The Rise of Crowdsourcing. Wired</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Howe</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2006-06-30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Phylo: a citizen science approach for improving multiple sequence alignment</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kawrykow</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">31362</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Algorithm discovery by protein folding game players</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Khatib</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="18949" to="18953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Crystal structure of a monomeric retroviral protease solved by protein folding game players</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Khatib</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Struct. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1175" to="1177" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">GENIA corpus—semantically annotated corpus for bio-textmining</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">D</forename>
				<surname>Kim</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="180" to="182" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Harnessing the wisdom of crowds in wikipedia: quality through coordination</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kittur</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">E</forename>
				<surname>Kraut</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM conference on Computer supported cooperative work</title>
		<meeting>the 2008 ACM conference on Computer supported cooperative work<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="37" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">CrowdForge: crowdsourcing complex work</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kittur</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual ACM symposium on User interface software and technology</title>
		<meeting>the 24th annual ACM symposium on User interface software and technology<address><addrLine>Santa Barbara, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">The anatomy of a large-scale human computation engine</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Kochhar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD Workshop on Human Computation</title>
		<meeting>the ACM SIGKDD Workshop on Human Computation<address><addrLine>Washington DC</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<monogr>
		<title level="m" type="main">New videogame lets amateur researchers mess with RNA. Wired Science</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">I</forename>
				<surname>Koerner</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2012-06-30" />
		</imprint>
	</monogr>
	<note>date. last accessed</note>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Prize-based contests can provide solutions to computational biology problems</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">R</forename>
				<surname>Lakhani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotech</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="108" to="111" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Galaxy Zoo: morphologies derived from visual inspection of galaxies from the sloan digital sky survey</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">J</forename>
				<surname>Lintott</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mon. Not. R. Astron. Soc</title>
		<imprint>
			<biblScope unit="volume">389</biblScope>
			<biblScope unit="page" from="1179" to="1189" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Little</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TurKit. In: Proceedings of the 23nd annual ACM symposium on User interface software and technology—UIST &apos;10</title>
		<meeting><address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">57</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Crowdsourcing malaria parasite quantification: an online game for analyzing images of infected thick blood smears</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Luengo-Oroz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Internet Res</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">167</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Wisdom of crowds for robust gene network inference</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Marbach</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="796" to="804" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Distributed medical image analysis and diagnosis through crowd-sourced games: a malaria case study</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Mavandadi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">37245</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Crowd-sourced BioGames: managing the big data problem for next-generation lab-on-a-chip platforms</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Mavandadi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lab Chip</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="4102" to="4106" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">Development and evaluation of a crowdsourcing methodology for knowledge base construction: identifying relationships between clinical problems and medications</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">B</forename>
				<surname>Mccoy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMIA</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="713" to="718" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<analytic>
		<title level="a" type="main">Progress in ion torrent semiconductor chip based sequencing</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Merriman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electrophoresis</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3397" to="3417" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b37">
	<analytic>
		<title level="a" type="main">Distributed human intelligence for colonic polyp classification in computer-aided detection for CT colonography</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">B</forename>
				<surname>Nguyen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">262</biblScope>
			<biblScope unit="page" from="824" to="833" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b38">
	<analytic>
		<title level="a" type="main">Human computation: a survey and taxonomy of a growing field</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Quinn</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">B</forename>
				<surname>Bederson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;11 SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting><address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1403" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b39">
	<analytic>
		<title level="a" type="main">Protein structure prediction using Rosetta</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">A</forename>
				<surname>Rohl</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods Enzymol</title>
		<imprint>
			<biblScope unit="volume">383</biblScope>
			<biblScope unit="page" from="66" to="93" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b40">
	<analytic>
		<title level="a" type="main">Crowdsourcing research opportunities</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Sabou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Knowledge Management and Knowledge Technologies—i-KNOW &apos;12</title>
		<meeting>the 12th International Conference on Knowledge Management and Knowledge Technologies—i-KNOW &apos;12<address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b41">
	<analytic>
		<title level="a" type="main">The power of many</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Sansom</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="201" to="203" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b42">
	<analytic>
		<title level="a" type="main">Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Snow</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b43">
	<analytic>
		<title level="a" type="main">Crowdsourced health research studies: an important emerging complement to clinical trials in the public health research ecosystem</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Swan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Internet Res</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">46</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b44">
	<analytic>
		<title level="a" type="main">A million dollar ideaffl potential biomarker for ALS</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Talan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurology Today</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient replication of over 180 genetic associations with self-reported medical data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">Y</forename>
				<surname>Tung</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">23473</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b46">
	<analytic>
		<title level="a" type="main">Fusion of machine intelligence and human intelligence for colonic polyp detection in CT colonography</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Wang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Biomedical Imaging: From Nano to Macro</title>
		<meeting><address><addrLine>Chicago, Illinois, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="160" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b47">
	<analytic>
		<title level="a" type="main">Raising money for scientific research through crowdfunding</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">E</forename>
				<surname>Wheat</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Ecol. Evol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="71" to="72" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b48">
	<monogr>
		<title level="m" type="main">Preliminary experience with amazon&apos;s mechanical turk for annotating medical named entities In: CSLDAMT &apos;10 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Yetisgen-Yildiz</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="180" to="183" />
			<pubPlace>Stroudsburg, PA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b49">
	<analytic>
		<title level="a" type="main">Cheap, fast, and good enough for the non-biomedical domain but is it usable for clinical natural language processing? Evaluating crowdsourcing for clinical trial announcement named entity annotations</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Zhai</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Second International Conference on Healthcare Informatics</title>
		<meeting><address><addrLine>La Jolla, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="106" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b50">
	<monogr>
		<title level="m" type="main">Crowdsourcing for bioinformatics</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>