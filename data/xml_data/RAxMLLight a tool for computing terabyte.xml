
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Phylogenetics RAxML-Light: a tool for computing terabyte phylogenies</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012">2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">A</forename>
								<surname>Stamatakis</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="laboratory">The Exelixis Lab, Scientific Computing Group</orgName>
								<orgName type="institution">Heidelberg Institute for Theoretical Studies</orgName>
								<address>
									<addrLine>Schloss-Wolfsbrunnenweg 35</addrLine>
									<postCode>D-68159</postCode>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">A</forename>
								<forename type="middle">J</forename>
								<surname>Aberer</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="laboratory">The Exelixis Lab, Scientific Computing Group</orgName>
								<orgName type="institution">Heidelberg Institute for Theoretical Studies</orgName>
								<address>
									<addrLine>Schloss-Wolfsbrunnenweg 35</addrLine>
									<postCode>D-68159</postCode>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">C</forename>
								<surname>Goll</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="laboratory">The Exelixis Lab, Scientific Computing Group</orgName>
								<orgName type="institution">Heidelberg Institute for Theoretical Studies</orgName>
								<address>
									<addrLine>Schloss-Wolfsbrunnenweg 35</addrLine>
									<postCode>D-68159</postCode>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">S</forename>
								<forename type="middle">A</forename>
								<surname>Smith</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Ecology and Evolutionary Biology</orgName>
								<orgName type="laboratory">Blackrim Lab</orgName>
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">S</forename>
								<forename type="middle">A</forename>
								<surname>Berger</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="laboratory">The Exelixis Lab, Scientific Computing Group</orgName>
								<orgName type="institution">Heidelberg Institute for Theoretical Studies</orgName>
								<address>
									<addrLine>Schloss-Wolfsbrunnenweg 35</addrLine>
									<postCode>D-68159</postCode>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">F</forename>
								<surname>Izquierdo-Carrasco</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="laboratory">The Exelixis Lab, Scientific Computing Group</orgName>
								<orgName type="institution">Heidelberg Institute for Theoretical Studies</orgName>
								<address>
									<addrLine>Schloss-Wolfsbrunnenweg 35</addrLine>
									<postCode>D-68159</postCode>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Phylogenetics RAxML-Light: a tool for computing terabyte phylogenies</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS APPLICATIONS NOTE 2071A Kraus Natural Science Building</title>
						<meeting> <address><addrLine>USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">28</biblScope>
							<biblScope unit="issue">15</biblScope>
							<biblScope unit="page" from="2064" to="2066"/>
							<date type="published" when="2012">2012</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/bts309</idno>
					<note type="submission">Received on March 7, 2012; revised on May 15, 2012; accepted on May 18, 2012</note>
					<note>Copyedited by: TRJ MANUSCRIPT CATEGORY: APPLICATIONS NOTE [12:10 20/7/2012 Bioinformatics-bts309.tex] Page: 2064 2064–2066 Advance Access publication Associate Editor: Jonathan Wren Supplementary Information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Due to advances in molecular sequencing and the increasingly rapid collection of molecular data, the field of phyloinformatics is transforming into a computational science. Therefore, new tools are required that can be deployed in supercomputing environments and that scale to hundreds or thousands of cores. Results: We describe RAxML-Light, a tool for large-scale phylogenetic inference on supercomputers under maximum likelihood. It implements a lightweight checkpointing mechanism, deploys 128-bit (SSE3) and 256-bit (AVX) vector intrinsics, offers two orthogonal memory saving techniques and provides a fine-grain production-level message passing interface parallelization of the likelihood function. To demonstrate scalability and robustness of the code, we inferred a phylogeny on a simulated DNA alignment (1481 taxa, 20 000 000 bp) using 672 cores. This dataset requires one terabyte of RAM to compute the likelihood score on a single tree. Code Availability: https://github.com/stamatak/RAxML-Light-1.0.5 Data Availability: http://www.exelixis</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Phyloinformatics is facing a paradigm shift toward becoming a 'real' computational science. Molecular sequencing technologies are developing at a rapid pace, generating enormous amounts of new data. Due to the necessity to process (and store) huge amounts of data, we expect the field to undergo an analogous transition that physics or computational fluid dynamics underwent 20–30 years ago. Projects such as the 1000 insect transcriptome sequencing project (www.1kite.org) already face these challenges. Such evolutionary studies require software that scales beyond a single node, that can be checkpointed and restarted, and that can accommodate the memory requirements of whole-genome datasets under likelihood-based models. RAxML-Light is a production-level tool for phylogenetic inference on supercomputers that implements new approaches for handling load imbalance, checkpointing and reducing the RAM * To whom correspondence should be addressed. requirements of likelihood computations. Implementation details are discussed in the online supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FEATURES</head><p>We briefly discuss the features that distinguish RAxML-Light from standard RAxML, other likelihood-based phylogeny programs, and the BEAGLE library (<ref type="bibr" target="#b0">Ayres et al., 2011</ref>). One important feature (in contrast to BEAGLE, MrBayes (<ref type="bibr" target="#b5">Ronquist and Huelsenbeck, 2003</ref>) or GARLI (<ref type="bibr" target="#b10">Zwickl, 2006)</ref>) is that RAxML-Light implements a finegrain message passing interface (MPI) parallelization to compute the likelihood on a single huge dataset and a single tree across several nodes. We introduced the proof-of-concept implementation in (<ref type="bibr" target="#b3">Ott et al., 2007</ref>). The work is split by distributing alignment sites or entire partitions (depending on the selected command line options) among processors. Another essential feature is the light-weight checkpointing and restart capability, that is required on typical HPC systems that have queues with 24-or 48-h run-time limits. Light weight means that only those data-structures are stored in a checkpoint which are really required to restart the code. The design goal is to minimize checkpoint writing/reading times and file sizes. To the best of our knowledge, RAxML-Light comprises the only fine-grain parallelization of the likelihood function that also incorporates load balance mechanisms as described in (<ref type="bibr" target="#b8">Stamatakis and Ott, 2009</ref>) and (<ref type="bibr" target="#b9">Zhang and Stamatakis, 2012</ref>). Load imbalance can deteriorate parallel efficiency in partitioned phylogenetic analyzes. RAxML-Light also contains a production-level implementation of two orthogonal memory saving techniques that can be used simultaneously (described in<ref type="bibr" target="#b1">Izquierdo-Carrasco et al., 2011a</ref>,b). These techniques allow for deploying RAxML-Light on systems that do not have enough RAM to store all conditional probability vectors required for likelihood calculations. Finally, RAxML-Light also uses 256-bit wide AVX vector intrinsics to accelerate likelihood computations on Intel SandyBridge and AMD Bulldozer CPUs that will become available in many HPC systems over the next 2–3 years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PERFORMANCE AND STRESS TESTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parallel scalability</head><p>We measured the relative speedup of the MPI version of RAxMLLight on a dataset with 150 taxa and 20 000 000 bp (extracted from the above simulated dataset) on an AMD Magny-Cours-based cluster</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RAxML-Light</head><p>with a Qlogic Infiniband interconnect and a total of 50 48-core nodes equipped with 128 GB (46 nodes) or 256 GB (4 nodes) of RAM per node. For comparison, we also measured execution times of the PThreads-based version on a stand-alone 48-core AMD server with 256 GB RAM. In<ref type="figure">Figure 1</ref>, we provide execution times for the PThreads and MPI versions on multiples of 48 cores under the CAT (<ref type="bibr" target="#b6">Stamatakis, 2006</ref>) and (using four discrete rate categories) models of rate heterogeneity. The test dataset requires almost 256 GB of RAM under which explains the bad initial performance under on one and two cluster nodes. The nodes in the cluster have slightly different swapping configurations than the stand-alone node (same server type) we used. Overall, the code scales well up to 1536 cores. On 1536 cores, RAxML-Light requires &lt;2 h (6108 s) to complete a full tree search under .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Load balance</head><p>The initial work on improving load balance for partitioned datasets (<ref type="bibr" target="#b8">Stamatakis and Ott, 2009</ref>) is hard-coded in RAxML-Light and can improve parallel efficiency by &gt;50%. The recent work on the 'multi-processor scheduling problem in phylogenetics' ((<ref type="bibr" target="#b9">Zhang and Stamatakis, 2012</ref>),-Q option) should only be applied when the number of partitions/genes is substantially larger than the cores that shall be used. This alternative data distribution scheme improved parallel run times by one order of magnitude on a partitioned protein alignment with 1000 partitions under CAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Computing a terabyte tree</head><p>To conduct a thorough stress test, we simulated a DNA alignment with 1481 taxa and 20 000 000 bp using SeqGen (<ref type="bibr" target="#b4">Rambaut and Grass, 1997</ref>). The 1481 taxon tree we used to generate the alignment is a ML tree inferred on a real-world single gene dataset. Under the CAT model of rate heterogeneity, this dataset requires about 1 TB of RAM to compute the likelihood on a single tree. We executed a single tree search on 672 cores (14 48-core nodes) which required 40 h to converge for the standard RAxML search algorithm. The relative Robinson–Foulds (RF) distance to the true tree was 7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Computing a 116 334 taxon tree</head><p>For the NSF plant tree of life grand challenge project, we deployed the PThreads version (running on a single 48-core node) to perform 100 ML searches (each search starting from distinct randomized stepwise addition order parsimony starting tree) on a real-world DNA alignment (116 334 taxa, 16 079 bp) under CAT and a partitioned model. With the ML search convergence criterion enabled (see<ref type="bibr" target="#b7">Stamatakis, 2011</ref>) the runs required two automatic restarts (using appropriate Sun Grid Engine scripts) from checkpoints to complete within three 48 h queue slots. While the runs were successful, the resulting trees did not make 'biological sense'. This is in part a result of trying to construct, with the data available at the time in GenBank (ca. 2008), alignments with &gt;100 000 species. Viridiplantae did not have data for &gt;100 000 species with any of traditionally well-sampled gene regions. So we had to (1) expand the dataset with less well-sampled gene regions for plants, (2) include ribosomal regions 18S and 26S and (3) include a large fungi outgroup. These complications allowed us to construct a large dataset with &gt;100 000 species but led to some unexpected taxonomic placements. Nonetheless, we make the alignment and trees available for benchmarking purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. Parallel execution times of the MPI and PThreads versions under CAT and on a DNA dataset with 150 taxa and 20 000 000 sites</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Memory saving techniques</head><p>CopyeditedUnvectorized execution times have been measured using the standard RAxML version. and averaged execution times over three runs. For reference, we also included the execution times of the standard RAxML version without vectorization in<ref type="figure" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION AND FUTURE WORK</head><p>We have presented, RAxML-Light, a scalable, AVX-vectorized and checkpointable open-source code for large-scale phylogenetic inference on supercomputers. User support will be provided through groups.google.com/group/raxml and continued development will be provided through the github repository. For partitioned whole-genome datasets with thousands of partitions, the code requires some substantial re-engineering (in addition to the techniques presented here) to further reduce communication costs. Under the current fork-join parallelization paradigm (also used in BEAGLE), communication to trigger parallel regions for partitioned whole-genome datasets becomes bandwidthbound instead of latency-bound. This problem is independent of and orthogonal to the load balance issues discussed here and only became apparent in the course of some currently on-going partitioned whole-genome analyses. We also expect energy efficiency and core failure tolerance to become important future research topics with respect to scaling phylogenetics codes to exascale HPC systems. Funding: Part of this work was funded by German Science Foundation (DFG) grants STA 860/2 and STA 860/3, the NSF (National Science Foundation) iPlant collaborative, and via institutional funding from the Heidelberg Institute of Theoretical Studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest: none declared.</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>Copyedited by: TRJ MANUSCRIPT CATEGORY: APPLICATIONS NOTE [12:10 20/7/2012 Bioinformatics-bts309.tex] Page: 2065 2064–2066</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>Table 1.</figDesc><table>Execution times of unvectorized, SSE3-and AVX-vectorized 
RAxML versions 

Data 
Model 
Unvectorized 
SSE3 
AVX 

DNA 
CAT 
100 
87 
76 
DNA 

520 
433 
353 
PROT 
CAT 
117 
83 
49 
PROT 

423 
249 
187 

</table></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">The scalability of RAxML-Light is limited by the number of sites in the alignment, because RAxML-Light is parallelized over sites/partitions. Hence, it does not make sense to analyze datasets as the one above (116 334 taxa) in parallel on more than one node. On such gappy datasets with missing data, we can deploy the subtree equality vector technique (-S option) to substantially reduce memory requirements. The key idea of this technique is to keep track of subtrees in partitions (genes) that entirely consist of missing data and omit computing as well as storing the ancestral probability vectors for these &apos;empty&apos; subtrees. In the above case, using-S led to a reduction of RAM requirements from 66 GB down to 26.5 GB. This allowed us to also execute some tree searches on single nodes of the Texas Advanced Computing Center, that only have 32 GB of RAM available. The-S option generally also decreases execution times, because a large number of unecessary computations are omitted (see (Izquierdo-Carrasco et al., 2011a) for performance details). Note that performance of this technique also depends heavily on the memory allocator being used (see Supplementary Material). We also tested the MPI version of the recomputation technique (with reduction factors of-r 0.2 and-r 0.15) on just a single 48-core node with 256 GB RAM on the dense simulated 1 TB dataset that does not contain any gaps. The recomputation technique saves memory by not storing all ancestral probability vectors, but only a fraction of them as specified by the-r switch (e.g. setting-r 0.2 means that only 20% of the ancestral vectors are stored). When an ancestral vector needs to be read that has not been stored in RAM, we simply recompute it. Evidently, execution times will increase because of recomputations, but the increase is small (≈40%) even when storing only 10% (-r 0.1) of the required vectors in RAM (Izquierdo-Carrasco et al., 2011b). Our tests showed that using the recomputation technique, a dataset requiring 1 TB of RAM can be successfully and correctly analyzed on a single multi-core server with only 256 GB RAM (for additional details see Supplementary Material). 3.6 Vector intrinsics for likelihood We tested the performance of the AVX-vectorization in RAxMLLight using a DNA dataset with 150 taxa and 1269 bp (1130 distinct site patterns) and a protein dataset with 40 taxa and 1104 bp (958 distinct site patterns) on a single Intel i7-2620M core running at 2.7 GHz. We measured execution times under the CAT and under 2065 at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">BEAGLE: an application programming interface and highperformance computing library for statistical phylogenetics</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Ayres</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Syst. Biol</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="170" to="173" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Algorithms, data structures, and numerics for likelihood-based phylogenetic inference of huge trees</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Izquierdo-Carrasco</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">470</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<monogr>
		<title level="m" type="main">Trading memory for running time in phylogenetic likelihood computations</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Izquierdo-Carrasco</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<pubPlace>Heidelberg Institute for Theoretical Studies</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale maximum likelihood-based phylogenetic analysis on the IBM BlueGene/L</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ott</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE/ACM Supercomputing Conference</title>
		<meeting>IEEE/ACM Supercomputing Conference</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Seq-gen: an application for the monte carlo simulation of dna sequence evolution along phylogenetic trees</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Rambaut</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Grass</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Appl. Biosci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">235</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">MrBayes 3: Bayesian phylogenetic inference under mixed models</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Ronquist</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Huelsenbeck</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1572" to="1574" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Phylogenetic models of rate heterogeneity: a high performance computing perspective</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Stamatakis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IPDPS2006, HICOMB Workshop, Proceedings on CD</title>
		<meeting>IPDPS2006, HICOMB Workshop, Proceedings on CD<address><addrLine>Rhodos, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Phylogenetic search algorithms for maximum likelihood</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Stamatakis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithms in Computational Molecular Biology: Techniques, Approaches and Applications</title>
		<editor>M. Elloumi and Albert Y. Zomaya</editor>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="547" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Load balance in the phylogenetic likelihood Kernel</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Stamatakis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ott</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Parallel Processing (ICPP&apos;09</title>
		<meeting>International Conference on Parallel Processing (ICPP&apos;09</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="348" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<monogr>
		<title level="m" type="main">The multi-processor scheduling problem in phylogenetics</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Stamatakis</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>Heidelberg Institute for Theoretical Studies</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<monogr>
		<title level="m" type="main">Genetic algorithm approaches for the phylogenetic analysis of large biological sequence datasets under the maximum likelihood criterion</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Zwickl</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>