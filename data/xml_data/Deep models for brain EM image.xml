
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep models for brain EM image segmentation: novel insights and improved performance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Ahmed</forename>
								<surname>Fakhry</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Old Dominion University</orgName>
								<address>
									<postCode>23529</postCode>
									<settlement>Norfolk</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Hanchuan</forename>
								<surname>Peng</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="institution">Allen Institute for Brain Science</orgName>
								<address>
									<postCode>98103</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Shuiwang</forename>
								<surname>Ji</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Washington State University</orgName>
								<address>
									<postCode>99164</postCode>
									<settlement>Pullman</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep models for brain EM image segmentation: novel insights and improved performance</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btw165</idno>
					<note type="submission">Received on December 10, 2015; revised on March 3, 2016; accepted on March 5, 2016</note>
					<note>Bioimage informatics *To whom correspondence should be addressed. Associate Editor: Robert Murphy Contact: sji@eecs.wsu.edu</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Accurate segmentation of brain electron microscopy (EM) images is a critical step in dense circuit reconstruction. Although deep neural networks (DNNs) have been widely used in a number of applications in computer vision, most of these models that proved to be effective on image classification tasks cannot be applied directly to EM image segmentation, due to the different objectives of these tasks. As a result, it is desirable to develop an optimized architecture that uses the full power of DNNs and tailored specifically for EM image segmentation. Results: In this work, we proposed a novel design of DNNs for this task. We trained a pixel classifier that operates on raw pixel intensities with no preprocessing to generate probability values for each pixel being a membrane or not. Although the use of neural networks in image segmentation is not completely new, we developed novel insights and model architectures that allow us to achieve superior performance on EM image segmentation tasks. Our submission based on these insights to the 2D EM Image Segmentation Challenge achieved the best performance consistently across all the three evaluation metrics. This challenge is still ongoing and the results in this paper are as of June 5, 2015. Availability and Implementation: https://github.com/ahmed-fakhry/dive</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Anatomical connections between neurons in the brain form circuits that are responsible for the rapid information flow. Knowledge of the circuit structure is crucial for the investigation of its function. Mapping the structure and components of these circuits is one of the top priority research areas in neuroscience. It provides a foundation for understanding what the brain is made of at the cellular and structural levels, and how these properties change across the normal life span and in brain disorders. The reconstruction of such circuits at a very high resolution using electron microscopy (EM) is considered to be the gold standard for circuit mapping (Brain Research through Advancing Innovative Neurotechnologies, BRAIN, 2014). Currently, sparse circuit reconstruction has been widely used on a small-scale. Most of the studies focused on the very small</p><p>Caenorhabditis elegans or small parts of the nervous systems of the Drosophila. Recently, some of those efforts were extended to reconstruct the inner plexiform layer in the mouse retina (<ref type="bibr" target="#b3">Briggman et al., 2011;</ref><ref type="bibr" target="#b10">Helmstaedter et al., 2013;</ref><ref type="bibr" target="#b16">Kim et al., 2014</ref>). Using EM data in large-scale studies are currently a challenge, where the main bottleneck is data analysis. Better automatic image segmentation techniques would substantially amplify the impact of dense EM reconstruction. Machine learning and artificial intelligence approaches are expected to be the main driver for the desired advancements in this area. In this work, we focused on the automatic segmentation of serial-section Transmitted Electron Microscopy (ssTEM) images. We provided a novel design of deep neural networks (DNNs) (<ref type="bibr" target="#b18">LeCun et al., 1989</ref>) that extends the techniques described in<ref type="bibr">Ciresan</ref>(2012a). We built a pixel DNN classifier that predicts the probability of every individual pixel in a given image being a membrane (border) pixel or not. Our DNN classifier accepts raw pixel intensities as input without any preprocessing and learns highly discriminative features automatically before producing final probability maps. These probability maps were fed later to another machine learning classifier based on random forests (<ref type="bibr" target="#b1">Breiman, 2001</ref>) to produce final segmentations. DNNs have been widely used in a number of applications in computer vision. It achieved the state-of-the-art performance on tasks like large-scale image and video recognition (<ref type="bibr" target="#b14">Ji et al., 2013;</ref><ref type="bibr" target="#b17">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b29">Zeiler and Fergus, 2014</ref>), digit recognition (<ref type="bibr" target="#b6">Ciresan et al., 2012b</ref>) and object recognition tasks (<ref type="bibr" target="#b19">LeCun et al., 2004</ref>). Recently, many attempts have been made to extend the usage of these models to the field of image segmentation, leading to improved performance (<ref type="bibr" target="#b2">Briggman et al., 2009;</ref><ref type="bibr" target="#b12">Jain and Seung, 2009;</ref><ref type="bibr" target="#b13">Jain et al., 2007;</ref><ref type="bibr" target="#b25">Ronneberger et al., 2015;</ref><ref type="bibr" target="#b28">Turaga et al., 2010;</ref><ref type="bibr" target="#b30">Zhang et al., 2015</ref>). Although some of the existing popular models have proved the ability to generalize well for different recognition tasks like the model in<ref type="bibr" target="#b17">Krizhevsky et al. (2012)</ref>, most of these models that excelled on image classification and recognition tasks cannot be applied directly to EM image segmentation tasks, given the difference in objectives between those tasks as well as the difference between EM and natural images (more details in Section 2.1). As a result, it is desirable to develop an optimized architecture that utilizes the full power of DNNs and tailored specifically for EM image segmentation. In this work, we developed a DNN model architecture that is highly optimized for ssTEM image segmentation. The key contribution is in the model itself and the novel insights about the specific kernel configuration leading to substantially improved results. We evaluated the effect of model configuration along with kernel structures and depth on the final segmentation outcome. We validated our approach by applying it to the ISBI 2012 EM Segmentation Challenge (<ref type="bibr" target="#b11">ISBI, 2012</ref>) (http://brainiac2.mit.edu/ isbi_challenge/), achieving the best performance on all evaluation metrics out of more than 40 participating groups (more groups are participating as the challenge is ongoing). Our model was one of the few that were able to beat the performance of a second human observer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In our work, we used a full stack of EM image slices of the Drosophila first-instar larva ventral nerve cord (<ref type="bibr" target="#b4">Cardona et al., 2010</ref>) provided by the organizers of the ISBI 2012 EM Segmentation Challenge (ISBI, 2012). The training stack consists of 30 grayscale sections of 512 Ã‚ 512 pixels each, where there is a corresponding label map for each image slice representing whether the pixels are membrane or non-membrane. We trained a deep convolutional neural network (DNN) pixel classifier to predict the label of every pixel separately. We applied this classifier on another stack of 30 sections representing the testing data where the ground truth is only known to the organizers of the challenge. In order to use a pixel classifier, we adopted a patch-based training technique. For every pixel in every slice, we extracted a square patch of a fixed size with the target pixel in its center. For boundary pixels, we mirrored the pixels across the slice borders. Upon testing, we obtained a membrane probability map for each slice of the testing data. These probability maps then underwent post-processing to generate the labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Key insights</head><p>The key contributions of this work are the optimized architecture of a DNN model for EM image segmentation and the underlying motivation and observations. Although the use of DNN architecture usually leads to good performance on similar segmentation tasks, a careful design of the network architecture and choices of kernel sizes and placement are the key to utilize the full performance power of the model. For example, in the model suggested by<ref type="bibr" target="#b17">Krizhevsky et al. (2012)</ref>, the kernel sizes were chosen to be very large at the bottom layers of the network and then reduced gradually. Later, Zeiler and<ref type="bibr" target="#b29">Fergus (2014)</ref>showed that a better performance could be obtained by reducing the receptive field size and choosing a smaller stride for the first convolutional layer in the same model. On the other hand, in Simonyan and Zisserman (2014), a network of a very small kernel size which was fixed for all layers proved to achieve the best performance on image localization and classification tasks. This highlights the importance of these architectural details in achieving record-breaking performance on different computer vision tasks. Unfortunately, the application of the very powerful models like Simonyan and Zisserman (2014) and<ref type="bibr" target="#b27">Szegedy et al. (2014)</ref>to the task of EM image segmentation does not yield as good results as it did on natural image recognition tasks. The reason is that the two tasks are genuinely different in terms of their objective and training characteristics. In segmentation, the objective is to assign a label for every single pixel in the image as opposed to a single label assigned to the entire image in classification. In addition, training for segmentation tasks includes a lot of redundancy in the input data due to the patch-based technique, as opposed to training on the whole image or random crops of it in classification tasks. In terms of the data itself, EM images are characterized by their high density and the invariability of the objects it composes unlike the natural images that are regularly used in classification tasks like the ImageNet data (<ref type="bibr" target="#b8">Deng et al., 2009</ref>). We experimented with several networks that are considered state-of-the-art for image classification and recognition tasks and the results were inferior. In particular, the VGG net (<ref type="bibr" target="#b26">Simonyan and Zisserman, 2014</ref>) has performed very poorly on this specific segmentation task. We attribute this to the very small kernel sizes (3 Ã‚ 3) in the network which does not include enough contextual information that this segmentation task requires based on our experiments. The key observation about this task is how important the context information is for building discriminative features especially in the bottom layers of DNN in EM image segmentation tasks. It is crucial to provide each kernel with a large enough receptive field especially in the bottom layers in order to be able to learn better features. At the same time, if the context is too wide for the bottom layers, the performance will drop due to the excessively large neighborhood which will contain some noise. For example, in the VGGnet (<ref type="bibr" target="#b26">Simonyan and Zisserman, 2014</ref>) all the kernels are fixed to a size of 3 Ã‚ 3 which is very small and not enough for the features to be learned from the underlying data. In Krizhevsky's network, they started with an excessively large kernel size of 11 Ã‚ 11 at the bottom. When we tried both networks on the EM data, the results were both inferior. In our architecture, we focused on starting with a kernel size of 8 Ã‚ 8 at the bottom which is moderately large and sufficient for the network to be able to learn discriminative features without being affected by noise. Increasing the kernel sizes of the bottom layers in the network increases the receptive fields of each unit in the resulting feature maps, thereby increasing the impact of context information in generating these features. This leads to learning more discriminative features which improves the overall</p><p>Deep models for brain EM image segmentationperformance. On the contrary, the pixels in the feature maps of the top layers already correspond to a very large receptive field due to the presence of max pooling layers beneath them regardless of the kernel sizes in the layer directly beneath them as illustrated in<ref type="figure">Figure 1</ref>. Using smaller kernel sizes for the upper layers also allows the model to grow deeper as we can add more convolution layers. As a result, we decreased the kernel sizes gradually as we went deeper in the network. Another key insight is the impact of non-linearity and network depth on the overall network performance. We argue that increasing the number of convolution layers along with their corresponding rectified linear unit (RELU) layers usually increases the networkes accuracy. Increasing the number of convolution layers increases the number of features to be learned, while the RELU layers are responsible for increasing the non-linearity in the network and preventing the gradient from saturation. In Krizhevsky network (<ref type="bibr" target="#b17">Krizhevsky et al., 2012</ref>), although the input image size was 224 Ã‚ 224, the network did not have enough non-linearity with only five convolution layers and five corresponding RELU layers in between them. On the other hand, our best performing network had six convolution layers while the input size is only 95 Ã‚ 95. This high non-linearity in our network was crucial for obtaining a better performance. We validated these insights through the network design in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The architecture</head><p>We used DNN with multiple convolution, pooling and fully connected layers for our pixel classifier. We experimented with a wide range of window sizes, network depth and kernel sizes to assess the effect of each parameter on the final segmentation outcome. We used window sizes ranging between 35 and 95, depth between 6 and 8 trainable layers and kernels between 3 Ã‚ 3 up to 10 Ã‚ 10. Although our architectures are quite different from each others, they share some common properties that experimentally proved to be the best for the current task. No preprocessing was applied to any of the networks except for mean subtraction. The overall mean of all the pixels in all sections was subtracted from each pixel value. In our designs, we limited the number of max pooling layers in favor of more convolution layers in all the architectures. Increasing the number of convolution layers helped the model find more discriminative features. The key challenge was to add as many convolution layers as possible without losing the translation invariance advantage that the max pooling layers provide. In addition, the choice of the window size would always affect the network depth and the number of convolution layers in turn. We compared several architectures containing different numbers of max pooling layers ranging between two and five and we found that three max pooling layers always give the best results. As a result, each network we trained is divided into three blocks; each block contains a number of convolution layers (differs per network configuration and per block) which are followed by a single max pooling layer of size 2 Ã‚ 2 and stride 2 by the end of the block. We also introduced back to back convolution layers interleaved by only RELU as the non-linear transformation in all architectures. Instead of using a single convolution layer with a very large kernel size in every block, we chose to stack multiple convolution layers above each other with moderate kernel sizes while adding RELU layers in between them. This was done mainly to increase the nonlinearity in the model and thus encouraging it to learn more complex features. In addition, breaking down a single large kernel into several smaller ones reduced the total number of parameters need to be trained, thus reducing the overall computation time (<ref type="bibr" target="#b26">Simonyan and Zisserman, 2014</ref>). Our networks concluded with two fully connected layers after their third block. The last fully connected layer contains only two neurons corresponding to the segmentation tasks. The outcomes of these two neurons were finally passed through a softmax layer to produce probability values that represent either membrane or nonmembrane classes. We trained four DNNs sharing the common characteristics described earlier but with different configurations. These architectures were inspired by the networks used in<ref type="bibr" target="#b5">Ciresan et al. (2012a)</ref>;<ref type="bibr" target="#b17">Krizhevsky et al. (2012)</ref>; Simonyan and Zisserman (2014). The full architectures of the four networks can be found in<ref type="figure" target="#tab_1">Table 1</ref>. Networks A and B are both shallow with six trainable weight layers each. Network A has a very small window size of 35 Ã‚ 35 pixels which highly restricted the depth and the kernel sizes used. Network B starts with a window size of 65 Ã‚ 65 pixels which we utilized to test the effect of using excessively large kernels while keeping the depth constant. On the other hand, Networks C and D have a relatively large window size of 95 Ã‚ 95 pixels each to include more contextual information and to allow them to grow deep. Both networks use slightly larger kernels than the ones used in network A and slightly less than the ones used in network B. In our experiments, we hypothesized that starting with a large kernel size in the bottom layers of the network and reducing the size as we move upwards is much better than the opposite direction where we start with small kernels at the bottom. We tested this hypothesis through the configuration of networks C and D. Network C started with a small kernel size for its bottom layer, and then the<ref type="figure">Fig. 1</ref>. In this figure, we demonstrate that the upper layers in the network always correspond to a large receptive field due to the presence of max pooling layers. In the figure, even with a shallow network that uses 2 Ã‚ 2 convolution for its bottom layer and only 2 max pooling layers, the generated feature map in the last layer corresponds to a receptive field equals to the entire input size kernel size was constantly increased till the top convolution layer. For network D, we started by a moderately large kernel size which is double the initial kernel size of network C then we constantly decreased it. Another advantage of beginning with a large kernel size was that we aggressively reduced the resolution of the feature maps, leading to a reduction in the computations. To ensure learning in networks C and D, we applied local response normalization (LRN) before the max pooling layers in the first two blocks. This was because the number of parameters to be trained for these networks is higher than the other two due to the larger kernel sizes used and the increasing number of feature maps. Although RELUs are not easily saturated, adding LRNs is supposed to increase model generalization (<ref type="bibr" target="#b17">Krizhevsky et al., 2012</ref>). Experimentally, we found that the performance improvement using LRNs is minimal and does not contribute to the overall performance gain. We still included it in our models for the sake of completeness and to allow reproducibility of the results. We also applied dropout after the first fully connected layer in both networks C and D (details given in Section 2.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Test data augmentation</head><p>Ensemble learning techniques are well known to improve performance on various learning tasks. Random forest (<ref type="bibr" target="#b1">Breiman, 2001</ref>) is a clear example that proved to be one of the most effective ensemble learning techniques. In random forests, many decision trees are built based on some random variations in the input and feature spaces, and eventually each decision tree votes for a specific class. The combined vote is then considered as the output decision of the random forest. Extending a similar scheme to neural networks is however computationally expensive. A single DNN usually takes several days of training even using GPU-based implementation for a data set with a million of samples. Training several of these networks would be computationally inefficient. We chose to perform augmentation upon testing instead of the computationally expensive ensemble of DNNs [similar to the technique used in<ref type="bibr" target="#b7">CiresÂ¸an et al. (2013)]</ref>. We applied several linear transformations on the input image before testing. The transformations were combinations of horizontal and vertical mirroring, and/ or rotations by Ã¾90, Ã€90 and 180. After passing the transformed image through the network and obtaining a probability map, a reverse transformation was applied. In total, eight variations were applied to each testing image and then we took the average. The augmentation was implemented so that each variation received a vote in the final decision produced by the model. This technique is computationally more efficient than training several models as testing time is typically very fast. Our experiments showed a considerable advantage of applying those variations as the segmentation error dropped by half as compared with its original value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Overfitting reduction</head><p>One of the challenges of using a DNN is overfitting. Because our deep networks have a huge number of parameters (up to tens of millions of them), we needed a very large data set to avoid overfitting. In our data set, we extracted all the patches of the membrane pixels in every slice and an equivalent number of non-membrane pixels sampled randomly per slice. This generated about 3 million training patches in total, a number that may not be enough to train a very deep network without the risk of overfitting. We applied data augmentation to increase the variability in our training data. At the beginning of each epoch, a linear transformation to the input patch was randomly selected. We rotated each patch either by Ã¾90 or Ã€90 and/or mirrored it either horizontally or vertically. The data augmentation significantly improved the accuracy of the classifier. In addition, we applied dropout with 0.5 dropping ratio after the first fully connected layer in networks C and D to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Model design and implementation</head><p>Our patch-based classifier implementation is based on the publicly available C Ã¾Ã¾ Caffe (<ref type="bibr" target="#b15">Jia et al., 2014</ref>) (branched out in October 2014) with several modifications. We implemented our custom data augmentation where we decoupled cropping from mirroring as it is not desirable in our experiments in addition to implementing rotations with several angles. We also added random shuffling of data at the beginning of each epoch through randomly dropping a few samples in each mini-batch with a specific percentage. A window size can also be supplied before augmentation to determine the patch size desired for each configuration to avoid the recalculation of the patches and the databases. We trained our DNN classifier using back propagation (<ref type="bibr" target="#b20">LeCun et al., 2012</ref>) with stochastic gradient decent. We used a mini-batch size of 256, a momentum of 0.9 and a weight decay of 0.0005. We started with a base learning rate of 10 Ã€2 and decreased it by a factor of 10 every 100K iterations. We used random initialization for the weights through sampling from a normal distribution with a zero mean and 10 Ã€2 variance. The biases were initialized to either 0 or 1. Experimentally, we found that the model requires 30 epochs of training to achieve the desired accuracy on an NVIDIA K80 GPU. The training time took typically several days to complete. We used an image-based approach for generating the probability maps for the testing data set. We implemented our own image-based forward propagation code (<ref type="bibr" target="#b9">Giusti et al., 2013</ref>). The model was trained first by Caffe using a patch-based approach then the weights of the kernels and biases were extracted and fed to our image-based code. Our image-based prediction speeded up the testing time dramatically as compared to a patch-based GPU forward propagation even though our code was running on a CPU (takes roughly 2â€“4 minDeep models for brain EM image segmentationon a CPU machine). In addition, our image-based code did not require any computational and storage overhead for generating the patches for the testing data, making it much more convenient and efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Post-processing</head><p>Our network D achieved the best pixel accuracy on the ISBI 2012 data set without any post-processing. However, the best pixel accuracy is not necessarily accompanied with the best segmentation. Even the slightest mis-prediction of certain boundary pixels could severely hurt the overall segmentation. Our model is in nature a pixel classifier that was designed to achieve the best pixel accuracy but was not optimized to produce a better segmentation. We used the watershed merge tree post-processing technique used in<ref type="bibr" target="#b21">Liu et al. (2012</ref><ref type="bibr" target="#b22">Liu et al. ( , 2013</ref>). The technique starts by generating an initial segmentation using watershed for each probability map and then gradually raising the water level to produce hierarchical segmentations forming a watershed merge tree. A decision of merging two nodes in the merge tree is based on the result of a random forest boundary classifier that predicts the merge based on various features extracted from each two nodes. This scheme reduced the segmentation error to less than half of its original value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and discussion</head><p>We created a validation data set from the training slices for which the truth labels are available. We divided the 30 training slices into 20 training and 10 validation slices to obtain quantitative evaluations for our models before submitting the final results to the ISBI 2012 challenge. The evaluation of the testing data in this challenge was done through an automated online system where the submitted segmentations were compared with the hidden ground truth based on three different metrics: Minimum Splits and Mergers Warping error is a segmentation metric that penalizes topological disagreements, i.e.: the number of splits and mergers required to obtain the desired segmentation. Foreground-restric Rand error is defined as 1 Ã€ the maximal F-score of the foreground-restricted Rand index, a measure of similarity between two segmentations. Pixel error is defined as 1 Ã€ the maximal F-score of pixel similarity, or squared Euclidean distance between the original and the result labels. We note that these evaluation metrics have been modified by the challenge organizers after this manuscript has been submitted. We demonstrate most of the results and model comparisons on the validation data before we show the performance of the best model on the testing data. It is worth mentioning that the results based on the validation data set may be affected by the reduction in the training data set size. This is because we only trained on the 20 slices instead of all the 30 slices, since the remaining 10 slices are in the validation set. Nonetheless, the results obtained on the validation data set are very useful for model comparison.<ref type="figure" target="#tab_2">Table 2</ref>illustrates the comparison between the four networks evaluated on the validation data set. Although all the pixel error values are similar, the rand error gives a better interpretation in terms of segmentation. We observed that the post-processing reduced the rand error, but increased the pixel and warping errors. This is because the DNN is a pixel classifier aiming at optimizing the pixel error directly, and the warping error is closely related to the pixel error. Thus, we chose to report the pixel and warping errors before we applied the post-processing. After post-processing, the pixel and warping errors were highly altered in favor of obtaining a better segmentation. For that, the rand error values reported in<ref type="figure" target="#tab_2">Table 2</ref>are obtained after applying the postprocessing. Our results are clearly in favor of the deeper networks C and D, which highlights the impact of the depth on the outcome of the segmentation. Networks A and B used less number of trainable layers, and this hurt the performance regardless of the kernel sizes used or the model configuration. On the other hand, networks C and D used a large window size with a deeper architecture and relatively large kernel sizes. Network D in particular achieved the best performance among all the other networks in terms of rand index. This validates our hypothesis that starting with a large kernel size in the bottom layers indeed helps the model learn more useful features than starting with small kernels as in network C. We report the effectiveness of the techniques described in Sections 2.3 and 2.4 in<ref type="figure" target="#tab_3">Table 3</ref>. The comparison was made using our best performing network D on the validation data set. We noticed that both the data augmentation and rotations during testing are highly effective for improving the segmentation accuracy. The model suffered from severe overfitting without augmentation while the rotations applied during testing provided robustness to the classifier, thereby reducing the effect of input data randomization to a great extent. After selecting the best architecture and evaluating different techniques applied upon training and testing, we extended our experiments to the testing data. We trained network D on the 30 training slices and evaluated them using the online system for the ISBI 2012 Challenge. Note that, although winners have been declared before ISBI 2012, the challenge is still ongoing.<ref type="figure">Figure 2</ref>reports the final results on two slices of the testing stack. On the ISBI 2012 Challenge data set, our approach achieved the best results on all the three evaluation metrics, breaking the record maintained by other participants for years. A complete comparison between our approach and other competing methods is shown inBoth the pixel and warping errors are reported before applying the postprocessing while the rand error is reported after the post-processing.In comparison to the second best approach (<ref type="bibr" target="#b5">Ciresan et al., 2012a</ref>) which was also based on a DNN and uses the same postprocessing technique, our network is deeper and uses a highly optimized network architecture. The largest kernel size used in that network was 5 and the bottom layer starts with a kernel size of 4. They also implemented an actual ensemble of models where they trained several networks with different window sizes and then averaged their results. We believe that applying variations upon testing is much faster and more efficient. The larger kernels utilized in our architecture, the specific configuration of these kernels along with data augmentation techniques we applied are key factors in outperforming all the other competitive approaches in all metrics. We demonstrate the training time requirements for the different architectures in<ref type="figure" target="#tab_5">Table 5</ref>. The comparison is based on training on 20 slices of the training stack on a GPU machine. Network C was the most computationally expensive network with the training time reaching roughly 5 days as the number of parameters to train was much higher than the other architectures. However the testing time of all networks was very fast even on our own Matlab-based CPU code with an average of few seconds on network A to roughly 4 min on network C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Most recent results</head><p>Due to the ongoing nature of the ISBI 2012 challenge, the evaluation metrics along with the leading groups are continuously changing. During the review process of this work, the organizers have published new evaluation metrics, leading to new rankings in the leader board along with new participating teams. In this section, we list the new metrics along with the updated team rankings. The challenge organizers revealed that the old metrics were not sufficiently robust to variations in the widths of neurite borders (<ref type="bibr" target="#b0">Arganda-Carreras et al., 2015</ref>). They proposed two new metrics based on specially normalized versions of the rand error and variation of information, namely foreground-restricted rand scoring after border thinning (rand score thin) and foreground-restricted information theoretic scoring after border thinning (information score<ref type="figure">Fig. 2</ref>. Slices 1 and 12 from the testing stack are shown along with their segmentations in the top and bottom rows, respectively. The first column represents the raw input image, the second column represents the probability map output of the pixel classifier and the third column represents the final segmentation after the post-processingDeep models for brain EM image segmentationthin). The latest team rankings based on these two evaluation metrics are given in<ref type="figure" target="#tab_6">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The key contributions of our work are to provide novel insights on DNN for brain EM image segmentation and to provide recordbreaking results. Our network is deep, wide and carefully designed to achieve the full performance power of DNN. We achieved the best results in the challenge without having to combine several models together, which makes the approach even more efficient. We believe that pretraining of our model on other data sets like ImageNet and combining the results of several models would further improve the accuracy. In addition, our model can generalize easily to different EM segmentation tasks due to the nature of DNNs, which learn features from data. We are also extending this deep learning scheme on the Vaa3D (<ref type="bibr" target="#b23">Peng et al., 2010</ref>) platform for other neuron images in large brain projects such as the BigNeuron (<ref type="bibr" target="#b24">Peng et al., 2015</ref>).We list only the top-8 groups.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>Each of the networks from A to D is divided into three blocks ending with a max pooling layer each. The convolution layer parameters are illustrated as 'conv &lt;kernel size &gt;-&lt;number of kernels &gt;-&lt;padding size if any &gt;'. All the max pooling layers are 2 Ã‚ 2 with stride 2 while all the convolution layers are of stride 1. The RELU layers are omitted here for brevity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>et al. V C The Author 2016. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 2352 Bioinformatics, 32(15), 2016, 2352â€“2358 doi: 10.1093/bioinformatics/btw165 Advance Access Publication Date: 25 March 2016 Original Paper</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><figDesc>Table 1. The complete architecture of the four DNNs used in our experiments</figDesc><table>A 
B 
C 
D 

input 35 Ã‚ 35 
input 65 Ã‚ 65 
input 95 Ã‚ 95 
input 95 Ã‚ 95 
conv4-30 
conv10-30 
conv4-48 
conv8-48 
LRN 
LRN 
maxpool 
conv3-50-pad1 
conv5-50 
conv5-128 
conv6-128 
conv5-128 
conv6-128 
LRN 
LRN 
maxpool 
conv3-60-pad1 
conv3-60 
conv6-256 
conv4-256 
conv3-60-pad1 
conv3-60 
conv6-256 
conv3-256 
conv6-256 
conv3-256 
maxpool 
FC-100 
FC-100 
FC-500 
FC-500 
Drop-0.5 
Drop-0.5 
FC-2 
softmax 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>Table 4.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><figDesc>Table 2. Comparison between the four networks using the validation data set</figDesc><table>Network 
Rand 
error [0.10 23 ] 

Warping 
error [0.10 26 ] 

Pixel 
error [0.10 23 ] 

A 
82 
1548 
52 
B 
95 
1729 
50 
C 
75 
1775 
51 
D 
47 
1684 
49 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><figDesc>Table 3. Comparison between the results obtained using network D before and after using train data augmentation and test data augmentation</figDesc><table>Network 
Rand 
error [0.10 23 ] 

Warping 
error [0.10 26 ] 

Pixel 
error [0.10 23 ] 

D 
47 
1684 
49 
Dâ€”no training 
augmentation 

271 
2905 
61 

Dâ€”No testing 
augmentation 

212 
2176 
61 

These results were obtained by evaluation on the validation data set. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><figDesc>Table 4.</figDesc><table>Comparison between our method and the other compet-
ing techniques 

Group 
Rand 
error [0.10 23 ] 

Warping 
error [0.10 26 ] 

Pixel 
error [0.10 23 ] 

DIVE-SCI 
(Our method) 

17 
307 
58 

IDSIA-SCI 
18 
616 
102 
optree-idsia 
22 
807 
110 
motif 
26 
426 
62 
SCI 
28 
515 
63 
Image Analysis Lab 
Freiburg 

38 
352 
61 

Connectome 
45 
478 
62 
IDSIA-V 
46 
462 
61 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><figDesc>Table 5.</figDesc><table>Comparison of the training time and number of param-
eters for different architectures 

A 
B 
C 
D 

Training hours 
5 
30 
120 
104 
No. of parameters 
169,580 
196,100 
7,615,208 
5,719,016 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><figDesc>Table 6. The most recent leader board with the updated metrics that were published during the review process of this work</figDesc><table>Group 
Rand score thin Information score thin 

IAL LMC 
0.9803 
0.9883 
IAL MC 
0.9795 
0.9869 
CUMedVision 
0.9768 
0.9886 
CUMedVision-motif 
0.9765 
0.9883 
DIVE-SCI (Our method) 
0.9762 
0.9873 
r1q 
0.9747 
0.9848 
IDSIA-SCI 
0.9730 
0.9874 
Image Analysis Lab Freiburg 
0.9727 
0.9866 

</table></figure>

			<note place="foot">at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">A.Fakhry et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This work was supported in part by research grants from National Science Foundation (DBI-1147134 and DBI-1350258), Old Dominion University, and Washington State University. Conflict of Interest: none declared.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Crowdsourcing the creation of image segmentation algorithms for connectomics</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Arganda-Carreras</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brain Research through Advancing Innovative Neurotechnologies (BRAIN) Working Group Report to the Advisory Committee to the Director</title>
		<imprint>
			<publisher>SCIENTIFIC VISION</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Breiman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Maximin affinity learning of image segmentation</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Briggman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1865" to="1873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Wiring specificity in the direction-selectivity circuit of the retina</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">L</forename>
				<surname>Briggman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">471</biblScope>
			<biblScope unit="page" from="183" to="188" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">An integrated micro-and macroarchitectural analysis of the drosophila brain by computer-assisted serial section electron microscopy</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Cardona</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Biol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1000502</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep neural networks segment neuronal membranes in electron microscopy images</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Ciresan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2843" to="2851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<monogr>
		<title level="m" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Ciresan</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<monogr>
		<title level="m" type="main">Mitosis detection in breast cancer histology images with deep neural networks</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">C</forename>
				<surname>CiresÂ¸an</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="411" to="418" />
		</imprint>
	</monogr>
	<note>In. : MICCAI</note>
</biblStruct>

<biblStruct   xml:id="b8">
	<monogr>
		<title level="m" type="main">Imagenet: a large-scale hierarchical image database</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Deng</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR IEEE</publisher>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast image scanning with deep max-pooling convolutional neural networks</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Giusti</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv Preprint arXiv</title>
		<imprint>
			<biblScope unit="page" from="1302" to="1700" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Connectomic reconstruction of the inner plexiform layer in the mouse retina</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Helmstaedter</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="page" from="168" to="174" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<monogr>
		<title level="m" type="main">Segmentation of Neuronal Structures in EM Stacks challengeISBI</title>
		<author>
			<persName>
				<forename type="first">Isbi</forename>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<monogr>
		<title level="m" type="main">Natural image denoising with convolutional networks</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Jain</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Seung</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<monogr>
		<title level="m" type="main">Supervised learning of image restoration with convolutional networks</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Jain</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Ji</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">convolutional neural networks for human action recognition. PAMI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<monogr>
		<title level="m" type="main">Caffe: convolutional architecture for fast feature embedding. arXiv Preprint arXiv</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Jia</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1408" to="5093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Space-time wiring specificity supports direction selectivity in the retina</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">S</forename>
				<surname>Kim</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">509</biblScope>
			<biblScope unit="page" from="331" to="336" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Krizhevsky</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>In. :7 NIPS</note>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Lecun</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Lecun</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>CVPR IEEE</publisher>
			<biblScope unit="page">97</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<forename type="middle">A</forename>
				<surname>Lecun</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<editor>Montavon, G., Orr, G.B., MÃ¼ ller, K.-R.</editor>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="9" to="48" />
		</imprint>
	</monogr>
	<note>2nd. edn. Springer, Heidelberg</note>
</biblStruct>

<biblStruct   xml:id="b21">
	<monogr>
		<title level="m" type="main">Watershed merge tree classification for electron microscopy image segmentation</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Liu</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="133" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<monogr>
		<title level="m" type="main">Watershed merge forest classification for electron microscopy image stack segmentation</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Liu</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>NIH Public Access</publisher>
			<biblScope unit="page">4069</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">V3d enables real-time 3d visualization and quantitative analysis of large-scale biological image data sets</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Peng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="348" to="353" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Bigneuron: large-scale 3d neuron reconstruction from optical microscopy images</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Peng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="252" to="256" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Ronneberger</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Interventionâ€“MICCAI 2015</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. arXiv Preprint</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Simonyan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Zisserman</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions. arXiv Preprint arXiv</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Szegedy</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1409" />
			<biblScope unit="page">4842</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional networks can learn to generate affinity graphs for image segmentation</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">C</forename>
				<surname>Turaga</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="511" to="538" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">D</forename>
				<surname>Zeiler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Fergus</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for multi-modality isointense infant brain image segmentation</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="214" to="224" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>