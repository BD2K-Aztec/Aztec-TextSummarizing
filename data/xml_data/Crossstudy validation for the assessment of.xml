
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-study validation for the assessment of prediction algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014">2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Christoph</forename>
								<surname>Bernau</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department for Medical Informatics, Biometry and Epidemiology</orgName>
								<address>
									<settlement>Munich, Germany, Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Markus</forename>
								<surname>Riester</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department">Dana-Farber Cancer Institute</orgName>
								<address>
									<settlement>Boston</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Harvard School of Public Health</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Anne-Laure</forename>
								<surname>Boulesteix</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department for Medical Informatics, Biometry and Epidemiology</orgName>
								<address>
									<settlement>Munich, Germany, Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Giovanni</forename>
								<surname>Parmigiani</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department">Dana-Farber Cancer Institute</orgName>
								<address>
									<settlement>Boston</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Harvard School of Public Health</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Curtis</forename>
								<surname>Huttenhower</surname>
							</persName>
							<affiliation key="aff3">
								<orgName type="institution">Harvard School of Public Health</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName>
								<forename type="first">Levi</forename>
								<surname>Waldron</surname>
							</persName>
							<email>levi.waldron@hunter.cuny.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">City University of New York School of Public Health, Hunter College</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Lorenzo</forename>
								<surname>Trippa</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department">Dana-Farber Cancer Institute</orgName>
								<address>
									<settlement>Boston</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Harvard School of Public Health</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Leibniz Supercomputing Center</orgName>
								<address>
									<settlement>Garching</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-study validation for the assessment of prediction algorithms</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">30</biblScope>
							<biblScope unit="page" from="105" to="112"/>
							<date type="published" when="2014">2014</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btu279</idno>
					<note>BIOINFORMATICS Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Numerous competing algorithms for prediction in high-dimensional settings have been developed in the statistical and machine learning literature. Learning algorithms and the prediction models they generate are typically evaluated on the basis of cross-validation error estimates in a few exemplary datasets. However, in most applications, the ultimate goal of prediction modeling is to provide accurate predictions for independent samples obtained in different settings. Cross-validation within exemplary datasets may not adequately reflect performance in the broader application context. Methods: We develop and implement a systematic approach to &apos;cross-study validation&apos;, to replace or supplement conventional cross-validation when evaluating high-dimensional prediction models in independent datasets. We illustrate it via simulations and in a collection of eight estrogen-receptor positive breast cancer microarray gene-expression datasets, where the objective is predicting distant metastasis-free survival (DMFS). We computed the C-index for all pair-wise combinations of training and validation datasets. We evaluate several alternatives for summarizing the pairwise validation statistics, and compare these to conventional cross-validation. Results: Our data-driven simulations and our application to survival prediction with eight breast cancer microarray datasets, suggest that standard cross-validation produces inflated discrimination accuracy for all algorithms considered, when compared to cross-study validation. Furthermore, the ranking of learning algorithms differs, suggesting that algorithms performing best in cross-validation may be suboptimal when evaluated through independent validation. Availability: The survHD: Survival in High Dimensions package (http:// www.bitbucket.org/lwaldron/survhd) will be made available through Bioconductor. Contact:</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Cross-validation and related resampling methods are de facto standard for ranking supervised learning algorithms. They allow estimation of prediction accuracy using subsets of data that have not been used to train the algorithms. This avoids over-optimistic accuracy estimates caused by 're-substitution'.</p><p>This characteristic has been carefully discussed in<ref type="bibr" target="#b26">Molinaro et al. (2005</ref><ref type="bibr" target="#b0">), Baek et al. (2009</ref><ref type="bibr" target="#b31">) and Simon et al. (2011</ref>. It is common to evaluate algorithms by estimating prediction accuracy via cross-validation for several datasets, with results summarized across datasets to rank algorithms (<ref type="bibr" target="#b5">Boulesteix, 2013;</ref><ref type="bibr" target="#b9">Dem sar, 2006</ref>). This approach recognizes possible variations in the relative performances of learning algorithms across studies or fields of application. However, it is not fully consistent with the ultimate goal, in the development of models with biomedical applications, of providing accurate predictions for fully independent samples, originating from institutions and processed by laboratories that did not generate the training datasets. It has been observed that accuracy estimates of genomic prediction models based on independent validation data are often substantially inferior to cross-validation estimates (<ref type="bibr" target="#b7">Castaldi et al., 2011</ref>). In some cases this has been attributed to incorrect application of cross-validation; however even strictly performed crossvalidation may not avoid over-optimism resulting from potentially unknown sources of heterogeneity across datasets. These include differences in design, acquisition and ascertainment strategies (<ref type="bibr" target="#b30">Simon et al., 2009</ref>), hidden biases, technologies used for measurements, and populations studied. In addition, many genomics studies are affected by experimental batch effects (<ref type="bibr" target="#b1">Baggerly et al., 2008;</ref><ref type="bibr" target="#b20">Leek et al., 2010</ref>). Quantifying these heterogeneities and describing their impact on the performance of prediction algorithms is critical in the practical implementation of personalized medicine procedures that use genomic information. There are potentially conflicting, but valid, perspectives on what constitutes a good learning algorithm. The first perspective is that a good learning algorithm should perform well when trained and applied to a single population and experimental setting, but it is not expected to perform well when the resulting model is applied to different populations and settings. We call such an algorithm specialist, in the sense that it can adapt and specialize to the population at hand. This is the mainstream perspective for assessing prediction algorithms and is consistent with validation procedures performed within studies (<ref type="bibr" target="#b0">Baek et al., 2009;</ref><ref type="bibr" target="#b26">Molinaro et al., 2005;</ref><ref type="bibr" target="#b31">Simon et al., 2011</ref>). However, we argue that it does not reflect the reality that 'samples of convenience' and uncontrolled specimen collection are the norm in genomic biomarker studies (<ref type="bibr" target="#b30">Simon et al., 2009</ref>). We promote another perspective: a good learning algorithm should be generalist, in the sense that it yields models that may be suboptimal for the training population, or not fully representative of the dataset at hand, but that perform reasonably well *To whom correspondence should be addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>y</head><p>The authors wish it to be known that, in their opinion, the last two authors should be regarded as Joint Last Authors. across different populations and laboratories employing comparable but not identical methods. Generalist algorithms may be preferable in important settings, for instance when a researcher develops a model using samples from a highly controlled environment, but hopes the model to be applicable to other hospitals, labs, or more heterogeneous populations. In this article we systematically use independent validations for the comparison of learning algorithms, in the context of microarray data for disease-free survival of estrogen receptorpositive breast cancer patients. Although concern has been often expressed about the lack of independent validation of genomic prediction models (<ref type="bibr" target="#b21">Micheel et al., 2012;</ref><ref type="bibr" target="#b33">Subramanian and Simon, 2010</ref>), independent validation has not been systematically adopted in the comparison of learning algorithms. This deficiency cannot be addressed for prediction contexts where related, independent datasets are unavailable. For many cancer types, however, several micro-array studies have been performed to develop prognostic models. These datasets pave the way for a systematic approach based on independent validations. For instance, a recent meta-analysis of prognostic models for late-stage ovarian cancer provides a comparison of publicly available microarray datasets (<ref type="bibr" target="#b39">Waldron et al., 2014</ref>). Furthermore,<ref type="bibr" target="#b27">Riester et al. (2014)</ref>showed that combining training datasets can increase the accuracy of late-stage ovarian cancer risk models. Thus situations exist in genomic data analysis where comparable, independent datasets are available, and these present an opportunity to use independent validation as an explicit basis for assessing learning algorithms. We propose what we term 'leave-one-dataset-in' cross-study validation (CSV) to formalize the use of independent validations in the evaluation of learning algorithms. Through data-driven simulations, and an example involving eight publicly available estrogen receptor-positive breast cancer microarray datasets, we assess established survival prediction algorithms using our 'leaveone-dataset-in' scheme and compare it to conventional crossvalidation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notation and setting</head><p>&gt;We consider multiple datasets i = 1,. .. , I with sample sizes N 1 ,. .. , N I. Each observation s appears only in one dataset i, i.e. datasets do not overlap, and the corresponding record includes a primary outcome Y s i and a vector of predictor variables X s i ; throughout this article X s i will be geneexpression measurements. Our goal is to compare the performance of different learning algorithms k = 1,. .. , K that generates prediction models for the primary outcome using the vector of predictors. Throughout this article, the primary outcome is a possibly censored survival time. We are interested in evaluating and ranking competing prediction methods k = 1,. .. , K. Since the ranking may depend on the application, the first step is to define the prediction task of interest. We focus on the prediction of metastasis-free survival time in breast cancer patients based on high-throughput gene-expression measurements. Our approach and the concept of CSV, however, can be applied to different types of response variables and any other prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Algorithms</head><p>We assess six learning algorithms (k = 1,. .. , 6) appropriate for highdimensional continuous predictors and possibly censored time-to-event outcomes: Lasso and Ridge regression (<ref type="bibr" target="#b15">Goeman, 2010</ref>), CoxBoost (<ref type="bibr" target="#b3">Binder and Schumacher, 2008</ref>), SuperPC (<ref type="bibr" target="#b4">Blair and Tibshirani, 2004</ref>), Unicox (<ref type="bibr" target="#b36">Tibshirani, 2009</ref>) and Plusminus (<ref type="bibr" target="#b40">Zhao et al., 2013</ref>). All parameters were tuned either by default methods included in their implementation (Ridge and Lasso regression: R-package glmnet) or by testing a range of parameters in internal cross-validation. Our focus is not to provide a comprehensive array of algorithms, but simply to use a few popular, representative algorithms to investigate CSV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">CSV matrices</head><p>We refer in this article to m-fold cross-validation and related resampling methods collectively as cross-validation (CV). Our ranking procedure for learning algorithms is based on a square matrix Z k of scores (k = 1,. .. , K). The (i, j) element in the matrix measures how well the model produced by algorithm k trained on dataset i performs when validated on dataset j. Since we consider K methods we end up with K method-specific square matrices Z 1 ;. .. ; Z K : We set the diagonal entries of the matrices equal to performance estimates obtained with 4-fold CV in each dataset. We will call Z k the CSV matrix. Possible definitions for the Z k i;j scores include the concordance index in survival analysis (<ref type="bibr" target="#b18">Harrell et al., 1996</ref>), the area under the operating characteristic curve in binary classification problems, or the mean squared distance between predicted and observed values in regression problems. We use survival models and focus on a concordance index, the Cindex, which is a correlation measure (<ref type="bibr" target="#b16">Gnen and Heller, 2005</ref>) between survival times and the risk scores, such as linear combinations of the predictors, provided by a prediction model. The heatmap in<ref type="figure" target="#fig_0">Figure 1A</ref>displays the CSV matrix of C-statistics obtained through validation of eight models trained on the studies in<ref type="figure" target="#tab_1">Table 1</ref>with Ridge regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Summarization of a CSV matrix</head><p>In order to rank learning algorithms k = 1,. .. , K, we summarize each matrix Z k by a single score. We consider following two candidate approaches.</p><p>(1) The Simple Average of all non-diagonal elements of the Z k matrix:</p><formula>CSV= X i X i6 ¼j Z k i;j IðI À 1Þ :</formula><p>(2) The Median or more generally a quantile of the non-diagonal entries of Z k. Quantiles offer robustness to outlier values, and the possibility to reduce the influence of those studies that are consistently associated with poor validation scores, both when used for training and validation, and independently of the learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">True global ranking</head><p>Throughout our analyses the score Z k i;j is a random variable. First, studies i and j can be seen as randomly drawn from a population of studies. Second, observations within each study can be considered as randomly drawn from the unknown and possibly different distributions F i and F j underlying studies i and j. With this view of Z k i;j as random variable, we consider the theoretical counterparts of the empirical aggregating scores (simple average and quantiles) described in Section 2.4 to summarize Z k. The theoretical counterparts are the expected value or quantiles of each Z k i;j score, i 6 ¼ j; obtained by integrating the two levels of randomness that we described. The true global ranking of the learning algorithms k = 1,. .. , K is then defined by these expected values (or quantiles), one for each algorithm. We will call the ranking global because it depends on the super-population (<ref type="bibr" target="#b19">Hartley and Sielken, 1975</ref>) and not which populations were sampled by the available datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>i106</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.Bernau et al.</head><p>The true global ranking can be considered as the estimation target of evaluation procedures such as CV or CSV. In Section 2.7 we present the design of a data-driven simulation study in which we can compute the true ranking through Monte Carlo integration. This allows us to evaluate and compare the ability of CV and CSV to recover the true global ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Datasets</head><p>We used a compendium of breast cancer microarray studies curated for the meta-analysis of Haibe<ref type="bibr" target="#b17">Kains et al. (2012)</ref>and available as supplement to their article. We selected all eight datasets (<ref type="figure" target="#tab_1">Table 1</ref>) for which distant metastasis-free survival (DMFS), the most commonly available time to event endpoint, as well as Estrogen Receptor (ER) status, were available. These studies were generated with Affymetrix HGU GeneChips HG-U133A, HG-U133B and HG-U133PLUS2. We considered exclusively ER-positive tumors. Of these datasets, only one originated from a population-based cohort (<ref type="bibr" target="#b29">Schmidt et al., 2008</ref>). Four studies considered only patients who did not receive hormone therapy or adjuvant chemotherapy. Only four provided date ranges of patient recruitment (<ref type="bibr" target="#b8">Chin et al. 2006;</ref><ref type="bibr" target="#b10">Desmedt et al., 2007;</ref><ref type="bibr" target="#b12">Foekens et al., 2006;</ref><ref type="bibr" target="#b29">Schmidt et al., 2008</ref>).<ref type="figure" target="#tab_1">Table 1</ref>points also to important differences in survival (for instance 3Q survival) that are not easily explicable based on known characteristics of these studies. This variability in design strategies, reporting, as well as outcomes, highlights the prevalence of 'samples of convenience' in biomarker studies discussed by<ref type="bibr" target="#b30">Simon et al. (2009)</ref>. Samples from dataset ST1 duplicated in dataset VDX were removed. Expression of each gene was summarized using the probeset with A B CThe heatmap for each pair of studies (i, j), the average C-index when we fit Ridge regression on a simulated dataset generated by resampling gene expression data and censored time to event outcomes from the i-th study in<ref type="figure" target="#tab_1">Table 1</ref>, and validate the resulting model on a simulated dataset generated by resampling study j. Computation of each diagonal element averages over pairs of independent datasets obtained by resampling from the same study. The heatmaps strongly resemble each other. CAL and MSK are outlier studies: cross-study C-index is $0.5 when they are used either for training or validation. The values of the arrays in (A) and (B) that involve these two studies constitute the blue 'bad performance' cluster in (C) which contrast the C-indices obtained for study pairs ði; jÞ; i 6 ¼ j, on simulated data (y-axis) and experimental data (x-axis). Pearson correlation is $0.9. The three plots illustrate similarity between our simulation model and the actual datasets in<ref type="figure" target="#tab_1">Table 1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Simulation design</head><p>We simulate heterogeneous datasets with gene-expression profiles and time to event outcomes from a joint probability model. We define the model through a resampling procedure that we apply to the eight breast cancer datasets in<ref type="figure" target="#tab_1">Table 1</ref>. The resampling scheme is a combination of parametric and nonparametric bootstrap (<ref type="bibr" target="#b11">Efron and Tibshirani, 1993</ref>). The goal of our simulation study is to compare CV and CSV when used for ranking and evaluation of competing learning algorithms. Here we use resampling methods to iteratively simulate realistic ensembles of breast cancer datasets from a hierarchical probability model that we define using the actual datasets in<ref type="figure" target="#tab_1">Table 1</ref>. CV and CSV are then assessed with respect to their ability to recover the true global ranking, which we compute through Monte-Carlo integration. We will quantify the ability to recover the ranking by using the Kendall correlation between the true global ranking and the estimates obtained with CV or CSV. For b = 1,. .. , B = 1000 iterations, we generate a collection of I = 8 datasets as follows. First, we sample eight study labels with replacement from the list of breast cancer studies in<ref type="figure" target="#tab_1">Table 1</ref>. This step only involves simulations from a multinomial Mult(8,[1/8,. .. ,<ref type="bibr">1/8]</ref>) distribution. We resample the collection of study labels to capture variability in study availability, and heterogeneity of study characteristics. Second, for each of the eight randomly drawn labels, we sample N = 150 patients from the corresponding original dataset, with replacement. If a study is randomly assigned to the j-th label, then each of the N = 150 vectors of predictive variables is directly sampped from the empirical distribution of the j-th study in<ref type="figure" target="#tab_1">Table 1</ref>. Finally, we simulate the corresponding times to event using a proportional hazards model (parametric bootstrap) fitted to the j-th dataset: M j true : j tjx ð Þ= j 0 t ð Þ Â exp x T j À Á ; ð1Þ where j (tjx) is the individual hazard function when the vector of predictors is equal to x and j denotes a vector of regression coefficients. We combine the truncated inversion method in<ref type="bibr" target="#b2">Bender et al. (2005)</ref>and the Nelson–Aalen estimator for cumulative hazard functions to simulate survival times that reflect survival distributions and follow-up of the real studies. We set the vector j to be the coefficients fitted in study j = 1,. .. , I using the CoxBoost method (<ref type="bibr" target="#b3">Binder and Schumacher, 2008</ref>). A different regression method could have been used at this stage. The collections of simulated datasets are then used both (i) to compute by Monte-Carlo method the true global ranking defined in Section 2.5, and (ii) to compute ranking estimates through CV and CSV.<ref type="figure" target="#fig_0">Figure 1A</ref>displays, for each pair of studies (i, j) in<ref type="figure" target="#tab_1">Table 1</ref>, the C-index obtained when training a model by Ridge regression on dataset i (rows), and validating that model on dataset j (columns). We computed the diagonal elements (i = j) by 4-fold CV.<ref type="figure" target="#fig_0">Figure 1B</ref>displays mean C-indices for each (i, j) combination across simulations, when the training and validation studies are generated resampling the i-th and j-th study. Here diagonal elements are computed by averaging C-indices with the training and validation datasets independently generated by resampling from the same study. The strong similarity between the two panels is reassuring, in particular with regard to the clear separation of the eight studies into two groups. The first group includes studies MNZ, ST1, ST2, TRP, UNT and VDX, and produces more accurate prediction models than the remaining studies. The datasets in this group are also associated with higher values of the concordance index when used for validation. This difference between the two groups is also illustrated in<ref type="figure" target="#fig_0">Figure 1C</ref>. It displays the non-diagonal entries of the matrices represented in the left and middle panels, that is the average C-indices from simulated datasets, against the C-indices from real data. This scatterplot shows a clear two-cluster structure: the yellow dots display the 30 training and validation combinations within studies MNZ, ST1, ST2, TRP, UNT and VDX. We will return to this cluster structure in the discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Evaluation criteria for simulations</head><p>In simulation studies we can assess and rank algorithms based on their ability to recover the true underlying models M i true ; i=1;. .. ; I: In this subsection, we introduce a criterion that reflects the degree of similarity between the true regression coefficients i that were used to simulate the i-th in silico dataset and the coefficients b ðkÞ j fitted through algorithm k on the j-th simulated dataset. We consider the i = j and i 6 ¼ j cases separately. Similarity between vectors is usually quantified by computing the Euclidean distance between them. However, since our focus is on prediction, we use c corðX i i ; X i b ðkÞ j Þ; the correlation between true and estimated patientspecific prognostic scores, to measure the similarity between the true i and estimated regression coefficients b ðkÞ j : Here X i is the matrix of predictors of dataset i and c cor denotes Pearson's correlation. The average</p><formula>S k self = 1=I ð ÞÁ X i c cor X i i ; X i b k ð Þ i ; ð2Þ</formula><p>over the I studies, provides a measure of the ability of learning algorithm k to recover the model that has generated the training dataset, hence the index self. Another criterion of interest is the ability of a learning algorithm k to recover the vector of regression coefficients i when it is trained on a separate dateset j 6 ¼ i and the unknown models underlying datasets i and j might differ from each other. This can be quantified with</p><formula>S k across = 1= I I À 1 ð Þ ð Þ ð Þ Á X i X j6 ¼i c cor X i i ; X i b k ð Þ j ; ð3Þ</formula><p>where the index across emphasizes the focus on cross-study similarity, i.e. on the ability of algorithm k to recover the coefficients i when fitted on dataset j, with j 6 ¼ i: In alternative to averaging across studies, or pairs of datasets, as in Equations (2–3) one can also use different summaries, e.g. quantiles, as we do in Section 2.4. Both S k self and S k across are summary statistics to assess and compare learning algorithms. We denote the ranking obtained by ordering the algorithms according to S self (S across ) by R self (R across ). Both S k self and S k across vary across simulations of the datasets ensembles, although the hierarchical simulation model remains fixed and their computations involve the vectors i , i = 1,. .. , I. We will therefore call the rankings R self and R across local because they are specific to the collection of datasets at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Simulated data</head><p>Our focus in the simulation study is on differences between the rankings and performance estimates obtained by using CV and CSV. We will use CV and CSV to denote the means of the diagonal and non-diagonal elements of a CSV matrix, respectively. Recall that we compute the diagonal elements through CV.<ref type="figure" target="#fig_1">Figure 2A</ref>shows, for K = 6 algorithms, the distributions of CSV and CV; and<ref type="figure" target="#fig_1">Figure 2B</ref>shows the distribution of the rankings estimates, across 1000 simulated collections of eight datasets.<ref type="figure" target="#tab_2">Table 2</ref>compares the medians of the distributions in<ref type="figure" target="#fig_1">Figure 2B</ref>with the true global rankings that we obtained using the criteria in Section 2.4. The rank of method k is 1 if it i108</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.Bernau et al.</head><p>outperforms the remaining K – 1 training algorithms. We observe large differences in the distributions of CSV and CV across simulations (<ref type="figure" target="#fig_0">Fig. 1A</ref>): the average of the CV scores, under all the algorithms we considered, is close to 0.65, while the CSV scores are centered at $0.55. The variability of CV and CSV across simulations, however, is comparable. Performance differences across algorithms, whether estimated by CV or CSV, are relatively small compared to the overall difference between CV and CSV performance estimates. We also observe differences between the rank distributions produced by CV and CSV. Accordingly, to both CV and CSV, in most of the simulations, Lasso regression is ranked as one of the worst performing algorithms, while Ridge regression and Plusminus are ranked first or second. However, the CV summaries suggest an advantage of Ridge regression over Plusminus across most of the simulations while CSV rank Plusminus as the best performing algorithm in $50% of the simulations. The median rank of CoxBoost across simulations has an improvement of two positions when it is estimated through CV and compared to the CSV summaries; in this case CSV results are more consistent with the true global rankings (<ref type="figure" target="#tab_2">Table 2</ref>). When we consider the criteria described in Section 2.4, Ridge regression and Plusminus exchange the top-two positions of the true global rankings (see<ref type="figure" target="#tab_2">Table 2</ref>), although for these two algorithms the Z i,j distributions under our simulation scenario are nearly identical. The local rankings R across and R self of the K = 6 algorithms defined by S k across and S k self in Section 2.8 vary across the 1000 simulated collections of studies. The median Kendall's correlation between R across and R self across simulations is $0.5, i.e. the performance measures S k across and S k self tend to define distinct rankings of the competing algorithms, see also the Supplementary<ref type="figure" target="#fig_0">Figure S1</ref>. We illustrate the extent to which CV and CSV recover the unknown rankings R across and R self. The boxplots in<ref type="figure">Figure 3</ref>display the Kendall's correlation between local rankings (i) R across or(ii) R self , and the rankings estimated through CV (gray boxes) and CSV (white boxes) across simulations.<ref type="figure">Figure 3C</ref>shows the Kendall's correlation between the true global ranking and the ranking estimates. The median Kendall's correlation between R self and the corresponding CSV estimates across simulations is %0.5. The CV ranking estimates tend to be less correlated with the local rankings R across than the CSV estimates. In contrast, the CV estimates tend to be more correlated with R self than the CSV estimates. We recall that both CV and R self are defined summarizing performance measures, Z k i;i and c corðX i i ; X i b ðkÞ i Þ; that refer to a single study, while CSV and R across summarizes performance measures computed using two distinct studies that are used for training and validation. Finally, CSV tends to be more correlated with the true global ranking than CV. This suggests that CSV is more suitable for recovering the true global ranking. When we removed the two outlier studies (CAL and MSK) and repeated the simulation study, the advantage of CSV over CV in recovering the true global ranking was confirmed (median Kendall's correlation 0.8 versus 0.6, see also Supplementary Figs S2–S4), moreover after their removal Kendall's correlations between R self and the CSV estimates tend to be larger than those between R self and the CV estimates. Overall, as displayed by the Supplementary<ref type="figure">Figure  S3</ref>, it appears that, after outlier studies are removed, CSV outperforms substantially CV when used for ranking algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Application to breast cancer prognostic modeling</head><p>We apply CV and CSV to the I = 8 breast cancer studies described in Section 2. Generally, the results resemble those obtained on simulated data. The top panel in<ref type="figure">Figure 4</ref>Median estimates across 1000 simulations are displayed for CV and CSV; individual columns refer to summarization of the Z k i;j statistics by using the mean or the median as discussed in Section 2.4. We also computed the true global ranking as well as CV and CSV estimates by using the third quartile of the Z k i;j summaries, and obtained results identical to those displayed for the rankings obtained by summarizing validation results through their median. Both CV and CSV tend to rank Ridge regression and Plusminus as best performing algorithms. Variability of CV and CSV rank estimates across simulations is shown inCV estimates are $0.06 higher than CSV estimates on the C-index scale. To interpret the magnitude of this shift on the C-index scale consider a population with two groups of patients, high and low risk patients, covering identical proportions 0.5 of the population. A perfect discrimination model that correctly recognizes the subpopulation of each individual, when the hazard ratio between high versus low risk patients is 2.7, achieves on average a C-index of 0.62. It is necessary to double the hazard ratio to 5.4 to increase the average C-index of the perfect discrimination model to 0.68. Thus, it is fair to say that the CV results are considerably more optimistic than the CSV estimates. The ranking defined by CSV, using median summaries of the Z k i;j scores, is nearly identical to the global ranking in our simulation example (see Supplementary Table ST1 and<ref type="figure" target="#tab_2">Table 2</ref>). With both, median and third quartile aggregation of the Z k i;j statistics, the rankings defined by CV and CSV differ substantially (Kendall's correlations 0.6 and 0.07). This is consistent with the results of the simulation study, where median correlation of the rankings estimated through CSV and CV was $0.4 (see Supplementary<ref type="figure" target="#fig_0">Fig. S1</ref>). The presence of outlier studies (CAL and MSK) has a strong effect on the ranking estimates when we use the mean to summarize the Z k matrices. After aggregating the validation statistics by averaging, both CSV and CV rank Superpc first. This result might be due to the high variability, $0.5, of the Z k i;j validation scores corresponding to models trained by outlier studies. In particular, Superpc and Unicox are the only algorithms that produce models with substantial prediction performances when trained on the MSK study. With median summarization the ranking estimates are less influenced by the presence or absence of outlier studies. We therefore recommend the use of the median to summarize Z k matrices.</p><formula>ðI À 1Þ À1 Z k j;i with Z k i;i :</formula><p>The CV performance statistics Z k i;i are only moderately correlated with the CSV statistics X j6 ¼i ðI À 1Þ À1 Z k i;j (correlation = 0.2), and negatively correlated with the CSV summaries X j6 ¼i ðI À 1Þ À1 Z k j;i (correlation = –0.33).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CV and CSV summaries</head><p>Correlation between CSV and CV summary statistics, as displayed in<ref type="figure">Figure 4B</ref>, suggests that cross-and within-study performances are less redundant than one might expect. In<ref type="figure">Figure 4B</ref><ref type="figure" target="#tab_1">Table 1</ref>(outliers CAL and MSK were removed). Cross-validation statistics on the y-axis are moderately correlated to the CSV summaries on the x-axis; identical considerations hold for all K = 6 algorithms that we used A B C<ref type="figure">Fig. 3</ref>. Kendall's correlation between true global or local rankings and estimates obtained with CSV (white box-plots) or CV (cross-validation, gray box-plots) across simulations. Panels (A) and (B) compare CV and CSV in terms of their correlation to the local rankings (R across and R self ), while panel (C) considers the true global ranking. Each box-plot represents a correlation coefficient that was computed in each of the 1000 iterations of our simulation study. CSV tend to achieve a higher correlation with the global ranking and R across than CV. The results displayed have been computed using the mean criterion discussed in Section 2.4 i110</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.Bernau et al.</head><p>Z-matrix column-and row-wise. In the column-wise case correlations, between CSV and CV summaries, vary across algorithms $0.5, while in the row-wise case all the correlations are negative. Overall, we can consider cross-and within-study prediction as two related but distinct problems. We also noted that CV is less suitable for detection of outlier studies than CSV; in particular CV can estimate encouraging prediction performances even on studies associated, under each training algorithm, with poor CSV summaries Z k i;i : For instance, with the SuperPC algorithm all but one C-index estimates obtained with CV are above 0.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Specialist and generalist algorithms</head><p>Our analyses lead to the question of whether some algorithms can be considered as generalist or specialist procedures according to our definitions. Our examples are not exhaustive and additional comparisons, within the development of new prognostic models, are necessary in order to determine 'specialist' or 'generalist' tendencies of these algorithms. However, the fact that Ridge regression, Lasso regression and CoxBoost are ranked distinctly better accordingly to CV than CSV, in most iterations of our simulation study, suggests that these algorithms might be specialist procedures and adapt to the specific properties of the individual dataset. The status of generalist versus specialist, for each algorithm, can be discussed using the local performance criteria S self and S across , which are conceived to measure within-single-studies and generalizable prediction performances. We note that CoxBoost and Ridge regression tend to achieve better ranks in R self than in R across. In particular CoxBoost improves its position by 1 or 2 ranks in most simulations, which is similar to what we observed comparing CoxBoost's CSV and CV rankings. In summary, in our study, these two algorithms seem to have—accordingly to all the criteria that we considered—a tendency to specialize to the dataset at hand. We mention that, as one can expect, for all the algorithms S self is consistently higher than S across. We also compared CV to independent within-study validation using our simulation model. For the independent within-study validation, we iteratively pair two datasets generated using identical regression coefficients and gene expression distributions. Subsequently, we train a model on the first dataset and evaluate it on the second one. As can be seen in Supplementary<ref type="figure">Figure S5</ref>, CV values, as expected, are slightly smaller than for the independent within-study validations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION AND CONCLUSION</head><p>In applying genomics to clinical problems, it is rarely safe to assume that the studies in a research environment faithfully represent what will be encountered in clinical application, across a variety of populations and medical environments. From this standpoint, study heterogeneity can be a strength, as it allows to quantify the degree of generalizability of results, and to investigate the sources of the heterogeneity. This aspect has long been recognized in meta-analysis of clinical trials (<ref type="bibr" target="#b25">Moher and Olkin, 1995</ref>). Therefore, we expect that an increased focus on quantifying cross-study performance of prediction algorithms will contribute to the successful implementation of the personalized medicine paradigm. In this article we provide a conceptual framework, statistical approaches and software tools for this quantification. The conceptual framework is based on the long-standing idea that finite populations of interest can be viewed as samples from an infinite 'super-population' (<ref type="bibr" target="#b19">Hartley and Sielken, 1975</ref>). This concept is especially relevant for heterogeneous clinical studies originating from hospitals that sample local populations, but where researchers hope to make generalizations to other populations. As an illustrating example, we demonstrate CSV on eight independent microarray studies of ER-positive breast cancer, with metastasis-free survival as the endpoint of interest. We also develop a simulation procedure involving two levels of nonparametric bootstrap (sampling of studies and sampling of observations within studies) in combination with parametric bootstrap, to simulate a compendium of independent datasets with characteristics of predictor variables, censoring, baseline hazards, prediction accuracy and between-dataset heterogeneity realistically based on available experimental datasets. Cross-validation is the dominant paradigm for assessment of prediction performance and comparison of prediction algorithms. The perils of inflated prediction-accuracy estimations by incorrectly or incompletely performed cross-validation are well known (<ref type="bibr" target="#b26">Molinaro et al., 2005;</ref><ref type="bibr" target="#b33">Subramanian and Simon, 2010;</ref><ref type="bibr" target="#b31">Simon et al., 2011;</ref><ref type="bibr" target="#b37">Varma and Simon, 2006</ref>). However, we show that even strictly performed cross-validation can provide optimistic estimates relative to CSV performance. All algorithms, in simulation and example, showed distinctly decreased performance in CSV compared to cross-validation. Although it would be possible to further reduce between-study heterogeneity, for example by stricter filtering on clinical prognostic factors, we believe this degree of heterogeneity reflects the reality of clinical genomic studies and likely other applications. Some sources of biological heterogeneity are unknown, and it is impossible to ensure consistent application of new technologies in laboratory settings. Prediction models are used in presence of unknown sources of variation. Formal CSV provides a means to assess the impact of unknown or unobserved confounders that vary across studies. In simulations, the ranking of algorithms by CSV was closer to the true rankings defined by cross-study prediction, both when we considered R across and the global true ranking. Surprisingly, CSV was also competitive with CV for recovering true rankings based on within-study prediction, such as R self. Although the performance differences we observed between algorithms were smaller than the difference between CV and CSV, Lasso consistently compared poorly with most of the competing algorithms, both under CV and CSV evaluations. Lasso, and other algorithms that ensure sparsity have been shown to guarantee poor prediction performances in previous comparative studies (<ref type="bibr" target="#b6">Bøvelstad et al., 2007;</ref><ref type="bibr" target="#b38">Waldron et al., 2011</ref>). Systematic CSV provides a means to identify relevant sources of heterogeneity within the context of the prediction problem of interest. By simple inspection of the CSV matrix we identified two outlier studies that yielded prediction models no better than random guessing in new studies. This may be related to known differences in these studies: smaller numbers of observations, higher proportions of node positive patients, different treatments and larger tumors (Supplementary Figs S6–S9). Conversely, other known between-study differences do not seem to have created outlier studies or clusters of studies as seen in the Z i111 CSV for the assessment of prediction algorithms matrix, such as between studies where all or no patients received hormonal treatment. We note that incorporation of clinical prognostic factors into genomic prognostic models could likely produce gains in CSV accuracy, and that such multi-factor prognostic models could also be assessed by the proposed matrix of CSV statistics. In practice it is neither possible nor desirable to eliminate all sources of heterogeneity between studies and between patient populations. The adoption of 'leave-one-in' CSV, in settings where at least two comparable independent datasets are available, can provide more realistic expectations of future prediction model performance, identify outlying studies or clusters of studies, and help to develop 'generalist' prediction algorithms which will hopefully be less prone to fit to dataset-specific characteristics. Further work is needed to formalize the identification of clusters of comparable studies, to develop databases for large-scale cross-study assessment of prediction algorithms, and to develop better 'generalist' prediction algorithms. Appropriate curated genomic data resources are available in Bioconductor (<ref type="bibr" target="#b14">Gentleman et al., 2004</ref>) through the curatedCRCData, curatedBladderData and curatedOvarianData (<ref type="bibr" target="#b13">Ganzfried et al., 2013</ref>) packages, and in other common cancer types through InSilicoDB (<ref type="bibr" target="#b35">Taminau et al., 2011</ref>). In realms where such curated resources are available, CSV is in practice no more difficult or CPU-consuming than cross-validation, and should become an equally standard tool for assessment of prediction models and algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. CSV matrices Z k in simulated and experimental data for Ridge regression. (A) C-indices for training and validation on each pair of actual datasets in Table 1. The diagonal of this matrix shows estimates obtained through 4-fold CV. (B) The heatmap for each pair of studies (i, j), the average C-index when we fit Ridge regression on a simulated dataset generated by resampling gene expression data and censored time to event outcomes from the i-th study in Table 1, and validate the resulting model on a simulated dataset generated by resampling study j. Computation of each diagonal element averages over pairs of independent datasets obtained by resampling from the same study. The heatmaps strongly resemble each other. CAL and MSK are outlier studies: cross-study C-index is $0.5 when they are used either for training or validation. The values of the arrays in (A) and (B) that involve these two studies constitute the blue 'bad performance' cluster in (C) which contrast the C-indices obtained for study pairs ði; jÞ; i 6 ¼ j, on simulated data (y-axis) and experimental data (x-axis). Pearson correlation is $0.9. The three plots illustrate similarity between our simulation model and the actual datasets in Table 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.2.</head><figDesc>Fig. 2. Comparison of CSV and CV on simulated data. Each panel represents evaluations of K = 6 algorithms across 1000 simulations of a compendium of I = 8 datasets. For each simulation the diagonal or off-diagonal elements of the Z k matrix of validation C-statistics is summarized by (A) mean and (B) rank of the mean across algorithms. CV estimates tend to be much higher than the CSV estimates. In most of the simulations Lasso is ranked as one of the worst algorithms, both by CV and CSV, while Ridge and Plusminus are ranked among the best prediction methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure2B.</head><figDesc>i109 CSV for the assessment of prediction algorithms represents validation scores within a single Z k-matrix, whereas in Figure 2 each box-plot displays a summary of 1000 Z k matrices, one for each simulation. This explains the higher variance observed in Figure 4. We also observe the following.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure</head><figDesc>Figure 4B illustrates lack of agreement between CSV and CV performance estimates. The black digits contrast, for each dataset i, the CSV summary X j6 ¼i ðI À 1Þ À1 Z k i;j versus the CV summary Z k i;i : Performance measures refer to Ridge regression. Similarly, the gray digits in this panel contrast X j6 ¼i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>Table 1.</figDesc><table>Breast cancer microarray datasets curated by Haibe-Kains et al. (2012) 

Number Name Adjuvant 
therapy 

Number 
of patients a 

Number of ER+ 3Q survival 
[mo.] 

Median 
follow-up [mo.] 

Original 
identifiers b 

Reference 

1 
CAL Chemo, hormonal 118 
75 
42 
82 
CAL 
Chin et al. (2006) 
2 
MNZ none 
200 
162 
120 
94 
MAINZ 
Schmidt et al. (2008) 
3 
MSK combination 
99 
57 
76 
82 
MSK 
Minn et al. (2005) 
4 
ST1 
hormonal 
512 a 
507 b 
114 
106 
MDA5, TAM, VDX3 Foekens et al. (2006) 
5 
ST2 
hormonal 
517 
325 
126 
121 
EXPO, TAM 
Symmans et al. (2010) 
6 
TRB none 
198 
134 
143 
171 
TRANSBIG 
Desmedt et al. (2007) 
7 
UNT none 
133 
86 
151 
105 
UNT 
Sotiriou et al. (2006) 
8 
VDX none 
344 
209 
44 
107 
VDX 
Minn et al. (2007) 

Datasets acronyms: CAL, University of California, San Francisco and the California Pacific Medical Center (USA); MNZ, Mainz hospital (Germany); MSK, Memorial 
Sloan-Kettering (United States). ST1 and ST2 are meta-datasets provided by Haibe-Kains et al. (2012), TRB denotes the TransBIG consortium dataset (Europe), UNT 
denotes the cohort of untreated patients from the Oxford Radcliffe Hospital (UK), VDX = Veridex (the Netherlands). Number of ER+ is the number of patients classified as 
Estrogen Receptor positive. 3Q survival indicates the empirical estimate of the 75-th percentile of the distribution of the survival times (in months). Median follow-up (in 
months) is computed using the reverse Kaplan–Meier estimate to avoid under-estimation due to early deaths (Schemper and Smith, 1996). a Numbers shown are after removal 
of samples duplicated in the dataset VDX. b Dataset identifiers specified in Haibe-Kains et al. (2012). 

i107 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><figDesc>Table 2. True global rankings and estimates with CV and CSV on simulated data</figDesc><table>Algorithm 
Global 
true ranking 

CSV 
(median ranks) 

CV 
(median ranks) 

Criterion Average Medium Average Medium Average Medium 

Ridge 
1 
2 
2 
2 
1 
2 
Plusminus 2 
1 
2 
2 
2 
2 
Superpc 
3 
3 
4 
3 
4 
4 
Unicox 
4 
4 
4 
4 
5 
4 
CoxBoost 5 
5 
5 
5 
3 
4 
Lasso 
6 
6 
6 
6 
5 
6 

</table></figure>

			<note place="foot">ß The Author 2014. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We wish to thank Benjamin Haibe-Kains for making the curated breast cancer datasets used in this study publicly available.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Development of biomarker classifiers from high-dimensional data</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Baek</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brief. Bioinform</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="537" to="546" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Run batch effects potentially compromise the usefulness of genomic signatures for ovarian cancer</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">A</forename>
				<surname>Baggerly</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Oncol</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1186" to="1187" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating survival times to simulate Cox proportional hazards models</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Bender</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Med</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1713" to="1723" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Allowing for mandatory covariates in boosting estimation of sparse high-dimensional survival models</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Binder</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Schumacher</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinform</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised methods to predict patient survival from gene expression data</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Blair</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Biol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="511" to="522" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">On representative and illustrative comparisons with real data in bioinformatics: response to the letter to the editor by smith et al</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">L</forename>
				<surname>Boulesteix</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2664" to="2666" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting survival from microarray data–a comparative study</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">M</forename>
				<surname>Bøvelstad</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2080" to="2087" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">An empirical assessment of validation practices for molecular classifiers</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">J</forename>
				<surname>Castaldi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brief. Bioinform</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="189" to="202" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Genomic and transcriptional aberrations linked to breast cancer pathophysiologies</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Chin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Cell</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="529" to="541" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Dem Sar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Strong time dependence of the 76-gene prognostic signature for node-negative breast cancer patients in the transbig multicenter independent validation series</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Desmedt</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Cancer Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="3207" to="3214" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<monogr>
		<title level="m" type="main">An Introduction to the Bootstrap</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Efron</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">J</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Chapman and Hall</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Multicenter validation of a gene ExpressionBased prognostic signature in lymph NodeNegative primary breast cancer</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">A</forename>
				<surname>Foekens</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Oncol</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1665" to="1671" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<monogr>
		<title level="m" type="main">curatedOvarianData: clinically annotated data for the ovarian cancer transcriptome. Database, [Epup ahead of print, doi: 10.1093/ database/bat013</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">F</forename>
				<surname>Ganzfried</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2013-04-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Bioconductor: open software development for computational biology and bioinformatics</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">C</forename>
				<surname>Gentleman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">1 penalized estimation in the cox proportional hazards model</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Goeman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometr. J</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="70" to="84" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Concordance probability and discriminatory power in proportional hazards regression</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Gnen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Heller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="965" to="970" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">A three-gene model to robustly identify breast cancer molecular subtypes</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Haibe-Kains</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Natl Cancer Inst</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="311" to="325" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Multivariate prognostic models: issues in developing models, evaluating assumptions and adequacy, and measuring and reducing errors</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">E</forename>
				<surname>Harrell</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stati. Med</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="361" to="387" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">A &apos;Super-Population viewpoint&apos; for finite population sampling</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">O</forename>
				<surname>Hartley</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">L</forename>
				<surname>Sielken</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Jr</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="page" from="31" to="411" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Tackling the widespread and critical impact of batch effects in high-throughput data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">T</forename>
				<surname>Leek</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Genet</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="733" to="739" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<monogr>
		<title level="m" type="main">Evolution of Translational Omics: Lessons Learned and the Path Forward</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Micheel</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>National Academies Press</publisher>
			<pubPlace>Wahington, D.C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Strategies for aggregating gene expression data: the collapserows R function</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">A</forename>
				<surname>Miller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinform</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">322</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Genes that mediate breast cancer metastasis to lung</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Minn</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">436</biblScope>
			<biblScope unit="page" from="518" to="524" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Lung metastasis genes couple breast tumor size and metastatic spread</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Minn</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6740" to="6745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Meta-analysis of randomized controlled trials: A concern for standards</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Moher</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Olkin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">274</biblScope>
			<biblScope unit="page" from="1962" to="1964" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Prediction error estimation: a comparison of resampling methods</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">M</forename>
				<surname>Molinaro</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="3301" to="3307" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Risk prediction for Late-Stage ovarian cancer by meta-analysis of 1525 patient samples</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Riester</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JNCI J Natl Cancer Inst., [Epup ahead of print</title>
		<imprint>
			<date type="published" when="2014-04-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">A note on quantifying follow-up in studies of failure time</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Schemper</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">L</forename>
				<surname>Smith</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Trials</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="343" to="346" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">The humoral immune system has a key prognostic impact in node-negative breast cancer</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Schmidt</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Res</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="5405" to="5413" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Use of archived specimens in evaluation of prognostic and predictive biomarkers</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">M</forename>
				<surname>Simon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Natl Cancer Inst</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="1446" to="1452" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Using cross-validation to evaluate predictive accuracy of survival risk classifiers based on high-dimensional data</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">M</forename>
				<surname>Simon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brief. Bioinform</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="203" to="217" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Gene expression profiling in breast cancer: understanding the molecular basis of histologic grade to improve prognosis</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Sotiriou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Natl Cancer Inst</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="262" to="272" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Gene expression-based prognostic signatures in lung cancer: ready for clinical use?</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Subramanian</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Simon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Natl Cancer Inst</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="464" to="474" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Genomic index of sensitivity to endocrine therapy for breast cancer</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">F</forename>
				<surname>Symmans</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Oncol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="4111" to="4119" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">inSilicoDb: an R/Bioconductor package for accessing human affymetrix expert-curated datasets from GEO</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Taminau</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3204" to="3205" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<monogr>
		<title level="m" type="main">uniCox: Univarate shrinkage prediction in the Cox model</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>R. package version 1.0</note>
</biblStruct>

<biblStruct   xml:id="b37">
	<analytic>
		<title level="a" type="main">Bias in error estimation when using cross-validation for model selection</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Varma</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Simon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">91</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b38">
	<analytic>
		<title level="a" type="main">Optimized application of penalized regression methods to diverse genomic data</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Waldron</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3399" to="3406" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b39">
	<analytic>
		<title level="a" type="main">Comparative meta-analysis of prognostic gene signatures for Late-Stage ovarian cancer</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Waldron</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JNCI J Natl Cancer Inst., [Epub ahead of print</title>
		<imprint>
			<date type="published" when="2014-04-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b40">
	<monogr>
		<title level="m" type="main">Mas-o-menos: a simple sign averaging method for discrimination in genomic data analysis</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Zhao</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2013-10-24" />
		</imprint>
	</monogr>
	<note>date. last accessed</note>
</biblStruct>

<biblStruct   xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Bernau</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>