
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequence analysis Improving protein secondary structure prediction using a simple k-mer model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010">2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Martin</forename>
								<surname>Madera</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<addrLine>Woodland Road</addrLine>
									<postCode>BS8 1UB</postCode>
									<settlement>Bristol</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Ryan</forename>
								<surname>Calmus</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<addrLine>Woodland Road</addrLine>
									<postCode>BS8 1UB</postCode>
									<settlement>Bristol</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Grant</forename>
								<surname>Thiltgen</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomolecular Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>95064</postCode>
									<settlement>Santa Cruz</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Kevin</forename>
								<surname>Karplus</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomolecular Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>95064</postCode>
									<settlement>Santa Cruz</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName>
								<forename type="first">Julian</forename>
								<surname>Gough</surname>
							</persName>
							<email>gough@cs.bris.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<addrLine>Woodland Road</addrLine>
									<postCode>BS8 1UB</postCode>
									<settlement>Bristol</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sequence analysis Improving protein secondary structure prediction using a simple k-mer model</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">26</biblScope>
							<biblScope unit="issue">5</biblScope>
							<biblScope unit="page" from="596" to="602"/>
							<date type="published" when="2010">2010</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btq020</idno>
					<note type="submission">Received on September 21, 2009; revised on December 24, 2009; accepted on January 12, 2010</note>
					<note>[14:46 5/2/2010 Bioinformatics-btq020.tex] Page: 596 596–602 Associate Editor: Burkhard Rost Contact:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Some first order methods for protein sequence analysis inherently treat each position as independent. We develop a general framework for introducing longer range interactions. We then demonstrate the power of our approach by applying it to secondary structure prediction; under the independence assumption, sequences produced by existing methods can produce features that are not protein like, an extreme example being a helix of length 1. Our goal was to make the predictions from state of the art methods more realistic, without loss of performance by other measures. Results: Our framework for longer range interactions is described as a k-mer order model. We succeeded in applying our model to the specific problem of secondary structure prediction, to be used as an additional layer on top of existing methods. We achieved our goal of making the predictions more realistic and protein like, and remarkably this also improved the overall performance. We improve the Segment OVerlap (SOV) score by 1.8%, but more importantly we radically improve the probability of the real sequence given a prediction from an average of 0.271 per residue to 0.385. Crucially, this improvement is obtained using no additional information.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The prediction of secondary structure remains very important in the field of protein biology, even if the methods have matured and development of the algorithms is a far less active area than a decade ago. One of the reasons for this decline in activity is that most of the competing methods have converged on a similar level of performance beyond which they have been unable to improve, and possibly because the level of performance that they achieve is, by bioinformatics standards, exceptionally good. This is reflected in the fact that the Critical Assessment of Techniques for Protein Structure Prediction (CASP;<ref type="bibr" target="#b19">Moult et al., 1995</ref>) competition for protein structure prediction ceased to assess this as an official category some years ago, as has the EVA (<ref type="bibr" target="#b13">Koh et al., 2003</ref>) continuous benchmarking project. Accurate prediction of secondary structure elements from an amino acid sequence remains very useful to * To whom correspondence should be addressed. biologists in its own right, but it is worth pointing out that it is also an essential component of tertiary structure prediction, which, in contrast, is far from solved and continues to be a highly active area of research. In addition, sequence comparison methods have more recently incorporated local structure tracks (such as secondary structure or burial). The extra information utilized by the new methods has led to considerable improvements in fold recognition and alignment accuracy. There are many different methods for secondary structure prediction (e.g.<ref type="bibr" target="#b4">Cuff et al., 1998;</ref><ref type="bibr" target="#b8">Jones, 1999;</ref><ref type="bibr" target="#b12">Katzman et al., 2008;</ref><ref type="bibr" target="#b21">Ouali and King, 2000;</ref><ref type="bibr" target="#b22">Pollastri and McLysaght, 2005;</ref><ref type="bibr" target="#b24">Rost, 1996</ref>), all using neural networks. Analysis of the results of the last CASP competition to include secondary structure (<ref type="bibr" target="#b0">Aloy et al., 2003</ref>) gives a good indication of the state of the art, and the range of methods available; for this work we chose to use PREDICT-2ND (<ref type="bibr" target="#b12">Katzman et al., 2008</ref>), which is named as one of the three leading original methods. Despite secondary structure prediction methods being able to correctly assign either helix, strand or loop to roughly 80% of the individual positions in a protein sequence, the overall prediction is not protein like. For example these methods are capable of predicting a helix of a single amino acid in length (although consequently most have implemented an ad hoc filter to remove them). What we aim to achieve in this article is to create a model which makes the overall predictions of existing methods more realistic and protein like without loss of performance as measured on a per-residue basis. It is possible that in doing this we may improve prediction accuracy, even if it is not our original goal. The idea of implicitly using more than one amino acid has been around for some time (<ref type="bibr" target="#b20">Nagano, 1973;</ref><ref type="bibr" target="#b3">Chou and Fasman, 1978</ref>), but many protein sequence comparison methods, when making a prediction, implicitly treat positions in a protein sequence with some level of independence. Even in cases where a sliding window is used, predictions are dependent on the neighbouring amino acids, but not usually directly on neighbouring predictions. While an independence assumption is an acceptable approximation when comparing individual amino acid sequences, it fails dramatically for many local structure alphabets. To give a specific example, when a position in a sequence is an α-helix, the adjacent positions are highly likely (∼90% chance) to also be α-helical. In fact, we have observed three broad types of correlations that violate the independence assumption:</p><p>(1) Short range: if H stands for a helix and A for an anti-parallel strand, there are three times more occurrences of HH, 14 times more occurrences of AA, but at least 10 000 times fewerPage: 597 596–602</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simple k-mer model</head><p>occurrences of HA and AH than one would expect based on the frequencies of H and A under the independence assumption.</p><p>(2) Medium range: the lengths of helices, strands and loops form well-defined distributions with exponential tails. The points at which exponential decay sets in are different for each structural type, and so are the decay constants. Further, adjacent secondary structure elements are frequently of a comparable physical length; for example, we found that a strand of length 4 is followed by another strand of length 4 twice as often as it is followed by a strand of length 5, even though both lengths occur with roughly the same frequency.</p><p>(3) Long range: if a residue lies within a parallel strand, a strand residue 100 residues away is roughly six times more likely to also be in a parallel strand than if the first position were an anti-parallel strand.</p><p>As detailed below, in this article, we concentrate in this first instance on using the first of these three correlations to our advantage, partly because they are amenable to exact analysis to verify the results. We have, however, ensured that in formulating our approach we developed a model general enough to be applied to the other two, and in principle other higher order sequence information at medium and long range. Please see Section 4 for more detail. More specifically the problem we have chosen as a starting point is that of sampling from a profile of secondary structure sequences, e.g. one generated by neural networks for structure prediction. We can measure from real sequences how often we observe each individual amino acid (1mer), each possible pair (2mer) or every combination of up to k amino acids (k-mer). Our goal is to change sequence probabilities to reward k-mers that are typically under-predicted compared with real sequences, and penalize k-mers that are overpredicted, so that sequences sampled from the modified system look protein like across k amino acids. Here, we present a conditional random field (CRF;<ref type="bibr" target="#b14">Lafferty et al., 2001</ref>) model as a solution to the problem. CRFs have previously been used in bioinformatics (<ref type="bibr" target="#b5">Do et al., 2006;</ref><ref type="bibr" target="#b15">Liu et al., 2004;</ref><ref type="bibr" target="#b27">Sato and Sakakibara, 2005</ref>) and may be gaining popularity. Protein amino acid sequences have traditionally been handled with hidden Markov models (HMMs;<ref type="bibr" target="#b17">Madera and Gough, 2002</ref>), but except for HMMSTR (<ref type="bibr" target="#b1">Bystroff et al., 2000</ref>) and a more recent attempt by Krogh (<ref type="bibr" target="#b31">Won et al., 2007</ref>), they have not made much of an impact in secondary structure prediction. This is because traditional first-order HMMs cannot handle very well the sorts of overlapping long-range features that are necessary for a good model of local structure. CRFs are an appropriate response to precisely this shortcoming of HMMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A k-mer model of correlated sequences</head><p>As a preparation for our full model, we start with a reformulation of a simple Markov chain of order n−1 in terms of log-odds scores. This formulation will play a key role in the full model. Let a sequence y of length L be denoted y 1...L , and a subsequence of y be denoted y m...n. Let us now suppose that sequences in some large training dataset T can be modelled as Markov chains of order n−1, i.e. that P(y) = P(y 1...n−1 )P(y n |y 1...n−1 )P(y n+1 |y 2...n )...P(y L |y L−n+1...L−1 ).<ref type="bibr">(1)</ref>We can express the individual probabilities in (1) in terms of the distribution of k-mers in T. Let us denote the relative frequency of a k-mer a in T by T k (a), where for each value of k the relative frequencies of all k-mers sum to one. The probability of the initial (n−1)-mer is then simply P(y 1...n−1 |T ) = T n−1 (y 1...n−1 ),</p><formula>(2)</formula><p>and the transition probabilities are P(y m |y m−n+1...m−1 ,T ) = T n (y m−n+1...m ) T n−1 (y m−n+1...m−1 ) .</p><formula>(3)</formula><p>Substituting (2) and (3) into (1) gives P(y) in terms of the distribution of k-mers T , so we shall henceforth denote it by P(y|T ). We have noticed that P(y|T ) can also be expressed in the following alternative form: lnP(y|T ) = S·F(y),</p><formula>(4)</formula><p>where F is a feature vector of k-mer counts and S are the corresponding k-mer scores, defined as follows:,</p><formula>S 1 (a 1 |T ) = lnT 1 (a 1 )</formula><formula>(7)</formula><p>and so on up to n-mers. An important aspect of this formulation is that Equations (6) and</p><p>(7) can be understood as log-odds scores, in the following sense. For k &gt; 1, the denominator is in fact the frequency of the k-mer under a (k −1)-mer model, so</p><formula>S k (a 1...k |T ) = ln T k a 1...k P(a 1...k |T k−1 ) .</formula><formula>(8)</formula><p>In other words, the score is the log-ratio of the observed frequency of a given k-mer to its expected frequency under a (k −1)-mer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Correcting profile emissions with a k-mer model</head><p>We apply the formulation in the previous subsection to the problem of generating realistic emissions from secondary structure profiles. Our goal here is to down-weight sequences with k-mers that are frequently produced in profile samples but occur rarely in real sequences, and conversely to up-weight k-mers that are sampled less frequently than they occur in real sequences. We define a profile X as a sequence of L probability vectors X 1 ...X L , where P(a|X l ) gives the probability of observing the letter a at position l of a sequence emitted from the profile. The total probability for a sequence y to be emitted from the profile is then</p><formula>P(y|X) = L l=1 P y l |X l .</formula><formula>(9)</formula><p>Our approach is to modify this emission probability by introducing a joint profile + k-mer model M,</p><formula>P(y|M) = 1 Z(X,R) exp lnP(y|X)+R ·F(y) ,</formula><formula>(10) where Z(X,R) = y exp lnP(y |X)+R ·F(y ) (11)</formula><p>is the normalization factor (also called the partition function) and R is a set of k-mer scores. The challenge is to come up with scores that would make the distribution of k-mers in sequences sampled from the joint model M as close as possible to the training distribution T. We have discovered that the following simple iterative procedure converges on the right answer:</p><formula>R (0) k (a) = 0 (12) R (i) k (a) = R (i−1) k</formula><p>(a)+S k (a|T )−S k (a|B (i−1) ).</p><formula>(13)</formula><p>Here, we use the superscript (i) to denote variables pertaining to iteration i of the model, and B (i−1) is the distribution of k-mers observed in a large set of sequences B (i−1) sampled from iteration i−1 of the model. Page: 598 596–602</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M.Madera et al.</head><p>The S k scores in (13) are undefined when a is absent from T or B (i−1). We deal with this in one of the following two ways: (i) When a is absent from both sequence sets, we simply set R</p><formula>(i) k (a) to R (i−1) k</formula><p>(a). (ii) When a is absent from one set but not the other (without loss of generality, let us assume that it is absent from T but present in B (i−1) ), we reset T k (a) to satisfy S k (a|T )−S k (a|B (i−1) ) = 0.</p><formula>(14)</formula><p>If the new value of T k (a) is greater than a cut-off value corresponding to an absolute frequency of 0.5 in the sequence set T , we further reset it to the cut-off value. We do not renormalize T k. In simple terms, the motivation behind the regularization scheme is to leave R</p><p>(i) k (a) unchanged as much as possible, unless the absence of a from T is too stark and demands an adjustment. We have tried traditional approaches such as simple pseudocounts, or pseudocounts based on expectations from (k −1)-mer models, but found that the present algorithm performs considerably better. We assess convergence of B (i) towards T using the Kullback–Leibler relative entropy,</p><formula>D (i) k = a 1...k T k (a)log 2 T k (a) B (i) k (a) .</formula><formula>(15)</formula><p>In cases where a is missing from one or both sequence sets we follow a procedure similar to the one described above: (i) when a is absent from both sets, the T log T B score is taken to be zero and both T k (a) and B</p><formula>(i)</formula><p>k (a) are kept at zero. (ii) When a is absent from one set but not the other, we use (14) to reset the zero frequency, subject to the same cut-off as above. Once a decision has been reached on all zero frequencies, both T k and B</p><p>(i) k are renormalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Exact inference for short k-mer models</head><p>For short k-mers we can perform exact inference in our model (10) using standard dynamic programming algorithms (<ref type="bibr" target="#b6">Durbin et al., 1998</ref>). For example, we can use the Viterbi algorithm to calculate the most likely sequence,</p><formula>ˆ y Vit = argmax y P(y|M),</formula><formula>(16)</formula><p>or the forward–backward algorithm to perform posterior decoding (also known as marginalization), which for each position i computes the letterˆy letterˆ letterˆy post i most likely observed at that position,</p><formula>ˆ y post i = argmax y i y 1...i−1 y i+1...L P(y|M).</formula><formula>(17)</formula><p>We can also use forward–backward to calculate the partition function Z(X,R) from (11). However, these exact algorithms require keeping track of all possible 'sticky ends' of length n−1. The memory requirements for doing so become prohibitive even for moderately large n, so we need to turn to sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">MCMC sampling from k-mer models</head><p>We used the Metropolis algorithm (<ref type="bibr" target="#b18">Metropolis et al., 1953</ref>), which is the oldest and best known Markov Chain Monte Carlo (MCMC) sampling method though not necessarily the most efficient. Using our joint model (10), for each profile we carried out 30 runs of 1000 mutations per position, retaining only the last sequence from each run and discarding all other sequences as burn-in. Note that we do not need to know Z for sampling, because it cancels out in the Metropolis probability ratio. In retrospect we are aware that the sampling could be done better. An improvement would be made at no additional cost by doing a smaller number of longer sampling runs, and keeping significantly more samples from each run. Also, Gibbs sampling would be more efficient than the Metropolis algorithm (<ref type="bibr" target="#b2">Casella and George, 1992</ref>). The computational complexity of the exact posterior calculation will not scale well to long-range models. However, we do not have to calculatethe posterior decoding described above to get a good approximation ofˆy ofˆ ofˆy post. Instead, we can calculate an estimate y † d of the posterior decoding by analysing a set of d samples drawn from the distribution P(y|M). In this case, y † d i is simply the classification most frequently observed at position i across the sample set. As d tends to infinity, y † d becomes equivalent tô y post .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">The STR2 alphabet</head><p>The UCSC STR alphabet, described in<ref type="figure" target="#fig_1">Figure 1</ref>, is an enhancement of the DSSP alphabet (<ref type="bibr" target="#b9">Kabsch and Sander, 1983</ref>) that was conceived as a response to the observation that parallel and anti-parallel strands exhibit different hydrophobicity patterns. This implies that it should be possible to distinguish between them when predicting secondary structure from sequence (<ref type="bibr" target="#b10">Karchin et al., 2003</ref>). Possibly for this reason, it has been the most successful alphabet at UCSC in protein alignment and fold recognition tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Training and test datasets</head><p>The training and test data used to generate and assess the performance of the neural networks providing our k-mer model with profiles, and the k-mer model itself, were drawn from a set of 1763 protein chains known as dunbrack-30pc-1763, created by<ref type="bibr" target="#b12">Katzman et al. (2008)</ref>. The set is based upon output from Dunbrack's PISCES server (<ref type="bibr" target="#b29">Wang and Dunbrack, 2003</ref>) containing 1875 chains with a maximum sequence identity of 30%, of which 112 were removed; 77 because their chain lengths were less than 50, 26 because the chains were non-globular and 9 because the chains exhibited very bad clashes as determined by the UNDERTAKER (<ref type="bibr" target="#b11">Karplus, 2009</ref>) clash detector.<ref type="bibr">Katzman et al.</ref>used 3-fold cross-validation on their dataset to test their neural networks, randomly splitting it into three subsets of 588, 588 and 587 chains and training each one of three networks on two of the subsets while testing on the remaining one. We cross-validated correspondingly using the Page: 599 596–602</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simple k-mer model</head><p>same three training and test subsets to produce three k-mer models, and the scores we report are averages over these three sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.1">Neural network inputs and training protocol</head><p>For each chain in the dataset, we generated two local structure profiles using the PREDICT-2ND neural networks (<ref type="bibr" target="#b12">Katzman et al., 2008</ref>): one using an alignment consisting solely of a guide sequence, describing the amino acid at each position of the target sequence, and one from the alignment generated by SAM-T06 seeded with the guide sequence; we refer to these as single-sequence and alignment inputs to the k-mer model, respectively. The PREDICT-2ND neural networks feeding predictions to the k-mer models are four-layered (others use two layers) feed-forward networks taking as input a sliding window of 27 residues worth of multiple alignment profile information (i.e. for residues i−13...i+13) centred around the residue for which a secondary structure classification is required; a single output is returned, consisting of 13 STR2 classification probabilities for the given residue (i), one for each letter in the alphabet. The networks and the software with which they may be utilized are available at http://www.soe.ucsc.edu/~karplus/predict-2nd/ and the SAM-T08 (<ref type="bibr" target="#b11">Karplus, 2009</ref>) web site.<ref type="figure" target="#fig_2">Figure 2</ref>, we can see an example of the improvement typically obtained by using the k-mer model to produce secondary structure predictions over sampling directly from the columns of the profile. The rows generated directly from the profile frequently include unrealistic features, such as helices or beta strands of only one or two residues in length, and the major secondary structure elements are often fragmented. The k-mer model can only improve on the profile it is given. For example, in<ref type="figure" target="#fig_2">Figure 2</ref>, the first strand of the sequence within the profile is evidently incorrect (at the very top) with respect to the true STR2 sequence (at the very bottom). In this case the k-mer model has no hope of correcting the prediction because it only has available prior information on secondary structure in general, not specific knowledge of the protein in question. Tables 1 and 2 compare the performances (using several different measures) of the secondary structure prediction under various decodings of our k-mer model for the given profile, versus the original profile-only performance (classification based on maximal probability at each residue position). A profile can be generated by the neural network from a single sequence, or it can take as input a multiple sequence alignment. The results are shown for both single sequence and alignment inputs. The measures presented in the table are computed as the average over all predictions in the test dataset, and are normalized for sequence length. We observe that while achieving the goal of making the predicted secondary structure more realistic, the k-mer model (sampled decoding): suffers no significant loss of accuracy as assessed by the Q 3 and Q 13 measures; improves somewhat the accuracy according to the Segment OVerlap (SOV) measure; and dramatically improves the chance of predicting the real secondary structure sequence. Unsurprisingly the results are all consistently better when using profiles derived from multiple sequence alignments rather than a single sequence. The sampled posterior decoding performs better than the exact posterior, which in turn performs better than the Viterbi. Key performance measures: The first column in<ref type="figure" target="#tab_1">Table 1</ref>(SOV) is the primary key performance measure employed within CASP (<ref type="bibr" target="#b19">Moult et al., 1995</ref>). SOV in its current form was definedThe improvements can be seen by comparing the two blocks of secondary structure sequences: above are the results from sampling columns independently and below are results from correlated sampling using the k-mer model (10). The STR2 profile is shown graphically above the alignments, and the true secondary structure is shown at the bottom, and in (B) which has the same colouring scheme showing the elements on the PDB structure (1aba). N.B. The quality of individual rows is important, not the alignment. by<ref type="bibr" target="#b32">Zemla et al. (1999)</ref>and is a segment-oriented definition of prediction accuracy measured as a percentage (calculated on the 3letter alphabet). Also used in CASP is Q 3 (<ref type="bibr" target="#b25">Rost and Sander, 1993</ref>), which is simpler per-residue measure of percentage prediction accuracy for the standard 3-letter EHL secondary structure alphabet. We generalize the definition of Q to the STR2 alphabet to produce the third column, Q 13 , where 13 the size of the STR2 alphabet. Historically, Q 3 has been an important measure of secondary prediction accuracy and so is included for reference, although evidently predicting true sequences under the 13-letter STR2 alphabet is a far more difficult problem. To calculate Q 3 , we translated sequences from their 13-state representation to the EHL alphabet using the mapping: CST → L, HG → H, AYZMPQBE → E. Page: 600 596–602<ref type="figure" target="#tab_1">Table 1</ref>X is the profile and M is the joint profile + k-mer model. The probabilities are reported per residue; that is, the quantity shown is [</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview: In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M.Madera et al.</head><formula>P(y)] 1/L ,</formula><p>where the product is over all real sequences y in the test set and L is the sum of their lengths. It can be seen that posterior sampling from our k-mer model on alignment inputs produces a superior SOV accuracy score to all other input types and decoding methods (83%), and that performance is not significantly different for the Q 3 measure (77.4%). SOV and Q 3 results are, respectively, 1.8% and 0.1% better than those produced directly from a profile, and this rises to 4% and 1.5% when using single sequence rather than alignment inputs. The harder Q 13 measure actually shows a small decrease in performance (55.8 to 55.2% and 45.3 to 44.2% for alignment and single-sequence inputs, respectively). The difference observed between the sampled and exact posterior is due to undersampling; if you sample sufficiently this difference goes away. Furthermore, if we did the training on the exact posterior instead of the sampled posterior, we would expect the difference to reverse, with the exact posterior improving on the current sampled posterior (although perhaps not noticeably). Confidence scores: the key performance measures above are commonly used in the field but fail to measure a crucial aspect of the prediction: almost as valuable as the predicted secondary structure states, for practical applications, is knowing the confidence of the predictions. A good way of measuring the overall prediction quality, taking into account the accuracy of the confidence at each position, is to calculate the probability of emitting the correct sequence.<ref type="figure" target="#tab_2">Table 2</ref>reports the probabilities per residue of observing the real sequence being emitted from the profile and from the joint profile + k-mer model, respectively. It can be seen that, for both single-sequence and alignment inputs, the odds of observing the real sequence increase dramatically in the joint model. This improvement is substantial and represents a major new contribution of this work. It is therefore worth examining in more detail and we do so in<ref type="figure" target="#fig_4">Figure 3</ref>, which shows sampling from the model using the same profile that we show in the example in<ref type="figure" target="#fig_2">Figure 2</ref>. Sampling sequences fromthe original profile produces the red cloud. This cloud has largely negative k-mer scores (vertical axis) highlighting an unprotein-like characteristic of the sequences. By combining the k-mer model with the profile, the red sequences are heavily penalized and would no longer be likely to be sampled. The k-mer model restricts the space from which sequences can be sampled and what remains is the blue cloud, which is far more likely to contain the real sequence (circle). Notable characteristics of the output k-mer distributions<ref type="figure">Table 3</ref>shows geometric averages over the three training sets for alignment inputs for some of the most encouraged and discouraged k-mers. These reflect structural expectations, e.g. the five most discouraged 4mers all feature two-residue helices (none of which is possible in the real world) and QEZ and ZEQ feature among the Page: 601 596–602</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simple k-mer model</head><p>most discouraged 3mers due to the necessity of maintaining the parallel/anti-parallel nature of a strand across a beta-bulge. Other 3mer observations include: all single-residue helices are discouraged; so are single-residue edge strands; and single-residue turns except immediately after or right before a helix, and same for the reverse transitions from parallel or anti-parallel to mixed. The 4mer observations reinforce some 3mer observations but also include: 3-turns (characterized by i to i+3 hydrogen bonds) are unlikely to occur except when adjacent to helices or edge β-strands; as with β-bulges, strands are more likely to be contiguous than broken; and double-partner β-strand residues A and P are unlikely to occur in runs of less than three. The most encouraged 3-and 4mers are so rare that they are of no importance and most of them are artefacts, e.g. PM occurs only once in the training set and it happens to be PMS. In 3-fold validation of the training, the mean deviation of scores for the k-mers between sets was relatively low with a value of 0.479, i.e. on average, scores will not be further than half the mean score across sets. This indicates that training on each subset resulted in convergence to similar distributions. Convergence of the training procedure: for simplicity we will restrict our discussion to profiles built from alignments, as the behaviour for profiles built from single sequences is similar. For sequences sampled straight from the profile, which is the zeroth iteration of our procedure, the distribution of 1mers is very close to that in the training set, with the Kullback–Leibler divergence (15) ranging from D (0) 1 = 8×10 −5 to 6×10 −4 for the three training sets. On the other hand, the 2-to 4mer distributions are very different and get progressively worse as k increases, from D</p><formula>(0) 2 ∼ 4×10 −1 to D (0) 4 ∼ 1.</formula><p>This is expected behaviour, because the neural network is essentially trained on 1mer accuracy. After the first iteration, the 1mers worsen to D</p><formula>(1)</formula><p>1 ∼ 1×10 −2 , but 2-to 4mers improve to D</p><formula>(1) 2 ∼ 2×10 −2 and D</formula><p>(1) 2 ∼ 5×10 −2. After a total 15 iterations, the final divergences for models used in the rest of this section are as follows: D</p><formula>(15) 1 ∼ 5×10 −4 , D (15) 2 ∼ 7×10 −4 and D (15)</formula><p>4 ∼ 1×10 −3. For comparison, the divergences among the three training sets are</p><formula>D T −T 1 ∼ 4×10 −4 , D T −T 2 ∼ 8×10 −4 and D T −T 4 ∼ 1×10 −2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION OF THE METHOD</head><p>The method we present here can be thought of as a graphical model. The formal structure is that of a dynamic CRF (<ref type="bibr" target="#b14">Lafferty et al., 2001;</ref><ref type="bibr" target="#b23">Rohanimanesh et al., 2007</ref>). Although our model (10) is a CRF, compared with usual practice in the field there is a major difference, which is our simple training algorithm (13). The algorithm was inspired by our reformulation of a Markov chain with memory as a hierarchical model of k-mers, where the k-mer scores are logratios of the observed frequency relative to the expected frequency based on the k −1 level of the model. The appearance of logodds scores is particularly exciting, because they underlie much of sequence alignment theory, including statistical assessment of alignment significance. The conclusion during recent rounds of CASP for tertiary structure prediction has been to try many potential alignments and secondary structure predictions and to defer judgement until a full 3D model has been built, and to assess that model. In profile–profile alignment (<ref type="bibr" target="#b16">Madera, 2008;</ref><ref type="bibr" target="#b26">Sadreyev and Grishin, 2003;</ref><ref type="bibr" target="#b28">Soeding, 2005</ref>), likewise, one is not interested in the single best sequence, but rather in a large number of samples of plausible ones. For this reason we argue that the P(real seq|M) quality measure is more important than the SOV, Q 3 or Q 13 measures, although these remain the most popular in the field. The P(real seq|M) measure requires confidences to be assigned to the sequence at each position and rewards for accurate confidence as well as correct prediction; a best guess is far less useful without knowledge of which parts to trust. Calculation of the partition function (11) is not needed for sampling and majority voting, i.e. for almost any practical application, but we did this for the purposes of assessing P(real seq|M); for more complex future incarnations of the model this in turn may need to be handled using sampling methods (<ref type="bibr" target="#b30">Wang and Landau, 2001</ref>). Another issue affecting future extensions of the model is that sampling is currently slow. In our simple 4mer model, the most accepted mutations lie on the ends of helices and sheets, either extending or shortening them by one residue. Medium-and long-range models are likely to further slow the sampling process and create lock-ins due to very long-range repulsive interactions between parallel and anti-parallel sheets, so approaches which avoid local minima will need to be explored, e.g. parallel tempering (<ref type="bibr" target="#b7">Earl and Deem, 2005</ref>). To apply our framework to medium-and long-range interactions, we need a hierarchical model of whole helices, loops, parallel, antiparallel and mixed strands. We can sample the distribution of real secondary structure lengths and correlations between neighbouring element lengths, giving us k-mer scores on an alphabet of whole secondary structure elements. We have already solved the correlated null model (though this is equivalent to Markov model of order n−1), for STR2 k-mers by sampling realistic sequences from a profile. This can be used for alignment, the next step being to generate a pairwise scoring function which would be a mix of the traditional substitution matrix and our sequence model. Not many people are using extended secondary structure alphabets. Clearly the richer alphabets contain more information, and in many cases, whether profile–profile, or using multi-track models, the more information the better. It is likely that the reason more advances have not been seen in homology recognition due to the addition of secondary structure is that, we are not using the information correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 602 596–602</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M.Madera et al.</head><p>gain the above improvements. Strings of length 4 are well inside the +/− 13 residue window of the neural network, so the improvements are achieved without using any new information beyond what the neural networks are already using. There is still great potential for further improvement in the future by extending this approach in different ways: most simply with longer k-mers, but also by creating an alphabet of whole secondary structure elements. Data indicates that this will especially improve predictions for parallel/anti-parallel sheets. Although we demonstrated our method on a specific neural network, k-mer models can be trained to correct the emissions of any other neural networks for secondary structure prediction. The work we present here not only improves on secondary structure prediction, but also our theoretical framework for modelling higher order interactions in proteins opens up a way forward for the advancement of protein sequence analysis in general.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>[14:</head><figDesc>46 5/2/2010 Bioinformatics-btq020.tex]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.1.</head><figDesc>Fig. 1. The STR2 alphabet. This 13-state alphabet uses DSSP hydrogen bond definitions and is defined strictly from DSSP output. The main difference is that STR2 subdivides the DSSP class E (β-sheet) into seven classes: A M P, anti-parallel, mixed or parallel β-strand, hydrogen bonded to two partners; Y Z, anti-parallel edge strand residue, bonded and non-bonded, respectively; Q, parallel edge strand, both bonded and non-bonded residues; and E, all other β-sheet residues, typically β-bulges. STR2 groups together DSSP classes H (α-helix) and I (π-helix) into a single STR2 class H. The remaining five classes are identical to DSSP: G, 3 10 helix; T, turn; S, bend; C, coil; and B, β-bridge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.2.</head><figDesc>Fig. 2. Improvement due to k-mer model. (A) The improvements can be seen by comparing the two blocks of secondary structure sequences: above are the results from sampling columns independently and below are results from correlated sampling using the k-mer model (10). The STR2 profile is shown graphically above the alignments, and the true secondary structure is shown at the bottom, and in (B) which has the same colouring scheme showing the elements on the PDB structure (1aba). N.B. The quality of individual rows is important, not the alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>.</head><figDesc>The accuracy of predictions as measured by standard performance measures: SOV on 3-states, Q3, Q13accuracy in each column is shown in bold, and the standard error of the mean is shown after each number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.3.</head><figDesc>Fig. 3. Distribution of scores for samples from profile and corresponding joint model. Each dot represents a sequence. The axes are the two components of the joint model M. The red cloud (bottom) represents 50 000 samples from the profile X; the blue cloud (top) represents 50 000 samples from the joint model M; circle is the real sequence. The profile used is the same as in Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><figDesc>Funding: European Commission 7th framework programme (grant number 213037). Conflict of Interest: none declared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>Table 2. Quality of predictions P(real seq|X) P(real seq|M)</figDesc><table>Alignment 
0.271 
0.385 
Single sequence 
0.189 
0.325 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 3. The five most encouraged and discouraged k-mers for multiple alignments</figDesc><table>Encouraged k-mers 
Discouraged k-mers 

k-mer 
Mean score 
k-mer 
Mean score 

MM 
3.4 
MA 
−13.0 

EE 
3.0 
PM 
−11.7 

ZE 
2.2 
HY 
−11.4 

GG 
2.1 
GY 
−11.2 

YZ 
1.9 
TP 
−11.1 

PMS 
5.7 
CTZ 
−9.1 

HQE 
5.1 
QEZ 
−9.0 

TMA 
4.8 
YTC 
−8.9 

YQY 
4.7 
CTC 
−8.3 

ZQZ 
4.1 
ZEQ 
−8.3 

YEQY 
7.6 
CGGC 
−7.5 

HQBB 
7.2 
CGGS 
−7.0 

QEZM 
6.9 
CGGT 
−7.0 

QBBQ 
6.8 
CHHT 
−6.6 

BTQM 
6.6 
CGGH 
−6.3 

</table></figure>

			<note place="foot">at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="5"> CONCLUSION We have succeeded in producing a new method, which is an additional layer on top of existing neural network-based secondary structure prediction methods meaning that any improvements we make, de facto, represent an advance on the state of the art. Our method has succeeded in our goal of sampling more realistic secondary structure sequences from a profile without loss of accuracy; in fact, we have surpassed this goal and actually increased the prediction performance. We have managed to significantly increase on the SOV scores (+1.8%) and there is no significant difference in the less sophisticated Q 3 scores (+0.1%) which are the two industry standard measures, e.g. used in CASP. A more important measure of the quality of predictions, however, is P(real seq|M), the probability of sampling the correct sequence from the model; this takes into account the confidence scores for each position, essential for practical applications using the prediction. We dramatically improve this probability from 0.271 to 0.385. Using the K-mer model, we have demonstrated that when K = 4 we can 601 at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="602"> at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Predictions without templates: new folds, secondary structure, and contacts in CASP5</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Aloy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins Struct., Funct. Genet</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="436" to="456" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">HMMSTR: a hidden Markov model for local sequencestructure correlations in proteins</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Bystroff</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">301</biblScope>
			<biblScope unit="page" from="173" to="190" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Explaining the Gibbs sampler</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Casella</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">I</forename>
				<surname>George</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. Stat</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="167" to="174" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Prediction of the secondary structure of proteins from their amino acid sequence</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">Y</forename>
				<surname>Chou</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">D</forename>
				<surname>Fasman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Enzymol. Relat. Areas Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="45" to="148" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">JPred: a consensus secondary structure prediction server</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">A</forename>
				<surname>Cuff</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="892" to="893" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">CONTRAfold: RNA secondary structure prediction without physics-based models</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">B</forename>
				<surname>Do</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="90" to="98" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<monogr>
		<title level="m" type="main">Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Durbin</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Parallel tempering: theory, applications, and new perspectives</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">J</forename>
				<surname>Earl</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">W</forename>
				<surname>Deem</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Chem. Chem. Phys</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3910" to="3916" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Protein secondary structure prediction based on position-specific scoring matrices</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">T</forename>
				<surname>Jones</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">292</biblScope>
			<biblScope unit="page" from="195" to="202" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Dictionary of protein secondary structure: pattern recognition of hydrogen-bonded and geometrical features</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Kabsch</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Sander</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biopolymers</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2577" to="2637" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Hidden Markov models that use predicted local structure for fold recognition: alphabets of backbone geometry</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Karchin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins Struct. Funct. Genet</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="504" to="514" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">SAM-T08, HMM-based protein structure prediction</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Karplus</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="492" to="497" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">PREDICT-2ND: a tool for generalized protein local structure prediction</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Katzman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2453" to="2459" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">EVA: evaluation of protein prediction servers</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<forename type="middle">Y Y</forename>
				<surname>Koh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3311" to="3315" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Conditional random fields: probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Lafferty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Machine Learning</title>
		<meeting>the 18th International Conference on Machine Learning<address><addrLine>Morgan Kaufmann, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Comparison of probabilistic combination methods for protein secondary structure prediction</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Liu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="3099" to="3107" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Profile comparer: a program for scoring and aligning profile hidden Markov models</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Madera</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2630" to="2631" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">A comparison of hidden Markov model procedures for remote homology detection</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Madera</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Gough</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4321" to="4328" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Equation of state calculations by fgast computing machines</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Metropolis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Phys</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1087" to="1092" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">A large-scale experiment to assess protein structure prediction methods</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Moult</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">ii–v</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Logical analysis of the mechanism of protein folding. I. Prediction of helices, loops and β-structures from primary structure</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Nagano</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="401" to="420" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Cascaded multiple classifiers for secondary structure prediction</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ouali</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">D</forename>
				<surname>King</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1162" to="1176" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Porter: a new, accurate server for protein secondary structure prediction</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Pollastri</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Mclysaght</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1719" to="1720" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Rohanimanesh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="693" to="723" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">PHD: predicting one-dimensional protein structure by profile-based neural networks</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Rost</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods Enzymol</title>
		<imprint>
			<biblScope unit="volume">266</biblScope>
			<biblScope unit="page" from="525" to="539" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Prediction of protein secondary structure at better than 70% accuracy</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Rost</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Sander</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">232</biblScope>
			<biblScope unit="page" from="584" to="599" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">COMPASS: a tool for comparison of multiple protein alignments with assessment of statistical significance</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">I</forename>
				<surname>Sadreyev</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<forename type="middle">V</forename>
				<surname>Grishin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">326</biblScope>
			<biblScope unit="page" from="317" to="336" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">RNA secondary structural alignment with conditional random fields</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Sato</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Sakakibara</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="237" to="242" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Protein homology detection by HMM-HMM comparison</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Soeding</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="951" to="960" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">PISCES: a protein sequence culling server</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">L</forename>
				<surname>Dunbrack</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1589" to="1591" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient, multiple-range random walk algorithm to calculate the density of states</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">P</forename>
				<surname>Landau</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2050" to="2053" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">An evolutionary method for learning HMM structure: prediction of protein secondary structure</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">J</forename>
				<surname>Won</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">357</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">A modified definition of SOV, a segment-based measure for protein secondary structure prediction assessment</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Zemla</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="220" to="223" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>