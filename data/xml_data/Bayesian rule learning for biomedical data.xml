
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining Bayesian rule learning for biomedical data mining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010">2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Vanathi</forename>
								<surname>Gopalakrishnan</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Informatics</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<addrLine>200 Meyran Avenue Suite M-183</addrLine>
									<postCode>15260</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Jonathan</forename>
								<forename type="middle">L</forename>
								<surname>Lustgarten</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Informatics</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<addrLine>200 Meyran Avenue Suite M-183</addrLine>
									<postCode>15260</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Shyam</forename>
								<surname>Visweswaran</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Informatics</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<addrLine>200 Meyran Avenue Suite M-183</addrLine>
									<postCode>15260</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Gregory</forename>
								<forename type="middle">F</forename>
								<surname>Cooper</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Informatics</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<addrLine>200 Meyran Avenue Suite M-183</addrLine>
									<postCode>15260</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining Bayesian rule learning for biomedical data mining</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">26</biblScope>
							<biblScope unit="issue">5</biblScope>
							<biblScope unit="page" from="668" to="675"/>
							<date type="published" when="2010">2010</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btq005</idno>
					<note type="submission">Received on April 9, 2009; revised on December 11, 2009; accepted on January 5, 2010</note>
					<note>[11:53 9/2/2010 Bioinformatics-btq005.tex] Page: 668 668–675 Associate Editor: Thomas Lengauer Contact: vanathi@pitt.edu Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Disease state prediction from biomarker profiling studies is an important problem because more accurate classification models will potentially lead to the discovery of better, more discriminative markers. Data mining methods are routinely applied to such analyses of biomedical datasets generated from high-throughput &apos;omic&apos; technologies applied to clinical samples from tissues or bodily fluids. Past work has demonstrated that rule models can be successfully applied to this problem, since they can produce understandable models that facilitate review of discriminative biomarkers by biomedical scientists. While many rule-based methods produce rules that make predictions under uncertainty, they typically do not quantify the uncertainty in the validity of the rule itself. This article describes an approach that uses a Bayesian score to evaluate rule models. Results: We have combined the expressiveness of rules with the mathematical rigor of Bayesian networks (BNs) to develop and evaluate a Bayesian rule learning (BRL) system. This system utilizes a novel variant of the K2 algorithm for building BNs from the training data to provide probabilistic scores for IF-antecedent-THEN-consequent rules using heuristic best-first search. We then apply rule-based inference to evaluate the learned models during 10-fold cross-validation performed two times. The BRL system is evaluated on 24 published &apos;omic&apos; datasets, and on average it performs on par or better than other readily available rule learning methods. Moreover, BRL produces models that contain on average 70% fewer variables, which means that the biomarker panels for disease prediction contain fewer markers for further verification and validation by bench scientists.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>High-throughput 'omic' data that measure biomarkers in bodily fluids or tissues are accumulating at a rapid pace, and such data have the potential for the discovery of biomarkers for early diagnosis, monitoring and treatment of diseases such as cancer. Data mining methods that learn models from high-dimensional data are being increasingly used for the multivariate analyses of such biomedical datasets. Together with statistical univariate analyses, some insights into predictive biomarkers of disease states can be gleaned, though * To whom correspondence should be addressed. the results may not generalize due to the small sizes of available training data, typically less than 200 samples. Due to the large imbalance between variable dimensionality (several thousand) and the sample size (a few hundred), there is a need for data mining methods that can discover significant and robust biomarkers from high-dimensional data. Rule learning is a useful data mining technique for the discovery of biomarkers from highdimensional biomedical data. We have previously developed and applied rule learning methods to analyze 'omic' data successfully (<ref type="bibr" target="#b16">Gopalakrishnan et al., 2004</ref><ref type="bibr" target="#b15">Gopalakrishnan et al., , 2006</ref><ref type="bibr" target="#b34">Ranganathan et al., 2005</ref>). Rules have several advantages, including that they are easy for humans to interpret, represent knowledge modularly and can be applied using tractable inference procedures. In this article, we develop and evaluate a novel probabilistic method for learning rules called the Bayesian rule learning (BRL) algorithm. This algorithm learns a particular form of a Bayesian network (BN) from data that optimizes a Bayesian score, and then translates the BN into a set of probabilistic rules. The use of the Bayesian approach allows prior knowledge (as probabilities) to be incorporated into the learning process in a mathematically coherent fashion. The possibility of over-fitting is attenuated by the incorporation of prior probabilities into the rule-discovery process. BRL outputs the predictive rule model with the best Bayesian score, which represents the probability that the model is valid given the data. The remainder of the article is organized as follows. Section 2 presents the BRL algorithm and briefly reviews other popular rule learning methods. Section 3 describes the datasets and the experimental setup to evaluate BRL. Section 4 presents the results of applying BRL to 24 published 'omic' datasets, and compares its performance with multiple rule-learning algorithms. Section 5 presents our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head><p>In biomedical data mining, a typical task entails the learning of a mathematical model from gene expression or protein expression data that predicts an individual phenotype, such as disease or health. Such a task is called classification and the model that is learned is termed as a classifier. In data mining, the variable that is predicted is called the target variable (or simply the target), and the features used in the prediction are called the predictor variables (or simply the predictors). Rule learning is a useful technique for knowledge discovery from data that is discrete. In this article, we present a Bayesian method for learning BNs and translating it into rules as shown in<ref type="figure">Figure 1</ref>. A rule model is a set of rules that together comprise a classifier that can be applied to new data to predict the target. The main contribution of this BRL method is its ability to quantify uncertainty about the validity of a rule model using a Bayesian<ref type="bibr">[</ref><ref type="bibr" target="#b14">Golub et al., 1999</ref>) and its equivalent set of rules. @Class refers to the target variable that can take on values of either 0 (ALL) or 2 (AML) in this example. Each rule is associated with statistics from the training data (see note 1). score. This score is used for model selection. We now discuss in detail the BRL method. This algorithm learns BN models from the data and the model is then translated into a set of rules with associated statistics. 1 These rules are mutually exclusive and exhaustive over the values of the predictor variables, and hence inference using these set of rules becomes trivial. Given a new test case, the rule that matches its values for the predictor variables is used to infer the value of the target variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bayesian networks</head><p>A BN is a probabilistic model that consists of two components: a graphical structure and a set of probability parameters. The graphical structure consists of a directed acyclic graph, in which nodes represent variables and variables are related to each other by directed arcs that do not form any directed cycles. Associated with each node (let us call it a child node) is a probability distribution on that node given the state of its parent nodes, and all the probability distributions for all the nodes taken together provide a factored (and often concise) representation of the joint probability distribution over all the variables (<ref type="bibr" target="#b27">Pearl, 1988</ref>). Learning a BN is a two-step process corresponding to learning the structure and the parameters of the model, and several methods have been developed to automatically learn BNs from data (<ref type="bibr" target="#b25">Neapolitan, 2004</ref>). Here, we use the Bayesian method called K2 (both the K2 scoring measure and the K2 forward stepping search heuristic) for learning BNs (<ref type="bibr">Cooper and Herkovits, 1992</ref>). The K2 scoring measure (<ref type="bibr" target="#b8">Cooper and Herskovits, 1992</ref>) assumes that the variables are discrete, the cases (training examples) occur independently, there are number of cases that have variables with missing values 2 and there is a uniform prior probability distribution over the space of all possible network structures. The K2 measure also assumes that every possible probability distribution over the values of a node given a state of its parents is equally likely (uniform). Under these assumptions, a closed form solution for the Bayesian score is given by the following equation (<ref type="bibr" target="#b8">Cooper and Herskovits, 1992</ref>):</p><formula>P(D|M) = n i=1 q i j=1 (r i −1)! (N ij +r i −1)! r i k=1 N ijk !,</formula><formula>(1)</formula><p>where M is the BN structure under consideration, D the data used to learn M, n the number of variables in M, q i the number of parent states of child variable i, r i the cardinality (number of values or states) of variable i and N ijk the number of instances in the training database D for which variable i has the value k and the parents of i have the value state denoted by index j. Also, N ij is the sum over k of N ijk. Since it is usually computationally intractable to search over all possible BN structures to locate the one with the highest score, a greedy search in the space of BNs is employed. The greedy search method used by K2 (<ref type="bibr" target="#b8">Cooper and Herskovits, 1992</ref>), requires an ordering on the variables and a userspecified parameter for the maximum number of parents any variable in the network can have.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Bayesian rule learning</head><p>The BRL algorithm uses the score given by Equation (1) to learn simple BNs. In particular, (i) BRL considers constrained BNs where the BN consists of one child variable, which is the target to be predicted, and other variables are parents of it; (ii) only the target node is evaluated with the Bayesian score; and (iii) models are searched by utilizing a beam (memory of particular size) to store high-scoring BNs. The beam refers to a restricted memory size for storing BN structures and is implemented as a priority queue of fixed width (beam size), where BNs are stored according to their score. This reduces the memory requirement for heuristic, best-first search, by exploring only those BN structures that are high scoring, while at the same time providing the ability to improve upon greedy search with a beam size of 1. Using the constrained structure of the algorithm in<ref type="figure" target="#fig_0">Figure 2</ref>, as illustrated by the model shown in<ref type="figure">Figure 1</ref>, Equation (1) simplifies further to the following equation:</p><formula>P(D|M) = q j=1 (r −1)! (N j +r −1)! r k=1 N jk !,</formula><formula>(2)</formula><p>where q is the number of joint parent states of the target variable, r the cardinality (number of values or states) of the target variable and N jk the number of instances in the training database D for which the target variable has the value k and its parents have the joint value state denoted by index j. Also, N j is the sum over k of N jk .<ref type="figure">Figure 1</ref>depicts an example of BN structure M learned by BRL, where @Class represents a target variable and two genes M23197_at and U46499_at are its parents. The values or expression of these genes influence the target class.<ref type="figure">Figure 1</ref>depicts M and the set of rules derived from M. A rule set is defined as the conditional probability, such as the conditional probability P(@Class | M23197_at, U46499_at) for all values of M23197_at</p><p>Page: 670 668–675</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.Gopalakrishnan et al.</head><p>INPUT: Discrete predictor variables (X1..n) and target variable (T), an upper bound MAX_CONJS on the number of parents that T can have, beam-width b, and training data D containing m cases OUTPUT: A disjunction of conjunctive probabilistic IF-THEN rules DEFINITIONS: M = Bayesian Network structure; P(D | M) = function that returns the Bayesian score (marginal likelihood) for model M and data D; B = Beam of size b that sorts models by their score in descending order; V = Set of all variables Xi ; F = Priority queue containing final structures (that cannot be improved further by adding a single variable) sorted by their scores in descending order; A = Subset of V containing Xi already appearing in final structures; Parents(M) = function that returns the set of parents Xi of T in M.and U46499_at. In the example, each of the two genes can take on two discrete ranges of values. Hence, the total number of possible combinations of the values for the predictor variables is four (Rules 0–3). The Bayesian score [<ref type="bibr">Equation (2)</ref>] represents the joint probability of the model and the data under the assumption of uniform structure priors. Since P(M |D) ∝ P(D|M), given the assumption that all models are equally likely a priori, the Bayesian score can be directly utilized as a measure of model uncertainty and used to prioritize and select models. Breadth-first marker propagation (<ref type="bibr" target="#b2">Aronis and Provost, 1997</ref>) is utilized to record the matching statistics (or counts) and greatly speeds up the calculations by requiring just one pass through the training data to record the counts. BRL is thus very efficient and runs in O(n 2 m) time given n + 1 variables and m training examples, using the default constant values for beam size b and the maximum conjunct parameter MAX_CONJS, which are both user-specified. The BRL algorithm is shown in<ref type="figure" target="#fig_0">Figure 2</ref>. It takes as input a set of input variables X, a target variable T , an upper bound on the number of parents that T can have and training data containing vectors of values for X's and the corresponding value for T. The user can also provide a beam width b that restricts the number of BNs that are in the priority queue. The default beam width is 1000. In Step 1, a BN containing just the target node with zero parents is created, and it is scored using Equation (2).Bayesian Model Xperform one-step forward search by adding one more allowed variable as an additional parent of the target T to the structure. Specialization refers to the addition of a parent variable (not already present) to the structure of current model, such that the total number of parent variables in the model does not exceed the upper bound MAX_CONJS. The default value we use for MAX_CONJS is 5. In Step 9, a check is made to see whether the score of the model removed from the beam improved after all one-step specializations. If not, that model is placed on a priority queue containing final model structures ordered according to their Bayesian scores. Even though we store many final models on the final priority queue, only the best scoring model is presented to the user in the form of a rule model in Step 10. For each value of target variable T , its probability given each state of possible values of its parent variables is calculated from the training data. The certainty factor is calculated as shown in<ref type="figure" target="#fig_2">Figure 3</ref>. We perform a simple pruning in Step 10, wherein we output only the rule with the target value that has the highest probability given the particular state of its parent variables (<ref type="figure" target="#fig_2">Fig. 3</ref>). There are many other methods that could be used to perform pruning of the rules generated in Step 10, such as the number of training examples covered by the rule (<ref type="bibr" target="#b17">Han and Kamber, 2006</ref>). Also, we perform an optimization to increase search efficiency of the BRL algorithm. As can be seen in Step 9, the algorithm keeps track of those sets of variables that cannot be specialized further by addition of single variables as parents of the target such that the Bayesian score improves. The assumption is that if a predictor is in some final rule, then it is unlikely to be a strong predictor in another rule. While empirically, we observe that this assumption works well for most datasets that we have analyzed, it is certainly possible that there are datasets for which the assumption will not work well. Thus, extensions to this basic BRL algorithm can be explored along several directions to overcome some of the assumptions and limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Rule learning methods</head><p>For our experiments we used three readily available rule learning methods: Conjunctive Rule Learner, RIPPER and C4.5. Conjunctive Rule Learner is a simple rule learner that learns a set of simple conjunctive rules that optimizes the coverage and predictive accuracy. It uses a technique called Reduced Error Pruning (<ref type="bibr" target="#b12">Furnkranz and Widmer, 1994</ref>) to trim an initial set of rules to the smallest and simplest subset of rules that provide highest discrimination. Repeated Incremental Pruning to Produce Error Reduction (RIPPER) was developed by<ref type="bibr" target="#b6">Cohen (1995)</ref>and uses the REP technique in Conjunctive Rule Learner, but performs multiple runs (<ref type="bibr" target="#b7">Cohen, 1996</ref>). C4.5 is a decision tree learner developed by<ref type="bibr" target="#b32">Quinlan (1994)</ref>that extends the basic decision tree learner ID3 (<ref type="bibr" target="#b31">Quinlan, 1986</ref>) to improve classification. These improvements include parameterization of the depth of the decision tree,Page: 671 668–675</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian rule learning for biomedical data mining</head><p>rule post-pruning, ability to handle continuous-valued attributes, ability to choose the best attribute to use in growing the decision tree and an increase in computational efficiency (<ref type="bibr" target="#b13">Gabrilovich and Markovitch, 2004;</ref><ref type="bibr" target="#b49">Xing et al., 2007</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Discretization</head><p>The rule learners described above require variables with discrete values. We used a new discretization method called heuristic efficient Bayesian discretization (EBD;<ref type="bibr" target="#b22">Lustgarten, 2009</ref>), which we developed for transforming continuous data to discrete. EBD uses a Bayesian score to discover the appropriate discretization for a continuous-valued variable and runs efficiently on high-dimensional biomedical datasets. Compared with Fayyad and Irani's (FI) discretization method (<ref type="bibr" target="#b9">Fayyad and Irani, 1993</ref>), which is an efficient method commonly used for discretization, EBD had statistically significantly better performance in the evaluation report in<ref type="bibr" target="#b23">Lustgarten et al. (2008</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Biomedical datasets</head><p>The performance of BRL and the three comparison rule learning methods were evaluated on 24 biomedical datasets [21 publicly available genomic datasets, two publicly available proteomic datasets from the Surface Enhanced Laser/Desorption Ionization Time of Flight (SELDI–TOF;<ref type="bibr" target="#b48">Wright et al., 1999</ref>) mass spectrometry platform and one University of Pittsburgh proteomic dataset, which is a diagnostic dataset from a Amyotrophic Lateral Sclerosis study obtained using the SELDI–TOF platform]. The datasets, along with their type (prognostic/diagnostic), number of instances, number of variables and the majority target-value proportions are given in<ref type="figure" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data mining techniques and statistical analysis</head><p>As mentioned, for comparison, we used three rule learners, namely, Conjunctive Rule Learner, RIPPER and C4.5 as implemented in WEKA version 3.5.6 (<ref type="bibr" target="#b47">Witten and Frank, 2005</ref>). We used two versions of BRL, namely, BRL 1 (beam size set to 1) and BRL 1000 (beam size set to 1000). We used heuristic EBD for discretization; the discretization cutpoints were learned from the training set and then applied to both the training and test sets. We implemented EBD in Java, so that it can be used in conjunction with WEKA. We evaluated the rule learning methods using 10-fold crossvalidation performed two times. The methods were evaluated using two measures: balanced accuracy (BACC), which is the average of sensitivity and specificity over all one-versus-rest comparisons for every target value, and relative classifier information (RCI;<ref type="bibr" target="#b38">Sindhwani et al., 2001</ref>). These measures are described below. The BACC differs from accuracy in that it compensates for skewed distribution of classes in a dataset. BACC is defined as follows:</p><formula>BACC = c Sensitivity(c)+Specificity(c) |C| Sensitivity(c) = TP (c|c) TP (c|c) +FN (¬c|c) Specificity(c) = TN (¬c|¬c) TN (¬c|¬c) +FP (c|¬c)</formula><p>where C is the set of the target variable values, and Sensitivity(c)<ref type="bibr">[Specificity(c)</ref>] refers to the sensitivity (specificity) of the target value c versus all other values of the target. TP (c|c) is the number ofIn the type (T) column, P signifies prognostic and D signifies diagnostic. #C represents the number of classes, #A the number of attributes within the dataset, #S the number of samples and M is the fraction of the data covered by the most frequent target value. The first 21 datasets contain genomic data, whereas the last three datasets contain proteomic data. samples predicted to have the value c for the target variable given that the observed value is c, FN (¬c|c) is the number of samples predicted to have a value other than c for the target variable given that the observed value is c, TN (¬c|¬c) is the number of samples predicted to have a value other than c for the target variable given that the observed value is not c and FP (c|¬c) is the number of samples predicted to have the value c for the target variable given that the observed value is not c. RCI is an entropy-based performance measure that quantifies how much the uncertainty of a decision problem is reduced by a classifier relative to classifying using only the prior probability distribution of the values of the target variable uninformed by any predictor variables (<ref type="bibr" target="#b38">Sindhwani et al., 2001</ref>). The minimum value for RCI is 0%, which is achieved by a classifier that always predicts the majority target-value, and the maximum value is 100%, which is achieved by a classifier with perfect discrimination. RCI is sensitive to the distribution of the target values in the dataset, and thus compensates for the observation that it is easier to obtain high accuracies on highly skewed datasets. Like the area under the receiver operating characteristic curve (AUC), RCI measures the discriminative ability of classifiers. We did not use AUC since there are several interpretations and methods to compute the AUC when the target has more than two values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>The average BACCs obtained from 10-fold cross-validation performed two times for each of the 24 datasets are shown inAverages over the genomic datasets 1–21 (GA) and their SDs, as well as averages over the proteomic datasets 22–24 (PA) and their SDs. Bold numbers indicate highest performance on a dataset.</p><p>As can be seen from the average BACCs for the 24 datasets, both BRL 1 and BRL 1000 clearly perform better than the other rule learning methods. This holds for both the genomic datasets (1–21) and the proteomic datasets (22–24). We see that BRL 1000 has the highest BACC on 15 datasets, while BRL 1 has the highest BACC on 4 datasets. On the remaining five datasets, C4.5 has the highest BACC on three, ties with BRL on one and Conjunctive Rule Learner has the highest on one. Only the first dataset is very easy to classify by all rule learners. As seen in<ref type="figure" target="#tab_3">Table 3</ref>, the performance of both BRL 1 and BRL 1000 are statistically significantly better than C4.5, its nearest competitor in terms of BACC. When compared with each other, BRL 1000 outperforms BRL 1. The average RCIs obtained by the various rule learning methods are shown in<ref type="figure" target="#tab_4">Table 4</ref>. BRL 1000 has the highest RCI on 19 datasets, whereas BRL 1 has the highest RCI on 3 datasets. There was one tie among the two BRL methods and C4.5. In addition, C4.5 has the highest RCI on one dataset. In<ref type="figure" target="#tab_5">Table 5</ref>, we compare the difference in performance using the RCI measure between C4.5 with BRL 1 and BRL 1000 and both BRL methods are statistically significantly better than C4.5; the difference in performance using the RCI measure<ref type="figure" target="#tab_6">Table 6</ref>depicts a comparison of the average number of variables (markers) appearing in the rule models for C4.5, BRL 1 and BRL 1000 , when run with default parameter settings. The average was calculated over the models generated from 20 folds (obtained from stratified 10-fold cross-validation repeated two times) on the 24 datasets. As shown, the BRL models have ∼70% less variables on average in their models than C4.5. If each predictor variable has only two discretized ranges of values, then BRL with default parameters would generate between 2 3 and 2 5 rules on average. However, discretization could yield a larger number of value ranges for a variable, thereby increasing the number of rules generated by BRL. To reduce the number of rules, we can prune rules with zero coverage, that is, those rules whose left-hand side does not match any of the samples in the training data. We notice that pruning does not harm BRL's performance. However, rule pruning could cause problems during testing, since rules that do not match training data could still match test data. We include an example of pruned rules and also C4.5 rules in the Supplementary Material. The variables chosen in BRL's predictive models are often different from those chosen by C4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Discussion</head><p>There are several advantages that accrue from BRL that are not available in current rule learning algorithms. BRL allows for the evaluation of the entire rule set using a Bayesian score. Using such a score results in a whole model evaluation instead of a per rule (or local) evaluation, which often occurs with Ripper and C4.5. The Bayesian score allows us to capture the uncertainty about the validity of a rule set. BRL currently uses this score only for model selection. However, the score could be utilized in extensions to BRL for performing inference when rule sets can be weighted by this score, which would be a form of Bayesian model averaging. A Bayesian approach allows incorporation of both structure and parameter priors. When training data are scarce, such as in 'omic' data analysis, it is useful to incorporate prior knowledge to improve the accuracy of learned models. For example, a scientist could define all of the variable relationships using either a knowledge base or restrict the possible variables with which to build the model (<ref type="bibr" target="#b10">Frey et al., 2005;</ref><ref type="bibr" target="#b24">Miriam et al., 2005</ref>). In a Bayesian approach, a scientist might provide prior knowledge specifying conditional independencies among variables, constraining or even fully specifying the network structure of the BN. In addition to providing such structure priors, the scientist might also specify knowledge in the form of prior distributions over the parameter values of the model. Structure priors are arguably the most useful, however, because in our experience scientists are often more confident about structural relationships than about parameter values. We have not explored informative priors in this article. We used uniform parameter and uniform structure priors. Exploring informative structure priors in this domain is a direction for future research. There are different ways of representing non-informativeness of parameters using the Dirichlet priors. We have explored one approach, it would be useful to explore other approaches as well. An interesting open problem is to investigate methods for BRL rule ordering and pruning within a set of rules. For example, pruning a set of BRL rules based on using local structure and scores (<ref type="bibr" target="#b5">Chickering et al., 1997;</ref><ref type="bibr" target="#b11">Friedman and Goldszmidt, 1996;</ref><ref type="bibr" target="#b45">Visweswaran and Cooper, 2005</ref>) would be worth investigating. A major advantage of BRL is that it can find models with fewer variables (markers) that have equivalent or greater classification performance than those obtained from several other rule learning methods. Fewer variables mean fewer markers for biological verification and subsequent validation. This is important in biomarker discovery and validation studies that have to be designed carefully and under tight resource constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We have shown that using a BN approach to generate rule models not only allows a more parsimonious model (in terms of the number of variables), but also produces results that are statistically significantly superior to common rule learning methods. It also allows the creation of probabilistic rules that are optimized on the rule model level as opposed to the current method of evaluation per rule. Using BN as a Page: 674 668–675</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.Gopalakrishnan et al.</head><p>generating model also provides a coherent method for incorporating different types of prior information and updating the rule model. The basic BRL algorithm presented here can be extended in many ways, which include experimenting with different priors, pruning and rule ordering methods. The use of multiple data mining methods to analyze biomedical 'omic' datasets has become important as these techniques often complement one another in terms of discoveries. In this article, we present and evaluate a novel approach that can complement existing methods for biomedical data mining. We hope that researchers will find this approach useful for efficient knowledge discovery from biomedical datasets and that future extensions will yield additional improvements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.2.</head><figDesc>Fig. 2. The BRL algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>Step 2 initializes the list of variables X that appear in models that have good scores and cannot have a better score by the addition of any single parent of T. The loop condition in Step 3 checks to see whether there are still models on the beam that can be expanded further by the addition of a parent variable such that the score would improve. Steps 4–8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.3.</head><figDesc>Fig. 3. Example of a BN (left) to a set of rules (right), where the CF is expressed as the likelihood ratio of the conditional probability of the target value given the value of its parent variable. As seen in Rule 1, the CF is the likelihood ratio 0.8/0.2 = 4. The two rules in the middle are automatically pruned by BRL and only the higher CF rule for each unique rule antecedent is retained in the rule model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><figDesc>[11:53 9/2/2010 Bioinformatics-btq005.tex]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>ALGORITHM: 1. Create model M containing just target node T and place M on beam B. 2. A = {} 3. WHILE (Beam B is not Empty AND A ⊂ V) DO: 4. M Highest scoring model removed from B 5. X = V – {parents(M) ∪ A} /* Xi NOT in M or A */ 6. Set score_improves = false 7. IF (X not empty AND | parents(M) | &lt; MAX_CONJS) THEN 8. FOR (Each Xi in X) DO: Mnew Add Xi as parent of T in M IF (score(Mnew, D) &gt; score(M, D)) THEN Place Mnew on B Set score_improves = on F A = A ∪ {all Xi in M} ENDIF ENDWHILE 10. MF First model removed from priority queue F FOR each possible joint state j of values for all Xi in M F FOR each possible value k of target variable T Calculate CF(Rjk) ENDFOR Let s = argmaxk(maxj(CF(Rjk))) Output Rjs as: IF (Xi in state j) THEN T=s with CF(Rjs.) ENDFOR</figDesc><table>true 
ENDIF 
ENDFOR 
ENDIF 
9. 
IF (score_improves is false) THEN 
Place M </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 1. Biomedical datasets used for the comparison experiments</figDesc><table># 
T 
#C 
#A 
#S 
M 
Reference 

1 
D 
2 
6584 
61 
0.651 
Alon et al. (1999) 
2 
D 
3 
12 582 
72 
0.387 
Armstrong et al. (2002) 
3 
P 
2 
5372 
86 
0.795 
Beer et al. (2002) 
4 
D 
5 
12 600 
203 
0.657 
Bhattacharjee et al. (2001) 
5 
P 
2 
5372 
69 
0.746 
Bhattacharjee et al. (2001) 
6 
D 
2 
7129 
72 
0.650 
Golub et al., 1999 
7 
D 
2 
7464 
36 
0.500 
Hedenfalk et al. (2001) 
8 
P 
2 
7129 
60 
0.661 
Iizuka et al. (2003) 
9 
D 
4 
2308 
83 
0.345 
Khan et al. (2001) 
10 
D 
4 
12 625 
50 
0.296 
Nutt et al. (2003) 
11 
D 
5 
7129 
90 
0.642 
Pomeroy et al. (2002) 
12 
P 
2 
7129 
60 
0.645 
Pomeroy et al. (2002) 
13 
D 
26 
16 063 
280 
0.574 
Ramaswamy et al. (2001) 
14 
P 
2 
7399 
240 
0.145 
Rosenwald et al. (2002) 
15 
D 
9 
7129 
60 
0.506 
Staunton et al. (2001) 
16 
D 
2 
7129 
77 
0.746 
Shipp et al. (2002) 
17 
D 
2 
10 510 
102 
0.150 
Singh et al. (2002) 
18 
D 
11 
12 533 
174 
0.150 
Su et al. (2001) 
19 
P 
2 
24 481 
78 
0.562 
van't Veer et al. (2002) 
20 
D 
2 
7039 
39 
0.878 
Welsh et al. (2001) 
21 
P 
2 
12 625 
249 
0.805 
Yeoh et al. (2002) 
22 
D 
2 
11 003 
322 
0.784 
Petricoin et al. (2002) 
23 
D 
3 
11 170 
159 
0.364 
Pusztai et al. (2004) 
24 
D 
2 
36 778 
52 
0.556 
Ranganathan (2005) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>Table 2.</figDesc><table>Page: 672 668–675 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><figDesc>Table 2. BACC from 10-fold cross-validation performed two times on the 24 datasets depicted along with the overall averages (AVG) and their SD</figDesc><table># 
Conj_ RL 
Ripper 
C4.5 
BRL 1 
BRL 1000 

1 
9 8 .34 
98.65 
100.00 
100.00 
100.00 
2 
2 9 .83 
44.20 
66.91 
86.22 
100.00 
3 
4 7 .29 
54.39 
60.41 
50.71 
54.88 
4 
4 7 .46 
43.92 
43.47 
59.29 
69.15 
5 
3 0 .24 
32.45 
37.68 
41.17 
47.17 
6 
87.42 
81.83 
84.50 
83.75 
82.75 
7 
8 1 .70 
81.70 
81.70 
97.50 
100.00 
8 
2 0 .70 
25.54 
38.56 
45.00 
63.75 
9 
4 1 .89 
47.36 
42.56 
61.41 
74.71 
10 
37.19 
59.83 
58.96 
59.92 
61.24 
11 
26.04 
29.62 
38.81 
45.62 
48.90 
12 
51.76 
53.40 
55.53 
57.08 
47.50 
13 
51.73 
62.48 
70.28 
65.00 
68.46 
14 
40.81 
44.81 
42.93 
43.31 
49.63 
15 
43.55 
46.64 
46.39 
54.56 
57.78 
16 
47.13 
59.48 
71.69 
80.50 
83.17 
17 
40.93 
47.59 
40.73 
82.17 
74.67 
18 
23.91 
29.88 
33.80 
26.95 
55.78 
19 
40.52 
55.57 
48.71 
76.83 
77.50 
20 
50.22 
71.30 
83.81 
60.00 
73.75 
21 
40.98 
43.09 
42.52 
49.29 
51.27 
22 
53.54 
48.19 
54.92 
61.59 
65.07 
23 
44.60 
57.38 
50.05 
64.34 
53.20 
24 
61.99 
50.79 
64.47 
75.83 
63.33 
AVG 
47.57 
52.80 
56.64 
63.66 
67.65 
(SD) 
(19.03) 
(17.44) 
(17.91) 
(18.45) 
(16.57) 
GA 
46.74 
52.89 
56.66 
63.16 
68.67 
(SD) 
(20.08) 
(18.64) 
(19.06) 
(19.58) 
(17.39) 
PA 
53.38 
52.12 
56.48 
67.25 
60.53 
(SD) 
(8.70) 
(4.74) 
(7.34) 
(7.55) 
(6.41) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><figDesc>Table 3.</figDesc><table>Statistical comparisons between C4.5 and the two BRL algorithms 
using BACC 

Comparison 
Average 
Diff. 
t-test 
(t-score) 

Wilcoxon 
(Z-score) 

BRL 1 versus 
63.66 versus 7.02 
0.015 
0.011 
C4.5 
56.64 
(2.624) 
(2.555) 
BRL 1000 versus 67.65 versus 11.01 
0.001 
0.001 
C4.5 
56.64 
(3.988) 
(3.254) 
BRL 1000 versus 67.65 versus 3.99 
0.050 
0.029 
BRL 1 
63.66 
(2.071) 
(2.190) 

We do not compare Ripper and Conjunctive Rule Learner because C4.5 and the two 
BRL algorithms completely dominate on both performance measures. BRL 1 stands for 
BRL with beam size 1 and BRL 1000 represents BRL with beam size 1000. We use 
both two-sided t-test and two-sided Wilcoxon signed rank test. Those P-values that are 
significant (≤0.05) are in bold and scores with a positive value favor the first method 
in the comparison. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><figDesc>Table 4.</figDesc><table>RCI results on the 24 datasets (from 2 × 10-fold) 

# 
Conj_RL 
Ripper 
C4.5 
BRL 1 
BRL 1000 

1 
9 3 .70 
93.83 
100.00 
100.00 
100.00 
2 
3 1 .58 
46.79 
70.83 
91.27 
100.00 
3 
0 .09 
4.35 
6.81 
4.3 
20.04 
4 
1 8 .35 
46.16 
45.69 
62.31 
64.1 
5 
0 .38 
0.71 
1.07 
65.44 
75.71 
6 
4 2 .64 
43.64 
35.37 
44.28 
43.02 
7 
7 1 .59 
71.59 
71.59 
85.44 
100.00 
8 
0 .56 
1.22 
0.45 
35.56 
66.77 
9 
1 6 .28 
64.87 
58.30 
84.12 
85.6 
10 
23.63 
38.02 
37.47 
38.07 
38.25 
11 
4.32 
21.15 
27.71 
32.58 
43.66 
12 
3.13 
1.85 
3.01 
31.07 
39.7 
13 
13.55 
50.01 
61.84 
51.34 
62.36 
14 
0.17 
1.01 
0.61 
11.14 
23.57 
15 
5.30 
24.85 
24.47 
36.67 
42.59 
16 
16.12 
20.34 
24.52 
27.53 
54.65 
17 
33.54 
39.00 
33.38 
67.33 
36.37 
18 
7.25 
55.32 
62.59 
49.89 
64.21 
19 
19.56 
26.83 
23.51 
37.09 
41.79 
20 
15.39 
24.92 
28.75 
18.39 
20.83 
21 
0.23 
0.73 
0.59 
12.75 
7.2 
22 
1.02 
13.94 
7.21 
17.82 
19.02 
23 
9.18 
17.22 
12.60 
40.45 
57.5 
24 
13.61 
8.84 
14.66 
32.26 
38.39 
AVG 
18.38 
29.88 
31.38 
44.88 
51.89 
(SD) 
(23.16) 
(25.42) 
(27.46) 
(26.38) 
(26.60) 
GA 
19.87 
32.25 
34.22 
46.98 
53.83 
(SD) 
(24.38) 
(26.36) 
(28.24) 
(27.39) 
(27.29) 
PA 
7.9 
1 3 .3 
1 1 .5 
3 0 .2 
38.3 
(SD) 
(6.4) 
(4.2) 
(3.8) 
(11.5) 
(19.2) 

between BRL 1000 and BRL 1 is statistically significant in favor of 
BRL 1000 . 
</table></figure>

			<note place="foot">© The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oxfordjournals.org at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="1"> CF refers to certainty factor or degree of belief in the rule (Shortliffe et al., 1975; Heckerman, D., 1985), P, P-value from Fisher&apos;s exact test; TP, number of true positives (match both sides of the rule); FP, number of false positives (match rule antecedent, but not consequent); Pos, number of positive examples (match rule consequent); and Neg, number of negative examples (do not match rule consequent). Cost measures could be incorporated, but are not used for the experiments in this article.</note>

			<note place="foot" n="2"> Missing values can be accommodated by including an extra state for a missing value of a variable. That extra state can be labeled, for example, as &apos;missing&apos;. Thus, a variable with two domain values (e.g. true and false) becomes a variable with three possible values (e.g. true, false and missing).</note>

			<note place="foot">at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank the Bowser Lab at the University of Pittsburgh (Department of Pathology) for use of the proteomic dataset from<ref type="bibr" target="#b34">Ranganathan et al. (2005)</ref>. We thank Philip Ganchev, the University of Pittsburgh (Intelligent Systems Program) for help with additional verification runs of the experiments with BRL.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Alon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="6745" to="6750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">MLL translocations specify a distinct gene expression profile that distinguishes a unique leukemia</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">A</forename>
				<surname>Armstrong</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="41" to="47" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Increasing the efficiency of data mining algorithms with breadth-first marker propagation</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">M</forename>
				<surname>Aronis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">J</forename>
				<surname>Provost</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Third International Conference on Knowledge Discovery and Data Mining<address><addrLine>Newport, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="119" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Gene-expression profiles predict survival of patients with lung adenocarcinoma</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">G</forename>
				<surname>Beer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="816" to="824" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Classification of human lung carcinomas by mRNA expression profiling reveals distinct adenocarcinoma subclasses</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Bhattacharjee</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA, 98</title>
		<meeting>. Natl Acad. Sci. USA, 98</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="13790" to="13795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">A Bayesian approach to learning Bayesian networks with local structure</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">M</forename>
				<surname>Chickering</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth Conference on Uncertainty in Artificial Intelligence (UAI-97</title>
		<editor>De Raedt,L</editor>
		<meeting>the thirteenth Conference on Uncertainty in Artificial Intelligence (UAI-97<address><addrLine>Providence, RI ; Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="80" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast effective rule induction</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">W</forename>
				<surname>Cohen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Machine Learning</title>
		<meeting>the Twelfth International Conference on Machine Learning<address><addrLine>Tahoe City, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to classify english text with ILP methods</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">W</forename>
				<surname>Cohen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Inductive Logic Programming</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="124" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">A Bayesian method for the induction of probabilistic networks from data</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">F</forename>
				<surname>Cooper</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Herskovits</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="309" to="347" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-interval discretization of continuous-valued attributes for classification learning</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<forename type="middle">M</forename>
				<surname>Fayyad</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">B</forename>
				<surname>Irani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Joint Conference on AI (IJCAI-93)</title>
		<meeting>the Thirteenth International Joint Conference on AI (IJCAI-93)<address><addrLine>Chambery, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="1022" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Using prior knowledge and rule induction methods to discover molecular markers of prognosis in lung cancer</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Frey</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA Annual Symposium Proceedings</title>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="256" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning Bayesian networks with Local Structure</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Friedman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Goldszmidt</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference on Uncertainty in Artifiical Intelligence (UAI-96</title>
		<meeting>the 12th Conference on Uncertainty in Artifiical Intelligence (UAI-96<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="252" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Incremental reduced error pruning</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Furnkranz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Widmer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Machine Learning</title>
		<meeting>the 11th International Conference on Machine Learning<address><addrLine>New Brunswick, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="70" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Text categorization with many redundant features: using aggressive feature selection to make SVMs competitive with C4. 5</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Gabrilovich</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Markovitch</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Machine Learning</title>
		<meeting>the 21st International Conference on Machine Learning<address><addrLine>Banff, Alberta, Canada ; New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Molecular classification of cancer: class discovery and class prediction by gene expression monitoring</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">R</forename>
				<surname>Golub</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="page" from="531" to="537" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Rule learning for disease-specific biomarker discovery from clinical proteomic mass spectra</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Gopalakrishnan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lect. Notes Comput. Sci</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="volume">3916</biblScope>
			<biblScope unit="page" from="93" to="105" />
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Proteomic data mining challenges in identification of disease-specific biomarkers from variable resolution mass spectra</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Gopalakrishnan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM Bioinformatics Workshop. Society of Industrial and Applied Mathematics International Conference on Data Mining</title>
		<meeting><address><addrLine>Lake Buena Vista, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<monogr>
		<title level="m" type="main">Data Mining: Concepts and Techniques</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Han</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Kamber</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Franisco</pubPlace>
		</imprint>
	</monogr>
	<note>2nd. edn</note>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Probabilistic interpretations for MYCIN&apos;s Certainty Factor</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Heckerman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Uncertainty and Probability in Artificial Intelligence</title>
		<meeting>the Workshop on Uncertainty and Probability in Artificial Intelligence<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="page" from="9" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Gene-expression profiles in hereditary breast cancer</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Hedenfalk</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">344</biblScope>
			<biblScope unit="page" from="539" to="548" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Oligonucleotide microarray for prediction of early intrahepatic recurrence of hepatocellular carcinoma after curative resection</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Iizuka</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet</title>
		<imprint>
			<biblScope unit="volume">361</biblScope>
			<biblScope unit="page" from="923" to="929" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Khan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="673" to="679" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<monogr>
		<title level="m" type="main">A Bayesian rule generation framework for &apos;Omic&apos; biomedical data analysis</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">L</forename>
				<surname>Lustgarten</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">An evaluation of discretization methods for learning rules from biomedical datasets</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">L</forename>
				<surname>Lustgarten</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 International Conference on Bioinformatics and Computational Biology</title>
		<meeting>the 2008 International Conference on Bioinformatics and Computational Biology</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="527" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">DrC4.5: improving C4.5 by means of prior knowledge</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Miriam</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 ACM Symposium on Applied Computing</title>
		<meeting>the 2005 ACM Symposium on Applied Computing<address><addrLine>Santa Fe, NM</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="474" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning Bayesian Networks</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">E</forename>
				<surname>Neapolitan</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Alan Apt</publisher>
			<pubPlace>Upper Saddle River</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Gene expression-based classification of malignant gliomas correlates better with survival than histological classification</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">L</forename>
				<surname>Nutt</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Res</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="1602" to="1607" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Pearl</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Morgan-Kaufmann</publisher>
			<pubPlace>San Mateo, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Serum proteomic patterns for detection of prostate cancer</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">F</forename>
				<surname>Petricoin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Iii</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Natl Cancer Inst</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="1576" to="1578" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Prediction of central nervous system embryonal tumour outcome based on gene expression</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">L</forename>
				<surname>Pomeroy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">415</biblScope>
			<biblScope unit="page" from="436" to="442" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Pharmacoproteomic analysis of pre-and post-chemotherapy plasma samples from patients receiving neoadjuvant or adjuvant chemotherapy for breast cancer</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Pusztai</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="1814" to="1822" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">R</forename>
				<surname>Quinlan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">C4.5: programs for machine learning</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Quinlan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="235" to="240" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiclass cancer diagnosis using tumor gene expression signatures</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Ramaswamy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA, 98</title>
		<meeting>. Natl Acad. Sci. USA, 98</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="15149" to="15154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Proteomic profiling of cerebrospinal fluid identifies biomarkers for amyotrophic lateral sclerosis</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Ranganathan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurochem</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="1461" to="1471" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">The use of molecular profiling to predict survival after chemotherapy for diffuse large-B-cell lymphoma</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Rosenwald</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">346</biblScope>
			<biblScope unit="page" from="1937" to="1947" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<analytic>
		<title level="a" type="main">Diffuse large B-cell lymphoma outcome prediction by geneexpression profiling and supervised machine learning</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Shipp</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="68" to="74" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b37">
	<analytic>
		<title level="a" type="main">Computer-based consultations in clinical therapeutics: explanation and rule acquisition capabilities of the MYCIN system</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">H</forename>
				<surname>Shortliffe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biomed. Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="303" to="320" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b38">
	<analytic>
		<title level="a" type="main">Information theoretic feature crediting in multiclass support vector machines</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Sindhwani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st SIAM International Conference on Data Mining</title>
		<meeting>the 1st SIAM International Conference on Data Mining<address><addrLine>Chicago, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b39">
	<analytic>
		<title level="a" type="main">Gene expression correlates of clinical prostate cancer behavior</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Singh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Cell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="203" to="209" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b40">
	<analytic>
		<title level="a" type="main">Chemosensitivity prediction by transcriptional profiling</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">E</forename>
				<surname>Staunton</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA, 98</title>
		<meeting>. Natl Acad. Sci. USA, 98</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="10787" to="10792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b41">
	<analytic>
		<title level="a" type="main">Molecular classification of human carcinomas by use of gene expression signatures</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">I</forename>
				<surname>Su</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Res</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="7388" to="7393" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b42">
	<monogr>
		<title level="m" type="main">Gene expression profiling predicts clinical outcome of breast cancer</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">J</forename>
				<surname>Van &apos;t Veer</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="530" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b43">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">1153</biblScope>
			<biblScope unit="issue">92</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>btq005. .tex]</note>
</biblStruct>

<biblStruct   xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName>
				<surname>Page</surname>
			</persName>
		</author>
		<imprint>
			<biblScope unit="page" from="668" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b45">
	<analytic>
		<title level="a" type="main">Patient-Specific Models for Predicting the Outcomes of Patients with Community Acquired Pneumonia</title>
		<author>
			<persName>
				<surname>Bayesian</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Mining Visweswaran</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">F</forename>
				<surname>Cooper</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AMIA 2005 Annual Symposium</title>
		<meeting>AMIA 2005 Annual Symposium<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="759" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b46">
	<analytic>
		<title level="a" type="main">Analysis of gene expression profiles in normal and neoplastic ovarian tissue samples identifies candidate molecular markers of epithelial ovarian cancer</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">B</forename>
				<surname>Welsh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA, 98</title>
		<meeting>. Natl Acad. Sci. USA, 98</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1176" to="1181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b47">
	<monogr>
		<title level="m" type="main">Data Mining: Practical Machine Learning Tools and Techniques</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<forename type="middle">H</forename>
				<surname>Witten</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Frank</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b48">
	<analytic>
		<title level="a" type="main">Proteinchip(R) surface enhanced laser desorption/ionization (SELDI) mass spectrometry: a novel protein biochip technology for detection of prostate cancer biomarkers in complex protein mixtures</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">L</forename>
				<surname>Wright</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prostate Cancer Prostatic Dis</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="264" to="276" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b49">
	<analytic>
		<title level="a" type="main">Combination data mining methods with new medical data to predicting outcome of coronary heart disease</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Xing</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proccedings of the International Conference on Convergence Information Technology</title>
		<meeting>cedings of the International Conference on Convergence Information Technology<address><addrLine>Gyeongju, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="868" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b50">
	<analytic>
		<title level="a" type="main">Classification, subtype discovery, and prediction of outcome in pediatric acute lymphoblastic leukemia by gene expression profiling</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">J</forename>
				<surname>Yeoh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Cell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="133" to="143" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>