
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-11T00:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Genome analysis densityCut: an efficient and versatile topological approach for automatic clustering of biological data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Jiarui</forename>
								<surname>Ding</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<postCode>V6T 1Z4</postCode>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Molecular Oncology</orgName>
								<orgName type="institution">BC Cancer Research Centre</orgName>
								<address>
									<postCode>V5Z 1L3</postCode>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Sohrab</forename>
								<surname>Shah</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<postCode>V6T 1Z4</postCode>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Molecular Oncology</orgName>
								<orgName type="institution">BC Cancer Research Centre</orgName>
								<address>
									<postCode>V5Z 1L3</postCode>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName>
								<forename type="first">Anne</forename>
								<surname>Condon</surname>
							</persName>
							<email>condon@cs.ubc.ca or sshah@bccrc.ca or jiaruid@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<postCode>V6T 1Z4</postCode>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Genome analysis densityCut: an efficient and versatile topological approach for automatic clustering of biological data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btw227</idno>
					<note type="submission">Received on November 26, 2015; revised on March 22, 2016; accepted on April 18, 2016</note>
					<note>*To whom correspondence should be addressed. Associate Editor: Alfonso Valencia Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Many biological data processing problems can be formalized as clustering problems to partition data points into sensible and biologically interpretable groups. Results: This article introduces densityCut, a novel density-based clustering algorithm, which is both time-and space-efficient and proceeds as follows: densityCut first roughly estimates the densities of data points from a K-nearest neighbour graph and then refines the densities via a random walk. A cluster consists of points falling into the basin of attraction of an estimated mode of the underlining density function. A post-processing step merges clusters and generates a hierarchical cluster tree. The number of clusters is selected from the most stable clustering in the hierarchical cluster tree. Experimental results on ten synthetic benchmark datasets and two microarray gene expression datasets demonstrate that densityCut performs better than state-of-the-art algorithms for clustering biological datasets. For applications, we focus on the recent cancer mutation clustering and single cell data analyses, namely to cluster variant allele frequencies of somatic mutations to reveal clonal architectures of individual tumours, to cluster single-cell gene expression data to uncover cell population compositions, and to cluster single-cell mass cytometry data to detect communities of cells of the same functional states or types. densityCut performs better than competing algorithms and is scalable to large datasets. Availability and Implementation: Data and the densityCut R package is available from https://bit bucket.org/jerry00/densitycut_dev. Contact:</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Clustering analysis (unsupervised machine learning), which organizes data points into sensible and meaningful groups, has been increasingly used in the analysis of high-throughput biological datasets. For example, The Cancer Genome Atlas project has generated multiple omics data for individual patients. One can cluster the omics data of individuals into subgroups of potential clinical relevance. To study clonal evolution in individual cancer patients, we can cluster variant allele frequencies of somatic mutations, such that mutations in the same cluster are accumulated during a specific stage of clonal expansion. Emerging technologies such as single-cell sequencing have made it possible to cluster single-cell gene expression data to detect rare cell populations, or to reveal lineage relationships (<ref type="bibr" target="#b27">Pollen et al., 2014;</ref><ref type="bibr" target="#b37">Xu et al., 2015</ref>). One can cluster single-cell mass cytometry data to study intratumour heterogeneity (<ref type="bibr" target="#b19">Levine et al., 2015</ref>). As measurement technology advances have drastically enhanced our abilities to generate various high-throughput datasets, there is a great need to develop efficient and robust clustering algorithms to analyze large N (number of data points), large D (dimensions of data) datasets, with the ability to detect arbitrary shape clusters and automatically determine the number of clusters. The difficulties of clustering analysis lie in part with the definition of a cluster. Of the numerous proposed clustering algorithms, the density-based clustering algorithms (<ref type="bibr" target="#b11">Fraley et al., 2007;</ref><ref type="bibr" target="#b13">Fukunaga et al., 1975</ref>) are appealing because of the probabilistic interpretation of a cluster generated by these algorithms. Let D ¼ fx i g N i¼1 ; x i 2 R D be drawn from an unknown density function f ðxÞ; x 2 X &amp; R D. For model-based approaches such as Gaussian mixture models f ðxÞ ¼ P C c¼1 p c N ðxjl c ; R c Þ, a cluster is considered as the points generated from a mixture component, and the clustering problem is to estimate the parameters of the density function from D (<ref type="bibr" target="#b11">Fraley et al., 2007</ref>). To analyze datasets consisting of complex shape clusters, nonparametric methods such as kernel density estimation can be used to estimate b f ðxÞ ¼ P N i¼1 K h ðx; x i Þ, where K h ðÁÞ is the kernel function with bandwidth h. Here, a cluster is defined as the data points associated with a 'mode' of the density function f ðxÞ (<ref type="bibr" target="#b35">Wishart, 1969</ref>). The widely used 'mean-shift' algorithm (<ref type="bibr" target="#b3">Cheng, 1995;</ref><ref type="bibr" target="#b4">Comaniciu et al., 2002;</ref><ref type="bibr" target="#b13">Fukunaga et al., 1975</ref>) belongs to this category, and it locates the modes of the kernel density function b f ðxÞ by iteratively moving a point along the density gradient until convergence. This algorithm, however, is computationally expensive, having time complexity OðN 2 TÞ, where T is the number of iterations, typically dozens of iterations are sufficient for most cases. A more efficient, non-iterative graph-based approach (<ref type="bibr" target="#b16">Koontz et al., 1976</ref>) constructs trees such that each data point x i represents a node of a tree, the parent of node x i is a point x j which is in the direction closest to the gradient direction r b f ðx i Þ, and the root of a tree corresponds to a mode of b f ðxÞ. Then each tree constitutes a cluster. This algorithm has been used to reduce the time complexity of the mean-shift algorithm to OðN 2 Þ (<ref type="bibr" target="#b33">Vedaldi et al., 2008</ref>), and has been extended in several ways, e.g. constructing trees after filtering out noisy modes (<ref type="bibr" target="#b28">Rodriguez et al., 2014</ref>). Nonparametric clustering methods have been generalized to produce a hierarchical cluster tree (<ref type="bibr" target="#b15">Hartigan, 1975</ref>). Consider the k level set of a density function f(x): Lðk; f ðxÞÞ ¼ fxjf ðxÞ ! kg:</p><p>The 'high level clusters' at level k are the connected components of Lðk; f ðxÞÞ (in the topological sense, the maximal connected subsets of Lðk; f ðxÞÞ). As k goes from 0 to maxf ðxÞ, the high level clusters at all levels constitute the level set tree, where the leaves of the tree correspond to the modes of f ðxÞ (<ref type="bibr" target="#b31">Stuetzle et al., 2010</ref>). The widely used DBSCAN algorithm (<ref type="bibr" target="#b10">Ester et al., 1996</ref>) extracts the high level clusters at just one given level k. Many original approaches for level set tree construction in statistics (<ref type="bibr" target="#b22">Menardi et al., 2013</ref>) take the straightforward 'plug-in' approach to estimating the level set tree from b f ðxÞ by partitioning the feature space, i.e. X. Therefore, they are computationally demanding, especially for high-dimensional data. Recently, efficient algorithms have been proposed to partition the samples D directly (<ref type="bibr" target="#b2">Chaudhuri et al., 2010;</ref><ref type="bibr" target="#b17">Kpotufe et al., 2011</ref>). Recovering the level set tree from a finite dataset is more difficult than partitioning the dataset into separate clusters. Correspondingly, theoretical analyses show that for these algorithms to identify salient clusters from finite samples, the number of data points N needs to grow exponentially in the dimension D (<ref type="bibr" target="#b2">Chaudhuri et al., 2010;</ref><ref type="bibr" target="#b17">Kpotufe et al., 2011</ref>). Moreover, although the level set tree provides a more informative description of the structure of the data, many applications still need the cluster membership of each data point, which is not available directly from the level set tree. The spectral clustering algorithm (<ref type="bibr" target="#b25">Ng et al., 2002;</ref><ref type="bibr" target="#b30">Shi et al., 2000</ref>) works on an N by N pairwise data similarity matrix S, where each element S i;j measures the similarity between x i and x j. The similarity matrix can be considered as the adjacency matrix of a weighted graph G ¼ ðV; EÞ, where vertex v i represents x i and the edge weight E i;j ¼ S i;j. Given the number of clusters C, the spectral clustering algorithm partitions the graph G into C disjoint, approximately equal size clusters, such that the points in the same cluster are 'similar', while points in different clusters are 'dissimilar'. In contrast to density-based methods, the spectral clustering algorithm does not make assumptions on the probabilistic model which generates data D (<ref type="bibr" target="#b34">Von Luxburg, 2007</ref>). Therefore, selecting the number of clusters is a challenging problem for spectral clustering algorithms, especially in the presence of outliers or when the number of clusters is large. In addition, the spectral clustering algorithm is time-consuming because it needs to compute the eigenvalues and eigenvectors of the row-normalized similarity matrix S, requiring Hð N 3 Þ time. Instead of using single value decomposition to calculate the eigenvalues and eigenvectors, the power iteration clustering algorithm (PIC) (<ref type="bibr" target="#b20">Lin et al., 2010</ref>) iteratively smoothes a random initial vector by the row-normalized similarity matrix, such that the points in the same cluster will be similar in value. Then the k-means algorithm is used to partition the smoothed vector into C clusters. Although PIC has a time complexity of OðN 2 TÞ, where T is the number of iterations, PIC may encounter many difficulties in practice. First, the points from two quite distinct clusters may have very similar 'smoothed' densities, and therefore they may not be distinguishable by k-means. Second, the points in a non-convex shape cluster can break into several clusters. As the number of clusters increases, these problems become more severe (<ref type="bibr" target="#b20">Lin et al., 2010</ref>). In this article, we introduce a simple and efficient clustering algorithm, densityCut, which shares some advantages of both densitybased clustering algorithms and spectral clustering algorithms. As for spectral clustering algorithms, densityCut works on a similarity matrix; thus it is computationally efficient, even for high-dimensional data. Using a sparse K-nearest neighbour graph further reduced the time complexity. Besides, we can use a random walk on the K-nearest neighbour graph to estimate densities at each point. As for many density-based clustering algorithms, densityCut is simple, efficient, and there is no need to specify the number of clusters as an input. Moreover, densityCut inherits both methods' advantage of detecting arbitrarily shaped clusters. Finally, densityCut offers a novel way to build a hierarchical cluster tree and to select the most stable clustering. We first benchmark densityCut against widely used ten simulation datasets and two microarray gene expression datasets to demonstrate its robustness. We then use densityCut to cluster variant allele frequencies of somatic mutations to infer clonal architectures in tumours, to cluster single-cell gene expression data to uncover cell population compositions, and to cluster single-cell mass cytometry data to detect communities of cells of the same functional states or types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>The densityCut method consists of four major steps (Supplementary Algorithm 1): (i) density estimation: given a set of data points D ¼ fx i g N i¼1 ; x i 2 R D , form a directed unweighted K-nearest neighbour graph and estimate the densities of data points by using the K-nearest neighbour density estimator; (ii) density refinement: refine the initial densities via a random walk on the unweighted K-nearest neighbour graph; (iii) local-maxima based clustering: detect local maxima of the estimated densities, and assign the remaining points to the local maxima; (iv) hierarchical stable clustering: refine the initial clustering by merging neighbour clusters. This cluster merging step produces a hierarchical clustering tree, and the final clustering is obtained by choosing the most stable clustering as the threshold in merging clusters varies.<ref type="figure" target="#fig_0">Figure 1</ref>demonstrates how densityCut works on a toy example (<ref type="bibr" target="#b12">Fu et al., 2007</ref>). Below we discuss each step in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Density estimation</head><p>We adopt the K-nearest neighbour density estimator to estimate the density at x 2 R D (<ref type="figure" target="#fig_0">Fig. 1</ref>(a)):</p><formula>f K ðxÞ ¼ ðK À 1Þ=N V K ðxÞ (1)</formula><p>where V K ðxÞ ¼ V D Â ðr K ðxÞÞ D is the volume of the smallest ball centred at x containing K points from D. V D is the volume of the unit ball in the D-dimensional space, and r K ðxÞ is the distance from x to its Kth nearest neighbour. Compared to the widely used kernel density estimator, K-nearest neighbour density estimates are easier to compute, and also the parameter K is more intuitive to set than the kernel bandwidths for kernel density estimators. Here for simplicity, we only calculate the densities at data points from D, represented by f 0 ¼ ðf 0 1 ;. .. ; f 0 N Þ T , where f 0 i ¼ f K ðx i Þ and the superscript '0' indicates that this is the initial Knn density estimate since we will refine this density in the next section.<ref type="figure" target="#fig_0">Figure 1</ref>(b) shows the estimated densities at each data point. When computing the Knn densities, we can get the K-nearest neighbour graph G as a byproduct with the following adjacency matrix: W i;j ¼ 1 x j 2 Knnðx i Þ 0 otherwise (</p><formula>(2)</formula><p>As x i maps to node v i in the Knn graph G, we next use 'points' and 'nodes' interchangeably. The Knn graph G is a directed unweighted graph. While the sets of in-vertices and out-vertices of many nodes may overlap significantly, some outliers may have few, if any in-vertices. In addition, points at the boundary of density changes may also have quite different sets of in-vertices and out-vertices because their in-vertices commonly consist of points from low-density regions while out-vertices are usually from high-density regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A random walk based density refinement</head><p>As Knn density estimates are based on order statistics and tend to be noisy, we next introduce a way to refine the initial Knn density vector f 0 ¼ ðf 0 1 ;. .. ; f 0 N Þ T. Our refinement is based on the intuition that (i) a high-density vertex belongs to one of the K-nearest neighbours of many vertices, and (ii) a vertex tends to have high density if its invertices also have high densities. For example, point k in<ref type="figure" target="#fig_0">Figure 1</ref>(a) has nine in-vertices, and these vertices are in high-density regions, and indeed, the density of k is 0.0087 which is higher than averagefor this example. Based on the above assumptions, we get the following recursive definitions of densities:</p><formula>f tþ1 j ¼ a X i P i;j f t i þ ð1 À aÞf 0 j (3)</formula><p>where a 2 ½0; 1 specifies the relative importance of information from v j 's in-vertices and the initial density estimate f 0 j , which is the K-nearest neighbour density estimate. If each row of P sums to one, P is a Markov transition matrix, which contains the transition probability from a vertex to its K out-vertices. Therefore, Equation 3 defines a random walk with restart. The refined density vector is the stationary distribution of the random walk on the Knn graph with adjacency matrix P. We can row-normalize matrix W to get P ¼ D À1 W, where D ¼ diagð P j W 1;j ;. .. ; P j W N;j Þ. Compared to the original Knn densities in<ref type="figure" target="#fig_0">Figure 1</ref>(b), the refined densities in<ref type="figure" target="#fig_0">Figure 1</ref>(c) have fewer 'local maxima' (shown as triangle points, see next section for details). Similar methods have been used in information retrieval and semi-supervised learning applications (<ref type="bibr" target="#b26">Page et al., 1999</ref>). Equation 3 can be solved exactly in the limit by f ¼ ð1 À aÞf 0 ðI À aPÞ À1 if a &lt; 1. Therefore, the above iterations guarantee convergence. When a ¼ 1, f t converges to a left eigenvector of P with the maximum eigenvalue of 1. In our computational experiments, when a &lt; 1, e.g. a ¼ 0:90, Equation 3 typically converges within a few dozen iterations, and can be much faster than the case when a ¼ 1. This rough density estimation process is faster than methods which attempt to solve density estimation to a high degree of precision since density estimation is well known to be a difficult problem. Moreover, our method can be applied to data that are presented in the form of a graph, rather than as data points over the reals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Local-maxima based clustering</head><p>After obtaining densities for points, we estimate the 'modes' of the underlying probability density function. The modes are the 'local maxima' of the density function with zero gradients. For finite samples, the modes are rarely located exactly at points x i 2 D, so we use the points close to the modes instead. Mathematically, modes can be approximated by points fx i jmax jxjÀxij &lt; f j f i g, where is a small distance threshold. The distance is dataset dependent and difficult to choose in practice. Instead, we can define a mode as a vertex whose density is the largest among all of its in-vertices:</p><formula>fv j j8P i;j &gt; 0; f i &lt; f j g (4)</formula><p>Here, we use in-vertices instead of out-vertices in order to be able to detect small cluster with less than K data points. As the vertices of a small cluster with less than K vertices can form a clique (or are highly connected to each other) in the Knn graph, we can detect this small cluster based on the definition of local maxima above if these points are not among the K-nearest neighbours of points outside this cluster. A potential problem is that some outliers with very few in-vertices could be detected as local maxima. We simply remove those local maxima whose numbers of in-vertices are less than K=2. In other words, we treat a cluster less than K=2 in size as an 'outlier' cluster, and densityCut is unlikely to detect this small cluster. Data points that fall into the basin of attraction of each mode constitute a cluster. This process can be done by moving each point along its gradient direction to reach a mode. We modify the efficient graph-based hill-climbing algorithm (<ref type="bibr" target="#b16">Koontz et al., 1976</ref>) to build a unique forest (a set of trees) for a given dataset. The parent of vertex v i is defined as</p><formula>Parentðv i Þ ¼ arg min vj2N i ðjd j À d i jjf i &lt; f j Þ (5)</formula><p>where N i is the set of in-vertices of v i. In other words, the parent of v i is the vertex which is closest to v i among all of v i 's in-vertices that have higher densities than v i. From the construction of the trees, we can see that each vertex is associated with just one tree. Therefore, each tree can be considered as a cluster.<ref type="figure" target="#fig_0">Figure 1</ref>(d) shows the clusters by assigning data points to the seven local maxima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Hierarchical stable clustering</head><p>We then generate a hierarchical cluster tree and select the most stable clustering. First the density of the root of a tree T generated above is called the height of this tree, denoted by h T , which has the largest density among all the vertices in T. Then we define the boundary points between trees T 1 and T 2 :</p><formula>ValleyðT 1 ; T 2 Þ ¼ BðT 1 ; T 2 Þ [ BðT 2 ; T 1 Þ; (7)</formula><p>The height of the valley separating T 1 and T 2 is defined as</p><formula>h ValleyðT 1 ;T 2 Þ ¼ max v2ValleyðT 1 ;T 2 Þ f v (8)</formula><p>The saliency index of a valley represents the relative height of the valley compared to the shorter tree:<ref type="figure" target="#fig_1">Figure 2</ref>(a) shows the height of the valley (the length of the grey arrow) separating two adjacent trees, and the saliency index is the ratio between the length of the grey arrow and the black arrow. The saliency index defined in Equation 9 has several properties. First, 0 1, and is invariant under scaling of the densities by a positive constant factor. This 'scale-free' property is very useful for us to select a threshold for merging trees. Second, it automatically scales to the local densities of trees, e.g. to get the same saliency index, the height of the valley separating two trees in low-density regions is shorter than that separating two trees in high-density regions. We can merge two clusters if the saliency index between them is above a threshold. When the saliency index ¼ 1, no clusters get merged, and when ¼ 0, all the connected clusters get merged to form a single cluster. Therefore, by gradually decreasing the saliency index threshold, we can get a hierarchical clustering tree, which is useful for us to interpret the structure of data, especially for highshows the cluster tree from merging neighbouring clusters in<ref type="figure" target="#fig_0">Figure 1</ref>(d). For a dataset consisting of clusters whose densities are considerably different, a single K for density estimation may be insufficient. This is because the points from a high-density cluster need a larger K to estimate their densities, compared to the points from a low-density cluster. The high-density cluster could 'break' into several small clusters when K is small. Instead of picking the parameter K on a data point basis, we can adjust the height of a valley by:</p><formula>ðT 1 ; T 2 Þ ¼ h ValleyðT 1 ;T 2 Þ minðh T 1 ; h T 2 Þ (9)</formula><formula>b h ValleyðT 1 ;T 2 Þ ¼ 1 þ X i f i &lt; h ValleyðT 1 ;T 2 Þ N ! h ValleyðT 1 ;T 2 Þ (10)</formula><p>The intuition behind this density adjustment step is that a highdensity valley could be an artefact caused by splitting a high-density cluster.<ref type="figure" target="#fig_0">Figure 1(f)</ref>shows the cluster tree using the saliency indexes based on the adjusted valley heights. A potential problem of this step is that it also increases the valley height between two genuine clusters and increases their likelihood to merge. For example, as can be seen from<ref type="figure" target="#fig_0">Figure 1</ref>(e, f), the two clusters merge at saliency index around 0.30 before valley adjustment, but merge at saliency index around 0.35 after valley adjustment. We will further investigate the influence of this density adjustment step on clustering in later sections. We finally introduce a method to determine the number of clusters to produce the final clustering from the hierarchical cluster tree. The basic idea is that by gradually decreasing the saliency index, clusters will get merged. Noisy, non-salient clusters will get merged quickly, and true clusters will exist for a long period of time. We therefore can calculate the length of saliency index change for producing a fixed number of clusters, and select the cluster number which spans the longest saliency index changes. In our current implementation of densityCut, we decrease the saliency index evenly and therefore we can interpret the saliency index change interval as 'Frequency'.<ref type="figure" target="#fig_0">Figure 1</ref>(g) shows the cluster number frequency plot, and<ref type="figure" target="#fig_0">Figure 1</ref>(h) shows the final clustering by merging the initial clustering to produce two clusters as selected by the cluster number frequency plot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Complexity analysis and implementation</head><p>densityCut has been implemented in the statistical computation language R. densityCut has a worst-case time complexity of OðNK þC 2 Þ and a space complexity of OðNK þ C 2 Þ, where N is the number of data points, K is the number of neighbours and C is the number of clusters (local maxima). In practice, as a majority of clusters are only adjacent to few clusters, the time and space complexity is typically of OðNK þ CÞ. We did not consider the time used to compute the Knn graph in densityCut as numerous algorithms have been developed for efficient Knn search with different complexities, and typically it takes less time to compute the Knn graph compared to cluster the data. To build the Knn graph given a data matrix, efficient algorithms such as kd-trees can be used in low-dimensional spaces (D 20) with time complexity OðNlogðNÞÞ (<ref type="bibr" target="#b24">Mount, 1998</ref>). To build the Knn graph in high-dimensional spaces (D &lt; 1000), efficient software libraries based on random projection exist to repeatedly partition the data to build a tree (https://github.com/spo tify/annoy). This algorithm can run in O(NDT) time, where T is the number of trees, and typically dozens of trees are enough to preserve the accuracy of Knn search. To demonstrate the scalability of densityCut, we tested densityCut on a Mac desktop computer running OS X Version 10.9.5. The computer has 32 GB of RAM and a 3.5 GHz four-core Intel i7 processor with 8 MB cache. We carried out all the experiments presented in the article on this computer.<ref type="figure" target="#fig_1">Figure 2</ref>(b), the running time increased almost linearly in the number of data points. It took about 127 CPU seconds to cluster a million data points (N ¼ 2 20 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Parameter setting</head><p>Our algorithm has two parameters: the number of nearest neighbours K, and the damping factor a in density refinement. K should be small enough to detect local maxima, e.g. smaller than the number of data points in a cluster. However, very small K can result in poor density estimates and produce large numbers of clusters, thus 'overfitting' the data, and there may not exist a 'gap' in the cluster number frequency plot for us to select the number of clusters. On the contrary, for large K, densityCut may fail to detect detailed structures thus 'underfitting' the data. Theoretical analysis for spectral clustering shows that K should be XðlogðNÞÞ to produce a connected graph (<ref type="bibr" target="#b34">Von Luxburg, 2007</ref>), and limit results are obtained under conditions K=logðNÞ ! 1 and K=N ! 0. K is also dependent on the dimensionality D. For the density estimate at x (f ðx) is Lipchitz smooth in a neighbour of x) from its K-nearest neighbours, under conditions k=N 2=ð2þDÞ ! 0 and k ! 1, we can get j b f ðxÞ À f ðxÞjf ðxÞ= ffiffiffi k p (<ref type="bibr" target="#b5">Dasgupta et al., 2014</ref>). In practice, K should be dataset dependent. For example, if the Euclidean distance is used, K should be sufficiently small such that the Euclidean distance is a good measure of the distance between two close data points even the data lie in a manifold. If the number of clusters is small, K should increase to prevent generating too many local maxima. We therefore conducted an empirical study of the influence of K and a on clustering the data in<ref type="figure" target="#fig_0">Figure 1</ref>, for which N ¼ 240 (Supplementary Figs S1 and S2). First, when K ¼ log 2 ðNÞ ¼ 8, densityCut correctly detected the two clusters given different values for a (Supplementary<ref type="figure" target="#fig_0">Fig. S1</ref>). Small K ¼ log 2 ðNÞ ¼ 4 produced 'spiky' density estimates and resulted in many local maxima (Supplementary<ref type="figure" target="#fig_0">Fig. S1</ref>). On the contrary, large K produced flat density estimates, and the two true clusters tended to merge because of no deep valley between them (Supplementary<ref type="figure" target="#fig_0">Fig. S1</ref>). We therefore used a default value of K ¼ log 2 ðNÞ. In addition, when a ¼ 0:9 or 0.99, densityCut correctly detected the two clusters given different values for K. Increasing a produced better clustering results but it took much longer for the density refinement step to converge, e.g. median 176 iterations when a ¼ 0:99 compared to 41 iterations when a ¼ 0:90. We set the default value for a ¼ 0:90 as it made a good compromise between accuracy and execution time. The valley height adjustment step plays a role of 'smoothing' the density estimates. This functionality is especially useful for small K. For example, even when K ¼ 0:5log 2 ðNÞ ¼ 4, densityCut correctly detected the two clusters after adjusting the heights of valleys (Supplementary<ref type="figure" target="#fig_1">Fig. S2</ref>). For all the results presented in this article, we used the default parameter setting (K ¼ log 2 ðNÞ and a ¼ 0:9) with the valley height adjustment step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Benchmarking against state-of-the-art algorithms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Synthetic datasets</head><p>We benchmarked densityCut against state-of-the-art clustering algorithms for biological datasets (<ref type="bibr" target="#b36">Wiwie et al., 2015</ref>) on synthetic densityCut: an efficient and versatile clustering algorithmdatasets that have been widely used to evaluate clustering algorithms. Specifically, we compared densityCut with three best algorithms reported in<ref type="bibr" target="#b36">Wiwie et al. (2015)</ref>(Supplementary Materials), i. e. the hierarchical clustering algorithm (HC, from the R stats package) with average linkage, the partitioning around medoids (PAM, from the R cluster package) algorithm and the density-based clustering algorithm OPTICS (from the R dbscan package). We also compare densityCut with the Gaussian mixture model (GMM, from the R mclust package) based clustering algorithm and the normalized cut (NCut, from the R kernlab package) spectral clustering algorithm. We used ten synthetic datasets (the eight 'shape' datasets and the two most challenging 'S-sets'), downloaded from http://cs.joen suu.fi/sipu/datasets/. Seven out of the ten datasets were also used in<ref type="bibr" target="#b36">Wiwie et al. (2015)</ref>(except for the most challenging case 'S4' from the S-set, the challenging 'Jain' dataset and the 'D31' dataset with large number of clusters – 31 clusters). For the algorithms requiring the number of clusters as a input parameter (PAM, GMM and NCut), we set the number of clusters as the ground truth cluster number for each dataset. For HC, we cut the dendrogram to produce the ground truth number of clusters for each dataset. To measure clustering performance, we compared the clustering results from each algorithm to the ground truth to compute the maximum-matching measure (MMM, range from 0 to 1), the normalized mutual information (NMI, range from 0 to 1) and the adjusted Rand index (ARI, with expected value of zero and maximum value of one). For all these measures, high values mean high similarity between two sets. More information about these measures can be found in Supplementary Materials. As shown in Supplementary<ref type="figure" target="#fig_3">Figure S3</ref>, densityCut performed the best in terms of the above evaluation measures (with mean MMM, NMI and ARI of 0.911, 0.897 and 0.854). Overall, the clustering results on these synthetic benchmark datasets demonstrated that densityCut can produce excellent results if the high-density clusters were separated by low-density valleys. For the datasets in Supplementary<ref type="figure" target="#fig_3">Figure S3</ref>(a, c–e, g–j), densityCut detected the right number of clusters and revealed the structures of these datasets. The red colour cluster in Supplementary<ref type="figure" target="#fig_3">Figure S3</ref>(b) was considered as two separated clusters originally (<ref type="bibr" target="#b38">Zahn, 1971</ref>). However, the sparse background points and the centre high-density points could be considered as from the same cluster for density-based clustering. For the dataset in<ref type="figure" target="#fig_3">Figure 3</ref>(f), densityCut failed to detect the three clusters because there were no 'deep' valleys between the outer arc cluster and the two Gaussian clusters. Therefore, the outer arc cluster got merged to the right Gaussian cluster. Without the valley height adjustment step, densityCut generated the same clustering (Supplementary<ref type="figure">Fig. S4</ref>). The density-based clustering algorithm OPTICS ranked second with mean MMM, NMI and ARI of 0.840, 0.717 and 0.685, respectively (Supplementary<ref type="figure" target="#fig_3">Fig. S3</ref>, last column; Supplementary<ref type="figure" target="#fig_5">Fig. S5(a)</ref>). PAM and GMM performed poorly on the datasets consisting of irregular shape clusters (Supplementary<ref type="figure" target="#fig_3">Fig. S3</ref>(a–f), last column). PAM results on these datasets had mean MMM, NMI and ARI of 0. 687, 0.492 and 0.414, respectively, and GMM results on these datasets had mean MMM, NMI and ARI of 0.617, 0.441 and 0.311, respectively. However, PAM did very well on the datasets where the points in each cluster were sampled from a two-dimensional Gaussian distribution with mean MMM, NMI and ARI of 0.908, 0. 870 and 0.828 (Supplementary<ref type="figure" target="#fig_3">Fig. S3</ref>(g–j), last column). In contrast, GMM performed inferior to PAM on these datasets with mean MMM, NMI and ARI of 0.787, 0.826 and 0.716. For example, for 'D31' in Supplementary<ref type="figure" target="#fig_5">Figure S5</ref>(e), cluster two consisted of only a single data point, and cluster nine consisted of two data points. Cluster three and 18 consisted of points from multiple Gaussian distributions. One major reason for the failure of GMM was that the relatively large number of clusters (31) compared to the limited number of data points (3100) and the overlapped clusters resulted in many local maxima in its objective log-likelihood function, while the Expectation–Maximization algorithm for fitting GMM only searched for a local maximum of the objective function. By contrast, densityCut directly located the high-density peaks and selected the most stable clustering, and thus it was less likely influenced by spurious density peaks. Both HC and NCut can cluster datasets consisting of arbitrary shape clusters. On the datasets consisting of non-convex shape clusters, HC (with mean MMM, NMI and ARI of 0.788, 0.589 andMMM: 0.848, NMI: 0.831, ARI: 0.828Clustering the somatic mutations from sequencing a lung/pancreas metastasis pair of a melanoma patient. The possible 'driver' mutations in each cluster are labeled with a black plus sign 'þ'. The clustering validation indices (MMM, NMI and ARI) were from comparing densityCut results with sciClone results or the results reported in the original studies. (a) Three-dimensional VAF plot. The mutations in each cluster were assigned a unique colour. The mutations with a circle ' ' were considered as outliers in the original publication (<ref type="bibr" target="#b9">Engle et al., 2015</ref>) before clustering analysis. (b) The number of clusters produced by densityCut as we gradually increased K from log 2 ðNÞ to 10log 2 ðNÞ. (c) The mutation assigned to the violet colour cluster by sciClone but assigned to the red colour cluster by densityCut was labeled with a circle ' '. (d) densityCut and sciClone execution time based on repeated ten runs. The hierarchical clustering trees and the cluster number frequency plots are in Supplementary<ref type="figure">Figure S7</ref>0.577) and NCut (with mean MMM, NMI and ARI of 0.771, 0.628 and 0.55) performed slightly better than PAM and GMM (Supplementary<ref type="figure" target="#fig_3">Fig. S3</ref>(a–f), last column). On the datasets consisting of convex shape clusters, HC (with mean MMM, NMI and ARI of 0.810, 0.841 and 0.746) and NCut (with mean MMM, NMI and ARI of 0.746, 0.814 and 0.678) performed slightly worse than PAM and GMM (Supplementary<ref type="figure" target="#fig_3">Fig. S3</ref>(g–j), last column). Compared to HC and NCut, densityCut performed better in both the datasets consisting of arbitrary shape clusters (with mean MMM, NMI and ARI of 0.922, 0.919 and 0.886) and the datasets consisting of convex shape clusters (with mean MMM, NMI and ARI of 0.895, 0. 863 and 0.807). Moreover, densityCut had low complexity and was scalable to large datasets.</p><formula>(a) ( b) (c) (e) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Microarray gene expression data</head><p>Gene expression data have been used to stratify cancer patients into biologically or clinically meaningful subtypes, e.g. different subtypes of patients have distinct prognosis. Here, we tested densityCut on the two microarray gene expression datasets as in Baek et al.</p><formula>(2011)</formula><p>(Supplementary Materials). The results in Supplementary<ref type="figure">Figure S6</ref>show that densityCut performs the best on these two datasets (with MMM, NMI and ARI of 0.926, 0.780 and 0.853 on dataset one, and 0.981, 0.860 and 0.924 on dataset two) compared with PAM, HC, OPTICS, NCut and GMM. Although OPTICS produced good results on the previous two dimensional synthetic datasets, it performed poorly on these highdimensional gene expression datasets (ten dimensions are considered as high dimensions for density-based clustering in<ref type="bibr" target="#b18">Kriegel et al., 2011</ref>). OPTICS produced just one cluster for each dataset. Although the absolute distances between data points are not discriminative in high-dimensional spaces (the curse of dimensionality, the distances between any two points are approximately the same), the relative distances (the order of closeness) could still be meaningful, and could be captured by the Knn graph. densityCut explores the topology of the Knn graph thus performed better on high-dimensional spaces than OPTICS. GMM (with MMM, NMI and ARI of 0.626, 0.621 and 0.408 on dataset one, and 0.962, 0.765 and 0.850 on dataset two) and PAM (with MMM, NMI and ARI of 0.714, 0.583 and 0.411 on dataset one, and 0.952, 0.727 and 0.815 on dataset two) performed relatively well on these datasets. The results were consistent with the results from the previous study that GMM and PAM (more precisely, the k-means algorithm) performed well on clustering gene expression data (<ref type="bibr" target="#b6">de Souto et al., 2008</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inferring clonal architectures of individual tumours</head><p>Cancer cells are heterogeneous, and a subpopulation of cancer cells of the same patient could harbour different sets of mutations (<ref type="bibr" target="#b23">Miller et al., 2014</ref>). Moreover, cancer cells frequently accumulate additional mutations after treatment or in metastasis. Understanding the clonal architecture of each tumour provides insights into tumour evolution and treatment responses. We used densityCut to cluster the somatic variant allele frequencies (VAF) measured from DNA sequencing of multiple tumour biopsies. The mutations in each cluster were accumulated during a specific stage of clonal expansion. The clustering results provide valuable information of the clonal architectures of tumours. We first tested densityCut on the mutation data from a primary myelofibrosis (PMF) patient (<ref type="bibr" target="#b9">Engle et al., 2015</ref>). This patient was first diagnosed with PMF, and seven years later, this patient's tumour transformed to acute myeloid leukaemia (AML). After chemotherapy treatment, the patient underwent complete remission. However, 1.5 years later, the patient redeveloped PMF but no evidence of AML relapse. A total of 649 single nucleotide variants detected in whole genome sequencing of either PMF, AML or relapse PMF genomes were validated by targeted high-coverage sequencing. We used densityCut to jointly cluster the targeted sequencing VAFs from PMF, AML and relapse PMF tissues.<ref type="figure" target="#fig_3">Figure 3</ref>(a) shows that densityCut grouped the mutations into four clusters. The hierarchical clustering trees and the accompanying cluster number frequency plots are in Supplementary<ref type="figure">Figure S7</ref>(a). Overall, densityCut clustering results matched those presented in the original study. However, to produce the results, the authors (<ref type="bibr" target="#b9">Engle et al., 2015</ref>) used different algorithms and several pre-processing steps. For example, the authors used DBSCAN (<ref type="bibr" target="#b10">Ester et al., 1996</ref>) to detect outliers (the mutations with circles ' ' in<ref type="figure" target="#fig_3">Fig. 3</ref>(a)), and then used Mclust (<ref type="bibr" target="#b11">Fraley et al., 2007</ref>) for model selection and final clustering analysis. The maximum number of clusters was limited to four, and each cluster had to contain at least seven mutations (<ref type="bibr" target="#b9">Engle et al., 2015</ref>). In contrast, we directly used densityCut to cluster the VAFs and produced exactly the same results (MMM ¼ 1, the outliers were not considered in calculating MMM.) We also changed the parameter K from the default log 2 ðNÞ ¼ 10 to 2log 2 ðNÞ until 10log 2 ðNÞ. Only after K was set to 8log 2 ðNÞ, the red colour cluster and the violet colour cluster got merged, as can be seen from<ref type="figure" target="#fig_3">Figure 3</ref>(b). For K &lt; 8log 2 ðNÞ, densityCut produced the same four clusters. OPTICS, PAM, HC, NCut and GMM produced the same results as densityCut results (to be exact, only PAM assigned one point to different clusters, Supplementary<ref type="figure">Fig. S8</ref>(c)). However, except for OPTICS, these algorithms either need the number of clusters as input parameters or cut the dendrogram to produce the desired number of clusters (HC). Next, we tested densityCut on the acute myeloid leukaemia sample AML28 (<ref type="bibr" target="#b7">Ding et al., 2012</ref>). We jointly clustered the VAFs from sequencing both the primary tumour and the relapse tumour after 26 months of chemotherapy (<ref type="bibr" target="#b7">Ding et al., 2012</ref>).<ref type="figure" target="#fig_3">Figure 3</ref>(c)</p><p>shows that densityCut grouped the 804 detected somatic mutations into five clusters. The results matched those predicted by sciClone (<ref type="bibr" target="#b23">Miller et al., 2014</ref>), a variational Bayesian mixture model based clustering algorithm. Only one mutation (with circle ' ' in<ref type="figure" target="#fig_3">Fig. 3(c)</ref>) was assigned to the red cluster by densityCut, but originally assigned to the cyan cluster by sciClone (MMM ¼ 0.999). densityCut is much more efficient than sciClone as can be seen from<ref type="figure" target="#fig_3">Figure 3</ref>(d). sciClone took a median of 48.12 s to run on the AML28 dataset while densityCut took a median of 0.074 s to run. Because it took less than a CPU second to run densityCut, we ran both algorithms ten times to get a more accurate estimation of the time used. For competing algorithms, PAM split the large cluster into two clusters because it tends to generate equal-size clusters (with MMM, NMI and ARI of 0.619, 0.509 and 0.767; Supplementary<ref type="figure">Fig. S8(g)</ref>). HC assigned some 'outliers' to a distinct clusters, and merged the points from two clusters (with MMM, NMI and ARI of 0.939, 0.964 and 0.929; Supplementary<ref type="figure">Fig. S8</ref>(h)). Similarly, GMM modelled the outliers using a Gaussian component (with MMM, NMI and ARI of 0.925, 0.865 and 0.891; Supplementary<ref type="figure">Fig. S8</ref>(j)). OPTICS results differed from densityCut results by the assignment of only one data point (Supplementary<ref type="figure">Fig. S8</ref>(f)), and NCut produced the same results as densityCut results (Supplementary<ref type="figure">Fig. S8</ref>(i)). Finally, we used densityCut to cluster the somatic mutations from whole genome sequencing of the lung/pancreatic metastasis pair from the same melanoma patient MEL5 (<ref type="bibr" target="#b8">Ding et al., 2014</ref>). Compared to blood cancer genomes, melanoma genomes are much more complex, frequency harbouring copy number alterations. The combinations of copy number alterations, homozygous mutations densityCut: an efficient and versatile clustering algorithmof the GW21 cells (<ref type="bibr" target="#b27">Pollen et al., 2014)</ref>). These cells could possibly be separated by selecting a better set of features for clustering analysis. In addition, one GW21 cell was misclassified as a neural progenitor cell (NPC), and one NPC was in the human-induced pluripotent stem (iPS) cell cluster. For the other seven types of cells, densityCut perfectly put them into separate clusters. Supplementary<ref type="figure" target="#fig_0">Figure S12</ref>shows the hierarchical cluster trees and the cluster number frequency plots. Other clustering algorithms such as OPTICS, PAM, HC, NCut and GMM had inferior performance compared to densityCut results with PAM ranked second with MMM, NMI and ARI of 0.877, 0.916 and 0.854, respectively (Supplementary<ref type="figure" target="#fig_0">Fig.  S13</ref>(a)). densityCut grouped the 223 stem cells of dataset two into four clusters (<ref type="figure">Fig. 4</ref>(b)). Glutamate aspartate transporter þ / Prominin1 þ (GP) cells and polysialylated-neural cell adhesion molecule þ (PSA) cells were in separate clusters (except for one PSA cell). The GP cells were subdivided into three clusters, consistent with the original finding that the GP cells consisted of at least three subtypes of stem cells. We next used t-SNE (Van der<ref type="bibr" target="#b32">Maaten et al., 2008</ref>) to project the 1000-dimensional single-cell gene expression data to a two-dimensional space (Supplementary<ref type="figure" target="#fig_0">Fig. S14</ref>). The results also show four very distinct clusters. Compared with the original analysis using hierarchical clustering coupled with principle component analysis feature section (<ref type="bibr" target="#b21">Llorens-Bobadilla et al., 2015</ref>), densityCut can be used in a more unbiased way to cluster singlecell gene expression data and produce the same results. On this dataset, densityCut, PAM, HC and GMM results had average silhouette widths of 0.190, 0.190, 0.191 and 0.189, respectively (Supplementary<ref type="figure" target="#fig_0">Fig. S13</ref>(b))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Clustering single-cell mass cytometry datasets</head><p>Finally, we used densityCut to cluster two benchmark single-cell mass cytometry (aka CyTOF) datasets (<ref type="bibr" target="#b19">Levine et al., 2015</ref>). The first dataset consists of CyTOF data of bone marrow mononuclear cells from a healthy individual. Manually gating assigned 81 747 cells to 24 cell types based on 13 measured surface protein markers (<ref type="bibr" target="#b1">Bendall et al., 2011</ref>). Dataset two contains CyTOF data from two healthy adult donors H1 and H2. For H1, manual gating assigned 72 463 cells to 14 cell types based on 32 measured surface protein markers. Manual gating assigned 31 721 cells to the same 14 cell populations from H2 based on the 32 surface protein markers. These manually identified cell populations were used as ground truth to test densityCut. We compared densityCut to the recently proposed algorithm, the PhenoGraph algorithm (<ref type="bibr" target="#b19">Levine et al., 2015</ref>), in clustering the benchmark single-cell CyTOF datasets. As both densityCut andPhenoGraph first build a Knn graph, we used the same K ¼ log 2 ðNÞ for both algorithms. As can be seen from<ref type="figure" target="#fig_5">Figure 5</ref>, densityCut detected 12 distinct cell types (clusters) in dataset one, 9 cell types in H1, and 12 cell types in H2 (the hierarchical clustering trees and the cluster number frequency plots are in Supplementary<ref type="figure" target="#fig_0">Fig. S15</ref>). The PhenoGraph algorithm detected 18, 24 and 20 clusters in dataset one, H1 and H2, respectively. Based on MMM, NMI and ARI, densityCut performed slightly worse on dataset one (MMM: 0.879 versus 0.883, NMI: 0.878 versus 0.900 and ARI: 0.857 versus 0. 893), but performed better on H1 (MMM: 0.941 versus 0.682, AMI: 0.935 versus 0.833 and ARI: 0.96 versus 0.669) and H2 (MMM: 0.953 versus 0.67, NMI: 0.945 versus 0.829 and ARI: 0. 977 versus 0.638). As for efficiency, densityCut was around two times faster than PhenoGraph based on the current implementations (Supplementary<ref type="figure" target="#fig_0">Fig. S16</ref>). Other clustering algorithms such as PAM, HC, NCut and GMM are not scalable to these relatively large datasets. For example, we can only run OPTICS on the first dataset (OPTICS took about 17 minutes while densityCut took only 24 s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and discussion</head><p>We developed densityCut, a simple and efficient clustering algorithm. densityCut effectively clustered irregular shape synthetic benchmark datasets. We have successfully used densityCut to cluster variant allele frequencies of somatic mutations, single-cell gene expression data and single-cell CyTOF data. densityCut is based on density estimation on graphs. It could be considered as a variation of the spectral clustering algorithms but is much more time-and space-efficient. Moreover, it automatically selects the number of clusters and works for the datasets with a large number of clusters. In summary, densityCut does not make assumptions about the shape, size and the number of clusters, and can be broadly applicable for exploratory data analysis.</p><p>A recent study has shown that current strategies for whole genome sequencing studies missed many somatic mutations (<ref type="bibr" target="#b14">Griffith et al., 2015</ref>). By increasing the sequencing depths from 30Â in their original study (<ref type="bibr" target="#b7">Ding et al., 2012</ref>) to 300Â and using a consensus of somatic single nucleotide variant (SNV) callers, the number of identified SNVs increased from 118 to 1343. Based on the 1343 SNVs, they identified two extra sub-clones (<ref type="bibr" target="#b14">Griffith et al., 2015</ref>). Moreover, an additional 2500 SNVs were highly likely to be genuine somatic SNVs but still without enough evidence even at 300Â coverage. For more complex genomes such as melanoma and breast cancer genomes, the number of SNVs could be much larger. Therefore, efficient algorithms such as densityCut are necessary to infer the clonal structures in individual tumours as more genomes are sequenced at higher coverage in the near future. In recent years, single-cell techniques have empowered scientists to investigate cellular heterogeneity. Computational tools are necessary to analyze these single-cell measurements with high dimensionality and large numbers of cells. Efficient algorithms such as densityCut whose computational complexities are independent of the dimensionality of data, and can cluster millions of points in a few minutes can be valuable tools to process these datasets to distill single cell biology.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. Major steps of the densityCut algorithm. (a) Data points in D. (This dataset was introduced by Fu et al., 2007.) The dotted black circles represent the balls containing K ¼ 8 points from D centred at three example points, i, j and k, whose densities to be estimated. Each point connects to its K ¼ 8 nearest-vertices by orange arrows. Other points connect to i, j and k by green arrows if i, j and k are among the K in-vertices of these points. Notice the asymmetry of in-vertices and out-vertices of a vertex in a Knn graph, e.g. vertex v i has one in-vertex but K ¼ 8 out-vertices. (b) Colour coded Knn estimated densities at points from D. The modes of densities are represented by triangles ''. (c) The refined densities based on a random walk. (d) Initial clustering by assigning data points to modes. (e) The tree created by merging clusters without adjusting valley heights. (f) The tree created by merging clusters based on the saliency index using the adjusted valley heights. (g) The cluster number frequency plot. (h) The final clustering results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.2.</head><figDesc>Fig. 2. (a) Merging the first two trees (clusters) based on the relative height of the valley separating the two trees. (b) The time (in seconds, log 2 transformed) increases almost linearly with the number of data points (log 2 transformed)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.3.</head><figDesc>Fig. 3. Clustering variant allele frequencies (VAF) of somatic mutations. (a, b) Clustering multi-time sample data from initial primary myelofibrosis (PMF), later acute myeloid leukaemia (AML) and after treatment relapsed PMF using densityCut. (c, d) Clustering the somatic mutations from sequencing a primary/relapse pair of an AML patient. (e) Clustering the somatic mutations from sequencing a lung/pancreas metastasis pair of a melanoma patient. The possible 'driver' mutations in each cluster are labeled with a black plus sign 'þ'. The clustering validation indices (MMM, NMI and ARI) were from comparing densityCut results with sciClone results or the results reported in the original studies. (a) Three-dimensional VAF plot. The mutations in each cluster were assigned a unique colour. The mutations with a circle ' ' were considered as outliers in the original publication (Engle et al., 2015) before clustering analysis. (b) The number of clusters produced by densityCut as we gradually increased K from log 2 ðNÞ to 10log 2 ðNÞ. (c) The mutation assigned to the violet colour cluster by sciClone but assigned to the red colour cluster by densityCut was labeled with a circle ' '. (d) densityCut and sciClone execution time based on repeated ten runs. The hierarchical clustering trees and the cluster number frequency plots are in Supplementary Figure S7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.5.</head><figDesc>Fig. 5. Comparing densityCut and PhenoGraph in clustering single-cell mass cytometry data. The original high-dimensional mass cytometry data were projected onto two dimensional spaces by t-SNE just for visualization purpose. Cell types and clustering memberships of data points were colour coded. (a–c) The ground truth. (d–f) densityCut clustering results. (g–i) PhenoGraph clustering results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>We sampled f2 12 ; 2 14 ; 2 16 ; 2 18 ; 2 20 g data points from a mixture of 64 two-dimensional Gaussian distributions. As can be seen from</figDesc><table></table></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">J.Ding et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from and heterozygous mutations make it a challenging task to develop a model to uncover the clonal structure of these cancer genomes (Roth et al., 2014). The densityCut clustering results in Figure 3(e) show that the mutations in MEL5 could be grouped into 12 clusters, providing the starting point for detailed inspection of the clonal structure of this cancer genome. Additional information such as copy number alterations would be required to fully interpret the clonal architectures. We also ran sciClone, which produced ten clusters (Supplementary Fig. S9). Both algorithms agreed in clustering 84. 8% of the mutations (MMM: 0.848, Fig. 3(e)). densityCut clustering had an average silhouette width of 0.58 (Supplementary Fig. S10), which was higher than sciClone clustering average silhouette width of 0.55 (Supplementary Fig. S11). Other competing algorithms performed inferrer to densityCut with PAM and OPTICS performed second and third with average silhouette widths of 0.548 and 0.539, respectively (Supplementary Fig. S8(m, k)). 3.3 Clustering single-cell gene expression datasets We used densityCut to cluster two single-cell mRNA gene expression datasets. The first dataset consists of the low-coverage mRNA expression of 23 730 genes in 301 cells from 11 populations (Pollen et al., 2014). The second dataset consists of the single-cell mRNA expression of 43 309 genes in 223 stem cells from the subventricular zone of eight-week-old male mice (Llorens-Bobadilla et al., 2015). We did several pre-processing steps to only select a subset of genes (Pollen et al., 2014) for clustering analysis because the high technique noise in single-cell gene expression data (e.g. loss of cDNA in reverse transcription and bias in cDNA amplification) and Knn search in high dimensional spaces is still time-consuming. Specifically, we only kept the genes expressed in more than five cells because it is difficult to detect clusters less than five in size given the relatively large number of cells. Here, a gene was considered to be expressed in a cell if its reads per kilobase per million (RPKM) value (or fragments per kilobase per million (FPKM) value for dataset two (Llorens-Bobadilla et al., 2015)) was greater than or equal to one in the cell. We then further normalized the RPKM values by log transformation: log 2 ðx þ 1Þ. Here, x was the original RPKM value of a gene in a cell. A small value of one was added to prevent taking the log of zero or generating very small numbers. Figure 4(a) shows that densityCut produced nine clusters for dataset one (MMM: 0.917, NMI: 0.953 and ARI: 0.918). densityCut cannot distinguish the cells from GW16, GW21 and GW21.2 based on the 1000 genes. These cells were quite similar as they were all from the human cortex (GW16 cells were from the germinal zone of human cortex at gestational week 16, GW21 cells were from GW21 and GW21.2 cells were cultured cells of a subset</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank Xuehai Wang from the Terry Fox Laboratory for the help in processing the CyTOF data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixtures of common t-factor analyzers for clustering high-dimensional microarray data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Baek</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1269" to="1276" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Single-cell mass cytometry of differential immune and drug responses across a human hematopoietic continuum</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">C</forename>
				<surname>Bendall</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">332</biblScope>
			<biblScope unit="page" from="687" to="696" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Rates of convergence for the cluster tree</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Chaudhuri</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twenty-fourth Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Mean shift, mode seeking, and clustering</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Cheng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="790" to="799" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Mean shift: a robust approach toward feature space analysis</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Comaniciu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimal rates for k-nn density and mode estimation</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Dasgupta</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twenty-eighth Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2555" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Clustering cancer gene expression data: a comparative study</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">C</forename>
				<surname>De Souto</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Clonal evolution in relapsed acute myeloid leukaemia revealed by whole-genome sequencing</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Ding</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">481</biblScope>
			<biblScope unit="page" from="506" to="510" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Clonal architectures and driver mutations in metastatic melanomas</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Ding</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">111153</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Clonal evolution revealed by whole genome sequencing in a case of primary myelofibrosis transformed to secondary acute myeloid leukemia</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Engle</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Leukemia</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="869" to="876" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ester</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Model-based methods of classification: using the mclust software in chemometrics</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Fraley</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Softw</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">FLAME, a novel fuzzy clustering method for the analysis of DNA microarray data</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Fu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">The estimation of the gradient of a density function, with applications in pattern recognition</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Fukunaga</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="32" to="40" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimizing cancer genome sequencing and analysis</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Griffith</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="210" to="223" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<monogr>
		<title level="m" type="main">Clustering Algorithms</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">A</forename>
				<surname>Hartigan</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">A graph-theoretic approach to nonparametric cluster analysis</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">L</forename>
				<surname>Koontz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="936" to="944" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Pruning nearest neighbor cluster trees</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Kpotufe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 28th International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Density-based clustering</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">P</forename>
				<surname>Kriegel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdiscip. Rev. Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="231" to="240" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Data-driven phenotypic dissection of AML reveals progenitor-like cells that correlate with prognosis</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">H</forename>
				<surname>Levine</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="184" to="197" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Power iteration clustering</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Lin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 27th International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="655" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Single-cell transcriptomics reveals a population of dormant neural stem cells that become activated upon brain injury</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Llorens-Bobadilla</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Stem Cell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="329" to="340" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">An advancement in clustering via nonparametric density estimation</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Menardi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Comp</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="753" to="767" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">SciClone: Inferring clonal architecture and tracking the spatial and temporal patterns of tumor evolution</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">A</forename>
				<surname>Miller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1003665</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<monogr>
		<title level="m" type="main">ANN programming manual</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">M</forename>
				<surname>Mount</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">On spectral clustering: analysis and an algorithm</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">Y</forename>
				<surname>Ng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 15th Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Page</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Low-coverage single-cell mRNA sequencing reveals cellular heterogeneity and activated signaling pathways in developing cerebral cortex</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">A</forename>
				<surname>Pollen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1053" to="1058" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Clustering by fast search and find of density peaks</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Rodriguez</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">344</biblScope>
			<biblScope unit="page" from="1492" to="1496" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Pyclone: statistical inference of clonal population structure in cancer</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Roth</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="396" to="398" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Shi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">A generalized single linkage method for estimating the cluster tree of a density</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Stuetzle</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comp. Graph. Stat</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="397" to="418" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Van Der Maaten</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Quick shift and kernel methods for mode seeking</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Vedaldi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 10th European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="705" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Von Luxburg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Comp</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<monogr>
		<title level="m" type="main">Mode analysis: a generalization of nearest neighbor which reduces chaining effects</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Wishart</surname>
			</persName>
		</author>
		<editor>Cole,A. (ed.) Numerical Taxonomy</editor>
		<imprint>
			<date type="published" when="1969" />
			<publisher>Academic Press</publisher>
			<biblScope unit="page" from="282" to="311" />
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<analytic>
		<title level="a" type="main">Comparing the performance of biomedical clustering methods</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Wiwie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1033" to="1038" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b37">
	<analytic>
		<title level="a" type="main">Identification of cell types from single-cell transcriptomes using a novel clustering method</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Xu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="31" to="1974" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph-theoretical methods for detecting and describing gestalt clusters</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">T</forename>
				<surname>Zahn</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="68" to="86" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>