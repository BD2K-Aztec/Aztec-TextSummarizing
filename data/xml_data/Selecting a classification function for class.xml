
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Selecting a classification function for class prediction with gene expression data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Victor</forename>
								<forename type="middle">L</forename>
								<surname>Jong</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Biostatistics &amp; Research Support</orgName>
								<orgName type="department" key="dep2">Julius Center for Health Sciences and Primary Care</orgName>
								<orgName type="institution">University Medical Center Utrecht</orgName>
								<address>
									<postCode>3508 GA</postCode>
									<settlement>Utrecht</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Viroscience Lab</orgName>
								<orgName type="institution">Erasmus Medical Center Rotterdam</orgName>
								<address>
									<postCode>3015</postCode>
									<settlement>Rotterdam</settlement>
									<region>CE</region>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Putri</forename>
								<forename type="middle">W</forename>
								<surname>Novianti</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Biostatistics &amp; Research Support</orgName>
								<orgName type="department" key="dep2">Julius Center for Health Sciences and Primary Care</orgName>
								<orgName type="institution">University Medical Center Utrecht</orgName>
								<address>
									<postCode>3508 GA</postCode>
									<settlement>Utrecht</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Epidemiology &amp; Biostatistics Department</orgName>
								<orgName type="institution">Vrije University Medical Center Amsterdam</orgName>
								<address>
									<postCode>1081</postCode>
									<region>HV Amsterdam</region>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Kit</forename>
								<forename type="middle">C B</forename>
								<surname>Roes</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Biostatistics &amp; Research Support</orgName>
								<orgName type="department" key="dep2">Julius Center for Health Sciences and Primary Care</orgName>
								<orgName type="institution">University Medical Center Utrecht</orgName>
								<address>
									<postCode>3508 GA</postCode>
									<settlement>Utrecht</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Marinus</forename>
								<forename type="middle">J C</forename>
								<surname>Eijkemans</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Biostatistics &amp; Research Support</orgName>
								<orgName type="department" key="dep2">Julius Center for Health Sciences and Primary Care</orgName>
								<orgName type="institution">University Medical Center Utrecht</orgName>
								<address>
									<postCode>3508 GA</postCode>
									<settlement>Utrecht</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Selecting a classification function for class prediction with gene expression data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btw034</idno>
					<note type="submission">Received on 12 October 2015; revised on 18 December 2015; accepted on 15 January 2016</note>
					<note>Gene expression *To whom correspondence should be addressed. Associate Editor: Ziv Bar-Joseph Contact: v.l.jong@umcutecht.nl Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Class predicting with gene expression is widely used to generate diagnostic and/or prognostic models. The literature reveals that classification functions perform differently across gene expression datasets. The question, which classification function should be used for a given dataset remains to be answered. In this study, a predictive model for choosing an optimal function for class prediction on a given dataset was devised. Results: To achieve this, gene expression data were simulated for different values of gene-pairs correlations, sample size, genes&apos; variances, deferentially expressed genes and fold changes. For each simulated dataset, ten classifiers were built and evaluated using ten classification functions. The resulting accuracies from 1152 different simulation scenarios by ten classification functions were then modeled using a linear mixed effects regression on the studied data characteristics, yielding a model that predicts the accuracy of the functions on a given data. An application of our model on eight real-life datasets showed positive correlations (0.33–0.82) between the predicted and expected accuracies. Conclusion: The here presented predictive model might serve as a guide to choose an optimal classification function among the 10 studied functions, for any given gene expression data. Availability and implementation: The R source code for the analysis and an R-package &apos;SPreFuGED&apos; are available at Bioinformatics online.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Microarray gene expression profiling has become a widely used tool to identify particular disease subpopulations and to perform diagnostic and prognostic predictions (<ref type="bibr" target="#b8">Huang et al., 2010;</ref><ref type="bibr">van 't Veer et al., 2002</ref>). In clinical practice, they are used in diagnostic and prognostic analyses while in preclinical studies (toxicogenomics), they involve predicting the toxicity of compounds in animal models with the goal of speeding up the evaluation of toxicity for new drug candidates (<ref type="bibr" target="#b18">Shi et al., 2010</ref>). Though class prediction analysis is a common practice, the question that remains to be addressed is, given the wide availability of classification functions nowadays, which classification function do we use for a particular dataset? Classification functions have been shown to perform differently across gene expression datasets (<ref type="bibr" target="#b11">Lee et al., 2005</ref>). Moreover, the MAQC-II initiative has pointed out that classification function is one of the variables that explains the variabilitybetween gene expression class prediction performance (<ref type="bibr" target="#b18">Shi et al., 2010</ref>). While substantial amount of information is known about the characteristics of classification functions and class prediction building procedures, little is known about which data characteristics have impact on the performance of a class prediction model. For instance, diagonal linear discriminant analysis (DLDA) assumes no covariances and hence no correlations between variables and might fail if the data is highly correlated. On the other hand, linear discriminant analysis (LDA) assumes a common covariance matrix for the classes and thus to some extent, accounts for correlations (<ref type="bibr" target="#b7">Hastie et al., 2003</ref>). In addition, pernalized regressions like ridge, lasso, elastic net are capable to handle correlated variables. Support Vector Machine (SVM), though commonly understood as a method of finding the maximum-margin hyperplane, may also be seen as a regularization function estimation problem, corresponding to a hinge loss function with a quadratic penalty as that of ridge regression (<ref type="bibr" target="#b7">Hastie et al., 2003;</ref><ref type="bibr" target="#b27">Ye et al., 2011</ref>). And it has been shown by (<ref type="bibr" target="#b26">Yang et al., 2006</ref>) that if a group of non-distinct variables are selected as input variable set, its training time lengthened and the errors become bigger. On the other hand, tree-based methods are by nature designed to capture interactions between variables while neural networks might capture other complex structures within a given dataset. Given the above observations, it is obvious that the performance of these functions depends on the characteristics of the data in question. Despite this, the literature on how to choose a classification function for a given dataset is sparse. A common practice is comparing several classification functions and selecting the one with the minimum error rate but this has been pointed by<ref type="bibr" target="#b1">Bernau et al. (2013</ref><ref type="bibr" target="#b3">), Ding et al. (2014</ref>, Tibshirani and Tibshirani (2009) and Varma and Simon (2006) to lead to selection bias. As such, some experimenters adhere to one or a few classification functions irrespective of the dataset, disease or medical question being addressed. While others choose a classification function for their datasets by affinity or familiarity without taking into account the characteristics of such data. A simulation study by<ref type="bibr" target="#b10">Kim and Simon (2011)</ref>shows that correlation is one of the data characteristics that affect the performance of most probabilistic classification functions. In addition,<ref type="bibr" target="#b9">Jong et al. (2014)</ref>showed that correlation structures differ across gene expression data of different etiological diseases. The study by<ref type="bibr" target="#b13">Novianti et al. (2015)</ref>shows that microarray gene expression data characteristics like log 2 fold change of expression values, number of deferentially expressed genes and pairwise correlations between genes are associated to the accuracy of classification functions. However, this study was conducted in real-life gene expression datasets, where the magnitude and/or direction of association might have been confounded by unobserved data characteristics. In this study, we aim to provide a guideline for making a choice of a classification function for a binary class prediction problem based on observed magnitudes and directions of the data characteristics, using accuracy as a measure of evaluation. We investigate the effect of sample size, proportion of deferentially expressed (DE) genes, genes' variances, log fold changes, pairwise correlations between DE and noisy genes on the accuracy of classification functions using extensive simulations. The remainder of this article is organized as follows: methodology to simulate data, classification functions considered and the building and evaluation of class prediction models are presented in Section 2; Section 3 contains a predictive summary of the results of class prediction models for different simulated scenarios; Section 4 provides an application of our predictive model from the simulated results on real-life microarray gene expression datasets and Section 5 presents a discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Simulated data (scenarios)</head><p>To simulated gene expression data, we hypothesized that sample size, proportion of DE genes, genes' variances, log fold changes, pairwise correlations between DE and noisy genes might be associated to the performance of classification functions. These six variables were to be systematically varied in our simulations. From observed correlation structures in real-life gene expression datasets (<ref type="bibr" target="#b9">Jong et al., 2014</ref>), we generalized the structure as shown in<ref type="figure" target="#fig_1">Figure 1</ref>, containing three clusters referred to as up-regulated (UR), down-regulated (DR) and noisy genes. The absolute values of pairwise correlation for DE genes (q) were varied as 0.00, 0.25, 0.50 and 0.75 with UR cluster taking oppositely-signed correlation values for DR cluster. The pairwise correlations both within the noisy cluster and between the noisy and the DE clusters were per gene-pair randomly drawn from a normal distribution centered at zero with a standard deviation h i.e. Nð0; hÞ where h ¼ 0.00, 0.25, 0.50, 0.75. The scenario q ¼ h ¼ 0.00 corresponds to complete independence. Resulting correlation values lying outside the interval<ref type="bibr">[À1, 1]</ref>were uniformly converted to the intervals<ref type="bibr">[</ref>. With the correlation values and the variances, the within covariance matrices R 0 and R 1 were constructed for the two classes. In addition, the proportion of DE genes (p) was also allowed to take up 1, 3 and 5% of the total number of genes, as values. This resulted to 192 different complex covariance matrices that were used to simulate the data for different values of other variables. Finally, two different values of absolute log 2 fold change (D) and three different sample sizes (n) were considered (<ref type="figure" target="#tab_1">Table 1</ref>). For a fixed number of genes (p ¼ 1000) and n samples, the samples' labels (0,1) were generated from a Bernoulli distribution with a probability 0.5 and the gene expression data of p Â n dimension was generated from a multivariate normal distribution with mean vectors from a uniform distribution, U(6,10) of length p and the covariance matrices corresponding to the above description, using Cholesky decomposition Golub and van Loan (1996) as a method to determine the root of the covariance matrix. The mean log 2 expression values of DE genes were incremented or decremented with the corresponding log 2 fold change value for samples in class 1. The choice of multivariate normal(UR), down-regulated (DR) and noisy (Non-DE) genes distribution and mean vector corresponds to the practical assumption that gene expression data are normally distributed in log 2 scale and based on observation that the log 2 expression values often fall in the interval (0, 16). For each combination of the values of the data characteristics, the dataset was simulated as shown in<ref type="figure" target="#fig_2">Figure 2</ref>(Algorithm 1), yielding 1152 different simulation scenarios, each of which was randomly replicated 1000 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Classification functions</head><p>Ten elective choices of classification functions were chosen to represent the broad list in the literature that falls within the categories: discriminant analyses or Bayes classifiers, tree-based, regularization and shrinkage, nearest neighbors and neural networks methods. For discriminant analyses, linear discriminant analysis (LDA), quadratic discriminant analysis (<ref type="bibr" target="#b12">McLachlan, 1992</ref>) and shrunken centroid discriminant analysis (SCDA) or prediction analysis of microarrays (PAM) (<ref type="bibr" target="#b23">Tibshirani et al., 2002</ref>) were selected. Random forest (RF) (<ref type="bibr" target="#b2">Breiman, 2002</ref>) was chosen as tree-based method while support vector machines (SVM) (Schö lkopf and<ref type="bibr" target="#b17">Smola, 2002</ref>), ' 1 penalized logistic regression or Lasso (PLR1) (<ref type="bibr" target="#b21">Tibshirani, 1996</ref>), ' 2 penalized logistic regression or Ridge (PLR2) (<ref type="bibr" target="#b28">Zhu, 2004</ref>) as well as ' 1 and ' 2 penalized logistic regression or Elastic net (PLR12) (<ref type="bibr" target="#b29">Zou and Trevo, 2005</ref>) were considered for regularized and shrinkage methods. Finally, k-nearest neighbors (KNN) and feed-forward neural network (NNET) (<ref type="bibr" target="#b15">Ripley, 1996</ref>) were the lone choices for nearest neighbors and neural networks respectively. In machine learning, opinions are that super learners might provide good class predictions but model complexities of these learners are usually high. As such, super learners might not be useful in clinical practice where physicians often want simple class prediction models, that might yield a subset of genes (and possibly coefficients) for easy interpretation. This is because given a subset of genes, focus can be geared toward these genes rather than the entire genome for which experiments are often costly and time consuming. Thus, our choices of classification functions were driven by the choices often made and considered useful in clinical practice.Pairwise correlations of DE genes (q) 0, 0.25, 0.5, 0.75 Gene' variances (r 2 ¼ 1/k) $Exp(k) k ¼ 0.25, 0.50, 1, 1.5 Pairwise correlations of noisy genes (c) $N(0,h) h ¼ 0, 0.25, 0.5, 0.75 50% of p were each up-and down-regulated.For each value of the six variables, the covariance matrix was constructed in step 1, the learning and test data were simulated at steps 2–4 and class prediction models were built and validated in step 5. Steps 2–5 were then repeated 1000 times</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Building and evaluating classifiers</head><p>To assess the dependency of the chosen classification functions on characteristics of the simulated gene expression data, we built on each simulated dataset, class prediction models with all the classification functions listed above. The simulated dataset was considered as a learning set and for classifiers that require pre-selection of genes because of their limitation to accommodate a number of parameters greater than the number of samples (i.e. LDA, QDA and NNET), the genes were ranked by their moderated t statistics (<ref type="bibr" target="#b20">Smyth, 2004</ref>) using the learning set. The learning set was split into a 1 3 inner-test set and 2 3 learning set using 5-fold Monte-Carlo-cross-validation (MCCV) with stratification. The parameter(s) of the classification functions were subsequently tuned using the inner-learning set and evaluated with the inner-test set. These tuning parameters were: number of genes (top k) for LDA and QDA; shrinkage intensity of class centroids for SCDA; with a fixed forest size of 500 trees, the number of variables randomly sampled as candidates at each split and minimum size of terminal nodes for RF; with a linear kernel, the cost of regularization for SVM; ' 1 penalty for Lasso; ' 2 penalty for Ridge; ' 1 &amp; ' 2 penalties for Elastic net; number of nearest neighbors for KNN and finally, the number of genes (top k), number of units in a hidden layer and decay weights for NNET. With the optimal parameter(s) for each classification function, the class prediction models were built using the learning set. The resulting models were evaluated on a test set consisting of 5000 samples generated from the same model as the learning set (see<ref type="figure" target="#fig_2">Fig. 2</ref>). The error rates of the classification functions on this test set were recorded. The process was repeated 1000 times (sampling both learning and test sets) for each simulation scenario and the resulting error rates over the 1000 replications were used for further analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Random effects linear regression</head><p>An average of the error rates of each and every classification function over 1000 replications for each simulated scenario was computed yielding 11 520 data points resulting from the 1152 different simulation scenarios by 10 classification functions. The error rates were then transformed to accuracies (1 À ½error rate þ 0:001Þ and these accuracies were modeled using a linear random effects regression model with the classification function as the random effects clustering variable, by transforming the accuracies to an unbounded range using the logit function. For the ' th standardized study factor, the random effects model is written as:</p><formula>log p ðx ij Þ 1 À p ðx ij Þ ¼ Y ij ¼ b 0 þ # 0j þ ðb 1 þ # 1j ÞX ' ij þ ij</formula><p>where 0 &lt; pðx ij Þ &lt; 1 is the average accuracy in scenario i for classification function j, # j ¼ ð# 0j ; # 1j Þ 0 $ N 0; D ð Þare respectively the random intercepts and slopes of the classification functions while ij $ N 0; r 2 À Á are the independent and identically distributed residuals, also independent from the random effects # j. D is a 2 Â 2 covariance matrix of the random effects. All the aforementioned study factors were evaluated by univariate and multivariate linear random effects regression models. Multivariate regression evaluation was done by a backward selection approach. In each step, two nested models, with and without a particular study factor, were compared by log-likelihood ratio test at 5% significance. Each factor ' was also evaluated by its explained-variation defined as:</p><formula>Var ' ¼ MSE null À MSE ' MSE null</formula><p>where MSE null and MSE l are the mean square errors of the null (random intercept only) and the l th standardized study factor models respectively. The explained variation of the selected multivariate model was also evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Software</head><p>All statistical analyses were performed in R software version 3.2.0, and Bioconductor (<ref type="bibr" target="#b4">Gentleman et al., 2009</ref>) using the following packages: mvtnorm (<ref type="bibr" target="#b5">Genz and Bretz, 2009</ref>) for simulating data, limma (<ref type="bibr" target="#b16">Ritchie et al., 2015</ref>) for ranking genes via linear models, CMA (<ref type="bibr" target="#b19">Slawski et al., 2008</ref>) for predictive classification modeling, lattice (<ref type="bibr" target="#b17">Sarkar, 2008</ref>) for visualization and lme4 (<ref type="bibr" target="#b0">Bates et al., 2015</ref>) for linear random effects modeling. Additionally, we have developed an R package called 'SPreFuGED': Selecting a Predictive Function for Gene Expression Data, that allows researchers to determine an optimal function for a given dataset.<ref type="figure" target="#fig_3">Figure 3</ref>shows the average error rates over the 1000 random replicates (y-axis) of the functions (x-axis) for different combinations of variances, pairwise correlations of noisy (non-DE) genes and DE genes for a fixed sample size (n ¼ 100), proportion of DE genes (p ¼ 5%) and log 2 fold change (D ¼ 1). From this figure, one sees that, the error rates for all functions increase with increasing variances (from top-to bottom-row), pairwise correlation values of non-DE genes (from left-to right-column) and pairwise correlation values of DE genes (different colored lines). On the other hand, other scenarios for different values of sample size, proportion of DE genes and log 2 fold change (Supplementary<ref type="figure" target="#fig_1">Fig. S1A</ref>–C) indicate a negative association of sample size, proportion of DE gene and log 2 fold change to the error rates. The non-constant variability of the error rates between classification functions across scenarios indicates a scenario-specific optimality for each and every classification function. The average accuracies (1 À ½average error rates þ 0:001) of the simulations were summarized to a data matrix as shown on<ref type="figure" target="#tab_2">Table 2</ref>. For each of the predictive variables, a linear random effects regression model was fitted as described in the method section. The individually explained variances of the study factors are depicted on<ref type="figure" target="#fig_5">Figure 4</ref>. This figure shows that sample size, pairwise correlations of non-DE genes and the proportion of DE genes are the leading factors respectively accounting for approximately 17, 14 and 13% of the null variance. While genes' variances and fold change respectively account for 8 and 7% of the null variance, pairwise correlations between DE genes accounts for simply 1%. As observed graphically, the univariate models (results not shown) confirmed a positive association of sample size, proportion of DE genes and fold change, and a negative association of pairwise correlations of non-DE, DE and the genes' variances to the accuracies. For the multivariate linear random effects regression model, we started with a complex model of random intercepts and slopes and three ways interactions of the predictive factors. Starting with pairwise correlation between DE genes because of its low individually</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification in gene expression</head><p>explained variance, we eliminated variables using the log-likelihood ratio test. We ended up with the model presented on<ref type="figure" target="#tab_3">Table 3</ref>consisting of the fixed effects two ways interactions of all the six predictive factors, random intercepts and slopes. This model explains approximately 70% of the null variance as illustrated on<ref type="figure" target="#fig_5">Figure 4</ref>. The left panel of<ref type="figure" target="#tab_3">Table 3</ref>presents the estimates of fixed effects, the standard errors and the t statistics while the top-right panel presents the net effect of a standard deviation (SD) unit increase of a given factor conditional on common values of other factors. Finally, the bottom-right panel presents the performances of the classification functions at different values of the predictive factors. From the top-right panel of this table, one notices that a 1 SD unit increase in sample size, corresponding to n ¼ 89.67 will lead to an increase in the Log odds (accuracy), with the highest increase observed when other variables are at their highest values. A similar effect is observed for a 1 SD unit increase in the proportion of DE genes. Though a 1 SD unit increase in fold change leads to an crease in the Log odds, as sample size and proportion of DE genes, its effect is highest when the other variables are at their lowest values. While on the average a 1 SD unit increase in the genes' variances, pairwise correlations of non-DE and DE genes will lead to a decrease in the accuracy, these effects become very severe when other variables are at their highest values. For very low values of other variables, a 1 SD unit increase of pairwise correlations between DE genes could even lead to an increase (a positive effect) on the accuracy as was previously observed and illustrated diagrammatically by<ref type="bibr" target="#b13">Novianti et al. (2015)</ref>. A similar effect is observed for a 1 SD unit increase in the genes' variances at very low values of other variables. These varying effects, indicate the complex interactions between the study factors and hence illustrate why classification functions will perform differently on different datasets. Lastly, the bottom-right panel of the table shows that all classification functions will perform reasonably well if the predictive factors are at their average values (0 SD) with PLR1, PLR12, LDA and QDA having outstanding performances. For extremely small values<ref type="figure" target="#tab_1">Table S1</ref>where both PLR1 and PLR12 fail when other variables are fixed at-2SD and otherCorr or DECorr is varied from-1SD to 2SD. It must be noted however that the combination of all other variables simultaneously being at À2, or at þ2, is highly unlikely.</p><formula>. . .. . .. . .. . .. . .. . .. . .. . .. .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11515">LDA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Application</head><p>To evaluate the predictive ability of the here presented random effects regression model on real-life data, eight Affymetrix gene<ref type="figure" target="#tab_3">Table 3</ref>Classification in gene expressionexpression datasets of the 25 non-cancerous datasets described in one of our previous studies (<ref type="bibr" target="#b13">Novianti et al., 2015</ref>) were used. These datasets were selected to include a variety of Array platforms, both class-balance and class-imbalance, number of DE probesets, as well as various sample sizes. Three of these datasets were preprocessed without filtering while the other five were preprocessed and filtered as described by<ref type="bibr" target="#b13">Novianti et al. (2015)</ref>. We quantified the data characteristics studied and presented on<ref type="figure" target="#tab_4">Table 4</ref>as follows:</p><formula>$ p $ ~ r 2 q $ h $ D $ StdSampSize (n $ ) 0</formula><p>(i) sampSize, by counting the samples in the study, (ii) propDE, by ranking the probesets using limma (<ref type="bibr" target="#b16">Ritchie et al., 2015</ref>) and computing the proportion of DE probesets based on a log 2 fold change cutoff of 1 if the number of DE is !10 or 0.5 otherwise, (iii) variance, was determined as the mean of the variances of all the probesets, (iv) log2FC, computed as the mean log 2 fold changes of the DE probesets, (v) deCorr as the mean of the elements of the upper-(lower-) triangular of the correlation matrix of the DE probesets and (vi) otherCorr, was computed as the standard deviation (SD) of the elements of the upper-(lower-) triangular of the correlation matrix of non-DE probesets. This matrix was computed from all non-DE probesets if they were less than 20 000 or a sample of 20 000 from these non-DE probesets otherwise. These data characteristics were standardized using the mean and SD of the respective variables from the simulated data. And our model was used to predict the accuracies for all classification functions in each dataset (Supplementary<ref type="figure" target="#fig_2">Fig. S2</ref>). We then built and evaluated classifiers using the classification functions by splitting the data into 2 3 learning set and 1 3 test set with stratification and a 3-fold inner cross-validation on the learning set for parameters optimization. This step was repeated a hundred times, each time predicting the accuracies of classification functions on the learning set using the random effects model and also recording the expected (observed) accuracies on the test set. These predicted and observed accuracies over the 100 repetitions are respectively presented on Supplementary<ref type="figure" target="#fig_3">Figure S3A</ref>and B. To compare the predicted to observed accuracies, and considering that we are interested in the ordering of performance (i.e. determining an optimal function for a given data), we used the ranked base Spearman correlation between the average predicted accuracies and the average observed accuracies. The results of this comparison for each dataset are presented on<ref type="figure" target="#fig_6">Figure 5</ref>. The positive correlation values on this figure indicate agreement between our predicted and observed accuracies. Though these correlations are not very high in some datasets, our model more or less determined an optimal classification function for all the datasets except for UC7 where Ridge regression and SVM emerged first instead of fourth as predicted (i.e. 87.5% sensitivity). Nevertheless, the model was able to rule out on which classification(s) will perform poor on a given dataset, with approximately 100% certainty. As expected, the performance of the functions deteriorate on CF (small sample size and low proportion of DE probesets), Dia2 (high class-imbalance and small fold changes), UC2 (low proportion of DE probesets and small fold changes), UC3 (large variances and high correlations) and UC7 (low proportion of DE probesets). From<ref type="figure" target="#fig_6">Figure 5</ref>and Supplementary<ref type="figure" target="#fig_3">Figure S3A</ref>and B, one sees that except on the UC3 data, our model's accuracies are less than or equal to observed accuracies. The model performs well on dataset with large sample sizes and balanced classes (UC2, UC3 and UC5). It attained its lowest performance on Dia2 where there is high class-imbalance and hence few samples of the small class in the learning set and on HIV2 and CF datasets with small sample sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We hypothesized that the performance of classification functions on gene expression data depends on sample size, proportion of DE genes, genes' variances, log 2 fold changes between DE genes and magnitude of the pairwise correlation within DE genes and non-DE genes, and showed their association to the accuracies of ten often used and clinically relevant classification functions using simulations. Additionally, we built a predictive model to determine an optimal classification function among the studied functions using the simulation results. An application of the predictive model on eight non-cancerous real-life gene expression datasets predicted optimal function(s) for seven out of the eight and was able to rule out function(s) that will perform poor on almost all the datasets. This model may serve as a guide for choosing a classification function for a given gene expression data. Classification functions have been shown to perform differently across gene expression datasets (<ref type="bibr" target="#b11">Lee et al., 2005</ref>) and data characteristics have been shown to differ across datasets and are associated to the performance of classification functions (<ref type="bibr" target="#b9">Jong et al., 2014;</ref><ref type="bibr" target="#b13">Novianti et al., 2015</ref>). While sufficient knowledge is available on the properties of most classification functions and procedures to build class prediction models using gene expression data have been outlined by<ref type="bibr" target="#b25">Wessels et al. (2005)</ref>, little is known about data characteristics that accounts for the variability in the performance of classification functions and how to use these characteristics to choose an optimal classification function for a specific dataset. As such, most researchers adhere to specific classification function(s) or randomly choose a classification for their class prediction models irrespective of the disease or data under study. A common practice is to evaluate several classification functions and select the one with smallestmisclassification error but this leads to selection bias (<ref type="bibr" target="#b3">Ding et al., 2014</ref>). In this study, we outlined data characteristics together with clinically relevant and often used classification functions and investigated their effects on classification performance using simulation studies. Based on these simulation studies, we provided a guide for choosing an optimal classification function for a specific dataset using the data's characteristics and the studied classification functions through a linear random effects predictive model. As a metamodel one would expect it to explain close to 100% of the variance in the simulated data but our predictive model accounts for approximately 70% of the variability in the simulated data. The remaining 30% unexplained variance may be associated to sampling variability stemming from the several (192) random covariance matrices used to generate both learning and test sets as well as the different learning and test sets generated at each iteration. Although we used different classification functions and evaluated these functions using accuracy, our simulation results confirm the findings of Kim and Simon (2011) that classifiers tend to have poor performance on highly correlated data. Our results also agree with those of<ref type="bibr" target="#b13">Novianti et al. (2015)</ref>that correlations, the absolute log 2 fold changes and the number of DE probesets are associated to the accuracy of a class prediction model. In addition, these results specify clearly the directions of the association and point out the effects of other data characteristics like sample size, genes' variances that were not previously identified.</p><p>Most importantly, we have provided a predictive model that can serve as a guide to choose a classification function for a given dataset and its application on eight real-life datasets (both filtered and unfiltered) indicated a good predictive ability of the model. Although our model was reasonably good in its prediction on real-life data, we want to point out that it might have failed in some datasets because of the following reasons: (i) most of the eight non-cancerous datasets had small sample sizes and splitting these datasets to learning and test sets yielded even smaller sample sizes of the learning sets and hence might have led to poor estimates of the characteristics under study and (ii) the observed accuracies might not be the true accuracies because of the few Bootstrap samples. It could have been better if we had the means to perform several Bootstraps but due to the small sample sizes, the number of independent Bootstrap samples is limited. The fact that our predictions were most often slightly lower than the observed accuracies for almost all classification functions might indicate the general trend that the performance of a model usually decreases on an independent dataset. Hence, our model's predictions might reflect expected accuracies on independent datasets. In the simulated data, we assumed exponential and normal distributions for the variances and pair-wise correlation of non-DE genes respectively. These distributional assumptions might be violated in some datasets. As such, it will be worth trying different distributions. Also, we used accuracy as a measure of evaluation by minimizing the loss function but in clinical applications, probabilities are more informative than simple yes or no predictions because they quantify theuncertainty of a prediction (<ref type="bibr" target="#b14">Pepe, 2005</ref>). As such, it is worth evaluating these data characteristics on probabilistic classification functions where by the log-likelihood function is optimized, this might possibly provide a predictive model that will be most useful in clinical applications. Despite these limitations, our model was found to work well with data containing reasonably large and balanced sample sizes (n ! 30). As such, our results apply to balanced class data. For data with class-imbalance some classification functions will have deteriorating performance, for which several solutions are proposed. However, this topic is outside the focus of the current study. In summary, our results serve as a guide to use data characteristics to choose an optimal classification function for a given dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>À1, À0.15] and [0.15, 1] for negative and positive values respectively. The variances of the genes r 2 ¼ 1 k À ) were drawn from an exponential distribution i.e. expðkÞ where k ¼ 0.25, 0.50, 1.00, 1.50. The distributional assumptions were made based on observation from real-life datasets as experienced by Jong et al. (2014) and Novianti et al. (2015)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.1.</head><figDesc>Fig. 1. Assumed correlation structure. Contains 3 clusters of up-regulated (UR), down-regulated (DR) and noisy (Non-DE) genes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.2.</head><figDesc>Fig. 2. Algorithm to simulate data, build and validate class prediction models. For each value of the six variables, the covariance matrix was constructed in step 1, the learning and test data were simulated at steps 2–4 and class prediction models were built and validated in step 5. Steps 2–5 were then repeated 1000 times</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.3.</head><figDesc>Fig. 3. Average misclassification error rates of the ten classification functions for sample size of 100, log 2 fold change of 1 and 5% DE genes. Top-row to bottom row indicate increase in variance (1/k) while from left-column to right-column indicate increase in the pairwise correlation of non-DE genes and the different colored lines from (blue–red) indicate increase in the pairwise correlations of DE genes (Color version of this figure is available at Bioinformatics online.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><figDesc>PAM PLR1 PLR12 PLR2 QDA RF SVM StdPropDE*StdDECorr À0.119 0.009 À13.745 2 SD Log odds À1.797 1.237 À0.511 À1.540 2.117 2.037 À0.977 0.957 0.265 À1.018 StdVariance*StdDECorr 0.048 0.009 5.519 1 SD À0.445 1.835 0.550 À0.232 2.451 2.396 0.178 1.632 1.085 0.150 StdSampSize*StdVariance À0.161 0.009 À18.571 0 SD 0.089 1.617 0.794 0.258 1.968 1.938 0.517 1.491 1.088 0.500 StdPropDE*StdVariance À0.172 0.009 À19.815 À1 SD À0.193 0.580 0.221 À0.070 0.667 0.662 0.038 0.532 0.273 0.033 StdSampSize*StdPropDE 0.249 0.009 28.729 À2 SD À1.294 À1.273 À1.170 À1.214 À1.451 À1.430 À1.258 À1.245 À1.359 À1.251</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.4.</head><figDesc>Fig. 4. Proportion of the null variance explained by each and every studied factor. The selected model refers to the predictive model presented on Table 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig.5.</head><figDesc>Fig. 5. Predicted versus Expected (Observed) accuracies. Cor represents Spearman correlations between the predicted and observed accuracies (Color version of this figure is available at Bioinformatics online.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>V C The Author 2016. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 1814 Bioinformatics, 32(12), 2016, 1814–1822 doi: 10.1093/bioinformatics/btw034 Advance Access Publication Date: 11 February 2016 Original Paper</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><figDesc>Table 1. Simulated gene expression data characteristics</figDesc><table>Data characteristics 
Values 

Sample size (n) 
20, 50, 100 
Proportion of DE genes (p) 
1%, 3%, 5% 
Log 2 fold change of DE genes (D) 
0.5, 1 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 2. Structure of the performance data generated from evaluating the classification functions on the simulated data</figDesc><table>ID 
Classifier SampSize propDE Variance deCorr otherCorr log2FC Acc 

1 
SVM 
100 
5 
0.667 
0.00 
0.00 
1 
0.999 
2 
SVM 
100 
5 
0.667 
0.25 
0.00 
1 
0.984 
3 
SVM 
100 
5 
0.667 
0.50 
0.00 
1 
0.961 
4 
SVM 
100 
5 
0.667 
0.75 
0.00 
1 
0.950 
5 
KNN 
100 
5 
0.667 
0.00 
0.25 
1 
0.970 
. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><figDesc>Table 3. Fixed effects estimates (left panel) and their conditional on other factors net effects (right panel)</figDesc><table>Fixed effects 
Conditional net effects of study factors 

Parameter 
Estimate Std. 
Error 

t value 
1 SD unit increase 

Intercept 
1.026 0.220 
4.663 
Other 
variables 
n 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><figDesc>Table 4. Characteristics of the eight datasets used for evaluating the predictive model</figDesc><table>No. 
Study 
ID þ 
Affymetrix Platform 
Probesets 
SampSize 
propDE 
Variance 
deCorr 
otherCorr 
log2FC 

1 
CF* 
E-GEOD-10406 
HG U133 Plus 2.0 
54 675 
15 (09, 06) 
0.267 
0.143 
1.205 
0.414 
0.408 
2 
CS 
E-MEXP-2236 
HG U133 Plus 2.0 
5422 
20 (10, 10) 
1.881 
0.654 
1.200 
0.368 
0.418 
3 
Dia2 
E-CBIL-30 
HG U133A 
1749 
26 (18, 08) 
2.859 
0.444 
0.604 
0.611 
0.434 
4 
HIV2 
E-GEOD-14278 
HG U133 Plus 2.0 
11 286 
18 (09, 09) 
1.435 
0.523 
1.157 
0.616 
0.393 
5 
UC2* 
E-GEOD-21231 
HG 1.0 ST 
32 321 
40 (20, 20) 
0.402 
0.139 
0.631 
0.257 
0.268 
6 
UC3 
E-GEOD-36807 
HG U133 Plus 2.0 
6541 
28 (15, 13) 
6.849 
0.735 
1.381 
0.715 
0.503 
7 
UC5 
E-MTAB-331 
HG 1.0 ST/HG 1.1 ST 
1402 
59 (30, 29) 
1.427 
0.461 
1.390 
0.951 
0.286 
8 
UC7* 
E-GEOD-6731 
HG U95AV2 
12 625 
28 (11, 19) 
0.135 
0.116 
1.280 
0.474 
0.311 

*No filtering was performed.; þ: ArrayExpress accessing ID. 
The sixth to the eleventh columns correspond to the variable under study. (.,.) represent the sample sizes for each class. </table></figure>

			<note place="foot">at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">V.L.Jong et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank the VIRGO consortium and its sponsors, the Netherlands Genomics Initiative and the Dutch Government for the financial support. Next to this, we thank Martin Marinus and the HPC-team at UMC Utrecht for the high performing computing facilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This work has been supported by the VIRGO consortium, which is funded by the Netherlands Genomics Initiative and by the Dutch Government (FES0908). The funding agencies in no way influenced the outcome or conclusions of the study.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<monogr>
		<title level="m" type="main">lme4: Linear mixed-effects models using Eigen and S4. R package version 1</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Bates</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Correcting the optimal resampling-based error rate by estimating the error rate of wrapper algorithms</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Bernau</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="693" to="702" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Random forest</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Breiman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Bias correction for selecting the minimal-error classifier from many machine learning models</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Ding</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3152" to="3158" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Bioconductor: open software development for computational biology and bioinformatics</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Gentleman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<monogr>
		<title level="m" type="main">Computation of Multivariate Normal and T Probabilities</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Genz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Bretz</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Heidelberg, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Matrix Computations</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Golub</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Van Loan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Johns Hopkins</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note>3rd. edn</note>
</biblStruct>

<biblStruct   xml:id="b7">
	<monogr>
		<title level="m" type="main">The Elements of Statistical Learning: Data Mining, Inference and Prediction</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hastie</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer</publisher>
			<pubPlace>NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Genomic indicators in the blood predict drug-induced liver injury</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pharmacogenomics J</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="267" to="277" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploring homogeneity of correlation structures within and between gene expression datasets of different etiological disease categories</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Jong</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Appl. Genet. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="717" to="732" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Probabilistic classifiers with high-dimensional data</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Kim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Simon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="399" to="412" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">An extensive comparison of recent classification tools applied to microarray data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Lee</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Statist. Data Anal</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="869" to="885" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<monogr>
		<title level="m" type="main">Discriminant Analysis and Statistical Pattern Recognition</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Mclachlan</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Wiley</publisher>
			<pubPlace>NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Factors affecting the accuracy of a class prediction model in gene expression data</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Novianti</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinf</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="0199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluating technologies for classification and prediction in medicine</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Pepe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Med</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="3687" to="3696" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<monogr>
		<title level="m" type="main">Pattern Recognition and Neural Networks</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Ripley</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Limma powers differential expression analyses for RNA-sequencing and microarray studies</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ritchie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">47</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<monogr>
		<title level="m" type="main">Lattice: Multivariate Data Visualization with R Learning with Kernels</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Sarkar</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Ny</forename>
				<surname>Springer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Schö Lkopf</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Smola</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT Press</publisher>
			<pubPlace>MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">The MicroArray Quality Control (MAQC)-II study of common practices for the development and validation of microarray-based predictive models</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Shi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="827" to="838" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">CMA-a comprehensive Bioconductor package for supervised classification with high dimensional data</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Slawski</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinf</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">439</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Linear models and empirical Bayes methods for assessing differential expression in microarray experiments</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Smyth</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Appl. Genet. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Statist. Soc. B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">A bias correction for the minimum error rate in cross-validation</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="822" to="829" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Diagnosis of multiple cancer types by shrunken centroids of gene expression Gene expression profiling predicts clinical outcome of breast cancer</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Nat. Acad. Sci. USA</title>
		<meeting>. Nat. Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="6567" to="6572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Bias in error estimation when using cross-validation for model selection</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Varma</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Simon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinf</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">91</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">A protocol for building and evaluating predictors of disease state based on microarray data</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Wessels</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="3755" to="3762" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Correlation coefficient method for support vector machine input samples</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Yang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn. Cybern</title>
		<imprint>
			<biblScope unit="page" from="2857" to="2861" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient variable selection in support vector machines via the alternating direction method of multipliers</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Ye</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Statist</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="832" to="840" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Classification of gene expression microarrays by penalized linear regression</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Zhu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="427" to="443" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Zou</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Trevo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Statist. Soc. B</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>