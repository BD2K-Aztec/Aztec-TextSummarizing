
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gene expression Improved mean estimation and its application to diagonal discriminant analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012">2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName>
								<forename type="first">Tiejun</forename>
								<surname>Tong</surname>
							</persName>
							<email>tongt@hkbu.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Hong Kong Baptist University</orgName>
								<address>
									<settlement>Kowloon Tong, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Liang</forename>
								<surname>Chen</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Biological Sciences</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Hongyu</forename>
								<surname>Zhao</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Epidemiology and Public Health</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Genetics</orgName>
								<orgName type="institution">Yale University School of Medicine</orgName>
								<address>
									<postCode>06520</postCode>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gene expression Improved mean estimation and its application to diagonal discriminant analysis</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">28</biblScope>
							<biblScope unit="issue">4</biblScope>
							<biblScope unit="page" from="531" to="537"/>
							<date type="published" when="2012">2012</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btr690</idno>
					<note type="submission">Received on August 24, 2011; revised on November 24, 2011; accepted on December 8, 2011</note>
					<note>[15:29 9/2/2012 Bioinformatics-btr690.tex] Page: 531 531–537 Associate Editor: Martin Bishop Contact:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: High-dimensional data such as microarrays have created new challenges to traditional statistical methods. One such example is on class prediction with high-dimension, low-sample size data. Due to the small sample size, the sample mean estimates are usually unreliable. As a consequence, the performance of the class prediction methods using the sample mean may also be unsatisfactory. To obtain more accurate estimation of parameters some statistical methods, such as regularizations through shrinkage, are often desired. Results: In this article, we investigate the family of shrinkage estimators for the mean value under the quadratic loss function. The optimal shrinkage parameter is proposed under the scenario when the sample size is fixed and the dimension is large. We then construct a shrinkage-based diagonal discriminant rule by replacing the sample mean by the proposed shrinkage mean. Finally, we demonstrate via simulation studies and real data analysis that the proposed shrinkage-based rule outperforms its original competitor in a wide range of settings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the advent of high-throughput technologies, learning highdimensional complex models is critical in many disciplines such as biology, genetics, epidemiology, geology, ecology, neurology and engineering. One such example is microarray data, where the expression levels of thousands of genes are measured simultaneously from each sample. Due to the cost and/or other experimental difficulties such as the availabilities of biological materials, it is common that high-throughput data are collected only in a limited number of samples. They are referred to as high-dimensional data with small sample size, or 'large G small n' data, where G is the number of dimensions and n is the sample size. Highdimensional data pose many challenges to traditional statistics methods. Specifically, due to the small n, there are more uncertainties associated with standard estimations of parameters such as the mean and variance estimations. As a consequence, statistical analyses based on such parameter estimation are usually unreliable. To obtain * To whom correspondence should be addressed. more accurate estimation of parameters some statistical methods, such as regularizations through shrinkage, may yield better results. Shrinkage-based methods have been proposed in recent years to improve the variance estimation for 'large G small n' data. See for example,<ref type="bibr" target="#b2">Baldi and Long (2001)</ref>, Storey and<ref type="bibr" target="#b37">Tibshirani (2003)</ref>, Wright and Simon (2003), Smyth (2004),<ref type="bibr" target="#b7">Cui et al. (2005)</ref>, Tong and<ref type="bibr" target="#b38">Wang (2007)</ref>, Opgen-Rhein and Strimmer (2007) and<ref type="bibr" target="#b39">Wang et al. (2009)</ref>, among many others. In contrast to the advances on variance estimation, little attention has been paid to improving the mean estimation for high-dimensional data until recently (<ref type="bibr" target="#b17">Hausser and Strimmer, 2009;</ref><ref type="bibr" target="#b20">Hwang and Liu, 2010</ref>). In this article, we investigate the family of shrinkage estimators for the mean value which is tailored to the high-dimensional data such as microarrays. Specifically, we will propose the optimal shrinkage parameter under the quadratic loss function for the data when G tends to be infinite. Class prediction with high-dimensional data has been recognized as a very important problem and received much attention in different fields such as genomics, proteomics, brain images, medicine and machine learning. For high-dimensional data with small sample sizes, it is known that the traditional classification methods such as the linear discriminant analysis are not applicable as the sample covariance is going to singular when G is greater than n. To overcome the singularity problem,<ref type="bibr" target="#b10">Dudoit et al. (2002)</ref>introduced two diagonalized discriminant rules, the diagonal linear discriminant analysis (DLDA) and the diagonal quadratic discriminant analysis (DQDA). When the sample size is small, DLDA performed remarkably well compared with more sophisticated classifiers in terms of both accuracy and stability (<ref type="bibr">Dettling, 2004;</ref><ref type="bibr" target="#b25">Lee et al., 2005</ref>). In addition, DLDA is easy to implement and is not sensitive to the number of predictor variables. Though DLDA performed well for high-dimensional small sample size data, there is still room to improve it (<ref type="bibr" target="#b19">Huang et al., 2010;</ref><ref type="bibr" target="#b31">Pang et al., 2009;</ref><ref type="bibr" target="#b32">Pang et al., 2010</ref>). In particular, we notice that the mean estimation (the sample mean) in DLDA will be unreliable when the sample size is not sufficiently large. As a consequence, the performance of DLDA may also be unsatisfactory. With this insight, we propose in this article an improved version of DLDA, which replaces the sample mean by the optimal shrinkage estimator. We expect that the proposed shrinkage-based DLDA will improve the classification accuracy in practice. The remainder of the article is organized as follows. In Section 2, we investigate the family of shrinkage estimators for the mean value under the quadratic loss function. With the nature of highdimensional data, we assume that the variances are unequal and unknown. Under regularity conditions, we discuss the choices of thePage: 532 531–537</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T.Tong et al.</head><p>shrinkage parameter and then evaluate their practical performance via numerical studies. In Section 3, we construct a shrinkage-based diagonal discriminant rule by replacing the sample mean by the proposed shrinkage mean. Simulation studies will also be conducted to evaluate its performance over its original competitor. In Section 4, we use the leukemia data to demonstrate that the proposed method is widely applicable and performs well. Finally, we conclude the article in Section 5 with a brief discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">IMPROVED MEAN ESTIMATION</head><formula>Let X j = (X 1j ,...,X Gj ) T , j = 1</formula><p>,...,n, be independent G-dimensional vectors normally distributed with mean μ = (μ 1 ,...,μ G ) T and covariance matrix. Let ¯ X = n j=1 X j /n be the sample mean. Let ¯</p><formula>X 2 = ¯ X T ¯ X and ¯ X 2 = ¯ X T −1 ¯ X</formula><p>for any invertible matrix. In this section, we consider estimating μ with respect to the quadratic loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivation</head><p>Shrinkage estimation of means has a long history starting with the seminal paper of James and<ref type="bibr" target="#b22">Stein (1961)</ref></p><formula>ˆ μ JS = 1− (G−2)/n ¯ X 2 / ˆ σ 2 0 ¯ X,</formula><p>wherê σ 2 0 is the pooled estimator of σ 2. More generally, when is non-diagonal and unknown, the James-Stein type estimator has the form (<ref type="bibr" target="#b26">Lin and Tsai, 1973;</ref><ref type="bibr" target="#b14">Gleser, 1986</ref>)</p><formula>ˆ μ JS = ⎛ ⎝ 1− (G−2)/n ¯ X 2 ˆ ⎞ ⎠ ¯ X, wherê = n j=1 (X j − ¯ X)(X j − ¯ X) T /(n−G+2)</formula><p>is the pooled sample covariance matrix. To guaranteê non-singular, we require that n &gt; G. Note that this is not the case for high-dimensional data where G can be much larger than n. Therefore, the existing shrinkage methods for estimating μ break down and cannot apply to the high-dimensional data directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Proposed mean estimator</head><p>To overcome the singularity problem, we assume is diagonal and denote it by D = diag(σ 2 1 ,...,σ 2 G ). As mentioned in<ref type="bibr" target="#b21">Hwang et al. (2009)</ref>, most existing shrinkage estimators of μ in the literature required the variances σ 2 i , i = 1,...,G, to be either equal or unequal but known. When σ 2 i = σ 2 for all i, the problem reduces to D = σ 2 I with σ 2 unknown (<ref type="bibr" target="#b22">James and Stein, 1961;</ref><ref type="bibr" target="#b28">Montazeri et al., 2010</ref>). When the σ 2 i are unequal but known, the problem reduces to the case considered in<ref type="bibr" target="#b11">Efron and Morris (1973)</ref>. In this article, we consider the assumption of σ 2 i being both unequal and unknown. Note that the same assumption has been commonly used in the recent literature,</p><formula>X ij |μ i ,σ 2 i i.i.d. ∼ N(μ i ,σ 2 i ), μ i |σ 2 i ∼ N(0,σ 2 i /τ 0 ), σ 2 i ∼ Inv-χ 2 (ν 0 ,σ 2 0 ),</formula><p>where i = 1,...,G, j = 1,...,n, N(·) is the standard normal distribution, and Inv-χ 2 (·) is the scaled inverse chi-squared distribution with unknown hyperparameters (τ 0 ,ν 0 ,σ 2 0 ). The joint prior density of μ i and σ 2 i is given as</p><formula>f (μ i ,σ 2 i ) ∝ (σ 2 i ) −(ν 0 +3)/2 exp − 1 2σ 2 i (ν 0 σ 2 0 +τ 0 μ 2 i ) . Let ¯ X i = n j=1 X ij /n and s 2 i = n j=1 (X ij − ¯ X i ) 2 /(n−1)</formula><p>be the sample mean and the sample variance for gene i, respectively. Let</p><formula>¯ X = ( ¯ X 1 ,..., ¯ X G ) T and S = diag(s 2 1 ,...,s 2 G</formula><p>). By<ref type="bibr" target="#b13">Gelman et al. (2004)</ref>, the posterior distribution of (μ i ,σ 2 i ) can be represented as</p><formula>f (μ i ,σ 2 i |X i1 ,...,X in ) = N(μ i,p , σ 2 i τ 0 +n )·Inv-χ 2 (ν 0 +n,σ 2 i,p ) where μ i,p = n τ 0 +n ¯ X i ,</formula><formula>σ 2 i,p = 1 ν 0 +n ν 0 σ 2 0 +(n+1)s 2 i + nτ 0 τ 0 +n ¯ X 2 i .</formula><p>The posterior mean estimator of μ i is thenˆμ</p><formula>thenˆ thenˆμ i = μ i,p = 1− 1 n/τ 0 +1 ¯ X i , i = 1</formula><p>,...,G.</p><formula>(1)</formula><p>Note that τ 0 in (1) is an unknown parameter. We propose to estimate n/τ 0 +1 by the form of ¯ X 2 S /r, where r is a shrinkage parameter to be tuned. We then have the following estimator for μ,</p><formula>ˆ μ(r) = 1− r ¯ X 2 S ¯ X.</formula><formula>(2)</formula><p>Under the quadratic loss function<ref type="bibr" target="#b12">Fourdrinier et al. (2003)</ref>showed that the estimator (2) dominates ¯ X when 0 &lt; r &lt; 2(G−2)/(n+1).</p><formula>L( ˆ μ,μ,D) = n G ˆ μ−μ 2 D ,</formula><formula>(3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Optimal shrinkage parameter</head><p>In this section, we propose the optimal shrinkage parameter of r &gt; 0 within the family of estimator (2). In Appendix A, we show that the optimal shrinkage parameter is given as</p><formula>r opt = (G−2)E(1/ ¯ X 2 S ) nE ¯ X 2 D /( ¯ X 2 S ) 2 .</formula><formula>(4)</formula><p>Page: 533 531–537</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improved mean estimation</head><p>Note that for high-dimensional data, n is usually small but G is large. We have the follow asymptotic result.</p><formula>¯ X 2 D / ¯ X 2 S a.s.</formula><p>−→ (n−3)/(n−1) as G →∞.</p><p>The proof of Lemma 1 is given in Appendix B. By Lemma 1, for high-dimensional data, we havê r opt ≈<ref type="bibr">[</ref>and the optimal shrinkage estimator isˆμ</p><formula>isˆ isˆμ(ˆ r opt ) = 1−ˆr 1−ˆ 1−ˆr opt ¯ X 2 S ¯ X.</formula><formula>(5)</formula><p>Let˜rLet˜ Let˜r = (G−2)/(n+1) be the middle point of the range 0 &lt; r &lt; 2(G−2)/(n+1) in<ref type="bibr" target="#b12">Fourdrinier et al. (2003)</ref>. It is clear that when n is large, ˆ r opt and˜rand˜ and˜r are asymptotically equivalent. While for highdimensional data with small sample sizes, the difference between these two estimators can be large. For instance, when n = 5, the ratiô r opt /˜ r is given as 2.4; and when n = 6, the ratio is 1.9. The practical performance of these two estimators will be studied in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Evaluation</head><p>The first simulation is to evaluate the performance ofˆμofˆ ofˆμ). Assume that G = 100. We consider four different values of n, ranging from 5, 10, 20 to 50, to represent different levels of sample sizes. We draw σ 2 i from the scaled chisquare distribution χ 2 n−1 /(n−1), and μ i from N(0,τ 2 ), where τ = 0.2, 0.6 or 1 representing different levels of mean heterogeneity. For each gene i, we simulate n observations from the normal distribution N(μ i ,σ 2 i ). We repeat the process 5000 times for each setting and report the simulated average risk AR = 5000 k=1 L(<ref type="figure" target="#tab_1">Table 1</ref>for the four estimators, respectively. Under the quadratic loss function (3), the sample mean ¯ X has a constant risk at 1, as reported in the simulations. The standard errors of these average risks are all around 0.14/ √ 5000 = 0.0020. Therefore, the improvements of the three shrinkage estimators over ¯ X are all statistically significant. We observe thatˆμthatˆ thatˆμ(ˆ r opt ) has a smaller average risk than bothˆμbothˆ bothˆμ JS andˆμandˆ andˆμ(˜ r) in most settings, especially when the sample size is small. In addition, the improvement ofˆμofˆ ofˆμ(ˆ r opt ) over ¯ X increases when the mean heterogeneity decreases, especially when τ is near zero. We also observe that the improvements of the shrinkage estimators over the sample mean become smaller when n becomes larger. This indicates that for the large sample size scenario, it is no longer necessary to borrow information from other genes to improving the mean estimation. Note that we have assumed the grand mean to be zero in the above simulation. Note that when the grand mean has a shift from zero, the term G i=1 μ 2 i /σ 2 i will tend to be larger so that the improvement ofˆμofˆ ofˆμ(ˆ r opt ) over ¯ X will be diminished correspondingly. In such situations, Lindley (1962) suggested to apply the shrinkage method to the deviations X ij − ¯ X ·· rather than to the original observations X ij , which leads to the following variation of the estimator (5),</p><formula>˜ μ ˆ r opt = ¯ X ·· + 1−ˆr 1−ˆ 1−ˆr opt ¯ X − ¯ X ·· 2 S ( ¯ X − ¯ X ·· ),</formula><formula>(6)</formula><formula>μ 0 n = 5 n = 10 n = 20 n = 50 0 ˆ μ(ˆ r</formula><formula>= ( ¯ X ·· ,..., ¯ X ·· ) T</formula><p>is a vector of size G with common values. To evaluate the performance of the Lindley-type estimator (6), we simulate μ i from N(μ 0 ,0.5 2 ) for three different values of μ 0 : 0, 1 and 2. All other settings remain the same as before. We repeat 5000 simulations for each setting and report the average risks of bothˆμ bothˆ bothˆμ(ˆ r opt ) and˜μand˜ and˜μ(ˆ r opt ) in<ref type="figure" target="#tab_2">Table 2</ref>. We observe that the two shrinkage estimators perform similarly when μ 0 = 0. When μ 0 is away from zero, ˆ μ(ˆ r opt ) may perform unsatisfactory while the performance of˜μof˜ of˜μ(ˆ r opt ) remains the same. This indicates that the Lindley-type estimator is robust to the shift of the grand mean. In the remainder of the article, the estimator (6) will be adopted to estimate the mean value unless otherwise specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">IMPROVED DIAGONAL DISCRIMINANT ANALYSIS</head><p>As mentioned in Section 1, class prediction with high-dimensional data is an important problem in high dimensional data analysis. The objective of class prediction is to assign a new observation to one of the K classes based on its given profile. For ease of notation, we define the class labels to be integers ranging from 1 to K with n k observations belonging to class k,k, respectively. Let N = n 1 +···+n K be the total number of observations. For high-dimensional data such as microarrays, it is common that the dimension is much larger than the sample size. As a consequence, traditional classification methods such as the linear discriminant analysis are likely to be inapplicable to such high-dimensional data directly. For instance, the sample covariance matrix is singular when G is larger than N. To overcome the singularity problem,<ref type="bibr" target="#b10">Dudoit et al. (2002)</ref>introduced DLDA and DQDA that ignored the correlations among genes. Specifically, under the assumption that k = for all k, DLDA classifies a new observation y = (y 1 ,...,y G ) to class k which minimizes the following discriminant scorêscorê</p><formula>X k,1 ,...,X k,n k i.i.d. ∼ MVN G (μ k ,, k ), k = 1</formula><formula>d k (y) = (y− ¯ X k ) T [diag( ˆ )] −1 (y− ¯ X k )−2logˆπ−2logˆ −2logˆπ k (7) = G i=1 (y i − ¯ X ki ) 2 / ˆ σ 2 i −2logˆπ−2logˆ −2logˆπ k , k = 1,...,K, where ¯ X k = n k j=1 X k,j /n k = ( ¯ X k1 ,..., ¯ X kG )</formula><p>is the sample mean of class k,</p><formula>ˆ = K k=1 (n k −1) ˆ k /(N −K) = diag( ˆ σ 2 1 ,..., ˆ σ 2 G )</formula><p>is the pooled sample covariance matrix withˆkwithˆ withˆk</p><formula>= n k j=1 (X k,j − ¯ X k )(X k,j − ¯ X k ) T /(n k −1</formula><p>), andˆπandˆ andˆπ k = n k /N are the prior probabilities of which the next new observation is coming from class k. DLDA is also called a 'naive Bayes' classifier as it arises in a Bayesian setting (<ref type="bibr" target="#b5">Bickel and Levina, 2004</ref>).</p><formula>˜ d S k (y), where˜d where˜ where˜d S k (y) = (y−˜μy−˜ y−˜μ k (ˆ r opt )) T [diag( ˆ )] −1 (y−˜μy−˜ y−˜μ k (ˆ r opt ))−2logˆπ−2logˆ −2logˆπ k. (8)</formula><p>We refer to it as shrinkage-mean-based DLDA (SmDLDA). Similarly, we can propose a shrinkage-mean-based version for DQDA as well. The behavior of the proposed SmDLDA will be studied in the next section under various scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Simulated study</head><p>In this section, we conduct simulations to compare the performance of SmDLDA with DLDA. We consider a binary classification with simulate data from multivariate normal distributions MVN</p><formula>(μ 1 ,,) and MVN(μ 2 ,,), respectively.</formula><p>The first simulation study considers an identity covariance matrix, i.e. = I G. To differentiate the two classes, without loss of generality we assume the first d, 0≤ d ≤ G, components of μ 1 and μ 2 are the same and the left ones are different. Specifically, we let</p><formula>μ 1 = {0,...,0,μ 1,d+1 ,...,μ 1G } and μ 2 =−μ 1 , where {μ 1,d+1 ,...,μ 1G }</formula><p>is a random sample of size G−d from the uniform distribution U(0,0.5). We consider two choices of G (50 and 200), and for each G, we consider six different d values at 0, 0.1×G, …, 0.5×G, respectively. For each simulation, we generate a training set of size n k for each class k under the simulation setting described above, and a test set of size 5n k under the same setting to assess the misclassification rate. The overall misclassification rate is calculated by the percentage of misclassified observations over the total numberof observations in the test sets. Finally, we repeat the procedure 1000 times and report the average misclassification rates in<ref type="figure" target="#fig_5">Figure 1</ref>for n 1 = n 2 = 10 and 20, respectively. It is evident that SmDLDA has a smaller misclassification rate than DLDA in all settings. To evaluate and compare the performance of DLDA and SmDLDA under more realistic situations, we conduct here another simulation study in the case where the observations are correlated. Similarly as in<ref type="bibr" target="#b16">Guo et al. (2007) and</ref><ref type="bibr" target="#b31">Pang et al. (2009)</ref>, we use the following block diagonal structure to mimic the true covariance matrix,</p><formula>= ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ ρ 0 ··· ··· .. .</formula><formula>0 −ρ 0 ··· .. . .. . 0 ρ 0 .. . .. . .. . 0 −ρ .. . ··· ··· ··· ··· ··· ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ G×G ,</formula><p>where each diagonal block has the following auto-regressive structure</p><formula>ρ = ⎛ ⎜ ⎜ ⎜ ⎝ 1 ρ ··· ρ 9 ρ 1 ··· ρ 8 .. . .. . .. . .. . ρ 9 ··· ρ 1 ⎞ ⎟ ⎟ ⎟ ⎠ 10×10 .</formula><p>We consider five different values of the correlation coefficient ρ, ranging between 0, 0.2, 0.4, 0.6 and 0.8, to represent different levels of dependence. Note that ρ = 0 corresponds to the independent situation in the previous simulation. All other settings are the same as before except that we set d = 0.1×G in this new simulation. We repeat the procedure 1000 times and report the averagemisclassification rates in<ref type="figure" target="#fig_6">Figure 2</ref>for both DLDA and SmDLDA. Once again, SmDLDA outperforms DLDA in most situations. The comparison result is more evident when the correlation coefficient ρ is not large. Though we have restricted to a balanced binary classification with n 1 = n 2 due to the page limitation, extensive simulations (not shown) indicate that the above comparative conclusions remain the same for unbalanced designs as well as for other simulation settings, including the multiclass comparison problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPLICATION TO LEUKEMIA DATA</head><p>In this section, we apply the proposed discriminant rule, SmDLDA, to the leukemia data of<ref type="bibr" target="#b15">Golub et al. (1999)</ref>. The dataset is available in the website of www.bioconductor.org. By following the same pre-processing steps (thresholding, filtering and logarithm transformation) as described in<ref type="bibr" target="#b10">Dudoit et al. (2002)</ref>, we end up with a gene expression dataset with a total of 3571 genes for 47 acute lymphoblastic leukemia (ALL) patients and 25 acute myeloid leukemia (AML) patients. We further standardize the dataset so that each array has mean zero and variance one across genes. Note that in general, the shrinkage methods work well for dense data rather than sparse data. Thus, to better reveal the practical performance of SmDLDA we perform a preliminary screen of informative features before the case study. Different screen methods are available in the literature so as to identify biologically significant gene functional groups or pathways via Gene Ontology annotations (<ref type="bibr" target="#b30">Pan, 2006;</ref><ref type="bibr" target="#b36">Tai and Pan, 2007</ref>). In this study for simplicity, we will not carry out the screen by integrating the real biological knowledge, but instead, will perform it based on the ratio of the between-group to within-group sums of squares as in<ref type="bibr" target="#b10">Dudoit et al. (2002)</ref>. Note that the gene selection can also be based on other proposals, see forexample, Bayesian variable selection (<ref type="bibr" target="#b24">Lee et al., 2003</ref>), analysis of variance (<ref type="bibr" target="#b9">Draghici et al., 2003</ref>) and independent component analysis (<ref type="bibr" target="#b6">Calò et al., 2005</ref>). Let ALL be class 1 and AML be class 2. The ratio for gene j is given as</p><formula>BW(j) = 2 k=1 n k i=1 ( ¯ X k.j − ¯ X ..j ) 2 2 k=1 n k i=1 (X kij − ¯ X ..j ) 2 ,</formula><p>where ¯ X ..j is the averaged expression values across all samples and ¯ X k.j is that across samples belonging to class k. We select the top G genes (50 and 200) with the largest BW ratios for further study. To assess the misclassification rates for both SmDLDA and DLDA, we randomly divide the total 72 samples into training sets and test sets. We let the training set size for each class ranging from 5, 10, 15 to 20, respectively. The remaining samples are then used as the test sets. Recall that the improvement of shrinkage is inversely proportional to the mean heterogeneity (Section 4). To better improve the performance of the shrinkage-based rule, we propose an adaptive procedure that aims to reduce the possibly large level of mean heterogeneity that may appear in real data. Specifically, we shrink the distances between the class centroids and the overall centroids rather than to shrink the class centroids directly. This is a similar idea as in<ref type="bibr" target="#b37">Tibshirani et al. (2003)</ref>. Finally, for each setting, we repeat the procedure 1000 times and report the average misclassification rates in<ref type="figure" target="#fig_7">Figure 3</ref>. Similarly as in the simulation studies, it is evident again that SmDLDA outperforms DLDA in all settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>In this article, we proposed an optimal shrinkage estimator for the mean value under the 'large G small n' scenario. We then applied the proposed shrinkage estimator to high-dimensional classification problem by constructing a shrinkage-based diagonal discriminant rule. Its improvement over the original competitor was demonstrated through both simulations and real data analysis. Though the independence assumption in this article is popular in the literature (<ref type="bibr" target="#b5">Bickel and Levina, 2004;</ref><ref type="bibr" target="#b10">Dudoit et al., 2002;</ref><ref type="bibr" target="#b21">Hwang et al., 2009;</ref><ref type="bibr" target="#b38">Tong and Wang, 2007</ref>), it is unlikely to be true in practice and so certain remedy might be necessary for a further improvement when additional information is available. Langaas Page: 536 531–537 T.<ref type="bibr">Tong et al. et al. (2005)</ref>suggested that the clumpy dependence is a likely form of dependence, where the clumpy dependence means that the genes are dependent within groups and independent among groups. Inspired by that, one natural extension would be to propose new shrinkage estimators for the mean value under the clumpy dependence structure. To avoid the singularity problem, we might need to assume that the largest group size is not larger than the number of samples. Another future work is to examine if the proposed optimal shrinkage estimator has any good in its own right, or if it can be further improved by its positive-part estimator. Finally, we note that the proposed SmDLDA in this article is a shrinkage-mean-only-based DLDA, whereas in<ref type="bibr" target="#b31">Pang et al. (2009)</ref>the authors proposed a shrinkage-variance-only-based DLDA. As both the mean and variance estimations are crucial in the statistical analysis, further research might be needed to develop new classification rules that shrink both the mean value and the variance. Possible approaches can be either by plugging-in the existing shrinkage estimators, respectively, or by proposing new shrinkage estimators for the mean value and variances simultaneously.</p><formula>= ( ¯ X −μ)−r ¯ X/ ¯ X 2 S , the risk function is R( ˆ μ,μ,D) = n G E( ˆ μ−μ) T D −1 ( ˆ μ−μ) = n G E ( ¯ X −μ) T D −1 ( ¯ X −μ) + r 2 ( ¯ X 2 S ) 2 ¯ X T D −1 ¯ X − r ¯ X 2 S ( ¯ X −μ) T D −1 ¯ X − r ¯ X 2 S ¯ X T D −1 ( ¯ X −μ) = 1+ nr 2 G E ¯ X 2 D ( ¯ X 2 S ) 2 − 2nr G G i=1 E 1 ¯ X 2 S ¯ X i ( ¯ X i −μ i ) σ 2 i . By Stein formula, Eg(X)(X −μ) = σ 2 Eg (X) where X ∼ N(μ,σ 2 ), we have E 1 ¯ X 2 S ¯ X i ( ¯ X i −μ i ) σ 2 i = 1 n E ¯ X i ¯ X 2 S ¯ X i −μ i σ 2 i /n = 1 n E ∂ ∂ ¯ X i ¯ X i ¯ X 2 S = 1 n E 1 ¯ X 2 S − 2 ¯ X 2 i ( ¯ X 2 S ) 2 s 2 i . This leads to R( ˆ μ,μ,D) = nr 2 G E ¯ X 2 D ( ¯ X 2 S ) 2 − 2r(G−2) G E 1 ¯ X 2 S .</formula><p>Finally, by minimizing the above quantity, we have</p><formula>r opt = (G−2)E(1/ ¯ X 2 S ) nE ¯ X 2 D /( ¯ X 2 S ) 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B B1. PROOF OF LEMMA</head><p>Noting that f (x) = 1/x is a continuous function in (0,∞), it suffices to show that ¯</p><formula>X 2 S / ¯ X 2 D a.s.</formula><p>→ ρ n as G →∞. By<ref type="bibr" target="#b0">Assani (1997)</ref>, the following strong law holds under the condition sup m</p><formula>(N m /m) &lt; ∞ almost surely, 1 A G G i=1 a i [Z i −E(Z i )] a.s. −→ 0, as G →∞.</formula><p>Further, we have</p><formula>G i=1 a i Z i /A G a.s.</formula><p>→ EZ 1 = 1/(n−3) for any n ≥ 4. This proves the lemma by noting that A G == ¯</p><formula>X 2 D and ¯ X 2 S = (n−1) G i=1 a i Z i .</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>[15:29 9/2/2012 Bioinformatics-btr690.tex]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>(n−1)(G−2)]/[n(n−3)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><figDesc>It has been widely adopted to analyze high-dimensional data in the real sciences. See for example Speed (2003), Noushath et al. (2006), Asyali et al. (2006) and Heilemann and Schuhr (2008), among others. Note that DLDA uses the sample mean. We propose here a modified version of DLDA by replacing the sample mean ¯ X k in formula (7) by the proposed optimal shrinkage estimates˜μestimates˜ estimates˜μ k (ˆ r opt ) for each class k. We then classify a new observation y to class argmin k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.1.</head><figDesc>Fig. 1. Plots of the average misclassification rates for DLDA and SmDLDA when the observations are independent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig.2.</head><figDesc>Fig. 2. Plots of the average misclassification rates for DLDA and SmDLDA when the observations are correlated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig.3.</head><figDesc>Fig. 3. Plots of the average misclassification rates for DLDA and SmDLDA using leukemia data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>that the sample mean is inadmissible for G ≥ 3. When = I, James and Stein (1961) showed thatˆμ thatˆ thatˆμ JS = 1− (G−2)/n ¯ X 2 ¯ X dominates ¯ X for any G ≥ 3. When = σ 2 I with σ 2 unknown, the James-Stein type estimator is given as (Baranchik, 1970)</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>e.g. in Berger and Bock (1976), Dudoit et al. (2002), Bickel and Levina (2004), Cui et al. (2005), Tong and Wang (2007) and Hwang and Liu (2010) where the correlations among genes are ignored due to the small sample size. Consider the following hierarchical Bayesian model with conjugate priors,</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>Lemma 1. Define N m = #{k : (a k / k i=1 a i ) ≥ 1/m}, where a i = ¯ X 2 i /σ 2 i and #{A} denotes the total number of elements in set A. Assume that sup m (N m /m) &lt; ∞ almost surely. Then for any fixed n ≥ 4, we have</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><figDesc>Table 1. Average risks of the estimators under various settings τ n = 5 n = 10 n = 20 n = 50</figDesc><table>0.2 
¯ 
X 
0.997 
1.001 
0.996 
1.000ˆμ 000ˆ 000ˆμ JS 
0.337 
0.362 
0.485 
0.682ˆμ 682ˆ 682ˆμ(˜ r) 
0.514 
0.408 
0.493 
0.683ˆμ 683ˆ 683ˆμ(ˆ r opt ) 
0.339 
0.359 
0.483 
0.682 
0.6 
¯ 
X 
1.004 
1.001 
1.003 
0.999ˆμ 999ˆ 999ˆμ JS 
0.889 
0.836 
0.897 
0.951ˆμ 951ˆ 951ˆμ(˜ r) 
0.851 
0.840 
0.898 
0.951ˆμ 951ˆ 951ˆμ(ˆ r opt ) 
0.801 
0.827 
0.895 
0.951 
1 
¯ 
X 
0.998 
0.997 
0.998 
1.001ˆμ 001ˆ 001ˆμ JS 
0.973 
0.932 
0.957 
0.984ˆμ 984ˆ 984ˆμ(˜ r) 
0.933 
0.932 
0.957 
0.984ˆμ 984ˆ 984ˆμ(ˆ r opt ) 
0.912 
0.927 
0.956 
0.983 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>Table 2.</figDesc><table>Average risks of the estimators (5) and (6) under various settings 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><figDesc>,...,K, where μ k and k are the mean value and covariance matrix of the G-dimensional multivariate normal distribution for class</figDesc><table>Page: 534 531–537 

T.Tong et al. 

</table></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors are grateful to the editor, the associate editor and three referees for their constructive comments and suggestions that have led to a substantial improvement in the article. The authors are also grateful to Professor J.T. Gene Hwang for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Strong laws for weighted sums of independent identically distributed random variables</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Assani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Duke Math. J</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="217" to="246" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Gene expression profile classification: a review</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">H</forename>
				<surname>Asyali</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="55" to="73" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">A Bayesian framework for the analysis of microarray expression data: regularized t-test and statistical inferences of gene changes</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Baldi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">D</forename>
				<surname>Long</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="509" to="519" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">A family of minimax estimators of the mean of a multivariate normal distribution</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Baranchik</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Stat</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="642" to="645" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Combining independent normal mean estimation problems with unknown variances</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">O</forename>
				<surname>Berger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Bock</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="642" to="648" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Some theory of Fisher&apos;s linear discriminant function, &apos;naive Bayes&apos;, and some alternatives when there are many more variables than observations</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">J</forename>
				<surname>Bickel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Levina</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="989" to="1010" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<monogr>
		<title level="m" type="main">Variable selection in classification problems: a strategy based on independent component analysis New Developments in Classification and Data Analysis. Studies in Classification, Data Analysis, and Knowledge Organization</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">G</forename>
				<surname>Calò</surname>
			</persName>
		</author>
		<editor>Vichi,M. et al.</editor>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="21" to="30" />
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Improved statistical tests for differential gene expression by shrinking variance components estimates</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Cui</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="59" to="75" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Bagboosting for tumor classification with gene expression data</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Dettling</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="3583" to="3593" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Noise sampling method: an ANOVA approach allowing robust selection of differentially regulated genes measured by DNA microarrays</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Draghici</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1348" to="1359" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Comparison of discrimination methods for the classification of tumors using gene expression data</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Dudoit</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Stein&apos;s estimation rule and its competitors-an empirical Bayes approach</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Efron</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Morris</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="117" to="130" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust shrinkage estimation for elliptically symmetric distributions with unknown covariance matrix</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Fourdrinier</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Multivar. Anal</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="24" to="39" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<monogr>
		<title level="m" type="main">Bayesian Data Analysis</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Gelman</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Chapman and Hall</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
	<note>2nd. edn</note>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Minimax estimators of a normal mean vector for arbitrary quadratic loss and unknown covariance matrix</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">J</forename>
				<surname>Gleser</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1625" to="1633" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Molecular classification of cancer: class discovery and class prediction by gene expression monitoring</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">R</forename>
				<surname>Golub</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="page" from="531" to="537" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Regularized linear discriminant analysis and its application in microarrays</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Guo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="86" to="100" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Hausser</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Strimmer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1469" to="1484" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">On the evolution of german business cycles 1958–2004</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Heilemann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Schuhr</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Econ. Stat</title>
		<imprint>
			<biblScope unit="volume">228</biblScope>
			<biblScope unit="page" from="84" to="109" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Bias-corrected diagonal discriminant rules for high-dimensional classification</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="1096" to="1106" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimal tests shrinking both means and variances applicable to microarray data analysis</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">T G</forename>
				<surname>Hwang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Liu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Appl. Genet. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Empirical Bayes confidence intervals shrinking both means and variances</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">T G</forename>
				<surname>Hwang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="265" to="285" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Estimation with quadratic loss</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>James</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Stein</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fourth Berkeley Symp</title>
		<meeting>. Fourth Berkeley Symp</meeting>
		<imprint>
			<date type="published" when="1961" />
			<biblScope unit="page" from="361" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Estimating the proportion of true null hypotheses, with application to DNA microarray data</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Langaas</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="555" to="572" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Gene Selection: a Bayesian variable selection approach</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">E</forename>
				<surname>Lee</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="90" to="97" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">An extensive comparison of recent classification tools applied to microarray data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">W</forename>
				<surname>Lee</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Stat. Data Anal</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="869" to="885" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Generalized Bayes minimax estimators of the multivariate normal mean with unknown covariance matrix</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">E</forename>
				<surname>Lin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">L</forename>
				<surname>Tsai</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="145" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Discussion of professor Stein&apos;s paper: confidence sets for the mean of a multivariate normal distribution</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">V</forename>
				<surname>Lindley</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="285" to="287" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Shrinkage estimation of effect sizes as an alternative to hypothesis testing followed by estimation in high-dimensional biology: Applications to differential gene expression Diagonal Fisher linear discriminant analysis for efficient face recognition</title>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Montazeri</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Appl. Genet. Mol. Biol. Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">69</biblScope>
			<biblScope unit="page" from="1711" to="1716" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Accurate ranking of differentially expressed genes by a distribution-free shrinkage approach</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Opgen-Rhein</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Strimmer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Appl. Genet. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Incorporating gene functions as priors in model-based clustering of microarray gene expression data</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Pan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="795" to="801" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Shrinkage-based diagonal discriminant analysis and its applications in high-dimensional data</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Pang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="1021" to="1029" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Analyzing breast cancer microarrays from african americans using shrinkage-based discriminant analysis</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Pang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum. Genomics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="5" to="16" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Linear models and empirical Bayes methods for assessing differential expression in microarray experiment</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">K</forename>
				<surname>Smyth</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Appl. Genet. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<monogr>
		<title level="m" type="main">Statistical Analysis of Gene Expression Microarray Data</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Speed</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Chapman and Hall</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<monogr>
		<title level="m" type="main">SAM thresholding and false discovery rates for detecting differential gene expression in DNA microarrays The Analysis of Gene Expression Data: Methods and Software</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">D</forename>
				<surname>Storey</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
		<editor>Parmigiani,G. et al.</editor>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<analytic>
		<title level="a" type="main">Incorporating prior knowledge of gene functional groups into regularized discriminant analysis of microarray data</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Tai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Pan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="3170" to="3177" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b37">
	<analytic>
		<title level="a" type="main">Class prediction by nearest shrunken centroids, with applications to DNA microarrays</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Sci</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="104" to="117" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b38">
	<analytic>
		<title level="a" type="main">Optimal shrinkage estimation of variances with applications to microarray data analysis</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Tong</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Wang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="113" to="122" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b39">
	<analytic>
		<title level="a" type="main">Variance estimation in the analysis of microarray data</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Wang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="425" to="445" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b40">
	<analytic>
		<title level="a" type="main">A random variance model for detection of differential gene expression in small microarray experiments</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">W</forename>
				<surname>Wright</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">M</forename>
				<surname>Simon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2448" to="2455" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>