
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Systems biology Model building and intelligent acquisition with application to protein subcellular location classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011">2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">C</forename>
								<surname>Jackson</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Bioimage Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">E</forename>
								<surname>Glory-Afshar</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Bioimage Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">R</forename>
								<forename type="middle">F</forename>
								<surname>Murphy</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Bioimage Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Engineering</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Lane Center for Computational Biology</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Biological Sciences</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>4400 Fifth Ave</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Machine Learning Department</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">J</forename>
								<surname>Kovačevíkovaˇkovačeví</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Bioimage Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Engineering</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Lane Center for Computational Biology</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Olga</forename>
								<surname>Troyanskaya</surname>
							</persName>
						</author>
						<title level="a" type="main">Systems biology Model building and intelligent acquisition with application to protein subcellular location classification</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">27</biblScope>
							<biblScope unit="issue">13</biblScope>
							<biblScope unit="page" from="1854" to="1859"/>
							<date type="published" when="2011">2011</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btr286</idno>
					<note type="submission">Received on September 25, 2010; revised on March 28, 2011; accepted on April 20, 2011</note>
					<note>[15:19 8/6/2011 Bioinformatics-btr286.tex] Page: 1854 1854–1859 Associate Editor: Contact:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: We present a framework and algorithms to intelligently acquire movies of protein subcellular location patterns by learning their models as they are being acquired, and simultaneously determining how many cells to acquire as well as how many frames to acquire per cell. This is motivated by the desire to minimize acquisition time and photobleaching, given the need to build such models for all proteins, in all cell types, under all conditions. Our key innovation is to build models during acquisition rather than as a post-processing step, thus allowing us to intelligently and automatically adapt the acquisition process given the model acquired. Results: We validate our framework on protein subcellular location classification, and show that the combination of model building and intelligent acquisition results in time and storage savings without loss of classification accuracy, or alternatively, higher classification accuracy for the same total acquisition time. Availability and implementation: The data and software used for this study will be made available upon publication at</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The creation of accurate and predictive models for cells and tissues will require detailed information on the subcellular location of all proteins. The field of location proteomics (<ref type="bibr" target="#b6">Murphy, 2005</ref>) is concerned with learning this information for entire proteomes and with capturing it in the form of generative models that can be used in cell simulations. Given the scale of the problem, efforts to optimize acquisition of location information are highly desirable (<ref type="bibr" target="#b4">Jackson et al., 2009</ref>). When studying the spatiotemporal behavior of proteins in a single cell using fluorescence microscopy, we typically have to choose a priori the number of frames to acquire. This can be problematic as we do not generally know in advance how many frames will be necessary to obtain the information we seek. We thus risk acquiring either too few or too many frames than needed for our application, * To whom correspondence should be addressed. thereby wasting time and storage space. We use the term frame to represent a time point in the time-lapse imaging of live cells. Similarly, when learning about a homogeneous population of cells, here called class of cells, we may take several cells from this class and acquire a movie of each. However, we do not know in advance how many cell movies we should acquire to gain an understanding of the class nor how many frames to acquire for each cell movie. Once again, we risk acquiring too few cells and frames and not learning what we wish to know, or wasting time by acquiring too many cells and frames. Reducing the area and duration of exposure to the exciting light also protects the sample from photobleaching and phototoxicity in fluorescence microscopy. In this work, we propose intelligent model building and acquisition algorithms to deal with the above problems. These algorithms automatically determine, during acquisition, when to stop acquiring frames from a particular cell, and when to stop acquiring cells from a particular class. As shown in<ref type="figure" target="#fig_0">Figure 1</ref>, they work by building models during acquisition. This is in contrast to the sequential approach to processing microscopy images, which would view model building as a post-processing step. We apply the algorithms to 3D movies of 12 3T3 cell lines tagged with green fluorescent protein (GFP), with a different protein labeled in each cell line. We consider each cell line to be a different class and determine the parameters of acquisition (how many cell movies we need to learn about each class, and how many frames we need in each cell movie). We test these algorithms by trying to recognize (classify) the pattern of the labeled proteins and show that:</p><p>(1) We can build models both of an individual cell and of a class of cells, which can then be used to correctly classify the subcellular location pattern of a given cell.</p><p>(2) When we build models from data that is intelligently acquired, the models achieve a higher classification accuracy on an independent test set than when we build models from the same amount of data acquired with standard acquisition methods (a fixed number of frames per cell and a fixed number of cells per class).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model building and classification</head><p>We distinguish between two types of models: a cell model is based entirely on a movie of a single cell. A class model is based on all cell models from<ref type="bibr">[15:19</ref><ref type="bibr" target="#b12">Zhao et al. (2005)</ref>. In this study, we use only the features calculated from the protein channel that do not require a nuclear channel to be calculated. that class. In this application, a class consists of all the cells expressing a specific GFP-tagged protein. Note that while some of these proteins show similar static location patterns, they are largely distinguishable when temporal information is available (<ref type="bibr" target="#b2">Hu et al., 2006</ref><ref type="bibr" target="#b3">Hu et al., , 2010</ref>). We now describe how to build cell models and class models, as well as our classification method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model building and intelligent acquisition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Cell models</head><p>Our models are based entirely on objects present in the movie. The basis for this approach is our previous work demonstrating that subcellular patterns in static images can be adequately recognized by finding objects by thresholding, describing each object by numerical features, identifying object types by clustering and representing an image by the fraction of objects of each type that it contains (<ref type="bibr" target="#b10">Velliste, 2002;</ref><ref type="bibr" target="#b12">Zhao et al., 2005</ref>). The object detection stage aims to find sets of connected pixels in 3D that display a higher intensity level than their local environment. The pixel intensities represent the GFP signal in the protein channel, and the resulting objects represent the pattern displayed by a labeled protein within a cell. Because the background intensity is not uniform and objects may be touching, we use a multi-threshold approach for object detection (<ref type="bibr" target="#b8">Gonzalez and Woods, 2008</ref>). First, the background is removed in each z-slice by subtracting the most common pixel intensity below the average intensity. Then, we apply a Gaussian filter (diameter = 5 pixels, SD = 1) to reduce the noise of the image. Finally, we find local thresholds (above a minimum threshold determined empirically), such that objects represent the biggest set of 3D-connected pixels that contain only one maximum intensity. To assign object types, we compute seven static features based on previous work (<ref type="bibr" target="#b10">Velliste, 2002;</ref><ref type="bibr" target="#b12">Zhao et al., 2005</ref>), as shown in<ref type="figure" target="#tab_1">Table 1</ref>. Similarly to that previous work, we take all objects across all frames and movies, normalize their features to unit standard deviation, and cluster them based on these features using the batch k-means algorithm. An object's type is defined as the number of the cluster into which it falls. We choose the number of types, k, to maximize classification accuracy on the training set using nested cross-validation. The cell model then consists of three components</p><formula>(m λ ,m λ,λ , m λ,Ø ):</formula><p>(1) The first component, m λ ,is a k-by-1 vector representing the proportion of objects of type λ.</p><formula>(2)</formula><p>The second component, m λ,λ , is a k-by-k matrix representing the proportion of objects of type λ that have a nearby object of type λ in the subsequent frame. We define a nearby object as an object whose center is within a distance d max of the original object. We choose d max to be 0.5 µm (corresponding to 1 pixel in the z-direction, and 4.5 pixels in the x,y-directions).</p><formula>(3)</formula><p>The third component, m λ,Ø , is a k-by-1 vector representing the proportion of objects of type λ that have no nearby objects in the subsequent frame.</p><p>When computing these proportions, we take the posterior mean under the assumption of a uniform prior (<ref type="bibr" target="#b0">Bishop, 2006</ref>). Hence, to determine m λ , we count the number of objects of type λ across all frames, storing as N λ. If N is the total number of objects, we calculate m λ as:</p><formula>m λ = N λ +1 N +k</formula><p>Similarly, to determine m λ,λ , we iterate through all objects of type λ, and look for nearby objects in the subsequent frame of type λ. If the total number of such events is N λ,λ , we calculate m λ,λ as:</p><formula>m λ,λ = N λ,λ +1 N λ +k</formula><p>We derive m λ,Ø in the same fashion as m λ,λ , but replacing N λ,λ with N λ,Ø , where N λ,Ø is the number of objects of type λ with no nearby objects in the subsequent frame. Cell models can be built up while the movie is being acquired, simply by including all objects present up until the current frame. As more frames are acquired, the cell model is refined by adding in the newly available objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Class models</head><p>A class model is simply the collection of cell models for that class. Hence, we can view the class model as a mixture model, where its constituent cell models form the component models of that mixture. As additional cells from the class are acquired, the class model is refined to include the resulting cell models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.Jackson et al.</head><p>m, L(m |m), is found by looking at the three components of the model individually: L(m |m) = L 1 (m |m)L 2 (m |m)L 3 (m |m) with:</p><formula>L 1 (m |m) = k λ=1 m N λ λ (1−m λ ) (N−N λ ) L 2 (m |m) = k λ=1 k λ =1 m N λ,λ λ,λ (1−m λ,λ ) (N−N λ,λ ) L 3 (m |m) = k λ=1 m N λ,∅ λ,∅ (1−m λ,∅ ) (N−N λ,∅ )</formula><p>Note that the above expressions assume all components of the model are conditionally independent given the underlying cell model m. This is analogous to the assumption made in a naïve Bayes classifier (<ref type="bibr" target="#b7">Ng and Jordan, 2002</ref>), and has been shown to give good classification results even when it does not strictly hold (<ref type="bibr" target="#b11">Zhang, 2004</ref>). We classify by assigning the cell model to the class with the maximum likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Intelligent acquisition</head><p>We now describe our intelligent acquisition algorithms. We will show it results in a better model for a given amount of data (as verified by classification accuracy). We begin by discussing how many frames to acquire when building a cell model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Cell models: when to stop acquiring frames for each cell</head><p>When building a cell model with the end application of classification, the goal is to stop acquiring when the classification decision is unlikely to change. We distinguish between two scenarios:</p><p>(1) Extrinsic scenario: we have access to the class models during acquisition. This means that we can classify the cell after acquiring each frame, and stop acquiring when this classification result reaches a sufficient confidence.</p><p>(2) Intrinsic scenario: we do not have access to the class models during acquisition. Hence, we cannot classify during acquisition, and our choice of when to stop acquiring the cell must be based solely on the data from that cell.</p><p>As discussed earlier, we classify by choosing the class of maximum likelihood. Equivalently, we can use the log-likelihood of a class, (c), and calculate the standard error of this log-likelihood, e(c). To demonstrate this, we first rewrite the equations as log-likelihoods, by taking logarithms of both sides:</p><formula>(m |m) = 1 (m |m)+ 2 (m |m)+ 3 (m |m) with: 1 (m |m) = k λ=1 N λ ln(m λ )+(N −N λ )ln(1−m λ ) 2 (m |m) = k λ=1 k λ =1 N λ,λ ln(m λ,λ )+(N −N λ,λ )ln(1−m λ,λ ) 3 (m |m) = k λ=1 N λ,∅ ln(m λ,∅ )+(N −N λ,∅ )ln(1−m λ,∅ )</formula><p>A cell model m is built from all of the N individual object observations in that cell. We can see from the above equations that, when we wish to determine (m |m), the log-likelihood of a cell model m given the observed cell model m, each individual object observation contributes toward (m |m). Therefore, we could express (m |m) as the sum of the log-likelihoods given each individual object observation,</p><formula>(1) (m ), ... ,, (N) (m )</formula><p>. We can then express the standard error of (m |m) as the standard deviation of</p><formula>(1) (m ), ..., (N) (m )</formula><p>, multiplied by the square root of N, where there are N object observations. If c 1 is the most likely class and c 2 is the second most likely class, we define the classification confidence, C, as:</p><formula>C = (c 1 )−(c 2 ) e(c 1 ) 2 +e(c 2 ) 2</formula><p>In the extrinsic scenario, we acquire until C exceeds a given classification confidence threshold, α. In the intrinsic scenario, we cannot compute the log-likelihood of a class model, because the class models are unavailable. However, we can still compute the log-likelihood of some sample model, including its standard error, providing an indication of how accurately we expect to predict the log-likelihood of the class models when they become available. The natural choice of a sample model is the model of the cell being acquired. We refer to the standard error of the resulting log-likelihood as the likelihood uncertainty, and acquire until this likelihood uncertainty falls below the likelihood uncertainty threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Class models: when to stop acquiring frames for each cell</head><p>When acquiring a cell to build a class model, intelligent acquisition can give even better results. In the extrinsic scenario, we stop acquiring a cell if we recognize that it is similar to a previous cell in that class. This allows us to focus our time and resources on acquiring cells that are different from previously acquired cells of that class, as these provide the most new information. To assess whether a cell is similar to previous cells in the class, we simply classify it. If it is correctly classified with high confidence (exceeding the classification confidence threshold), we know that this must be the case and we stop acquiring. Moreover, we also stop acquiring when the likelihood uncertainty falls below the likelihood uncertainty threshold ensuring that we do eventually stop acquisition even when the cell is very different from previously acquired cells of that class. This method implicitly assumes that acquisition alternates between cells of all the classes, such that we can build and refine a class model of every class simultaneously. In the intrinsic scenario, we do not alternate between cells, and thus the classes are not available before starting the acquisition. In this scenario, we can still use the likelihood uncertainty threshold as before, which gives almost as good results for small numbers of cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Class models: how many cells to acquire</head><p>Finally, after discussing how many frames to acquire from each cell, we now discuss when to stop acquiring cells altogether. Although all class models will improve when we acquire more cells of that class, we want to identify those for which further acquisition is especially beneficial. We again consider two scenarios:</p><p>(1) Global scenario: we acquire N 1 cells from every class, but then have time to revisit r classes and acquire an additional N 2 cells. We want to decide which r classes to revisit.</p><p>(2) Local scenario: we acquire N 1 cells from a class, and then have to decide whether we should acquire an additional N 2 cells. Unlike the global scenario, we do not have access to the other classes, and thus must make this decision locally with the information available at that time.</p><p>In the global scenario, we revisit the r classes with the highest global marginal utility. We define this as the decrease in classification accuracy on the training set that results from removing a cell from that class. To measure this for a cell C, we begin by testing whether C is classified correctly when all other cells are used for training. Let γ 1 = 1 when it is classified incorrectly, and 0 otherwise. Next, we measure the classification accuracy of all other cells in the same class as C under two scenarios: (i) when training with all cells except the test cell, and (ii) when training with all cells except both the test cell and C. Let γ 2 be the mean drop in accuracy from the first scenario to the second. The global marginal utility of cell C is then given by γ 1 +γ 2. The global marginal utility of an entire class is the mean of the utility of its constituent cells. In the local scenario, for each cell model m in the class, we determine its closest match, m , which is the cell model that has the highest likelihood given m. We define the local marginal utility of a class as the proportion of Page: 1857 1854–1859The number of frames per cell varies depending on the amount of photobleaching incurred during acquisition. cell models in that class that are chosen at least once as a closest match. This estimates the probability that a new cell will affect the classification result of existing cells. We revisit the classes with high-local marginal utility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model building and intelligent acquisition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We perform our experiments on a collection of 304 3D movies of GFP-tagged proteins in NIH 3T3 cells over 12 different cell lines, with a different protein labeled in each cell line. The lines were generated by CD tagging (<ref type="bibr">Jarvik et al., 2002</ref>) and the microscope and image acquisition parameters are as described in<ref type="bibr" target="#b2">Hu et al. (2006</ref><ref type="bibr" target="#b3">Hu et al. ( , 2010</ref>). At each time point of the movie, we have a single-channel stack of 15 z-slices, 1280×1024 pixels each. The x,y-resolution is 0.11 microns, and the distance between pixels in the z-direction is 0.5 microns. There is a 45 s interval between frames. The set of proteins included in this study are located in six major subcellular structures.<ref type="figure" target="#tab_2">Table 2</ref>summarizes the tagged gene/protein for each cell line, along with the location in the cell where the protein is expressed, the number of movies for each cell line, and the number of frames in each movie. Note that we aim to distinguish proteins even when they appear in the same subcellular compartment. This is possible because such proteins still exhibit different dynamic behavior, and may also have a different spatial distribution within the subcellular compartment. As a known example, histones, RNA polymerases and nucleoporins are three nuclear proteins with very different distributions, which are respectively involved in the compaction of DNA into chromatin, the replication of DNA (nucleoli) and the nuclear pore complex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classification</head><p>We tested our classification method using leave-one-out crossvalidation (i.e. when classifying a test cell, we built the class models from all cells except for this test cell), and achieved 84.2% accuracy on our dataset. A previous result on this same 3T3dataset used only the middle z-slice at each time point, and with a range of morphological, texture and temporal features, achieved an accuracy of 80.9% (<ref type="bibr" target="#b3">Hu et al., 2010</ref>). Therefore, in bettering the classification accuracy, we verify that our models are capturing relevant discriminative information present in the movies. If we consider only the static component of our model—the proportion of objects in each type m λ —we get 70.4% accuracy. The addition of the dynamic components,</p><formula>(m λ,λ ,m λ,Ø )</formula><p>, completes the model, giving our final result of 84.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cell models: when to stop acquiring frames for each cell</head><p>To test our intelligent stopping algorithms, we take each movie in our dataset and determine how many frames would have been acquired for a given stopping algorithm and threshold. We then measure the average number of frames acquired and the average classification accuracy. For example, in the extrinsic scenario with α = 0.7, we acquire 3–35 frames per cell, with an average of 8.0 frames, yielding 80.3% classification accuracy. We compare this to the standard method that acquires exactly eight frames for each cell, yielding only 75.0% classification accuracy. To reach 80% classification accuracy using the standard method, we would need to acquire over 14 frames per cell—almost twice as many as with our intelligent method. In<ref type="figure" target="#fig_2">Figure 2</ref>, we compare our intelligent methods for the extrinsic and intrinsic scenarios with the standard method, using a range of stopping thresholds to get the different points on the curves. The intelligent algorithms achieve significantly higher accuracy for the same average number of frames acquired, with the best results for the extrinsic scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Class models: when to stop acquiring frames for each cell</head><p>To test these intelligent acquisition algorithms, we set aside 10 movies as our testing set. We take the remaining 294 training Page: 1858 1854–1859movies and determine which frames would have been acquired for each intelligent stopping method and threshold. We then use these to build a class model for each of the 12 classes, and attempt to classify the testing set. We use every frame for movies in the testing set. We ran 10 000 trials, randomizing the testing and training sets, including their order, in each trial. In<ref type="figure" target="#fig_3">Figure 3</ref>, we show the resulting classification accuracy against the average number of frames acquired per cell with (i) the intelligent acquisition algorithm for the extrinsic scenario, varying the classification confidence threshold to get the different points on the curve, (ii) the intelligent acquisition algorithm for the intrinsic scenario, varying the likelihood uncertainty threshold to get the different points on the curve and (iii) a standard acquisition algorithm that acquires a fixed number of frames per cell. Once again, we see that the intelligent algorithms achieve significantly higher accuracy than the standard method for the same average number of frames acquired; the results for the extrinsic scenario are best. In this scenario, the intelligent algorithm requires only 10 frames per cell to reach a classification accuracy of 80%, whereas the standard algorithm requires 18 frames. We expect that the difference between the two intelligent methods will increase as the number of cells per class increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.Jackson et al.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Class models: how many cells to acquire</head><p>To test our methods for how many cells to acquire, we randomly choose one movie (cell) per class to serve as the testing set. From the remaining movies, we randomly choose 10 movies per class as the training set. We choose a set of r classes to revisit and add five more movies from each of these classes to the training set. Finally, we classify the testing set, and record the accuracy. We vary the number of classes to revisit, r, from 0 (none) to 12 (all). We test three methods for choosing which r classes to revisit: those of high-global marginal utility, those of high-local marginal utility and randomly chosen. The results are averaged over 100 000. Initially, we acquire 10 movies from each class. We then acquire five more movies (a) or 10 more movies (b) from a selected set of classes. The acquired movies are used to train a classifier that is then evaluated on held-out testing data. The solid line shows results when revisit the classes with the high-global marginal utility. The dashed line shows when we revisit the classes with the high-local marginal utility. The dotted line shows the accuracy when we randomly choose which classes to revisit. We can see that both of the intelligent methods perform better than choosing the classes randomly, with the best results attained using the global marginal utility. trials and shown in<ref type="figure" target="#fig_4">Figure 4a</ref>, which displays classification accuracy as a function of r. We can see that intelligently choosing which classes to revisit gives higher classification accuracy, with the best results for the global scenario.<ref type="figure" target="#fig_4">Figure 4b</ref>repeats the above experiment, but acquires 10 additional movies from the revisited classes (instead of 5). Because we have fewer than 20 movies available for some classes, we do this for only six classes. The results are similar to those of<ref type="figure" target="#fig_4">Figure 4a</ref>, but the increase in accuracy with intelligent acquisition is greater.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Computation time</head><p>The time taken to build a model is about 1–3 s per frame (Intel Core Duo 2.2 GHz processor, 1.96 GB of memory), depending on the number of objects; the time taken for each of the intelligent acquisition algorithms is negligible. This is relatively fast in comparison with the acquisition time of 45 s per frame. Furthermore, for the algorithms that determine when to stop acquiring frames, we Page: 1859 1854–1859</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model building and intelligent acquisition</head><p>can immediately begin acquiring the subsequent frame even while we are evaluating whether we should stop acquiring or not; thus, this computational penalty only applies on the one frame at which we choose to stop acquiring. For the algorithm that determines how many cells to acquire, the added computation time for building the model of a cell is negligible in comparison with the time it takes to acquire that cell.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head><p>We have demonstrated that intelligently choosing when to stop acquiring frames and cells leads to an increased accuracy for a given amount of acquired data, or equivalently, a reduced acquisition time and resources for a given accuracy. The intelligent acquisition algorithms described here are not closely tied to the model building procedure used, and thus have broad applicability in other modeling scenarios. In addition, we have presented a model-building technique based solely on the locations and types of objects present within a cell, and shown that the resulting models can classify with a higher accuracy than previous results using all of the image data. Automated microscopy is increasingly used both for basic research in cell and systems biology and for drug screening and development (<ref type="bibr" target="#b9">Taylor et al., 2006</ref>). Approaches such as those we have described here can be directly incorporated into automated microscopes, such as high-content screening systems, which are typically designed to include decision making during acquisition. Alternatively, they can also be relatively easily added to conventional microscopes. In this case, the main challenge is to create a control loop between the model-building software (MBS) and the microscope control software (MCS), in particular to give the MBS the ability to control rudimentary aspects of microscope acquisition. The two critical requirements are for the MBS to be able to retrieve each cell image after it is acquired, and for the MBS to be able to either initiate acquisition of the next frame or stop acquisition of the next frame (assuming that continued acquisition of a large number of frames is the default). These are surprisingly difficult to achieve with the MCS of commercial microscopes as typically configured, because some microscopes wait until all (or a certain number) of frames have been acquired before writing them to disk, and because the MCS often cannot itself be controlled other than via a graphical user interface. At least three solutions present themselves. The first is to configure the microscope with optional 'macro' languages provided by the manufacturer that can incorporate external software into the control loop. The second is to use third-party MCS, such as the open-source MicroManager (<ref type="bibr" target="#b1">Edelstein et al., 2010</ref>), which give nearly complete microscope control. This is an excellent solution, with the main disadvantages being that manufacturer support may be lacking in the case of hardware problems and that the performance (latency, acquisition speed) may not be as good as the software that has been optimized for the manufacturer's hardware. The last is to use software that simulates interaction with the graphical user interface to perform basic control. This solution is the lowest cost and has the minimal impact, but the external control software may need to be extensively modified to work with new versions of the manufacturer software. Once the control loop is established, implementation of the approaches described here is straightforward. As discussed above, the time required for computing models is small relative to acquisition time. The one potential exception is the time required for defining the object types, which is not included in the modelbuilding time (the object types are assumed to be constant during the model building). To learn object types we currently use k-means clustering for many different values of k, and the time required can be many minutes. In the extrinsic scenario, this is not a problem since classes do not change. In the intrinsic scenario, the object types may need to be relearned when a new class is observed. Possible solutions include reducing the range of k over which the search is done, using more highly optimized clustering code, and/or using 'online' or incremental clustering approaches. For the future, we plan to extend the methods described here to allow models to be built from image series collected with spatial and temporal resolution that may vary under computer control.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. Framework for intelligent acquisition of a class model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>To classify a cell model of unknown class, we first measure the likelihood of each of the class's constituent cell models. The likelihood of a constituent cell model m given the observed cell model Page: 1856 1854–1859</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.2.</head><figDesc>Fig. 2. When to stop acquiring (cell models). The average classification accuracy is shown as a function of the average number of frames acquired with one of three methods to choose when to stop acquiring. The first method (solid line) uses the intelligent algorithm for the extrinsic scenario. The second method (dashed line) uses the intelligent algorithm for the intrinsic scenario. The third method (dotted line) acquires the same number of frames for each movie. The two intelligent methods outperform the standard one, with the best results in the extrinsic scenario.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.3.</head><figDesc>Fig. 3. When to stop acquiring (class models). The average classification accuracy is shown as a function of the average number of frames acquired with one of three methods to choose when to stop acquiring. The first method (solid line) uses the intelligent algorithm for the extrinsic scenario. The second method (dashed line) uses the intelligent algorithm for the intrinsic scenario. The third method (dotted line) acquires the same number of frames for each movie. The two intelligent methods outperform the standard one, with the best results in the extrinsic scenario.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.4.</head><figDesc>Fig. 4. How many cells to acquire. Initially, we acquire 10 movies from each class. We then acquire five more movies (a) or 10 more movies (b) from a selected set of classes. The acquired movies are used to train a classifier that is then evaluated on held-out testing data. The solid line shows results when revisit the classes with the high-global marginal utility. The dashed line shows when we revisit the classes with the high-local marginal utility. The dotted line shows the accuracy when we randomly choose which classes to revisit. We can see that both of the intelligent methods perform better than choosing the classes randomly, with the best results attained using the global marginal utility.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>Table 1.</figDesc><table>List of static subcellular object features (SOFs) 

Name 
Description 

3D-SOF1.1 
Number of voxels in object 
3D-SOF1.4 
Measure of eccentricity of the object 
3D-SOF1.6 
A measure of roundness of the object 
3D-SOF1.7 
The length of the object's skeleton by homotopic thinning 
3D-SOF1.8 
The ratio of skeleton length to the area of the convex hull 
of the skeleton 
3D-SOF1.9 
The fraction of object pixel contained within the skeleton 
3D-SOF1.11 
The ratio of the number of branch points in skeleton to 
length of skeleton 

The complete 3D-SOF1 set is described in Velliste (2002), based on the 2D-SOF1 set 
described by </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 2. Overview of the experimental dataset composed of 12 cell lines</figDesc><table>Gene 
Protein 
Location 
# cells # frames 

Dia1 
Cytochrome b-5 
reductase 

Cytoplasm 
20 
19–24 

Anxa5 Annexin A5 
Nucleus, cytoplasm 18 
11–25 
Sdpr 
Serum deprivation 
response protein 

Vesicles, cytoplasm 23 
20–31 

Adfp 
Adipose differentiation 
related protein 

Vesicles 
51 
21–27 

Timm23 ADP-ATP translocase 23 Mitochondria 
40 
16–22 
Atp5a1 ATP synthase 
Mitochondria 
20 
21 
Hspa9a Mitochondrial stress-70 
protein 

Mitochondria 
24 
9–46 

Glut1 
Glucose transporter 1 
Plasma membrane 17 
13–82 
Cav 
Caveolin 
Plasma membrane 16 
11–49 
Tctex1 t-complex testis 
expressed 1 

Cytoskeleton 
30 
13–36 

Actn4 
Actinin, alpha 4 
Cytoskeleton 
29 
9–25 
Cald1 
Caldesmon 1 
Cytoskeleton 
16 
12–34 

</table></figure>

			<note place="foot">© The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Probability distributions</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">M</forename>
				<surname>Bishop</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition and Machine Learning. Information Science and Statistics</title>
		<editor>Jordan,M. et al.</editor>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="76" to="78" />
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>1st. edn</note>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Computer control of microscopes using µManager</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Edelstein</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Protoc. Mol. Biol., Chapter</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Unit. 14.20</note>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Application of temporal texture features to automated analysis of protein subcellular locations in time series fluorescence microscope images</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Hu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 IEEE International Symposium on Biomedical Imaging</title>
		<meeting>the 2006 IEEE International Symposium on Biomedical Imaging<address><addrLine>Arlington, VA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1028" to="1031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Automated analysis of protein subcellular locations in time series images</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Hu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1630" to="1636" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Intelligent acquisition and learning of fluorescence microscope data models</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Jackson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Proc</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="2071" to="2084" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">In vivo functional proteomics: mammalian genome annotation using CD-tagging</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">W</forename>
				<surname>Jarvik</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioTechniques</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="852" to="867" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Location proteomics: a systems approach to subcellular location</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">F</forename>
				<surname>Murphy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biochem. Soc. Trans</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="535" to="538" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">On discriminative vs. generative classifiers: a comparison of logistic regression and naive Bayes</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">Y</forename>
				<surname>Ng</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">I</forename>
				<surname>Jordan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="841" to="848" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<monogr>
		<title level="m" type="main">Digital Image Processing</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">C</forename>
				<surname>Gonzalez</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">E</forename>
				<surname>Woods</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Prentice Hall</publisher>
			<biblScope unit="page" from="752" to="763" />
			<pubPlace>Upper Saddle River, NJ</pubPlace>
		</imprint>
	</monogr>
	<note>3rd. edn</note>
</biblStruct>

<biblStruct   xml:id="b9">
	<monogr>
		<title level="m" type="main">High Content Screening: A Powerful Approach to Systems Cell Biology and Drug Discovery</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">L</forename>
				<surname>Taylor</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<monogr>
		<title level="m" type="main">Image interpretation methods for a systematics of protein subcellular location</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Velliste</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<pubPlace>Pittsburgh, PA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">The optimality of naïve Bayes</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventeenth Florida Artificial Intelligence Research Society Conference</title>
		<meeting><address><addrLine>Florida</addrLine></address></meeting>
		<imprint>
			<publisher>The AAAI Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="562" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Object type recognition for automated analysis of protein subcellular location</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Zhao</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1351" to="1359" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>