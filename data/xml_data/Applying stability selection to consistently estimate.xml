
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gene expression Applying stability selection to consistently estimate sparse principal components in high-dimensional molecular data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName>
								<forename type="first">Martin</forename>
								<surname>Sill</surname>
							</persName>
							<email>: m.sill@dkfz.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Biostatistics</orgName>
								<orgName type="department" key="dep2">DKFZ</orgName>
								<address>
									<postCode>69120</postCode>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Maral</forename>
								<surname>Saadati</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Biostatistics</orgName>
								<orgName type="department" key="dep2">DKFZ</orgName>
								<address>
									<postCode>69120</postCode>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Axel</forename>
								<surname>Benner</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Division of Biostatistics</orgName>
								<orgName type="department" key="dep2">DKFZ</orgName>
								<address>
									<postCode>69120</postCode>
									<settlement>Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gene expression Applying stability selection to consistently estimate sparse principal components in high-dimensional molecular data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btv197</idno>
					<note type="submission">Received on October 27, 2014; revised on March 31, 2015; accepted on April 2, 2015</note>
					<note>*To whom correspondence should be addressed. Associate Editor: Janet Kelso Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Principal component analysis (PCA) is a basic tool often used in bioinformatics for visualization and dimension reduction. However, it is known that PCA may not consistently estimate the true direction of maximal variability in high-dimensional, low sample size settings, which are typical for molecular data. Assuming that the underlying signal is sparse, i.e. that only a fraction of features contribute to a principal component (PC), this estimation consistency can be retained. Most existing sparse PCA methods use L1-penalization, i.e. the lasso, to perform feature selection. But, the lasso is known to lack variable selection consistency in high dimensions and therefore a subsequent interpretation of selected features can give misleading results. Results: We present S4VDPCA, a sparse PCA method that incorporates a subsampling approach, namely stability selection. S4VDPCA can consistently select the truly relevant variables contributing to a sparse PC while also consistently estimate the direction of maximal variability. The performance of the S4VDPCA is assessed in a simulation study and compared to other PCA approaches, as well as to a hypothetical oracle PCA that &apos;knows&apos; the truly relevant features in advance and thus finds optimal, unbiased sparse PCs. S4VDPCA is computationally efficient and performs best in simulations regarding parameter estimation consistency and feature selection consistency. Furthermore, S4VDPCA is applied to a publicly available gene expression data set of medulloblastoma brain tumors. Features contributing to the first two estimated sparse PCs represent genes significantly over-represented in pathways typically deregulated between molecular subgroups of medulloblastoma. Availability and implementation: Software is available at https://github.com/mwsill/s4vdpca. Contact</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Principal component analysis (PCA) is the most popular method for dimension reduction and visualization that is widely used for the analysis of high-dimensional molecular data. In bioinformatics typical applications range from outlier detection as part of quality control (<ref type="bibr" target="#b11">Kauffmann et al., 2009</ref>) to exploratory data analysis for revealing new molecular subgroups (<ref type="bibr" target="#b17">Remke et al., 2011</ref>), as well as pathway and network analysis (<ref type="bibr" target="#b14">Ma and Dai, 2011</ref>). Common biological data sets for such applications are continuous molecular data typically generated by high-throughput profiling techniques, e.g. gene expression, copy number variation, methylation and micro RNA expression data. In general, PCA aims to project a high-dimensional data matrix into a lower dimensional space by seeking linear combinations of the original variables, called principal components (PCs). By construction, these PCs capture maximal variance and are orthogonal V C The Author 2015. Published by Oxford University Press.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2683</head><p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. to each other. As PCs are mutually uncorrelated, PCA is a practical method to aggregate correlated variables. The resulting PCs can then be used as input variables for further analysis, e.g. principal component regression (<ref type="bibr" target="#b7">Jolliffe, 1982</ref>). In gene expression data analysis PCs are often referred to as 'metagenes', 'eigengenes' or 'latent genes'. Moreover, PCs extracted from different molecular data sets can be combined to perform an integrated analysis. Although PCA was originally developed for the multivariate normal distribution, it is not restricted to this distribution and can generally be used for exploratory data analysis and dimension reduction. However, PCA can be strongly impacted by some types of non-Gaussianity such as outliers and extreme skewness. This might be a problem for some molecular data types, but often data can be transformed to approximately achieve normality. A major drawback of PCA is that resulting principal components are linear combinations of all variables and that the corresponding loadings vector involve only non-zero coefficients. Therefore, a practical interpretation of the loadings vectors is often complicated, especially for high-dimensional data. Furthermore, in high-dimensional, low-sample size settings (HDLSS), which are typical for molecular data sets, PCA is known to become inconsistent in estimating the leading eigenvectors of the underlying population variance covariance matrix (<ref type="bibr" target="#b9">Jung and Marron, 2009</ref>), i.e. with increasing dimensionality and fixed sample size the estimate of the first PC does not necessarily converge towards the true direction of maximal variance. A possible way to overcome these two drawbacks is to assume the data embodies a strong structure. This is characterized by two assumptions. First, it is assumed that the majority of variability in the data can be explained by the first few PCs and thus the data matrix can be sufficiently approximated by a matrix of lower rank. Secondly, it is assumed that only few variables contribute to the true signal of a PC. This so-called sparsity (or parsimony) assumption is supported by current knowledge about biological processes, which in most situations also involve only few genes or molecular features. In the context of PCA, we consider methods that search for PCs where only a few coefficients of the loadings vector are non-zero. So far several methods to find sparse PCA solutions have been proposed (<ref type="bibr" target="#b8">Jolliffe et al., 2003;</ref><ref type="bibr" target="#b13">Lee et al., 2010;</ref><ref type="bibr" target="#b19">Shen and Huang, 2008;</ref><ref type="bibr" target="#b22">Witten et al., 2009;</ref><ref type="bibr" target="#b23">Yang et al., 2014;</ref><ref type="bibr" target="#b26">Zou et al., 2004</ref>).<ref type="bibr" target="#b18">Shen et al. (2013)</ref>clearly characterized the asymptotics of sparse PCA in high-dimensional, low-sample size settings. They showed that under the assumption that the true loadings vector is sparse and given that the underlying signal is strong relative to the number of variables involved, sparse PCA methods are able to consistently estimate the direction of maximal variance. In addition, they proved that the regularized sparse PCA method (RSPCA) proposed by<ref type="bibr" target="#b19">Shen and Huang (2008)</ref>is a consistent sparse PCA method. The focus of their work is on consistency in terms of estimating the true direction of maximal variance which corresponds to consistency in the parameter estimation of a statistical model. However, despite parameter estimation consistency, model selection consistency, i.e. selecting the variables that truly contribute to a PC, also plays an important role. Particularly in case of molecular data, selecting the correct features might be crucial for further interpretation of the PCs. For example, supposing that the selected features are subsequently analysed by downstream pathway analysis, then falsely selected irrelevant features might give misleading results. The RSPCA algorithm applies L 1-penalized ordinary least squares, also known as the lasso (<ref type="bibr" target="#b20">Tibshirani, 1996</ref>), to estimate sparse loadings vectors. The lasso is a popular method whose model selection consistency has been widely explored in the literature (<ref type="bibr" target="#b15">Meinshausen and Bü hlmann, 2006;</ref><ref type="bibr" target="#b24">Zhao and Yu, 2006</ref>). The lasso selects variables by shrinking estimates towards zero such that small coefficients will become exactly zero. Choosing the penalization for the lasso usually results in a trade-off between large models with many falsely selected coefficients and small, biased models which underestimate the coefficients of truly relevant variables and thus fit the data poorly. Typically, the strength of the L 1-penalization is determined by the regularization parameter k. In practice, k is chosen so as to optimize the goodness of fit of the model. In case of PCA methods where each PC is a rank one approximation, the goodness of fit can be measured by the Frobenius norm which corresponds to L 2-norm for matrices and measures the closeness of a rank one approximation to the original data matrix. An optimal k leads to sparse PC loadings vectors, where not only the coefficients of the truly relevant variables are non-zero, but also the coefficients of some irrelevant features. This is particularly meaningful for highdimensional molecular data, where some irrelevant features are likely to be correlated with relevant features. The reason being that an optimal rank one approximation is achieved by unbiased estimates of the relevant features. To get nearly unbiased estimates penalization should not be too strong, thus increasing the chance of irrelevant features to be included in the model. To overcome this problem of estimation bias other penalty terms have been developed. Fan and Li (2001) suggest a non-concave penalty function referred to as the smoothly clipped absolute deviation (SCAD). The adaptive lasso proposed by Zou (2006) uses individual weights for the penalty of each coefficient. These weights are chosen by an initial model fit, such that features that are assumed to have large effects will have smaller weights than features with small coefficients in the initial fit. Both of these penalties fulfill the oracle property, i.e. the penalized estimator is asymptotically equivalent to the oracle estimator, namely the ideal unpenalized estimator obtained when only the truly relevant variables are used for PCA. However, even though the lasso does not fulfill the oracle property and can not achieve model selection consistency in highdimensional data, it selects the truly relevant variables with high probability (<ref type="bibr" target="#b1">Benner et al., 2010</ref>). To utilize this property we propose to apply stability selection (<ref type="bibr" target="#b16">Meinshausen and Bü hlmann, 2010</ref>) to the lasso estimator involved in the RSPCA algorithm. Stability selection is a general framework to combine variable selection methods such as penalized regression models with subsampling strategies. Variable selection probabilities are estimated by applying variable selection methods to subsamples of the data, drawn without replacement, and estimating the proportion of subsamples where the variable was included in the fitted model. These selection probabilities are used to define a set of stable variables. Meinshausen and Bü hlmann (2010) provide a theoretical framework for controlling Type I error rates of falsely assigning variables to the set of stable variables. Here we suggest to apply the subsampling scheme of stability selection to the lasso estimator involved in the RSPCA algorithm to estimate selection probabilities which are then used to identify the truly relevant variables contributing to a PC. As the lasso selects true variables with high probability the corresponding selection probabilities estimated with stability selection are expected to dominate those of irrelevant variables. Applying a classical forward model selection to the features ranked by these selection probabilities, sparse loadings vectors that are parameter estimation consistent as well as model selection consistent can be identified. This manuscript is structured as follows: Section 2 describes the PCA, the RSPCA and the proposed sparse PCA method that involves stability selection. In Section 3 we describe the design and results of the simulation study that was performed to compare the different PCA methods. In Section 4 we demonstrate the practicability of the proposed sparse PCA approach by applying it to a publicly available gene expression data set of medulloblastoma brain tumors (<ref type="bibr" target="#b17">Remke et al., 2011</ref>). Finally, we discuss our findings and their relevance for estimating sparse PCs in high-dimensional molecular data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Principal component analysis (PCA)</head><p>Suppose X is an n Â p data matrix with entries x ji and indices j ¼ 1; .. . ; n and i ¼ 1;. .. ; p and rank r, where p corresponds to the number of features measured over n samples. Further, X has been mean centered such that the means of all p variables are zero. PCA seeks a number of K r linear combinations of the p variables that capture maximal variance:</p><formula>~ u k ¼ X T v k ¼ X p i¼1 v k;i x i ; (1)</formula><p>where ~ u k is the kth principal component (PC), k ¼ 1;. .. ; K and v k is the so-called loadings vector. v k has unit length and maximizes the variance of the kth PC. The coefficients of the loadings vector are interpreted as the contribution of each variable to the kth PC. Typically, the PCs are uncorrelated, i.e. the first PC points in the direction of maximal variance and the second PC shows in the direction of maximal variance orthogonal to the first PC and so on. A PCA can be performed by either an eigenvalue decomposition of the covariance matrix R or by singular value decomposition (SVD) of the data matrix X. The SVD of X is:</p><formula>X ¼ UDV T ; (2)</formula><p>where U is a n Â r orthogonal matrix and the column vectors u k are the PCs scaled to unit length. V is a p Â r orthogonal matrix with columns v k , which represent the loadings vectors and are equal to the eigenvectors of the sample covariance matrix ^ R. D is a diagonal matrix and the diagonal entries d 1 ;. .. ; d r are the singular values, where d k u k ¼ ~ u k is the kth PC with variance d 2 k. Typically, we are interested in a low-rank approximation of X, i.e. the first few PCs that explain most of the variance. It is known that the SVD gives the closest rank one approximation of X with respect to the Frobenius norm (<ref type="bibr" target="#b5">Eckart and Young, 1936</ref>):</p><formula>ðd; u; vÞ ¼ arg min d;u;v X À duv T 2 F ; (3)</formula><p>where jj Á jj 2 F indicates the squared Frobenius norm, which is the sum of squared elements of the matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Regularized sparse principal component analysis (RSPCA)</head><p>Shen and Huang (2008) and later<ref type="bibr" target="#b13">Lee et al. (2010)</ref>showed that, with u fixed, the minimization in Equation (3) can be formulated as a least squares regression. For fixed u, the least squares coefficient vector of regressing the columns of X on u is ~ v ¼ dv. The ordinary least squares estimator (OLS) for ~ v is ^ ~ v ¼ Xu. Without loss of generality, holding v fixed the OLS for ~ u is ^ ~ u ¼ X T v. With this connection to least squares regression it is straightforward to use penalization terms to impose sparsity on ~ v.</p><formula>ðu; ^ ~ vÞ ¼ arg min u;~ v X À u ^ ~ v T 2 F þ kPð~ vÞ; (4)</formula><p>where Pð~ vÞ is a penalization term that induces sparsity on ~ v and k is a tuning parameter that determines the strength of the penalization.</p><p>The RSPCA algorithm uses the lasso penalty Pð~ vÞ ¼ j~ vj, however other sparsity inducing penalization terms such as the adaptive lasso (<ref type="bibr" target="#b25">Zou, 2006</ref>) and the SCAD-penalty (<ref type="bibr" target="#b6">Fan and Li, 2001</ref>) are conceivable. With the lasso penalization term in Equation (4) a softthresholding estimator (<ref type="bibr" target="#b20">Tibshirani, 1996</ref>) can be derived to estimate the elements of ^ ~ v:</p><formula>^ ~ v i ¼ signfðXuÞ i gðjðXuÞ i j À kÞ þ : (5)</formula><p>Using adaptive lasso weights, the soft-thresholding estimator is given by:</p><formula>^ ~ v i ¼ signfðXuÞ i gðjðXuÞ i j À ^ w i kÞ þ : (6)</formula><p>where the ^ w i 's are weights chosen by an initial model fit ^ w ¼ 1=Xu c. Here c determines the strength of the weighting, typical values are in the range 0 &lt; c 2. Using the SCAD-penalty, the estimator is given by:</p><formula>^ ~ v i ¼ signf Xu ð Þ i g j Xu ð Þ i j À k À Á if j Xu ð Þ i j k signf Xu ð Þ i g j Xu ð Þ i j À ak À f Xu ð Þ i g a À 1 if k &lt; j Xu ð Þ i j ak Xu i if j Xu ð Þ i j &gt; ak 8 &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; :</formula><formula>(7)</formula><p>Here a &gt; 2 is a tuning parameter. Fan and<ref type="bibr" target="#b6">Li (2001)</ref>showed that the SCAD prediction is not sensitive to selection of a and suggest to use a ¼ 3.7. The SCAD-penalty function corresponds to a quadratic spline function with knots at k and ak, which leaves large values of the vector ^ ~ v not excessively penalized.<ref type="bibr" target="#b13">Lee et al. (2010)</ref>proposed an algorithm that solves the minimization problem in Equation (4). Using the lasso estimator in Equation (5) the algorithm alternates between the following two steps until convergence:</p><formula>1. ^ ~ v i ¼ signfðXuÞ i gðjðXuÞ i j À kÞ þ v ¼ ^ ~ v=k ^ ~ vk 2. ^ ~ u ¼ X T v u ¼ ^ ~ u=k ^ ~ uk</formula><p>To choose an optimal penalization parameter k,<ref type="bibr" target="#b13">Lee et al. (2010)</ref>proposed to use the Bayesian Information Criterion (BIC). The BIC is a model selection criterion related to Bayesian variable selection that assesses the quality of a model by the goodness of fit while penalizing for the complexity of the model, i.e. the number of parameters in the model.</p><formula>BICðkÞ ¼ X À duv T 2 F np^ r 2 þ ^ dfðkÞ logðnpÞ np ; (8)</formula><p>where ^ df ðkÞ is the degree of sparsity of the loadings vector v with penalty parameter k, and ^ r 2 is the OLS estimate of the error variance of the model. Subsequent PCs are fitted by subtracting the rank one approximation corresponding to the estimated sparse PC from the data matrix and applying the algorithm to the residual matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sparse PCA by sparse SVD using stability selection (S4VDPCA)</head><p>In contrast to the approach described so far, we propose to identify the variables that truly contribute to the leading eigenvector by applying a subsampling technique motivated by stability selection (<ref type="bibr" target="#b16">Meinshausen and Bü hlmann, 2010</ref>). By applying the corresponding variable selection method to subsamples drawn without replacement, selection probabilities for each variable can be estimated as the proportion of subsamples where the variable is included in the fitted model. The selection probability of each variable along the S4VDPCAregularization path, e.g. along the range of possible penalization parameters, is called the stability path. Here we propose to estimate the selection probabilities of the variables that contribute to sparse PCs by applying this resampling scheme to the lasso estimator as defined in Equation (5). In addition, we adopt the idea of the 'randomized lasso' also described by Meinshausen and Bü hlmann (2010). In each resampling iteration and for each of the p components of ^ ~ v a randomized reweighing of the penalization parameter k is performed. In each iteration weights w 1 ;. .. ; w p are sampled from a uniform distribution, i.e. w i $ Uðj; 1Þ. Given these weights the 'randomized lasso' estimator is:</p><formula>^ ~ v i ¼ signfðXuÞ i g jðXuÞ i j À k w i þ : (9)</formula><p>In this context, the so called weakness parameter j 2 ð0; 1 describes the amount of additional randomization and the 'randomized lasso' changes the penalization parameter k to a randomly chosen value in the range of ½k; k=j. Meinshausen and Bü hlmann (2010) showed that this additional randomization achieves model selection consistency even in situations where the necessary conditions for consistency of the lasso are violated. The 'randomized lasso' decorrelates variables and therefore addresses the model selection inconsistency problem of standard lasso in the presence of correlations between relevant and irrelevant variables. According to Meinshausen and Bü hlmann (2010) a low value of j lowers the probability of irrelevant variables to be selected. They propose to choose j in the range (0.2, 0.8) in applications. Due to computational complexity we do not calculate the whole stability path but follow the idea of point-wise control described by Meinshausen and Bü hlmann (2010) and choose a single k at which selection probabilities are estimated. This k should not penalize too strong so that in each iteration of the stability selection the true nonzero coefficients are selected with high probability. To find such a k, we estimate the selection probabilities for several possible penalization parameter and choose the lambda that leads to minimal number of ties in the selection probabilities. Ranking the variables according to their estimated selection probability a forward selection procedure is applied: starting with the variable with highest selection probability, we subsequently add variables and calculate sparse PCA solutions by applying regular SVD to the reduced matrix involving only the variables with highest selection probability. The remaining coefficients of ^ v that correspond to variables with lower selection probability, are set to zero. The final sparse PCA solution can be selected by applying a model selection criterion. It is known that model selection criteria like the BIC used in the RSPCA may select more variables than necessary when the number of variables is larger than the number of observations. Instead, a generalized information criterion (GIC) according to<ref type="bibr" target="#b12">Kim et al. (2012)</ref>is applied:</p><formula>GICðkÞ ¼ kX À duv T k 2 F np^ r 2 þ ^ df ðkÞ logðlogðnpÞÞlogðpÞ np ; (10)</formula><p>Further PCs can be fitted by subtracting the rank one approximation, i.e. the estimated sparse PC, from the data matrix and applying the algorithm to the resulting residual matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Simulation study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Study design</head><p>In a simulation study the proposed S4VDPCA method is compared to conventional PCA, the RSPCA with lasso penalty, with adaptive lasso penalty and with SCAD penalty. Furthermore, these methods are also compared to that of an oracle PCA, i.e. a PCA that 'knows' the coefficients of the true PC solution. In order to guarantee comparability between the S4VDPCA and the RSPCA with different penalty functions, the BIC used in<ref type="bibr" target="#b13">Lee et al. (2010)</ref>to choose an optimal penalization parameter within the RSPCA algorithm is replaced by the GIC of Equation (10). Moreover, the tuning parameter c used in the adaptive lasso in Equation (6) was set to c ¼ 1 for all simulations. In the same way, parameter a of the SCAD-penalty in Equation (7) was set to a ¼ 3.7. The number of iterations for the stability selection was set to 500 and the weakness parameter was set to j ¼ 0:2. To simulate data the underlying true population covariance matrix R was generated according to the single-covariance spike model described by Amini and Wainwright (2008):</p><formula>R ¼ ðd À 1Þvv T þ I p : (11)</formula><p>Here d ¼ p a is the simulated, leading eigenvalue and a is the spike index 0 a, i.e. the dominance of the eigenvalue. v is the corresponding sparse eigenvector, the true loadings vector, of length p, where bp b c coefficients of v are non-zero with value 1= ffiffiffiffiffiffiffiffiffi ffi bp b c p , such that jjvjj ¼ 1. b is the sparsity index that measures the sparsity of v and is in the range of 0 b 1. For the simulation study all combinations of a and b from 0.1 to 1 with step size 0.05 were investigated. At each point of this considered parameter space, R was generated for p ¼ 1000 features using the formula of the single-covariance spike model in Equation (11). Given R, 100 data matrices with sample size n ¼ 50 were generated by sampling from a multivariate normal distribution X $ N ðl; RÞ, where l is a zero vector of length p. To estimate v, S4VDPCA, RSPCA, conventional PCA and the oracle PCA were applied to these matrices. Oracle PCA estimates are calculated by applying regular SVD to a reduced matrix that involves only variables that are known to have non-zero coefficients in v. The remaining coefficients of ^ v that correspond to the zero entries in v are set to zero. To evaluate the results regarding parameter estimation consistency, the angle between the true loadings vector, the leading eigenvector of R, and the estimates are calculated, Að^ v; vÞ arccos jh^ v; vij:</p><formula>(12)</formula><p>Here A denotes the angle and h; i is the inner product. Following<ref type="bibr" target="#b18">Shen et al. (2013)</ref>an estimator of v is considered consistent as long as Að^ v; vÞ ! p 0. Moreover, an estimator is considered marginally inconsistent if Að^ v; vÞ ! p ð0; p 2 Þ and strongly inconsistent if Að^ v; vÞ ! p p 2. In theory, sparse PCA methods are able to consistently estimate v in high-dimensional, low-sample size data as long as 0 b &lt; a 1, but are marginally inconsistent if b ¼ a and strongly inconsistent if b &gt; a. For situations in which the signal of the leading eigenvector is relatively strong, i.e. a ! 1, even conventional PCA is expected to give consistent estimates (<ref type="bibr" target="#b9">Jung and Marron, 2009</ref>). In addition, to assess whether the different PCA methods select the true non-zero coefficients in v the false discovery rates (FDR) were calculated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>The results of the simulation study comparing the S4VDPCA to RSPCA using different penalization functions, conventional PCA and oracle PCA are shown in<ref type="figure" target="#fig_1">Figures 1</ref>and 2.<ref type="figure" target="#fig_1">Figure 1</ref>displays the median angles between the estimated and the true loadings vectors on a heat color scale. Simulation scenario were defined by all possible combinations of the spike index a and the sparsity index b, themedian was calculated over 100 simulation runs. Furthermore, the 10 and 45 median angles are indicated by contour lines. In addition, true positive rates (TPR) and simulation results for other tuning parameters are shown in the Supplementary Material., i.e. RSPCA using the adaptive lasso with c ¼ 0:5 and c ¼ 2 and S4VDPCA with weakness parameter j ¼ 0:5 and j ¼ 0:8. As already shown in theory by Jung and Marron (2009), conventional PCA is strongly inconsistent as long as the strength of the underlying signal is weak, i.e. for spike indices a [ 0:45. With increasing a the PCA estimates get closer to the true eigenvector, thereby achieving marginal consistency for a Z 0:45 (as indicated by the 45 degree contour line) and consistency for a Z 1. The behaviour of the consistency is independent of the sparsity of the underlying signal, e.g. the sparsity index b (<ref type="figure" target="#fig_1">Fig. 1f</ref>). However, if the underlying signal is sparse and b &lt; a, all considered sparse PCA methods can consistently estimate the first loadings vector and become marginally consistent for b ¼ a (<ref type="figure" target="#fig_1">Fig. 1a</ref>–e). The oracle PCA, which 'knows' the true non-zero coefficients, gives unbiased estimates of the non-zero coefficients and therefore the best possible sparse solutions that are closest to the underlying first eigenvector of the population covariance matrix. The sparse loadings vectors estimated by the S4VDPCA are in all situations slightly closer to the unbiased, oracle estimates than RSPCA estimates using any considered penalty function. For b &gt; 0:2 the 10 degree contour line of the S4VDPCA is always closest to the marginal consistency boundary (b ¼ a). Furthermore, the 45 degree contour line lies in most situations further to the left hand side of the marginal consistency boundary. If the true eigenvector is very sparse and the signal is weak, i.e. b &lt; 0:3 and a &lt; 0:3, both the RSPCA and the S4VDPCA become marginally inconsistent in estimating the true eigenvector, even if a &gt; b.<ref type="figure" target="#fig_2">Figure 2</ref>displays the median FDR for the different RSPCA methods (<ref type="figure" target="#fig_2">Fig. 2a</ref>, c and d) and the S4VDPCA (<ref type="figure" target="#fig_2">Fig. 2b</ref>). The median FDR was calculated over 100 simulation runs and for all combinations of a and b, as described in the simulation design above. The FDR is shown on a heat color scale and median levels of 0.05 and 0.5 are indicated by contour lines. For relatively weak signals, starting at a ¼ 0:4, the RSPCA methods and S4VDPCA tend to falsely select coefficients resulting in FDRs around 0.05, especially in situations where the true signal is less sparse b &gt; 0:5. When a 0:25 and b &lt; 0:9 the FDR increases dramatically to 0.5. In situations where sparse PCA methods are expected to consistently estimate the direction of the true eigenvector, i.e. a &gt; b and b &gt; 0:2, the FDR for the RSPCA with the lasso is around 0.05 (<ref type="figure" target="#fig_2">Fig. 2a</ref>). This expected behaviour reflects the known variables selection inconsistency of the lasso in high dimensions (<ref type="bibr" target="#b15">Meinshausen and Bü hlmann, 2006;</ref><ref type="bibr" target="#b24">Zhao and Yu, 2006</ref>). In these simulation settings the unbiased coefficients are relatively large (a &gt; b), so that the penalization chosen by the GIC is not sufficient to screen out irrelevant variables that are correlated with truly relevant variables. Both the adaptive lasso and SCAD penalty are known to possess the oracle property and are thus expected to select only truly relevant variables and achieve approximately unbiased estimates. Nevertheless, the simulation results show that both penalties tend to select additional irrelevant variables in simulation settings where the signal intensity a and sparsity b are nearly equal (depicted by the orange tail along the diagonal in<ref type="figure" target="#fig_2">Fig. 2a</ref>, c and d), i.e. around the marginal consistency boundary (b ¼ a). This behaviour is more pronounced for the SCAD penalty compared to the adaptive lasso. In contrast, in almost all simulation settings where a &gt; b the S4VDPCA identifies the true non-zero coefficients without adding any irrelevant features. Therefore, we can conclude, that particularly in the challenging settings where b % a, the selection probabilities estimated by the S4VDPCA can successfully be used to filter the truly relevant features withouth selecting as many false positves as the RSPCA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Application</head><p>To demonstrate practicability to find sparse and interpretable PCs in high-dimensional molecular data sets, the proposed S4VDPCA method was applied to a gene expression data set of medulloblastoma brain tumors (<ref type="bibr" target="#b17">Remke et al., 2011</ref>). Medulloblastoma is the most common malignant pediatric brain tumor and comprises four distinct molecular variants. These subgroups are known as WNT, SHH, group C and group D. WNT tumours show activated Wnt signaling pathway and carry a favourable prognosis. SHH medulloblastoma show Hedgehog signaling pathway activation and are known to have an intermediate to good prognosis. While both WNT and SSH variants are molecularly already well characterized, the genetic programs driving the pathogenesis of group C and group D medulloblastoma remain largely unknown. Here we applied the proposed S4VDPCA method to gene expression data of 8 group C, 20 group D, 20 SHH and 16 WNT tumors. Gene expression has been measured by the 4x44K Agilent Whole Human Genome Oligo Microarray. After normalization and quality control the data set comprised gene expression values of 18406 annotated genes. The data set is publicly available at the National Center for Biotechnology Information (NCBI) Gene Expression Omnibus (GEO) database, Accession No. GSE28245. The first two sparse PCs have been extracted by applying the S4VDPCA and the results are visualized as biplot representation in<ref type="figure">Figure 3</ref>. The loadings vector of the first sparse PC comprises 2035 non-zero coefficients and the second PC involves 1532 non-zero coefficients. The biplot displays the projection of the tumor samples onto the two sparse PCs while also visualizing the covariance structure of the selected genes within this rank two approximation by grey arrows. Each arrow represents a gene and the length of the arrow reflects the size of the corresponding coefficient in the two loadings vectors. Arrows that point in similar directions represent positive correlated genes. Arrows parallel to a PC axis are genes with a non zero loadings coefficient only in one of the two loadings vectors of the two PCs. The four molecular subgroups can clearly be separated by projecting the samples in the space spanned by the first two sparse PCs. While most WNT and SHH medulloblastomas form clusters far away from samples of other subgroups, group D and group C tumors are closer to each other. A set of 2035 genes is still too large for a reasonable interpretation, but the most dominating genes, i.e. those showing the highest absolute coefficients can be highlighted. In<ref type="figure">Figure 3</ref>, 15 prominent oncogenes with a high absolute coefficient have been highlighted, including SFRP1 and its transcription factor GLI1. Both arrows point away from the WNT samples into the direction of SHH medulloblastoma. This means that the SFRP1 expression is up-regulated in SHH tumors and down-regulated in WNT sample and matches the current knowledge that SFRP1 is a tumor suppressor gene responsible for Hedgehog signaling mediated regulation of the WNT signaling pathway. Moreover, the arrows of DKK1 and WIF1, which are known target genes of the WNT signaling pathway, both point in direction of the WNT medulloblastoma. FSTL5, a known marker for poor prognosis in non-WNT/non-SHH medulloblastoma (<ref type="bibr" target="#b17">Remke et al., 2011</ref>), points into a direction of group C and group D tumors. Since the loadings coefficient of FSTL5 is zero in the second PC the arrow for FSTL5 is parallel to the first PC.</p><p>However, individual interpretation of all non-zero coefficients is still too complex. An alternative way to try to understand the importance of the genes selected by sparse PCA methods is to perform a pathway analysis. Here we performed hypergeometric testing of the genes selected in the first and second PC to evaluate whether these genes are overrepresented in KEGG pathways (Kyoto Encyclopedia of Genes and Genomes;<ref type="bibr" target="#b10">Kanehisa and Goto, 2000</ref>). To perform this analysis the R/Bioconductor package HTSanalyzeR was used (<ref type="bibr" target="#b21">Wang et al., 2011</ref>). The top six pathways most significantly overrepresented by genes selected in the first and second PC are shown as graphs above and to the right of the biplot. Each node or circle of the graph represents a pathway and the size of each node is proportional to the number of genes assigned to that pathway. Pathways are connected by edges and the width of each edge is proportional to the number of genes shared by two pathways. The white-red coloring of the nodes corresponds to FDR adjusted p-values that are also shown in the Supplementary Material. Among the top six pathways overrepresented by genes selected in the first sparse PC are the Wnt signaling pathway and Neuroactive ligandreceptor interaction. Even though these pathways include a wide range of genes, both pathways are expected to be deregulated in medulloblastoma. Interestingly, genes selected in the second, sparse PC are also significantly overrepresented in the Wnt signaling pathway and the Hedgehog signaling pathway. This result directly reflects the known interaction between these two pathways and is in agreement with the biplot where the largest distances between WNT and SHH samples are along the axis of the second PC. Similar results calculated by applying conventional PCA, the RSPCA with lasso, RSPCA adaptive lasso and the RSPCA with SCAD penalty are shown in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and conclusion</head><p>Here we have presented a simple and computationally efficient twostep approach to estimate consistent sparse PCA solutions in highdimensional, low sample-size situations. In a first step features are ranked by applying a subsampling scheme motivated by stability selection. In the second step a sparse PC is estimated by simple forward selection. While existing sparse PCA methods like the RSPCA focus on finding sparse PCs that are consistent in estimating the true direction of maximal variation, the proposed S4VDPCA also takes model selection consistency into account. Model selection consistency, i.e. selecting truly relevant variables is important for a correct interpretation and further downstream analysis, e.g. performing a subsequent pathway analysis. The stability selection applied within the S4VDPCA can be understood as an ensemble method such as bootstrap aggregation (<ref type="bibr">Bagging;</ref><ref type="bibr" target="#b2">Breiman, 1996a</ref>). Bagging is a popular method to estimate models with improved prediction performance by reducing the variance of a single weak prediction model by aggregating the predictions of several weak models that were fitted on bootstrap samples. Similarly, by counting the number of times a variable is selected in each of the sampled subsets, the stability selection combines the information of a collection of lasso models. Each of these lasso models is weak in model selection as it suffers from the model selection inconsistency of the lasso. Therefore, the selected features vary, e.g. are unstable, when compared over all models in the collection. However, ranking features by estimated selection probabilities, i.e. the proportion of subsamples where the variable is included in a fitted model, allows the S4VDPCA to identify the truly relevant molecular features as variables with high selection probabilities.<ref type="figure">Fig. 3</ref>. Biplot representation of the first two sparse PCs. The biplot displays the projection of the samples into the two dimensional space spanned by the first two sparse PCs. The arrows show the contribution of the selected genes to the two sparse PCs, i.e. the covariance structure of the selected genes. Each arrow represents a gene and the length of the arrow reflects the size of the corresponding coefficient in the two loadings vectors. Relevant oncogenes are highlighted in red. The nodes of the two graphs above and on the right side of the biplot represent pathways significantly overrepresented by the genes selected in the first and second PC, respectively</p><formula>DCX LPPR4 ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● Ribosome</formula><formula>Hedgehog signaling pathway ● ● ● ● ● ●</formula><p>The additional randomization of the 'randomized lasso' approach further decorrelates variables and leads to larger differences between the selection probabilities of correlated irrelevant and relevant variables. In the same spirit other ensemble methods like the popular Random Forests algorithm (<ref type="bibr" target="#b4">Breiman, 2001</ref>) decorrelate variables by limiting the number of variables that are allowed to be selected for each subsample. Surprisingly, in some simulation scenarios, i.e. when the spike index a and sparsity index b are close to each other, the S4VDPCA even outperformed the RSPCA with SCAD and the adaptive lasso penalty. Both of these penalization functions are explicity designed to overcome the model selection inconsistency of the lasso and were expected to consistently select only truly relevant variables. Applying subsampling or bootstrapping to estimate selection frequencies that are then used to rank variables for classical forward model selection can be seen as a computational shortcut to a stable 'best' subset selection. The best subset selection is a combinatorial procedure which evaluates all subsets by minimizing some selection criterion like the BIC. Conventional best subset selection is not feasible for high-dimensional data and is known to suffer from instability in variable selection (<ref type="bibr" target="#b3">Breiman, 1996b</ref>). The two-step approach of the S4VDPCA addresses the instability and the computational complexity by applying stability selection to rank variables. Moreover, this twostep procedure is a rather general idea that could be applied to all kinds of statistical prediction problems to find parameter consistent and model selection consistent estimates in high dimensions. The computational bottleneck for both the RSPCA and the S4VDPCA algorithm is the optimization of the information criterion, here the BIC or GIC. These information criteria are step functions of p that involve computationally costly matrix multiplications to calculate the goodness of fit of the sparse rank one approximation. To reduce the computation time we have implemented a parallelized search algorithm that can be used for both sparse PCA methods. However, by design the RSPCA optimizes the information criterion in each iteration of the algorithm while the S4VDPCA performs the stability selection once and requires only a single information criteria optimization step to select the final model via forward selection. In addition, the stability selection is an embarrassingly parallel computation problem that can easily be solved using parallel implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This work was partially supported by the Virtual Helmholtz Institute<ref type="bibr">[VHVI-404]</ref>. Conflict of Interest: none declared.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.1.</head><figDesc>Fig. 1. Angles between estimated and true leading eigenvector for (a) RSPCA with lasso penalty, (b) S4VDPCA, (c) RSPCA with adaptive lasso penalty, (d) oracle SPCA, (e) RSPCA with SCAD penalty and (f) conventional PCA. The colors correspond to the median angle calculated over 100 simulation runs. Angles with 10 and 45 degrees of deviation are indicated by contour lines. The sparsity index b and the spike index a define the sparsity, e.g. the number of truly non-zero coefficients, and the dominance of the signal, e.g. the eigenvalue, of the simulated first PC. Further, p and n denote the number of features and samples of the simulated data sets and nsim denotes the number of simulated data sets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.2.</head><figDesc>Fig. 2. Median FDR for (a) RSPCA with lasso penalty, (b) S4VDPCA, (c) RSPCA with adaptive lasso penalty and (d) RSPCA with SCAD penalty. FDRs of 0.05 and 0.5 are indicated by contour lines. The sparsity index b and the spike index a define the sparsity, e.g. the number of truely non-zero coefficients, and the dominance of the signal, e.g. the eigenvalue, of the simulated first PC. Further, p and n denote the number of features and samples of the simulated data sets and nsim denotes the number of simulated data sets</figDesc></figure>

			<note place="foot">M.Sill et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">High-dimensional analysis of semidefinite relaxations for sparse principal components</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">A</forename>
				<surname>Amini</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">J</forename>
				<surname>Wainwright</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2454" to="2458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">High-dimensional Cox models: the choice of penalty as part of the model building process</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Benner</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometr. J</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="50" to="69" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Breiman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Heuristics of instability and stabilization in model selection</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Breiman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2350" to="2383" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Breiman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">The approximation of one matrix by another of lower rank</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Eckart</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Young</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="211" to="218" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Variable selection via nonconcave penalized likelihood and its oracle properties</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Fan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Li</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="1348" to="1360" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">A note on the use of principal components in regression</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<forename type="middle">T</forename>
				<surname>Jolliffe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Stat</title>
		<imprint>
			<biblScope unit="page" from="31" to="300" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">A modified principal component technique based on the LASSO</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<forename type="middle">T</forename>
				<surname>Jolliffe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graph. Stat</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="531" to="547" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">PCA consistency in high dimension, low sample size context</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Jung</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Marron</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="4104" to="4130" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">KEGG: Kyoto encyclopedia of genes and genomes</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Kanehisa</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Goto</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="27" to="30" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">arrayQualityMetrics: a bioconductor package for quality assessment of microarray data</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kauffmann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="415" to="416" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Consistent model selection criteria on high dimensions</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Kim</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1037" to="1057" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Biclustering via sparse singular value decomposition</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Lee</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="1087" to="1095" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Principal component analysis based methods in bioinformatics studies</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Ma</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Dai</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brief. Bioinf</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="714" to="722" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">High dimensional graphs and variable selection with the lasso</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Meinshausen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Bü Hlmann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1436" to="1462" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Stability selection</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Meinshausen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Bü Hlmann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="417" to="473" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Fstl5 is a marker of poor prognosis in non-wnt/nonshh medulloblastoma</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Remke</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Oncol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3852" to="3861" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Consistency of sparse pca in high dimension, low sample size contexts</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Shen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Multivar. Anal</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="317" to="333" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparse principal component analysis via regularized low rank matrix approximation</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Shen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Multivar. Anal</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1015" to="1034" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">HTSanalyzeR: an R/Bioconductor package for integrated network analysis of high-throughput screens</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Wang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="879" to="880" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">M</forename>
				<surname>Witten</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="515" to="534" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">A sparse singular value decomposition method for highdimensional data</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Yang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graph. Stat</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="923" to="942" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">On model selection consistency of lasso</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Zhao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Yu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2541" to="2563" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">The adaptive LASSO and its oracle properties</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Zou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="1418" to="1429" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Sparse principal component analysis</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Zou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graph. Stat</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>