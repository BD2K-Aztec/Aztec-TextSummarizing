
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining Small-sample precision of ROC-related estimates</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010">2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Blaise</forename>
								<surname>Hanczar</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIPADE</orgName>
								<orgName type="institution">University Paris Descartes</orgName>
								<address>
									<addrLine>45 rue des Saint-Peres</addrLine>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Jianping</forename>
								<surname>Hua</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Computational Biology Division</orgName>
								<orgName type="institution">Translational Genomics Research Institute</orgName>
								<address>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Chao</forename>
								<surname>Sima</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Computational Biology Division</orgName>
								<orgName type="institution">Translational Genomics Research Institute</orgName>
								<address>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">John</forename>
								<surname>Weinstein</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Bioinformatics and Computation Biology</orgName>
								<orgName type="institution">MD Anderson Cancer Center</orgName>
								<address>
									<settlement>Houston</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Michael</forename>
								<surname>Bittner</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Computational Biology Division</orgName>
								<orgName type="institution">Translational Genomics Research Institute</orgName>
								<address>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName>
								<forename type="first">Edward</forename>
								<forename type="middle">R</forename>
								<surname>Dougherty</surname>
							</persName>
							<email>edward@mail.ece.tamu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computational Biology Division</orgName>
								<orgName type="institution">Translational Genomics Research Institute</orgName>
								<address>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Bioinformatics and Computation Biology</orgName>
								<orgName type="institution">MD Anderson Cancer Center</orgName>
								<address>
									<settlement>Houston</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<addrLine>College Station</addrLine>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining Small-sample precision of ROC-related estimates</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">26</biblScope>
							<biblScope unit="issue">6</biblScope>
							<biblScope unit="page" from="822" to="830"/>
							<date type="published" when="2010">2010</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btq037</idno>
					<note type="submission">Received on December 14, 2009; revised on January 22, 2010; accepted on January 25, 2010</note>
					<note>[15:24 19/2/2010 Bioinformatics-btq037.tex] Page: 822 822–830 Associate Editor: Jonathan Wren</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: The receiver operator characteristic (ROC) curves are commonly used in biomedical applications to judge the performance of a discriminant across varying decision thresholds. The estimated ROC curve depends on the true positive rate (TPR) and false positive rate (FPR), with the key metric being the area under the curve (AUC). With small samples these rates need to be estimated from the training data, so a natural question arises: How well do the estimates of the AUC, TPR and FPR compare with the true metrics? Results: Through a simulation study using data models and analysis of real microarray data, we show that (i) for small samples the root mean square differences of the estimated and true metrics are considerable; (ii) even for large samples, there is only weak correlation between the true and estimated metrics; and (iii) generally, there is weak regression of the true metric on the estimated metric. For classification rules, we consider linear discriminant analysis, linear support vector machine (SVM) and radial basis function SVM. For error estimation, we consider resubstitution, three kinds of cross-validation and bootstrap. Using resampling, we show the unreliability of some published ROC results. Availability: Companion web site at http://compbio.tgen.org/ paper_supp/ROC/roc.html Contact:</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>High-throughput technologies, such as those based on microarrays or 'Next-Generation' sequencing, make it possible to generate data on large numbers of genes, transcripts or proteins simultaneously in biological samples. Typical variables assessed include mutations, DNA copy number, DNA methylation, mRNA expression, microRNA expression, protein expression and post-translational modifications. A central goal of current biomedical research is to use those molecular profiles to identify biomarkers or multi-gene biosignatures for 'personalization' of medicine—that is, to use them for the full range of medical management choices—in disease risk assessment, sub-classification of disease, early diagnosis, prognosis, choice of optimal therapy, evaluation of response to therapy and/or identification of relapse. * To whom correspondence should be addressed.</p><p>The profile data are used to develop univariate or multivariate predictors of biologically or medically interesting outcomes. Often, the aim is to develop a binary classifier, for example, diseased versus normal, disease subtype 1 versus disease subtype 2, response versus non-response to a drug, or 5-year survival versus death. A large literature has developed on such classifiers, but the recurring question is, 'How accurate are their predictions and classifications?' This question is supposed to be answered by the error rate; however, recent Monte Carlo simulations have shown large uncertainty in the error estimates. In the presence of high-dimensional feature spaces and small samples, a ubiquitous situation with high-throughput technologies, resampling error estimation methods, for example, cross-validation (CV), suffer from high-deviation variance, that is, the variance of the difference between the true and estimated errors is large (Braga<ref type="bibr" target="#b1">Neto and Dougherty, 2004</ref>)<ref type="bibr">[see Glick (1978)</ref>for an early criticism of<ref type="bibr">CV]</ref>. Moreover, there tends to be a lack of correlation and regression between the true and estimated errors, to the extent that the regression line of the true error on the estimated error is nearly horizontal (<ref type="bibr" target="#b6">Hanczar et al., 2007</ref>). These Monte Carlo studies have been supported by analytical studies in the case of the discrete histogram rule (Braga<ref type="bibr" target="#b3">Neto and Dougherty, 2005b</ref>) and linear discriminant analysis (LDA;<ref type="bibr">Zollanvari et al., 2009</ref>). For assessment of binary classifiers, in addition to the error rate, a favorite analytical tool is the receiver operator characteristic (ROC) representation (<ref type="bibr" target="#b10">Pepe et al., 2004;</ref><ref type="bibr" target="#b14">Spackman, 1989</ref>)—for instance, with regard to gene-expression profiling in cancer, see<ref type="figure" target="#tab_1">Table C1</ref>on the companion web site (http://compbio.tgen.org/paper_supp/ROC/ roc.html). An ROC curve is formulated by plotting the sensitivity and specificity of the classifier against each other as a function of some threshold criterion, for example, based on a biomarker or biosignature. The resulting ROC curve presents graphically the trade-off between false positives (FP) and false negatives (FN) in the classification process. The area under the ROC curve provides a scalar parameter that reflects the overall quality of the classifier. A natural question is whether parameters associated with ROC curves, such as the area under the curve (AUC), would suffer the same degree of uncertainty as discovered in the previous analyses of classifier error. Accordingly, we have established the computational machinery to address this question for both simulated and real datasets, and have performed a variety of analyses based on different predictive algorithms and methods of validation. We have analyzed the effect of sample size and the effect of an unbalance in the number of cases per class. That type of imbalance is common in biological datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Small-sample precision of ROC-related estimates</head><p>Although ROC curves are insensitive to changes in class proportion, we show here that such imbalances have considerable impact on the estimation of both error rate and AUC. Through the simulations on both synthetic and real data, we identify how the training set size and class disproportion affect the performances of the different metrics. In particular, we show the unreliability of ROC performance metric estimations in small-sample settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SYSTEMS AND METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">ROC curves</head><p>Consider a two-class problem defined by the feature-label distribution F and a sample S ={(x 1 ,y 1 ),...,(x N ,y N )} of N examples drawn from F. An example is a pair (x,y), where x is a d-dimensional vector and y ∈{0,1} is the class. A classification rule is used to design a discriminant S : R d → R from S. The output of S is a probability or a score that reflects the degree of uncertainty with which an example is assigned to a class. A binary classifier S,T is derived from S via a threshold T according to S,T (x) = 0 if S (x) &gt; T and S,T (x) = 1 otherwise.</p><p>Given an example x, there are four possibilities when comparing the class predicted by S,T (x) to its true class y: true positive (TP) y = 0 and S,T (x) = 0; FN y = 0 and S,T (x) = 1; FP y = 1 and S,T (x) = 0; true negative (TN) y = 1 and S,T (x) = 1. From these four possibilities, we can define three performance metrics; classifier error, ERR = (FP+FN)/N; true positive rate, TPR = TP/(TP+FN); and false positive rate, FPR = FP/(TN+FP). An ROC graph is a 2D graph in which the x-axis represents the FPR and y-axis represents the TPR. The point (0,1) represents perfect classification: no negatives classified as positives and all positives classified as positive. On the diagonal, the points (0,0) and (1,1) correspond to all examples being assigned to the negative class and to the positive class, respectively. The performance of a classifier S,T for a fixed threshold T is represented by a single point in ROC space. If the decision threshold T is allowed to vary, then the performance of the discriminant S is a variable depending on T and is represented by a curve in ROC space. A common metric to estimate the performance of a classifier independently of the decision threshold is the Area Under the Curve (AUC). AUC ∈<ref type="bibr">[0,</ref><ref type="bibr">1]</ref>, AUC = 1 corresponds to the perfect classifier, for which the ROC curve goes directly from point (0,0) to (0,1) and then to (1,1), AUC = 0 corresponds to the classifier assigning all examples to the wrong class, and the ROC curve that follows the diagonal line has AUC = 0.5. A direct method to compute the AUC is to construct the ROC curve and then measure the AUC. If there are M test examples, then we obtain up to M +1 points in the ROC space with which to draw the curve. Accordingly, the AUC can be estimated by applying a rectangle or trapezoid area on each point. However, an alternative of AUC computation has been proposed in (<ref type="bibr" target="#b7">Hand and Till, 2001</ref>), where it is shown that the AUC corresponds to the probability that an example from the positive class has a higher classifier output S (x) than an example from the negative class. In their procedure, the examples are sorted in increasing order according to the values S (x) and the AUC is computed by the following formula:</p><formula>AUC = S 0 −n 0 (n 0 +1)/2 n 0 n 1 (1)</formula><p>where n 0 and n 1 are the numbers of examples of the positive and negative classes, respectively, in the test set, and S 0 is the sum of ranks of examples in the positive class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Models for synthetic data</head><p>We have performed a set offor the negative class, where the elements of A are evenly drawn from [0.5,1.5] and fixed throughout so as not to confound model variability with error estimation. An irrelevant feature follows the same normal distribution, N(0,σ 0 ), for both classes. Inside a class, all relevant features have a common variance. Two covariance matrix structures are considered: (i) is the identity matrix I in which the features are uncorrelated and the classconditional densities are spherical Gaussian. (ii) is a block-structured matrix in which the features are equally divided into blocks of size 4: features from different blocks are uncorrelated and every two features within the same block have a common correlation coefficient ρ = 0.8. In the linear models, the variances of covariance matrices of the two classes are equal, σ 1 = σ 0 = 1.8; in the non-linear models, the variances of covariance matrices are different, with σ 0 = σ 1 /1.5 = 1.4. With these model characteristics, we generate training and test sets with N and 10 000 examples, respectively, containing 20 relevant features and 180 irrelevant features. The large test set is used to compute the true metrics. The number of training examples (N) and class prior probabilities (p 0 for class 0, and p 1 = 1−p 0 for Class 1) of the two classes are parameters of the dataset, with N varying from 50 to 1000, and p 0 from 0.2 to 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Classification rules</head><p>We consider three classification rules: LDA, linear support vector machine (SVM) and radial basis function SVM (RBF-SVM). The output of an LDA classifier is readily transformed into the posterior probability of the positive class, so the usual decision threshold is 0.5. The output of an SVM is a score that represents the distance of the example from the separating hyperplane and the sign of the score defines the predicted class. The usual decision threshold of an SVM is 0. The RBF-SVM is in general a non-linear classifier, although linear SVM can be viewed as a special form of it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">IMPLEMENTATION</head><p>Our simulation study uses the following protocol:</p><p>(i) A training set S tr and a test set S ts are generated. For the synthetic data, examples are sampled from the distribution determined by the model, N examples for S tr and 10 000 examples for S ts. For the microarray data, the examples are randomly separated into training and test sets with N = 50 examples for the training set and the remaining for the test set.</p><p>(ii) To design a classifier based on a dataset S: first apply t-test to S and select 10 best features based on the t-test statistics; then build the classifier S from the reduced set. For classification, the decision threshold T is as defined in Section 2.3.</p><p>(iii) For true performance:</p><p>(a) Based on training data S tr , build the classifier Str .</p><p>(b) Apply Str to test data S ts. For each example x, compute class prediction Str ,T (x) and classifier output Str (x).</p><p>(c) Based on class predictions and true labels, compute true error rate, TPR and FPR.</p><p>(d) Sort Str (x), then compute true AUC according to Equation (1).</p><p>(iv) For estimated performance, consider the following estimators:</p><p>(a) resubstitution:</p><p>((2) Compute estimated error rate, TPR and FPR with class predictions, and estimated AUC with sorted classifier outputs.</p><p>(b) k-fold CV:</p><p>Page: 824 822–830</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.Hanczar et al.</head><p>(1) Randomly partition the training data into k folds S</p><p>(i) tr , i = 1,...,k. For each fold S</p><p>(i) tr , based on the remaining data in the training set S tr \S</p><p>(i) tr , build the classifier Str \S (i) tr</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Apply</head><p>Str \S (i) tr</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>to fold S</head><p>(i) tr to generate each example's class prediction and classifier output.</p><p>(2) Collect the class predictions and classifier outputs from all folds. Compute estimated error rate, TPR, FPR and AUC.</p><p>(3) For leave-one-out (LOO), set k to the training sample size; for 10-fold CV (10CV), set k = 10; for 10CV with 10 repetitions (10CV10), repeat Step (1) for 10 times before entering Step (2).</p><p>(c) .632 bootstrap (BOOT):</p><p>(1) Form a bootstrap sample S * of size N by drawing with replacement from S tr. Using S * , build the classifier S * . Apply S * to the examples that are in S tr but not in S * .</p><p>(2) Repeat the above step 100 times, collect class predictions and classifier outputs from all repetitions, compute estimated error rate, TPR, FPR and AUC, and denote them as ε 0 , TPR 0 , FPR 0 and AUC 0 , respectively.</p><p>(3) Obtain the resubstitution estimate of error rate, TPR, FPR and AUC, and denote them as ε resub , TPR resub , FPR resub and AUC resub , respectively.Replace ε in above equation with TPR, FPR and AUC to obtain the .632 bootstrap estimation of TPR, FPR and AUC, respectively.</p><p>(v) Repeat the above procedure for 5000 times and collect all results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS AND DISCUSSION</head><p>In this article, we discuss representative results with the full set of results being given on the companion web site. For the synthetic data, we restrict ourselves here to the linear SVM and CV error estimation for the linear model with uncorrelated data, unless specifically indicated. For real data, again we demonstrate only the results of linear SVM with CV error estimation, unless specifically indicated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results for synthetic data</head><p>Figure 1 presents the deviation distributions (true minus estimated metric) for classifier error, AUC, TPR and FPR, with N = 100 and p 0 = 0.5,0.7 and for five error estimators. As expected, leave-oneout is practically unbiased but has the largest deviation variance. Generally, for this case, bootstrap and the CV estimators are nearly unbiased. Bootstrap has the smallest deviation variance except for resubstitution, which suffers from severe bias. However, one must be careful about generalizing from the bootstrap bias results, since .632 bootstrap is known to have substantial bias for certain models and classifiers.<ref type="figure" target="#fig_3">Figure 2</ref>shows scatter plots comparing the true and estimated values of the four metrics for N = 50,100,200 (500 and 1000 are on the companion web site) and p 0 = 0.5,0.7. The estimated and true values are on the x-and y-axis, respectively, and the small triangles indicate the mean true and mean estimated values. The black line shows the linear regression for the true error on the estimated error. The lack of error regression and wide dispersion for small N is consistent with what we have previously reportedfor error estimation (<ref type="bibr" target="#b6">Hanczar et al., 2007</ref>). Of interest here, the dispersion is worse for the AUC than for the classifier error and that it is worse for the unbalanced prior (p 0 = 0.7) than the balanced prior (p 0 = 0.5). Note that the TPR variance is particularly bad for the unbalanced prior, a finding common throughout this study. There Page: 825 822–830</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Small-sample precision of ROC-related estimates</head><p>Error rates area under curve true positive rate false positive rateis also little regression for the true AUC on the estimated AUC. On the companion web site, it can be seen that AUC regression generally does not improve for large sample sizes; however, the variance decreases greatly for N = 500,1000. Hence, estimation is good, even with a lack of regression.<ref type="figure" target="#fig_5">Figure 3</ref>shows the root mean square (RMS) error and the correlation between the true and estimated metrics as functions of the class prior probabilities (on the x-axis) for N = 50,100,200,500,1000. RMS of the first row is defined by</p><formula>RMS = E[|ε est −ε tru | 2 ]</formula><p>where ε tru and ε est are the true and estimated metrics. The second row in the figure shows the correlation between true and estimated metrics. The RMS is strongly negatively correlated with the training set size, RMS decreases as N increases. The prior probability also impacts the RMS. The classifier error decreases slightly and the AUC increases slightly with imbalance between the classes. Relative to the prior probability, RMS for the TPR increases substantially and RMS for the FPR decreases substantially for increasing prior probability. In all cases, sensitivity to the prior is substantial for small samples and decreases with increasing N. RMS can be decomposed into the bias and deviation variance as<ref type="figure" target="#fig_3">Figure 2</ref>shows that the bias is small for the CV estimator being considered, so that both classifier error and AUC imprecision result from the deviation variance. The deviation variance can be further decomposed into the variances of the true and estimated metrics, along with the correlation, ρ, between true and estimated metrics:</p><formula>RMS = Var dev [ε est ]+Bias[ε est ] 2</formula><formula>Var dev [ε est ]=σ 2 est +σ 2 tru −2ρσ est σ tru .</formula><p>According to<ref type="figure" target="#fig_5">Figure 3</ref>, the correlation is typically not large and cannot offset the σ 2 est +σ 2 tru term in the deviation variance for the classifier error or the AUC. In sum, the AUC is poorly estimated for small samples, particularly so for N ≤ 100. The effect of imprecise estimation is observable in the ROC curves themselves.<ref type="figure" target="#fig_0">Figure 4</ref>shows ROC-related curves for LDA classification and the non-linear uncorrelated model.The left-and right-hand columns show results for p 0 = 0.5 and p 0 = 0.7, respectively, and N = 50,100 and 200. The cross-and circle-marked curves correspond to using LOO error estimation and the true error, respectively. The solid lines are the mean ROC curves and the upper and lower dashed lines are the 95% confidence interval. The circle-marked dashed lines represent the variation associated with computing ROC curves from samples, that is, constructing the curves from sample-based TPRs and FPRs. The extra variance represented by the cross-marked dashed lines results from using the estimated TPR and estimated FPR instead of the true TPR and true FPR. For N ≤ 100, this extra variance is substantial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analytic representation of estimated AUC variance</head><p>We have observed that the variance of the estimated AUC often exceeds the variance of the estimated error in the simulations. Although we cannot prove a general theorem to that effect, we can give an analytic proof for a special case. For simplicity, we consider the one-split training–testing scheme, but the conclusion can easily be extended to many other schemes. For the feature vector X and binary label variable Y , let the two class-conditional distributions be identical, the corresponding cumulative distribution functions (CDFs) be continuous over R, and the prior class probabilities be given by P{Y = 0}=p and P{Y = 1}=1−p. The Bayes error is min(p,1−p). For AUC estimation, the classification rule adopted will yield a discriminant S : R →<ref type="bibr">[0,</ref><ref type="bibr">1]</ref>from the training data. Let the CDFs of S (X|Y = 0) and S (X|Y = 1) be continuous over<ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. Assume there are altogether n testing sample points x 1 ,...,x n , with n 0 = pn points from class 0 and n 1 = (1−p)n points from Class 1. The AUC is computed according to Equation (1). Owing to the continuity assumption,</p><formula>P[ S (x i ) = S (x j )]=0</formula><p>. Hence, the ranking order is unique with probability 1. Since the class-conditional distributions are identical, S (X|Y = 0) and S (X|Y = 1). Thus, the rank-sum S 0 follows the same distribution of the null-hypothesis in the Mann–Whitney test (<ref type="bibr" target="#b9">Mann and Whitney, 1947</ref>), whose variance has been shown to be n 0 n 1 (n 0 +n 1 +1)/12. Hence, the variance of the estimated AUC is</p><formula>σ 2 est-AUC = n 0 +n 1 +1 12n 0 n 1 = n+1 12p(1−p)n 2 ≥ n+1 3n 2 &gt; 1 3n ,</formula><p>where we use p(1−p) ≤ 1/4. It is well known that the variance of the estimated error in this testing scenario is bounded according to σ 2 est-ERR ≤ 1 4n (<ref type="bibr" target="#b4">Devroye et al., 1996</ref>), which is smaller than 1 3n. The same argument applies to k-fold CV and many other resampling-based error estimation schemes, as long as the data partitioning is stratified so that the testing sample points are represented in the same proportion as the training data for every fold/resampling. For non-trivial distributions, where S (X|Y = 0) = S (X|Y = 1), the distribution of the rank-sum is generally unknown. Moreover, the variance of the true AUC and true error rate, which are functions of sample size and classification rule, are rarely known. Hence, we depend on simulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results for microarray data</head><p>We present two sets of experiments based on real microarray datasets. In the first experiment, we apply a hold-out-based Page: 827 822–830</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Small-sample precision of ROC-related estimates</head><p>prior = 0.5 prior = 0., LDA classifier, LOO. The x-axis is the FPR and the y-axis is the TPR. The curves with cross marks and curves with circle mark correspond to using LOO error estimation and the true error, respectively. The solid lines are the mean ROC curves and the upper and lower dashed lines are the 95% confidence interval. scheme on two existing cancer datasets to compare the true and estimated metrics and confirm the conclusions drawn from artificial data simulations. In the second experiment, we reproduce the experiments in two publications to verify the imprecise estimation observable in ROC plots. In the first set of experiments, we use microarray data from two published sources: breast cancer (van de<ref type="bibr">Vijver et al., 2002</ref>) and lung cancer (<ref type="bibr" target="#b0">Bhattacharjee et al., 2001</ref>) studies. The breast cancer dataset includes 295 patients, 115 belonging to the good-prognosis class and 180 belonging to the poor-prognosis class, with prior probabilities 0.39 and 0.61, respectively. The lung cancer dataset contains 203 tumor samples, 139 being adenocarcinoma and 64 being of some other type of tumor, the prior probabilities being 0.68 and 0.32, respectively. We have reduced the two datasets to a selection of the 2000 genes with highest variance. For each iteration of our procedure, the datasets are divided into a training set and a test set. The training set is formed by 50 examples drawn without replacement from the dataset. The examples not drawn are used as the test set. Note that the training sets are not fully independent. Since they are all drawn from the same dataset, there is an overlap between the training sets; however, for a training set size of 50 out of a pool of 295 or 203, the amount of overlap between the training sets is small. The average size of the overlap is about 8 examples for the breast cancer dataset and 12 examples for the lung cancer dataset. The dependence among the samples is, therefore, not expected to have a large impact on the results (see Braga-Neto and Dougherty, 2004, for a discussion of this issue). We apply on these data the same protocol used for artificial data and detailed in Section 3. The results allow us to compare the true values of the metrics to the estimated metrics.<ref type="figure" target="#fig_8">Figure 5</ref>shows the scatter plots for the breast and lung cancer microarray dataset with the linear SVM. As for the synthetic data, there is very little regression, wide dispersion, and the AUC dispersion is comparable or even greater than that for the classifier error. The RMS of the lung cancer dataset is smaller and there is less variance because the classification is easier (e.g. the error is low and the AUC is high).<ref type="figure" target="#tab_1">Table 1</ref>gives the RMS values and the correlation coefficients, where the latter are very small. In the second set of experiments, we demonstrate the large variance observable in ROC plots in real data cases. We use data in two published studies and repeat the same classification scheme multiple times with the only variance being the randomness in data partitioning. The first dataset is from a study on the prediction of Parkinson's disease based on gene expression from blood samples (<ref type="bibr" target="#b12">Scherzer et al., 2007</ref>). The authors have identified a set of biomarkers, constructed a classifier and validated their results with an ROC analysis of the classifier. The dataset contains 105 subjects, 50 at early stages of Parkinson's disease, 22 healthy and 33 having another brain disease. The classification task is to detect only the Parkinson's disease, so the prior of the problem is 0.52. The authors use an algorithm that selects the best genes based on the Pearson's correlation between their expression level and the class label. Then a template of each class is formed from the mean expression of the selected genes. The classification outcome is determined by the risk score, which is defined as its correlation with the Parkinson's disease template minus its correlation with the non-Parkinson disease template. To evaluate the performance of this classifier, the original dataset is randomly divided into a training set (66 examples) and a test set (39 examples). The training set is used for gene selection and classifier construction, and the test set for evaluation. The authors support the validity of the identified biomakers using the classifier ROC curves. These ROC curves are computed using both the test set and an LOO procedure. In our experiment, we apply the same scheme of gene selection, classification and evaluation as in the original study; however, we run this procedure 100 times to estimate the variance of the results.<ref type="figure">Figure 6</ref>shows the average curve for the 100 ROC curves computed by LOO (solid line with cross marks) and the test set (solid line with circle marks). The dashed lines represent the 95% confidence intervals. These represent a kind of internal confidence bounds relative to the sample. We see that the results of our experiments are more pessimistic than the ones in the original paper. The AUC for LOO is 0.561 with confidence interval<ref type="bibr">[0.354;0.766]</ref>. The AUC for the test set is 0.536 with confidence interval<ref type="bibr">[0.345;0.727]</ref>. Our ROC curves are much more closer to the axis y = x. The confidence intervals are wide and include the axis y = x. In these conditions it is not possible to validate the genes, identified by the methodology, as</p><p>Page: 828 822–830</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.Hanczar et al.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Breast cancer dataset</head><p>Lung cancer dataset Error AUC TPR FPR. The scatter plots of performance measures for breast and lung cancer dataset: linear SVM, 10CV. The x-axis is the estimated performance, whereas the y-axis is the true performance.predictors of Parkinson's disease. The situation is even worse than what is depicted here because the confidence intervals are internal to the sample. This only accounts for resampling variation, not the variance across samples, which would have to be included to obtain the full variance (Braga<ref type="bibr">Dougherty, 2004, 2005a</ref>). The second dataset is from a study of the loss of phosphatase and tensin homolog (PTEN) associated with the presence of solid tumor. The authors develop and validate a microarray gene expression signature for immunohistochemistry (IHC) detectable PTEN loss in breast cancer (<ref type="bibr" target="#b11">Saal et al., 2007</ref>). The data contain 105 examples, 70 being IHC negative and 35 being IHC positive. The genes set is reduced to genes containing &lt;20% of missing values, thereby resulting in 16 039 genes. The best discriminant genes are selected by a Mann–Whitney test and the classifier is constructed from a linear SVM. The performance of the classifier is estimated by 3-fold 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 FPR TPR<ref type="figure">Fig. 6</ref>. ROC curves for the Parkinson's disease dataset. Lines with cross marks are the ROC curves by LOO and lines with circle marks are by the test set. The solid lines represent the average ROC, whereas the dashed lines the 95% confidence intervals. CV with 10 repetitions. The AUC of the final classifier is estimated to be 0.758. We use the same procedure as described in the original study, except the feature size in classifier design, which was not clearly described in the original publication. In our experiment, the final feature size is fixed as 100. We compute the ROC curve and AUC of the obtained classifier. We also estimate the internal variance of the ROC curve and AUC via repetitions of the procedure.axis y = x. This means that, even if the ROC curve is above the axis y = x, we cannot reject the hypothesis that the classifier is meaningless. The AUC of the classifier is estimated to 0.642 and its 95% confidence interval is<ref type="bibr">[0.483; 0.810]</ref>. Note that the AUC of the original published classifier (AUC = 0.758) is included in our confidence interval, as is a random guess (AUC = 0.5). As with the Parkinson's data, the situation is worse because the confidence internal only reflects internal variance from resampling within the sample, not the full variance. These experiments demonstrate that there are insufficient examples in the microarray datasets to draw ROC-based conclusions with acceptable precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Concluding remarks</head><p>This article and several preceding it have shown that even for synthetic data of a simple type, two Gaussian distributions, it is difficult to find good feature sets (<ref type="bibr" target="#b13">Sima and Dougherty, 2006</ref>) and difficult to identify the error rate of a classifier composed of the features that one does find with a small sample. The essential reason is that one cannot make sufficiently good estimates of predictive error at any of the steps required to select features or to characterize the error of the classifier finally developed (Braga<ref type="bibr" target="#b1">Neto and Dougherty, 2004;</ref><ref type="bibr" target="#b6">Hanczar et al., 2007</ref>). Here, we have demonstrated that small sample size leads to large inaccuracies in the estimated validation parameters associated with ROC analysis, even from well-behaved distributions. A previous study (<ref type="bibr" target="#b11">Saal et al., 2007</ref>) applied permutation P tests to the AUC and obtained good P-values. There is no contradiction here because permutation P tests, when applied to classification, are essentially unrelated to classifier performance. Specifically, the P-values have virtually no regression with the error estimates (<ref type="bibr" target="#b8">Hsing et al., 2003</ref>). A procedure for error estimation cannot provide more information than that exists in the distribution of samples with which it is presented: if that distribution is a poor estimate of the actual distribution, then the error estimate will be poor as well. In the case of actual biological samples, it can be seen that the differences between true and estimated error are even larger—considerably larger—than for a similarly small sample of well-distributed synthetic data. ROC curves must be used with extreme caution unless one has a very large sample. In other cases, it would be nice to have some simple rule of thumb to determine if a sample is sufficiently large for the problem at hand; however, since in practice there is only a single sample available, no simple solution is possible. Nonetheless, an experimenter can take some precautions. First, one could use the kind of model-based analysis done in the present article. This would not reflect the actual population but it would allow the kind of confidence analysis demonstrated in<ref type="figure" target="#fig_0">Figure 4</ref>. This could be performed using a model developed for the specific technology being used or, lacking the availability of such a model, a Gaussian model like the one employed herein. It can be expected that the true biological population is less well behaved than the model so that the resulting confidence bounds could be taken as a performance floor for determining a sufficient sample size. A second approach would be to use the internal variance of the AUC if a resampling procedure has been employed, as in the PTEN example (see Appendix A for a description of the internal variance). Again, this would provide a floor because it only provides an estimate of the internal variance, not the full variance of the AUC. Neither method is perfect, but certainly if the 95% confidence interval contains the line y = x in either case, then the sample size is insufficient. If forced to choose between the two approaches, we would choose the model-based approach because often the internal variance is much less than the full variance, so the resampling approach may be more optimistic. If these approaches are used, prudence would dictate utilizing both and making no conclusion unless the lower 95% confidence bound for the AUC exceeds 0.5 for both. Funding: National Science Foundation (CCF-0634794, partially).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest: none declared.</head><p>Page: 830 822–830</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.Hanczar et al.</head><p>van de Vijver,M.J<ref type="bibr">. et al. (2002)</ref>A gene-expression signature as a predictor of survival in breast cancer. N. Engl. J.<ref type="bibr">Med., 347, 1999</ref><ref type="bibr">–2009</ref><ref type="bibr">. Zollanvari,A. et al. (2009</ref>On the sampling distribution of resubstitution and leave-oneout error estimators for linear classifiers. Pattern Recogn., 42, 2705–2723.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A</head><p>The internal variance of a randomized error estimator, such as k-fold CV, is the variance of the estimator given the sample, namely, the variance due only to its internal random factors (Braga-Neto and<ref type="bibr" target="#b1">Dougherty, 2004</ref><ref type="bibr" target="#b2">Dougherty, , 2005a</ref>). It is expressed as Var int = Var(ˆ |S), wherêwherê is a randomized error estimator and S is the sample. This variance is zero for non-randomized error estimators. The full variance, Var(ˆ ), of the error estimator is the one we are really concerned about, since it takes into account the uncertainty introduced by randomly sampling the data from the population. Using the well-known conditional-variance formula,</p><formula>Var(X) = E[Var(X|Y )]+Var(E[X|Y ]),</formula><p>we can break down Var(ˆ ) in the following way:</p><formula>Var(ˆ ) = E[Var int ]+Var(E[ˆ |S]).</formula><p>The second term on the right-hand side is the one that includes the variability due to random sampling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(4)</head><figDesc>Compute the .632 bootstrap estimated error rate by ε .632 = (1−0.632)ε resub +0.632ε 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.1.</head><figDesc>Fig. 1. Deviation distribution of various error estimation schemes for four performance measures: synthetic data, linear uncorrelated model, linear SVM and sample size N = 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.2.</head><figDesc>Fig. 2. The scatter plots of various performance measures at different sample size Ns and priors: synthetic data, linear uncorrelated model, linear SVM and 10CV. The x-axis is the estimated performance, whereas the y-axis is the true performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.3.</head><figDesc>Fig. 3. The precision of four performance measures, under different sample size Ns and priors: synthetic data, linear uncorrelated model, linear SVM and 10CV. The x-axis is the p 0 of the distribution and the y-axis is the precision based on either RMS (top row) or correlation (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig.4.</head><figDesc>Fig. 4. Comparison of ROC plots: non-linear uncorrelated model, LDA classifier, LOO. The x-axis is the FPR and the y-axis is the TPR. The curves with cross marks and curves with circle mark correspond to using LOO error estimation and the true error, respectively. The solid lines are the mean ROC curves and the upper and lower dashed lines are the 95% confidence interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig.5</head><figDesc>Fig. 5. The scatter plots of performance measures for breast and lung cancer dataset: linear SVM, 10CV. The x-axis is the estimated performance, whereas the y-axis is the true performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig.7.</head><figDesc>Fig. 7. ROC curve for the PTEN dataset. The solid line is the ROC curve by 3-fold CV with 10 repetitions and the dashed lines represent the 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>0 ) for the positive class and N(A,σ 1 )</figDesc><table>experiments on synthetic data generated to 
reflect key properties of microarray data: small number of examples, high 
dimension and large number of irrelevant features. We construct two types of 
features: relevant and irrelevant. Relevant features follow different normal 
distributions for each class: N(0,σ </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>1) Based on training data S tr , build the classifier Str. Apply Str back to training data S tr. For each example compute class</figDesc><table>prediction and classifier output. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>Table 1.</figDesc><table>The RMS and correlation of performance measures of breast and 
lung cancer datasets: linear SVM and 10CV 

Error 
AUC 
TPR 
FPR 

Breast cancer 
RMS 
0.089 
0.124 
0.149 
0.102 
Correlation 
0.070 
0.114 
0.421 
0.369 
Lung cancer 
RMS 
0.065 
0.064 
0.063 
0.153 
Correlation 
0.161 
0.197 
0.136 
0.226 

</table></figure>

			<note place="foot">© The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oxfordjournals.org at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Classification of human lung carcinomas by mRNA expression profiling reveals distinct adenocarcinoma subclasses</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Bhattacharjee</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA, 98</title>
		<meeting>. Natl Acad. Sci. USA, 98</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="13790" to="13795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Is cross-validation valid for small-sample microarray classification</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<forename type="middle">M</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="374" to="380" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Classification</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<forename type="middle">M</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Genomic Signal Processing and Statistics, EURASIP Book Series on Signal Processing and Communication</title>
		<editor>Dougherty,E.R. et al.</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Hindawi Publishing Corporation</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Exact performance of error estimators for discrete classifiers</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<forename type="middle">M</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1799" to="1814" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<monogr>
		<title level="m" type="main">A Probabilistic Theory of Pattern Recognition</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Devroye</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Additive estimators for probabilities of correct classification</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Glick</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="211" to="222" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Decorrelation of the true and estimated classifier errors in high-dimensional settings</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Hanczar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Bioinform. Syst. Biol</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Article. ID 38473</note>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple generalisation of the area under the ROC curve for multiple class classification problems</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">J</forename>
				<surname>Hand</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">J</forename>
				<surname>Till</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="171" to="186" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Relation between permutation-test p values and classifier error estimates</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hsing</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="11" to="30" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">On a test of whether one of two random variables is stochastically larger than the other</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">B</forename>
				<surname>Mann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">R</forename>
				<surname>Whitney</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Stat</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="50" to="60" />
			<date type="published" when="1947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Limitations of the odds ratio in gauging the performance of a diagnostic, prognostic, or screening marker</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">S</forename>
				<surname>Pepe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am J. Epidemiol</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="882" to="890" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Poor prognosis in carcinoma is associated with a gene expression signature of aberrant PTEN tumor suppressor pathway activity</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">H</forename>
				<surname>Saal</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7564" to="7569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Molecular markers of early parkinson&apos;s disease based on gene expression in blood</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">R</forename>
				<surname>Scherzer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="955" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">What should be expected from feature selection in small-sample settings</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Sima</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2430" to="2436" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Signal detection theory: Valuable tools for evaluating inductive learning</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">A</forename>
				<surname>Spackman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Workshop on Machine Learning</title>
		<editor>San Mateo,C.M.K.</editor>
		<meeting><address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>