
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining Click-words: learning to predict document keywords from a user perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010">. 21 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Rezarta</forename>
								<surname>Islamaj</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">National Center for Biotechnology Information</orgName>
								<orgName type="institution" key="instit1">National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">National Institutes of Health</orgName>
								<address>
									<postCode>20894</postCode>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Do</forename>
								<forename type="middle">˘</forename>
								<surname>Gan</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">National Center for Biotechnology Information</orgName>
								<orgName type="institution" key="instit1">National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">National Institutes of Health</orgName>
								<address>
									<postCode>20894</postCode>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Zhiyong</forename>
								<surname>Lu</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">National Center for Biotechnology Information</orgName>
								<orgName type="institution" key="instit1">National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">National Institutes of Health</orgName>
								<address>
									<postCode>20894</postCode>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining Click-words: learning to predict document keywords from a user perspective</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">26</biblScope>
							<biblScope unit="page" from="2767" to="2775"/>
							<date type="published" when="2010">. 21 2010</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btq459</idno>
					<note type="submission">Received on April 12, 2010; revised on July 27, 2010; accepted on August 8, 2010</note>
					<note>[09:31 12/10/2010 Bioinformatics-btq459.tex] Page: 2767 2767–2775 Associate Editor: Jonathan Wren Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Recognizing words that are key to a document is important for ranking relevant scientific documents. Traditionally, important words in a document are either nominated subjectively by authors and indexers or selected objectively by some statistical measures. As an alternative, we propose to use documents&apos; words popularity in user queries to identify click-words, a set of prominent words from the users&apos; perspective. Although they often overlap, click-words differ significantly from other document keywords. Results: We developed a machine learning approach to learn the unique characteristics of click-words. Each word was represented by a set of features that included different types of information, such as semantic type, part of speech tag, term frequency–inverse document frequency (TF–IDF) weight and location in the abstract. We identified the most important features and evaluated our model using 6 months of PubMed click-through logs. Our results suggest that, in addition to carrying high TF–IDF weight, click-words tend to be biomedical entities, to exist in article titles, and to occur repeatedly in article abstracts. Given the abstract and title of a document, we are able to accurately predict the words likely to appear in user queries that lead to document clicks. Contact:</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Finding relevant publications is critical for individual researchers to keep pace with the state of the art in their fields. Most scholars in the biomedical domain use PubMed , a free web service provided by the US National Center for Biotechnology Information (NCBI), to access over 20 million biomedical citations. Improving the retrieval effectiveness of PubMed while accommodating the exponential growth of biomedical literature is a challenging and critical task for the NCBI, as well as for the researchers in the field of biomedical information retrieval (<ref type="bibr" target="#b7">Hersh, 2003;</ref><ref type="bibr" target="#b18">Lu et al., 2009;</ref><ref type="bibr" target="#b26">Tsai et al., 2009;</ref><ref type="bibr" target="#b27">Tsuruoka et al., 2008;</ref><ref type="bibr" target="#b31">Zhu et al., 2009</ref>). Interaction with PubMed is generally initiated by a user query and that contains three words on average. An intelligent system should be able to use this information efficiently to retrieve the citation(s), * To whom correspondence should be addressed. which the user is looking for. The user may click to view one or more abstracts, modify the original query, issue a different query or leave the system (<ref type="bibr" target="#b10">Islamaj Do˘ gan et al., 2009</ref>). This sequence of interactions with PubMed is similar to what users experience with general web search engines. However, searching the biomedical literature in PubMed is also unique from at least two perspectives: (i) PubMed search is built as a Boolean system—by default, only the documents matching all the words in the query are retrieved; and (ii) PubMed results are listed in reverse chronological order. As a result, the top returned documents are the newly indexed citations but not necessarily the most relevant ones. On the other hand, 80% of retrievals are for the top 20 returned results in PubMed (<ref type="bibr" target="#b10">Islamaj Do˘ gan et al., 2009</ref>). The search in PubMed, therefore, is much more sensitive to the user selection of query terms than in search systems that weigh and rank results by relevance. Although a PubMed article can be retrieved by various user queries, only certain queries lead to user clicks (retrievals) for that article—a strong indication of relevance between the query and the clicked document. In this work, we identify document keywords from a list of PubMed queries that resulted in user clicks, and we name such keywords as click-words. In document retrieval, a keyword refers to a term that captures the essence of the topic of a document. They are integral to the document management both for indexing and for retrieval. One type of keywords used in MEDLINE citations is known as Medical Subject Heading® (MeSH) indexing terms, which are assigned by professional indexers. Another common type of keywords is author keywords, provided by authors when submitting an article. A third type of keywords may be computed using statistics, instead of relying on human annotation. For example, the classic term frequency–inverse document frequency (TF–IDF) weighting schema can be used to identify highly weighted words that stand out in an article when compared to the other articles in the collection (<ref type="bibr" target="#b21">Salton and Buckley, 1988</ref>). Comparing click-words with other document keywords we found that, although there was overlap, user click-words were quite different from other types of important keywords (see<ref type="figure">Fig. 1</ref>for an illustration). Document keywords are all meant to capture the important contributions of a document, but they rely on different weighting mechanisms, which may be the reason for their difference. Click-words are the product of click-through logs and they represent the 'wisdom of the crowds' as to what terms in an article may be important from the users' perspective. Top weighted TF–IDF words capture the importance of words with respect to other articles in a collection. In contrast, PubMed relies on indexers to assign<ref type="figure">1</ref>. An example of click-words, top-scoring TF–IDF words, author keywords and MeSH indexing terms for a PubMed article. User click-words are listed by the frequency in which they appear in user queries for this article. TF–IDF words are listed in decreasing order of their TF–IDF weight. MeSH terms follow the order in which they are listed in PubMed. Author keywords are listed as they appear in the article. the appropriate MeSH indexing terms to PubMed articles. As a result, these words are not immediately available for new articles. Moreover, they are not necessarily found in the title and abstract of the article. Author keywords, on the other hand, are not included in the MEDLINE citation. In addition, they are not easily procured— we found that they are available for only 13% of the articles in the PubMed Central full text database. As an illustrative example, we randomly selected an article from MEDLINE (PubMed ID: 18309290) and identified the document keywords.<ref type="figure">Figure 1</ref>illustrates the relationship between the click-words and other keyword types for this article. It also shows that the top five TF–IDF words list has the largest overlap with the click-words list: three of the four click-words of this article: arp2, actin and cofilin, were also ranked in the top five TF–IDF words of this article. In contrast, the other two words of the top five TF–IDF list: lamellipodium and complex, were not popular in user queries. Our objective is to learn what characteristics make document words important from a collective user perspective—words used as click-words. For each word, we consider several characteristics including the word itself, its part of speech (POS), its position in the title/abstract, etc. By learning the importance of each characteristic, we aim to build a learning system that will be able to predict which words are likely to become click-words for a given article. Thus, no search history would be required to automatically predict click-words in any documents, including those that lack user search history (e.g. new PubMed citations). The predicted click-words can not only serve as an alternative type of document keywords for indexing, but they can also provide assistance in curating MeSH terms or suggesting author keywords. Moreover, as briefly discussed in Section 6, predicted click-words could play important roles in many PubMed-related applications such as ranking relevant results and finding similar documents. Since top-weighted TF–IDF words provided the largest overlap with the user click-words in the documents in our data, we built a machine learning model with novel features to recognize click-words from a pool of candidate words with high TF–IDF weights. For example, take the five TF–IDF words in<ref type="figure">Figure 1</ref>. In our proposed approach, we aim to identify prediction scores such that the three click-words (arp2, cortactin and actin) are ranked higher than the other two words (lamellipodium and complex). The specific contributions of this article are the following: @BULLET We introduce click-words as representative document keywords. We observed that click-words (obtained from click-through logs) can represent the meaning of scientific text from a collective user perspective. Therefore, they are complementary to the current types of document keywords such as MeSH, and they may serve as an alternative type of keywords for document management.</p><p>@BULLET We developed and evaluated a machine learning method with novel features for identifying the unique characteristics of click-words. The proposed approach is capable of automatically finding click-words in any document, regardless of its search history.</p><p>@BULLET We performed empirical studies on large-scale real-world datasets (over 50 000 frequently accessed MEDLINE articles over the course of 6 months). Evaluation results show that the classifier is able to successfully perform automatic assignment of high-quality click-words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Automatically identifying click-words from a pool of candidates (e.g. top-scoring TF–IDF words) is closely related to the problem of searching for words that are key to the meaning of a document (also known as keyword extraction). Much research has been devoted to this problem in the field of Natural Language Processing because of its importance in categorizing and summarizing documents, and making efficient retrievals (<ref type="bibr" target="#b19">Manning et al., 2008</ref>). In the web context, keyword identification is important for content-targeted advertizing (<ref type="bibr" target="#b13">Lacerda et al., 2006;</ref><ref type="bibr" target="#b29">Yih et al., 2006</ref>), sponsored search (<ref type="bibr" target="#b2">Ciaramita et al., 2008;</ref><ref type="bibr" target="#b5">Fuxman et al., 2008</ref>) and search retrieval performance (<ref type="bibr" target="#b6">Hawking et al., 2006;</ref><ref type="bibr" target="#b11">Ji et al., 2009</ref>). A classic algorithm for weighing words in an article is the TF–IDF measure. Other unsupervised techniques with comparable performance to this measure include Matsuo and Ishizuka (2003) and<ref type="bibr" target="#b15">Liu et al. (2009)</ref>. The former proposed to build and use co-occurrence distributional characteristic of a word to evaluate its importance. The latter demonstrated that additional information from POS and sentence salience score can improve the classic TF–IDF approach. Alternatively, supervised methods have been used to classify keywords versus non-keywords. Commonly used features include those that are able to characterize context such as term frequency, term position and TF–IDF score. In addition, better classification results were achieved when advanced features such as linguistic knowledge (<ref type="bibr" target="#b8">Hulth, 2003</ref>) or graph-based features (<ref type="bibr" target="#b14">Litvak and Last, 2008</ref>) were added. A similar set of features was also used by a learning-to-rank method in a more recent study (<ref type="bibr" target="#b12">Jiang et al., 2009</ref>). On the other hand,<ref type="bibr" target="#b29">Yih et al. (2006)</ref>observed that keyword frequency in query log files substantially helped the linguistic features and information retrieval-based features in identifying content-based keywords of web pages. In their study, they relied on human annotations of advertizing keywords. Thus, they had only Page: 2769 2767–2775</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Click-words: learning to predict document keywords</head><p>a small number of annotated documents (30) with modest annotator agreement. In comparison, we use document-specific keyword frequency to label positive and negative instances of click-words. As a result, not only do we use this important information implicitly, but we are also able to obtain a sufficient number of training and test data for building and evaluating our classification algorithm. Click-through logs seem the perfect source of information when deciding which documents to show in response to a query. It can be thought of as the result of users voting in favor of the document that they found interesting. This source has been exploited for purposes of gaining insight into users browsing behavior (<ref type="bibr" target="#b3">Dupret and Piwowarski, 2008;</ref><ref type="bibr" target="#b10">Islamaj Do˘ gan et al., 2009</ref>), of improving the ranking of search results (<ref type="bibr" target="#b2">Ciaramita et al., 2008;</ref><ref type="bibr" target="#b11">Ji et al., 2009</ref>), of studying user queries to mine query relationships (<ref type="bibr" target="#b22">Shen et al., 2007</ref>) or of identifying queries related to an advertizing campaign in order to display ads alongside search results (<ref type="bibr" target="#b5">Fuxman et al., 2008</ref>). Studies mentioned above mostly dealt with processing web pages and news articles, perhaps due to the lack of large-scale datasets with scientific publications in the general domain. Regarding applications in the biomedical domain, Andrade and Valencia (1998) automatically extracted keywords from biomedical literature by their relative accumulation in comparison with a domain-specific background distribution.<ref type="bibr">Liu et al. (2004a, b</ref>) reported studies using extracted keywords from MEDLINE citations to describe the most prominent functions of the genes and the resulting weights of the keywords as feature vectors for gene clustering. In their work, they compared two keyword-weighing schemes: normalized z-score and TF–IDF.<ref type="bibr" target="#b25">Tudor et al. (2008)</ref>proposed another ranking algorithm for identifying keywords relating to a gene. In their work, the produced keywords were not limited to the gene functional terms. In our approach, we used machine learning to identify document keywords, which would likely be used frequently in user queries and become click-words for the given document. In addition to the previously examined features—details of feature contributions in Section 3.3—we included novel features such as 'named entity', which have not been explored before in this context. We used MetaMap (<ref type="bibr" target="#b1">Aronson, 2001</ref>), as the biomedical concept recognizer to identify biomedical entities in scientific text and included the recognized semantic types as classification features. As reported in Section 4, this feature showed a relatively strong discriminative power in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head><p>In this section, we describe how we compute the click-words in the context of highly accessed articles, we describe how we create our training and evaluation datasets from the information in the query logs data. Next, we describe how we build the click-word model, the features used for characterizing the click-words, the learning algorithm and the evaluation measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Computation of click-words and top-scoring TF–IDF words</head><p>Click-words were produced in two steps. First, we identified highly accessed articles from a large pool of query logs: articles whose citations have been clicked on, on average, at least once per day by different users for a given period of time (e.g. 2 months). When we collected data for our experiments, a user query followed by a click constituted an association from the query to the clicked article, regardless of the article's rank in the results page (because PubMed does not return results by relevance). Next, we computed click-words of a given article: popular words that appeared in at least 10% of the user queries that produced clicks for that article during the same period of time. The salience of query terms was not a factor for consideration because each term is treated equally in Boolean searches. Note that our definition of frequently accessed articles and click-words are empirically determined. However, based on our analysis, varying the selection criteria (e.g. relaxing from 10% to 5% in click-words selection) does not affect our overall observations. Because we are interested in words that represent a document's content rather than its bibliographic information, we discarded any query words that did not appear in the title or abstract (e.g. author names). Using this procedure, some documents did not produce any click-words. These documents were subsequently discarded. This procedure resulted in 4.5 click-words on average per article. We computed the TF–IDF weight for each of the words in the title and abstract of an article and ranked them according to their weight. (See Supplementary Material for the TF–IDF definition). Next, for each document in our dataset, we chose the top five TF–IDF-weighted words. We decided to pick the top five operational TF–IDF-weighted words for each document because this is consistent with the number of click-words per document (4.5). Note that in order to accurately capture the differences in words of various morphological classes, we preserved their original forms in the document and did not perform any stemming. In fact, PubMed does not employ any stemming algorithm (e.g. porter stemming algorithm). Instead, it relies on a newly introduced feature, which adds related terms (not necessarily always word stems) through a sophisticated mapping process (http://www .nlm.nih.gov/pubs/techbull/mj08/mj08_pubmed_atm_cite_sensor.html). As a result, word stems are very inconsistently recruited into original PubMed search, which makes it practically difficult to align results with any existing stemming algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Datasets</head><p>Page: 2770 2767–2775</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R.Islamaj Do ˘ gan and Z.Lu</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The click-word learning method</head><p>Our aim is to build a learning system that, given the title and abstract of a MEDLINE article, will be able to predict which words are likely to be used frequently in user queries and become click-words for this article. To build such a learning system, we needed to specify positive and negative instances of click-words in articles. Thus, for each article in our datasets (both training and evaluation), first, we selected the five top-scoring TF–IDF words, and then we labeled the pre-identified click-words positive and the rest negative. Accordingly, for the sample article in<ref type="figure">Figure 1</ref>, we labeled the words: arp2, cofilin and actin as positive, and the words lamellipodium and complex as negative. Once we labeled the top five TF–IDF word of each article, we computed the values for each of the designed click-word features. We applied a classification algorithm in a 5-fold cross-validation setting to learn a clickword classification model. This schema was fitted into an iterative feature selection algorithm to improve the classification results and identify the most important features for the final click-word model. We discuss these steps in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Click-word features</head><p>To deduce context and be able to use supervised machine learning methods to predict click-words, we represented each word instance by a set of features, determined by both our own studies and prior observations in the literature. These binary features are: @BULLET TF–IDF rank (TF–IDF): TF–IDF rank of word w denotes the rank position of a word w according to its weighted TF–IDF value. Because we only consider five words per article, we have five different TF–IDF rank features. @BULLET Part of speech (POS): The POS tag of word w denotes whether word w is a noun, verb, adjective or a different POS. The POS tags were computed using MedPost (<ref type="bibr" target="#b24">Smith et al. 2004</ref>) with the default settings. A total of 37 POS tags were obtained in our data and each was subsequently used as a binary feature in the classification task.</p><p>@BULLET Semantic type (SEM): The semantic type of word w denotes whether word w corresponds to a particular named entity in biomedicine. According to our previous study (<ref type="bibr" target="#b10">Islamaj Do˘ gan et al., 2009</ref>), the majority of the PubMed queries do not include any bibliographic information. Instead, they contain named entities such as gene and protein names. The semantic type feature is designed to capture this aspect of the click-words. Specifically, if a word can be mapped to a biological concept, its semantic type (category) information is used. We applied MetaMap (<ref type="bibr" target="#b1">Aronson, 2001</ref>) to our data and obtained 134 semantic types (e.g. molecular function—mofc), each of which is used as a binary feature.</p><p>@BULLET Word frequency rank (WFR): The frequency rank of word w denotes the rank position of word w according to its frequency value. This feature is similar to the local term frequency in the TF–IDF definition. However, instead of using raw term frequency, we chose to use rankings based on word frequency because word frequency can vary significantly in different articles for the most frequently occurring words. Specifically, after removing the stop words, we ranked each word by its frequency in the article. We assigned the same rank to words with equal number of occurrences. The resulting rank of the words in our training dataset ranged from 1 to 46, each of which was then used as a binary feature.</p><p>@BULLET Word location (LOC): the word location of word w denotes some specifics regarding whether word w appears in the title and/or abstract of the article. We created binary features to capture a given word's importance using four different locations: title, the first sentence, the last sentence or the middle of the abstract. When a word appeared in multiple locations, each individual position was marked, creating a combined location feature. We had 15 different binary features denoting the various combinations of word locations. @BULLET Abbreviation (ABBR): the abbreviation feature of word w denotes whether word w is an abbreviated form of a known concept. It has been shown that important biological concepts like gene names are often abbreviated in user queries (<ref type="bibr" target="#b4">Federiuk, 1999;</ref><ref type="bibr" target="#b10">Islamaj Do˘ gan et al., 2009</ref>). In fact, our previous analysis revealed that 13% of PubMed user queries contained at least one abbreviated term. Previous studies on PubMed describe highly precise methods that identify abbreviated terms in PubMed abstracts (<ref type="bibr" target="#b23">Sohn et al., 2008</ref>). We used their list of abbreviated terms in PubMed to build our ABBR feature. The value of this feature is 'is-an-abbreviation' if the word instance in our dataset is matched exactly to a term from that abbreviation list or 'is-not-an-abbreviation' otherwise.</p><p>@BULLET Phrase (PHR): the phrase feature of word w denotes whether word w is part of a common MEDLINE phrase. As shown in a previous study (<ref type="bibr" target="#b28">Yeganova et al., 2009</ref>), words in PubMed queries tend to associate in phrases. Hence, we hypothesized that given a PubMed abstract, a word having the tendency of appearing as a phrase with its neighboring words may have a higher probability of catching the readers' attention and being a click-word. For this reason, we formed MEDLINE phrase candidates of our dataset by combining the word of interest with one, two or three of its neighboring words. If such a phrase candidate was repeated somewhere else in the same article that it originated from, and was also found in at least one other article in our dataset, we considered it a valid phrase. After identifying all the valid phrases in our training dataset, we could readily determine whether or not a word is part of a phrase. Again, we considered both the confirmation 'is-part-ofphrase' and the negation 'is-not-part-of-phrase' features. For example, the document PMID:17850624 contains the word 'migration', which is part of 'neuronal migration', a valid phrase with multiple occurrences in 17850624 and other PubMed citations (e.g. PMID: 18075253). Thus, for the word 'migration', the value of its PHR feature 'is-part-of-phrase' is TRUE.</p><p>@BULLET Neighboring words (NBR): the neighbor of word w denotes the word(s) that w has as neighbors and its (their) relative position(s) to w. We considered up to the three words on each side of the word of interest, when available, as its neighbors. This was the most sparse feature type in our data. It resulted in more than 261 974 different individual binary features (i.e. unique words found in any of the considered six positions). @BULLET Word (WRD): the final feature used was the word instance itself. There were 41 152 unique words in our training dataset, which served as binary features. Of these, 17 662 (43%) words were repeated in at least two different documents. Of the repeated words, 27% were labeled as click-words for some articles but not for other articles even though they were listed in their top five TF–IDF weight lists. This suggested that we could not rely on only the word instances themselves as features.</p><p>In addition, using only word instances would have limited our ability to predict click-words that were not seen in our dataset. However, if we represent a click-word in the context of all the features that we defined above, our proposed model can make a prediction based on the learned click-word characteristics (e.g. a word's semantic type) in addition to the word itself, making it much more robust when handling new/unseen words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Classification algorithm</head><p>For our experiments, we considered a range of classifiers. We selected a wide margin classifier similar to a linear support vector machine as our principal learner, which was based on the modified Huber loss function (<ref type="bibr" target="#b30">Zhang, 2004</ref>). The Huber loss function is smooth, hence differentiable; therefore, it allows the application of a gradient search method for optimization. These properties, as well as the speed of optimization, make this algorithm suitable for large datasets, such as the one in this work. In addition, in the preliminary experiments, we also used the multivariate Bernoulli Naïve Bayesian classifier for the purpose of comparing different learning algorithms. The results presented in this article were obtained using the Huber algorithm, which produced better results when compared with the Naïve Bayes classifier.The set of all PubMed articles in our training dataset was randomly divided into five subsets and a 5-fold cross-validation was performed. At each round of cross-validation, the classifier was trained with four-fifths of the data, and tested on the one-fifth of the held-out data. The number of articles and the number of positive and negative class instances for each fold was balanced (see Supplementary Material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 2771 2767–2775</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Click-words: learning to predict document keywords</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Iterative feature selection method</head><p>Feature selection is usually employed to filter out redundant and/or irrelevant features, while maintaining or improving the accuracy of prediction, thus resulting in a better and simpler model. For this study, we used an iterative feature selection scheme to tackle the 290 000 features that we constructed to describe the click-words (<ref type="figure" target="#fig_1">Fig. 2</ref>). As illustrated in<ref type="figure" target="#fig_1">Figure 2</ref>, this iterative feature selection method first used the wide margin classifier (Huber algorithm) to learn a classification model for the training dataset, in a 5-fold cross-validation setting. Next, for each individual feature it examined the average weight assigned by the five wide-margin classifiers to judge whether the current feature is valuable for this problem. Usually, a non-contributing feature receives a weight close to zero. In contrast, a contributing feature is assigned a relatively high weight. We applied an aggressive feature selection procedure. After examining the features' weights, it eliminated 1000 lowest-weight features. Then, in an iterative fashion, we retrained the click-word model on the refined set of features using the Huber algorithm. We performed feature selection iterations until we eliminated all features. To select the best click-word model, we examined the performance levels of all feature subsets and selected the most significant one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation measures</head><p>Precision, or the positive predictive value, is calculated as the ratio of the number of correct answers to the number of all the answers given by the classification algorithm. Recall, or sensitivity, is calculated as the ratio of number of correct answers to the total number of possible correct answers (<ref type="bibr" target="#b7">Hersh, 2003</ref>). The identification of click-words, as mentioned earlier, is article dependent: a given word may act as a click-word for one article but not for another. For this reason, we evaluated our ability of identifying click-words per each article. After applying the learning model, we scored each word instance and ranked all of the word instances of each article in descending order of their scores. Instead of relying on a global score threshold for the whole set of articles, we computed the break-even point between precision and recall for each article in the test set (during the cross-validation procedure or the evaluation stage). The break-even point is calculated as the value where precision equals recall. We also computed the mean average precision, the area under the ROC curve (ROC score) and the precision at the first retrieved level (precision for the highest-scoring word of each article which received the highest confidence of being a click-word for that article). In the event that for a given article, two or more word instances received the same click-word score, we considered all of the possible rankings and computed the breakeven point of precision–recall and mean average precision averaging over all enumerations. Finally, the value of the corresponding measure was averaged over all articles in the test set. For the training dataset experiments, we report this value averaged over the five folds of cross-validation. Next, using the evaluation dataset, we performed two different evaluations. First, we applied the model to the top five TF–IDF words list of each article in the evaluation dataset. Second, in order to test the performance of our model in a more realistic setting, we applied the model to every single word in the title and abstract of each article in the evaluation dataset. In addition, as a comparison, we ranked the words of each article according to two different baselines. First, in the random selection baseline we assigned every word of an article the same score. Next, we computed the evaluation measures by considering all possible word rankings, and averaged the scores. Second, in the TF–IDF weight baseline we ranked the words of each article according to their TF–IDF weight values, and computed the evaluation measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>We conducted a wide range of experiments to identify user click-words from PubMed abstracts, and here we present a summary of them. We begin the presentation of results by describing the click-word prediction results for each individual feature type. This analysis allowed us to understand each of our feature types, and their individual contributions. Following this analysis, we combined all features and learned a click-word model consisting of a mix of all features. Next, we performed our iterative feature selection method. We show how we reduced the number of features and the corresponding break-even precision recall point average for each feature selection step. Finally, we show the results of our selected click-word prediction model when applied to evaluation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Click-word prediction results using single feature types</head><p>In the following experiments, we describe the click-word prediction performance for each individual feature type.<ref type="figure" target="#tab_1">Table 1</ref>summarizes the break-even results of precision–recall values averaged over five folds of cross-validation when using each feature type individually. In addition, it lists the number of contributing features for each individual feature type. As shown in<ref type="figure" target="#tab_1">Table 1</ref>, the click-word model trained on the Word features gave the best result, while the clickword model trained on the Abbreviation features gave the worst result. For comparison, we use two baseline methods: the random selection baseline and the TF–IDF weight baseline. The random selection baseline uniformly picks at random words from the article. The TF–IDF weight baseline assigns the TF–IDF weight as the score for each word in the article, and then ranks them accordingly. As shown in<ref type="figure" target="#tab_1">Table 1</ref>, three feature types: word location, neighboring words and word, gave (statistically significant) better classification results than the TF–IDF baseline. Each of the click-word models trained on individual feature types gave significantly better results than the random selection baseline. Finally, in the last row of the<ref type="figure" target="#tab_1">Table 1</ref>we list the result of the click-word model when we have combined all features. This result is statistically significant when compared to each of the individual models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R.Islamaj Do ˘ gan and Z.Lu</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature selection method</head><p>In the following set of experiments, we show the results of our iterative feature selection algorithm. After we combined all features together, we had a total of more than 290 000 features and, at that stage, the click-word model had a break-even point performance of 0.781. This is shown in the last row of<ref type="figure" target="#tab_1">Table 1</ref>. As illustrated in<ref type="figure" target="#fig_2">Figure 3</ref>, we used aggressive feature selection steps, removing 1000 features with each iteration. For each refined feature set, we retrained the Huber algorithm, and computed the break-even point of precision and recall for the resulting click-word model.<ref type="figure" target="#fig_2">Figure 3</ref>summarizes each break-even value, averaged over the five folds of the cross-validation test sets. We observe this value ultimately increases, despite slight fluctuations. We selected the click-word model that consisted of just 2000 features as our final model, because at that point, we reached the best performance of break-even point average, 0.794. This constituted a significantly simpler model when compared with the initial model of more than 290 000 features. Moreover, it exhibited a significantly better performance when compared with the initial model of 0.781. Its performance was also better when compared with the next model of 1000 features, represented as the last dot in the graph in<ref type="figure" target="#fig_2">Figure 3</ref>. Compared with the random baseline performance of 0.429, and with the TF–IDF weight performance of 0.613, the performance of the best click-word model represents an 85% and 30% increase, respectively. More details regarding the performance of the best click-word model are listed in<ref type="figure" target="#tab_4">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Feature analysis of the final click-word prediction model</head><p>In the following analysis, we considered the feature composition of the final click-word model, summarized in<ref type="figure" target="#tab_2">Table 2</ref>, and the relative predictive strength of each feature according to the weight assigned by the Huber algorithm, as illustrated in<ref type="figure" target="#tab_3">Table 3</ref>. We observe that, even though Abbreviation was the least predictive feature type among the other individual models (<ref type="figure" target="#tab_1">Table 1</ref>), the 'isan-abbreviation' and 'is-not-an-abbreviation' features were part of the final click-word model. The same is true for the 'in-phrase' and 'not-in-phrase' features. This means that even though they did not have enough predictive strength on their own, they worked well in combination with the rest of the features to have an impact in the final performance. In fact, the feature 'not-in-phrase' is listed in<ref type="figure" target="#tab_3">Table 3</ref>as one of the features having the highest negative weight. In<ref type="figure" target="#tab_3">Table 3</ref>, we also observe that the feature with the most positive weight was the feature that describes the location of the word both in the title and in the first sentence of the abstract. In contrast, the feature that described the word location in the middle of the abstract was assigned a negative weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Click-word prediction results using the evaluation dataset</head><p>To further evaluate the performance of our click-word prediction model, we applied this model to the documents in our evaluation dataset. To apply the click-word model to new articles, we followed this sequence of steps: First, we extracted its title and abstract. Second, we tokenized these into single words and removed the stop words. Third, for each single word we computed the values for each of the 2000 features in the final click-word model. For each article, each word was scored according to these values. Next, we ranked the words of each article according to the score of the click-word model. As a comparison, we also ranked the words of each article according to the random selection baseline and the TF–IDF weight baseline. Based on these rankings, we computed the mean average precision, break-even point of precision and recall, ROC score and precision at the first recall value. These results are summarized in Tables 4 and 5. The training and evaluation datasets results are contrasted in<ref type="figure" target="#tab_4">Table 4</ref>. For these results, we considered only the top five weighted TF–IDF words for each article in the evaluation dataset, as this is the equivalent to the results of the training dataset. Although our model was trained on the top five TF–IDF terms, we also tested it for every word present in the titles and abstracts of the articles in the evaluation dataset. These results are presented in<ref type="figure" target="#tab_5">Table 5</ref>. As shown in the<ref type="figure" target="#tab_5">Table 5</ref>, when we used the random selection baseline, the mean average precision of picking the user click-words was 11%. When we used the TF–IDF weight as a baseline, the mean average precision of picking the user click-words was 51%, and</p><p>Page: 2773 2767–2775when we used our click-word model to score the words, the mean average precision of picking the user click-words increased to 61%. In addition, in<ref type="figure" target="#tab_5">Table 5</ref>we list t-test results of comparing the click-word model with the TF–IDF baseline, and the corresponding P-values. Each evaluation measure is highly statistically significant. We conclude that our click-word model is very accurate at identifying the user click-words for any given article. Moreover, our click-word model is significantly more accurate at recognizing the user interest in a given article when compared with the TF–IDF weighting model, evident from every evaluation measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Click-words: learning to predict document keywords</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>In this work, we proposed click-words as an alternative representation of document keywords. We built a click-word model able to identify the words of a document that conveyed the readers' interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Click-word characteristics</head><p>A click-word model based only on words would not be useful for several reasons. First, new words in new articles would be a problem. Second, the words selected as click-words for certain articles but not for other articles would create confusion. Finally, it would not be possible to apply a model based solely on the words to rank the articles matching a given query. Such a model needs context. In this work, the click-word context was provided by the rich set of features that we designed and implemented. To understand the contributions of each feature type, we examined the feature weights learned with the Huber algorithm for each individual feature of the click-word model after feature selection 1. We found that a word was more likely to be a click-word if:</p><p>@BULLET It appeared in the title of the article. All 15 word location features were retained in the final best prediction model. They were divided into two distinct groups: 8 positively weighted features described words that appeared in the title, among other positions, while the 7 negatively weighted features described words that appeared in locations other than the title. This may also be related to the fact that in the PubMed search results pages, no parts of the abstracts are displayed to the users. @BULLET It was repeated several times in the abstract of the article. During feature selection, most of the initial 46 binary WFR features were removed. The final three features that were retained assign positive weights to the most frequent and second most frequent words in an abstract, and assign a negative weight to the less frequent words. @BULLET It was ranked first according to its TF–IDF weight. All five TF–IDF Rank features were retained in the final model. Interestingly, their weights were in accordance with their ranks. The feature for the top ranking in TF–IDF value (Rank 1) had the highest positive weight, whereas the one for the last rank (Rank 5) had the largest negative weight. The top 2 ranks were assigned positive weights.@BULLET It was found as part of a phrase. In addition, when a word was not found as part of a phrase, this increased its probability of not being a click-word (see<ref type="figure" target="#tab_3">Table 3</ref>). @BULLET It had certain word neighbors such as: background, syndrome, diagnosis, receptor, infection or cells. Neighboring words account for the largest number of final features in the final feature set. Although their number is significantly reduced, there are 745 positively and 577 negatively weighted neighboring word features in the final model. The positively weighted neighboring words tend to be general words providing context for specific content words. For instance, the word preceding syndrome is likely to be a click-word because it is typically a specific disorder name. @BULLET It belonged to one of the following semantic types: virus, neoplastic process or disease or syndrome, rather than research</p><p>Page: 2774 2767–2775@BULLET It was a word such as cancer, stem, mirna, diabetes, rather than proteins, genes, diseases, therapies and trials. This set of 556 words includes 308 words with positive weighs and 248 with negative weights. This is the second largest subset among the 2000 remaining features. We found that, the aforementioned click-word characteristics were largely applicable to those positively weighted Word features. For instance, it is shown that they are more likely to be singular nouns (e.g. mirna) than plural nouns (e.g. mirnas), and that they tend to be specific content words (e.g. diabetes) rather than the context words (e.g. diseases). Our further analysis shows that among the top 100 most frequent PubMed search words (e.g. cancer), 37 of them are positively weighted Word features. Interestingly, we also find 13 negatively weighted click-word features in the top 100 most frequent user search terms. Despite their frequent occurrences, those 13 words tend to represent general concepts such as: gene, infection and treatment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R.Islamaj Do ˘ gan and Z.Lu</head><p>Note that the designed feature types are not fully independent of one another. For instance, the WFR feature is implicated in computing both the word location and TF–IDF features. Yet, we found that all the designed feature types contributed features to the final click-word model. Although specific words on their own still carried some weight towards being recognized as click-words, all the other features that we designed provided strong characteristics that collectively identified the click-words. As a result, such a clickword model could also make it possible to rank and index the articles according to the perceived users' interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 2775 2767–2775</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Click-words: learning to predict document keywords</head><p>procure, they can be used to complement MeSH terms and author keywords that both require human annotation. In terms of document retrieval, the click-word model can produce a score for identifying word importance, similar to the classic TF–IDF measure. Thus, a natural extension of this work is to compare the retrieval effectiveness using such scores versus traditional TF–IDF scores. For example, the 'related articles' in PubMed are computed by adding up the TF–IDF weights of all the terms in common between two documents. Replacing the TF–IDF weights with the click-words weights might yield better results in that we assign higher scores to terms that represent the subject of an article from readers' perspective. Another possible direction is to expand the click-words to click phrases, as the Phrase feature was a prominent feature in the final click-word model. In this work, we used only exact query terms but not their translation(s) (due to PubMed's Automatic Term Mapping feature) in obtaining instances of click-words. A further direction is to expand the definition of click-words to all frequent query terms that result in document clicks, regardless of their appearance in the title or abstract. Finally, rather than treating query words as independent, we could explore the relationships between query words, which also may lead to a better identification of relevant articles.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.</head><figDesc>Fig. 1. An example of click-words, top-scoring TF–IDF words, author keywords and MeSH indexing terms for a PubMed article. User click-words are listed by the frequency in which they appear in user queries for this article. TF–IDF words are listed in decreasing order of their TF–IDF weight. MeSH terms follow the order in which they are listed in PubMed. Author keywords are listed as they appear in the article.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.2.</head><figDesc>Fig. 2. The iterative feature selection model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.3.</head><figDesc>Fig. 3. Results of break-even point of precision and recall averaged among the 5-folds of cross-validation test sets, through the progression of iterative feature selection method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>Table 1.</figDesc><table>The break-even precision recall point for each individual feature 
type and the corresponding number of contributing features (non-zero Huber 
weights) for each feature type, when learning to differentiate articles' 
click-words from the top five TF–IDF-weighted words 

Model 
Precision–recall 
Number of 
break-even point 
features 

Word 
0.748 
36489 
Word location 
0.663 
15 
Neighbor words 
0.661 
253 340 
TF–IDF rank 
0.613 
5 
WFR 
0.609 
46 
MetaMap semantic types 
0.594 
134 
POS tag 
0.524 
37 
Part of phrase 
0.510 
2 
Abbreviation 
0.448 
2 
Random selection 
0.429 
− 

TF–IDF weight 
0.613 
− 

ALL features 
0.781 
290 069 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><figDesc>Table 2. Total number of features: 2000, best break-even point avg: 0.794</figDesc><table>Feature 
Number 

Neighbors 
1322 
Word 
556 
Semantic types 
83 
Word location 
15 
POS tag 
12 
TF–IDF 
5 
WFR 
3 
Phrase 
2 
Abbreviation 
2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>Table 3.</figDesc><table>Top features of the best model 

Huber 
weight 

Positive features 
Huber 
weight 

Negative features 

0.455 
LOC: title + first sentence −0.409 LOC: middle abstract only 
0.368 
WRD: mirna 
−0.342 LOC: middle + last 

sentence 
0.343 
WRD: cancer 
−0.312 LOC: first + middle 

sentence 
0.337 
SEM: disease or syndrome −0.312 LOC: first + middle + last 
sentence 
0.328 
WRD: il 
−0.270 POS: plural noun 

0.317 
LOC: title + first 
sentence + middle 
abstract 

−0.263 PHR: not in phrase 

0.293 
SEM: bacterium 
−0.251 SEM: functional concept 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>Table 4.</figDesc><table>Performance evaluation of the click-word model when compared with the TF–IDF weighting and random selection, for the top five weighted TF–IDF 
words for each article in the evaluation dataset 

Classification model 
Mean average precision 
Break-even precision–recall 
ROC 
Precision@1 

Training dataset results of 5-fold 
cross-validation 

Random selection 
0.612 
0.428 
0.498 
0.428 

TF–IDF weight 
0.757 
0.611 
0.691 
0.671 
Click-word model 
0.888 
0.794 
0.868 
0.863 

Evaluation dataset results for top 
5 TF–IDF words 

Random selection 
0.596 
0.405 
0.495 
0.405 

TF–IDF weight 
0.737 
0.581 
0.681 
0.631 
Click-word model 
0.855 
0.743 
0.832 
0.810 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><figDesc>Table 5. Performance evaluation results for the baseline random selection model, TF–IDF weighting of the words model and the click-word prediction model, for all the words in the title and abstract of the articles in the evaluation dataset</figDesc><table>Classification model 
Mean average precision 
Break-even precision–recall 
ROC 
Precision@1 

Evaluation dataset 
results for all words 

Random selection 
0.112 
0.065 
0.499 
0.065 

TF–IDF weight 
0.513 
0.454 
0.861 
0.631 
Click-word model 
0.627 
0.547 
0.904 
0.806 

Statistical analysis 
t-test 
45.195 
31.824 
32.425 
32.877 
P-value 
0.0002 
0.0005 
0.0005 
0.0005 

activity (e.g. trials), population group (e.g. women), idea 
or concept (e.g. recommendation). After feature selection, 
the number of semantic type features decreased substantially 
from the initial 135 to the final 83, which still would 
cover almost all types of common information needs in 
biomedicine. For instance, diseases and disorders—frequently 
sought information in PubMed—are covered by the following 
semantic types: Disease or Syndrome, Neoplastic Process, 
Mental or Behavioral Dysfunction, Injury or Poisoning, Sign 
or Symptom. 

</table></figure>

			<note place="foot">at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">We used two separate datasets for the purposes of this study. The first dataset consisted of 47 609 PubMed articles, and was used in a 5-fold cross-validation setting to train the click-word model. The second dataset consisted of 11 237 articles that do not overlap with the articles in the training dataset, and was used for evaluating the click-word model. Training dataset: for the training dataset, we collected 2 months of PubMed log data (March 2008 and February 2009), consisting of more than 100 million user queries and 130 million abstract clicks in 51 million user sessions. We identified the highly accessed articles (i.e. accessed more than 60 times by different users during the 2 months) and computed their user click-words. Our training dataset consisted of 47 609 articles. From the articles in our training dataset, we identified a total of 237 155 top five TF–IDF words, of which 101 377 were click-words (42.7%). Of the top five TF–IDF words per article, we found 2.2 click-words on average. Evaluation dataset: for the evaluation dataset, we collected 6 months of PubMed log data (February, April, May, June, July and August, 2009), consisting of more than 333 million user queries, 329 million abstract views in 144 million user sessions. From these click-through logs, we identified a total of 38 852 highly accessed MEDLINE citations (i.e. accessed over 180 times by different users during the 6 months). We separated the articles which were different from the documents in the training dataset and extracted the click-words. Our evaluation dataset consisted of 11 237 highly accessed articles. From the articles in our evaluation dataset, we identified an average of 4.5 click-words per article. This number is in agreement with the average number of click-words computed for the 2-month dataset (Section 3.1). When we selected the top five TF–IDF words of the evaluation dataset articles, we identified a total of 62 310 words, of which 22 663 were click-words (36.4%). These were distributed with an average of 2.0 click-words per article.</note>

			<note place="foot" n="2770"> at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from [09:31 12/10/2010 Bioinformatics-btq459.tex] Page: 2772 2767–2775</note>

			<note place="foot" n="1"> The entire set of 2000 final features and their respective weights can be found in Supplementary Material. 2773 at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="6"> CONCLUSIONS AND FUTURE WORK In this work, we proposed click-words as document keywords as they refer to words that readers find important. Next, we showed that click-words overlap significantly with top-scoring TF–IDF words. We implemented a supervised method to learn what characteristics, in addition to TF–IDF weights, make click-words important for PubMed users. Our results show that a word&apos;s semantic type, location, POS, neighboring words and phrase information together could best determine if a word will be a click-word. In addition, we have detailed the individual contribution of each of the clickword features. For example, a word&apos;s location was found to have the strongest power in classification. Specifically, click-words were more likely to appear in the title of the document, rather than only in the middle of the abstract. Click-words tended to be names of recognized biological entities, and they could be identified by their neighboring words and their positions in a sentence. Clickwords tended to be abbreviated terms, and appeared in phrases. Although trained to identify click-words from the top-weighted TF–IDF words, our click-word model showed significant robustness when applied to identify click-words from all the words in the titles and abstracts of more than 11 000 PubMed articles. There are several directions for improving the current work and extending this research. In terms of document indexing, because click-words are readily available and significantly less costly to 2774 at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="2775"> at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic extraction of keywords from scientific text: application to the knowledge domain of protein families</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Andrade</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Valencia</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="600" to="607" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">R</forename>
				<surname>Aronson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AMIA Symp</title>
		<meeting>. AMIA Symp</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Online learning from click data for sponsored search</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ciaramita</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;08: Proceeding of the 17th International Conference on World Wide Web</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="227" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">A user browsing model to predict search engine click data from past observations</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">E</forename>
				<surname>Dupret</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Piwowarski</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR&apos;08: Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="331" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">The effect of abbreviations on MEDLINE searching</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Federiuk</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acad. Emerg. Med</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="292" to="296" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Using the wisdom of the crowds for keyword generation</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Fuxman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on World Wide Web (WWW)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving rankings in small-scale Web search using clickimplied descriptions</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Hawking</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Aust. J. Intell. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="17" to="24" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<monogr>
		<title level="m" type="main">Information Retrieval: A Health and Biomedical Perspective</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">R</forename>
				<surname>Hersh</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<monogr>
		<title level="m" type="main">Improved automatic keyword extraction given more linguistic knowledge</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Hulth</surname>
			</persName>
		</author>
		<editor>M.Collins and M.Steedman</editor>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<monogr>
		<title level="m" type="main">Understanding PubMed(R) user search behavior through log analysis. Database [Epub ahead of print</title>
		<author>
			<persName>
				<forename type="first">Islamaj</forename>
				<surname>Do˘ Gan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>doi. :10.1093/database/bap018]</note>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Global ranking by exploiting user clicks</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Ji</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR&apos;09: Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">A ranking approach to keyphrase extraction</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Jiang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR&apos;09: Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="756" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to advertise</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Lacerda</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR&apos;06: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="549" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph-based keyword extraction for single-document summarization</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Litvak</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Last</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MMIES&apos;08: Proceedings of the Workshop on Multisource Multilingual Information Extraction and Summarization</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised approaches for automatic keyword extraction using meeting transcripts</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Liu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL&apos;09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="620" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Comparison of two Schemes for automatic keyword extraction from MEDLINE for functional gene clustering</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Liu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CSB&apos;04: Proceedings of the 2004 IEEE Computational Systems Bioinformatics Conference</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="394" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Text mining functional keywords associated with genes. Stud</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Liu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Health Technol. Inform</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="292" to="296" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Evaluating relevance ranking strategies for MEDLINE retrieval</title>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Lu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Med. Inform. Assoc</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="32" to="36" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">D</forename>
				<surname>Manning</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Keyword extraction from a single document using word co-occurrence statistical information</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Matsuo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ishizuka</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Artif. Intell. Tools</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="157" to="169" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Term-weighting approaches in automatic text retrieval</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Salton</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Buckley</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Manag</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="513" to="523" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Mining web query hierarchies from clickthrough data</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Shen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;07: Proceedings of the 22nd National Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="341" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Abbreviation definition identification based on automatic precision estimates</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Sohn</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">402</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">MedPost: a part-of-speech tagger for bioMedical text</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Smith</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2320" to="2321" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Mining the biomedical literature for genic information</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">O</forename>
				<surname>Tudor</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BioNLP&apos;08: Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="28" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">PubMed-EX: a web browser extension to enhance PubMed search with text mining features</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">T</forename>
				<surname>Tsai</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">3031</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">FACTA: a text search engine for finding associated biomedical concepts</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Tsuruoka</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2559" to="2560" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">How to interpret PubMed queries and why it matters</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Yeganova</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JASIST</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="264" to="274" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Finding advertising keywords on web pages</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Yih</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW&apos;06: Proceedings of the 15th international conference on World Wide Web</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="213" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Solving large scale linear prediction problems using stochastic gradient descent algorithms</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-first International Conference on Machine Learning</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="918" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Enhancing MEDLINE document clustering by incorporating MeSH semantic similarity</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Zhu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1944" to="51" />
			<date type="published" when="2009-08-01" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>