Motivation: FASTQ is a standard file format for DNA sequencing data, which stores both nucleotides and quality scores. A typical sequen-cing study can easily generate hundreds of gigabytes of FASTQ files, while public archives such as ENA and NCBI and large international collaborations such as the Cancer Genome Atlas can accumulate many terabytes of data in this format. Compression tools such as gzip are often used to reduce the storage burden but have the disadvantage that the data must be decompressed before they can be used. Here, we present BEETL-fastq, a tool that not only compresses FASTQ-formatted DNA reads more compactly than gzip but also permits rapid search for k-mer queries within the archived sequences. Importantly, the full FASTQ record of each matching read or read pair is returned, allowing the search results to be piped directly to any of the many standard tools that accept FASTQ data as input. Results: We show that 6.6 terabytes of human reads in FASTQ format can be transformed into 1.7 terabytes of indexed files, from where we can search for 1, 10, 100, 1000 and a million of 30-mers in 3, 8, 14, 45 and 567 s, respectively, plus 20 ms per output read. Useful applications of the search capability are highlighted, including the genotyping of structural variant breakpoints and in silico pull-down experiments in which only the reads that cover a region of interest are selectively extracted for the purposes of variant calling or visualization. Availability and implementation: BEETL-fastq is part of the BEETL library, available as a github repository at
INTRODUCTIONMuch has been written about disruptive changes in DNA sequencing technology over the last decade and the need for compact ways to store the vast datasets that these new technologies have facilitated. The raw reads in a sequencing project are commonly stored in an ASCII-based format called FASTQ (), the entry for each read comprising a read ID string that holds free-text metadata associated with the read, a string for the sequence itself and a string of quality scores that encodes accuracy estimates for each base. The general purpose compression tool gzip (www.gzip.org, Jean-Loup Gailly and Mark Adler) is free and widely available, and is hence an appealing option for compressing FASTQ files. Many bioinformatics tools can read gzip-compressed data directly via the API provided by the zlib library (www.zlib.net, Jean-Loup Gailly and Mark Adler), which avoids the need to create the uncompressed file as an intermediate, but nevertheless still incurs the computational overhead of decompressing the entire file, which is considerable for the large file sizes we are typically dealing with. For many applications, we would like random access to the data without the need to decompress the file in its entirety. This need was recognized by the authors of the Samtools package (), which features two programs, razip (a tool available from razip.sourceforge.net appears to have similar aims to the eponymous Samtools program, but seems to be an entirely separate development. We deal exclusively with Samtools' razip here) and bgzip, that take a blockcompressed approach to random access while retaining some degree of backward compatibility with gzip. The data are divided into contiguous blocks that are compressed individually, allowing decompression to commence from any point in the file, while limiting the overhead to the need to decompress the data that precede that point within its block. Given this functionality, it is simple to index a set of records by some key of interest by sorting them in order of that key, blockcompressing them and retaining the mapping from key value to file offset for a subsampling of the records. Several of Samtools' user-level tools work in this way, e.g. tabix and indexed BAM files are both indexed by genomic coordinate, whereas faidx uses the name of the sequence as a key. However, we wish to search for any substring within the sequences, which is not possible with a key-based indexing strategy. Instead, we use an approach based on the Burrows Wheeler transform, or BWT (). The BWT is a reversible transformation of a string that acts as a compression booster, permuting the symbols of the string in a way that tends to enable other text-compression techniques to work more effectively when subsequently applied. When the BWT-transformed string is decompressed again, the reversible nature of the transform allows the original string to be recovered from it. Although it was originally developed with compression in mind,showed that, in combination with some relatively small additional data structures, the (compressed) BWT can act as a compressed index that facilitates rapid search within the original string. This concept has been highly influential in bioinformatics, being the means by which BWT-based aligners such as BWA () and Bowtie () accelerate searches against a reference genome. The core idea is that exact occurrences of some query within the original string can be found by applying a recursive backward search procedure to its BWT. *To whom correspondence should be addressed. Having found some exact matches to our query within the reads in this way, we continue the recursion to extend these hits into the entire sequences of the reads that contain them. Once the extension reaches the boundary of a read, a lookup into an additional table allows the original positions of the reads in the FASTQ file to be deduced. This last piece of information enables the quality score and read ID strings to be extracted from razip-compressed files that have been indexed using their ordering within the original FASTQ file as a key. Specifically, given a query DNA sequence q, our tool can provide, in increasing order of computational overhead: the number of occurrences of q in the reads, the full sequence of each of the reads that contain q, the quality score strings associated with the sequences that contain q, the read IDs of the reads whose sequences contain q, (For paired-read data) the read IDs, sequences and quality scores of the reads that are paired with the sequences that contain q.Three potential applications demonstrate the usefulness of this fast k-mer search: an 'in silico pull-down' experiment in which only the reads that cover a region of interest are selectively extracted for the purpose of variant calling or visualization, a de novo assembly of reads from insertion breakpoints and the genotyping of structural variants by means of tracking the k-mers that overlap their breakpoints.
DISCUSSIONOur focus here has been to represent the input data in a lossless fashion, buthighlights that the relative storage needs of the sequences, quality scores and read IDs perhaps do not reflect their relative importance, as the sequences and their indexes take up only 17% of the total size, barely more than the 14% consumed by the read IDs. There remains some scope for further lossless compression: in earlier work that focused exclusively on compression (), we achieved sub-0.5 bpb on the sequences using 7-zip (www.7-zip.org, Igor Pavlov). Here, however, we sacrifice some compression for the faster decompression, and thus faster search that our simple byte code format gives us. The byte coding can be further optimized, and it may also be advantageous to switch between run-length encoding and naive 2-bits-per-base encoding on a per-block basis, choosing whichever of the two strategies best compresses each block. However, to achieve significant further compression, some degree of lossy compression is likely to be necessary. Each of the three data streams is potentially amenable to this, and our methods can of course still be applied to the resulting data. The free format of the read IDs limits our ability to comment generally on the prospects of compressing such data further. Nevertheless, it could be argued that, for paired data, the most useful metadata to retain is which read is paired with which: this could be simply encapsulated in an array containing one pointer per read, consuming Ologn bits. The space taken up by the sequences themselves would be reduced by error correction, two possible strategies being the trimming of low-quality read ends [as demonstrated by] or a k-mer-based approach such as Musket (). Lastly, the majority of the archive's size is taken up by the quality scores. More recent Illumina data default to a reducedrepresentation quality scoring scheme that makes use of only 8 of the 40 or so possible quality values, but the PG data we tested still follow the full-resolution scoring scheme. The newer scheme would likely reduce the size of the scores by about half. We have also described a complementary approach () that uses the k-mer context adjacent to a given base to decide whether its quality score can be discarded without likely detriment to variant calling accuracy. Conflicts of Interest: All authors are employees of Illumina Inc., a public company that develops and markets systems for genetic analysis, and receive shares as part of their compensation.