Motivation: Sequence-variation analysis is conventionally performed on mapping results that are highly redundant and occasionally contain undesirable heuristic biases. A straightforward approach to single-nucleotide polymorphism (SNP) analysis, using the Burrowsâ€“Wheeler transform (BWT) of short-read data, is proposed. Results: The BWT makes it possible to simultaneously process collections of read fragments of the same sequences; accordingly, SNPs were found from the BWT much faster than from the mapping results. It took only a few minutes to find SNPs from the BWT (with a supplementary data, fragment depth of coverage [FDC]) using a desktop workstation in the case of human exome or transcrip-tome sequencing data and 20 min using a dual-CPU server in the case of human genome sequenc-ing data. The SNPs found with the proposed method almost agreed with those found by a time-consuming state-of-the-art tool, except for the cases in which the use of fragments of reads led to sensitivity loss or sequencing depth was not sufficient. These exceptions were predictable in advance on the basis of minimum length for uniqueness (MLU) and FDC defined on the reference genome. Moreover, BWT and FDC were computed in less time than it took to get the mapping results , provided that the data were large enough. Availability and implementation: A proof-of-concept binary code for a Linux platform is available on request to the corresponding
IntroductionSince the advent of so-called next-generation DNA sequencers (NGSs), which rapidly and cost-effectively generate billions of short reads, large-scale analysis of sequence data of a few-hundred giga base pairs (Gbp), requiring a large computational resources, is not uncommon anymore. The first step to extract biologically meaningful information from sequence data often involves analysis of the variation (mutation) of that data in comparison with a reference genome sequence data. Billions of short reads are first mapped onto the reference genome, and unambiguous and recurrent mismatches between the short reads and the reference genome are identified as candidate mutations (). This line of approach is hereafter referred to as the mapping-based approach. Although it is the most appreciated and most commonly used approach, it has the following basic weak points. (i) The computation of mapping is highly redundant because of large sequencing depth (typically ranging from 30 to 100). (ii) Some mutations can be lost by mapping tools because such tools use certain heuristics of their own to resolve mapping ambiguities. (iii) It is not easy to switch from one reference genome to another after the computation of mapping has been completed. To address these weak points, an alternative solution, the dictionary-based approach, is proposed. The short-read data are converted into a dictionary of reads, so that numbers of occurrences of any sequence in the short-read data are immediately obtained. Then,mutations can be inferred on the basis of these numbers by means of querying genomic subsequences with and without the mutations (). The dictionary can be implemented efficiently by means of the BurrowsWheeler transform (BWT) () [a.k.a. FM index (] because it is simple and particularly suitable for DNA sequences. Although the proposed approach appears to be too navenave, it has the following potential advantages. (i) Redundancy due to deep sequencing coverage is efficiently managed by the dictionary of reads. (ii) The dictionary of reads does not suffer information loss or heuristic bias because it is essentially constructed by means of sorting the data in alphabetical order. (iii) It is easy to switch from one reference genome to another one after the dictionary of reads is constructed. However, the following issues of the proposed approach remain to be addressed. 1. The construction of BWT for large data (more than 100 Gbp) is very time-consuming even with the fastest known algorithm, BCRext (). 2. A large number of occurrences in the short-read data of a genomic subsequence with a candidate mutation do not necessarily imply the existence of the mutation because some of them might be derived from different genomic regions with similar subsequences. 3. To get useful information from the dictionary of reads, it is necessary to prepare effective queries that are likely to contain candidate mutations. Namely, it is necessary to locate genomic positions with a significant chance of finding mutations.To address these issues, the following algorithm and concepts are introduced in this study. 1. BWT/WT, a modified and parallelized BCRext algorithm for computing the BWT of reads. 2. The minimum length for uniqueness (MLU), a simple criteria for evaluating the uniqueness of the subsequence. 3. The fragment depth of coverage (FDC), an estimate of sequencing depth of coverage on the basis of exact matching of read fragments at single-base resolution.