Motivation: Network-based gene function inference methods have proliferated in recent years, but measurable progress remains elusive. We wished to better explore performance trends by controlling data and algorithm implementation, with a particular focus on the performance of aggregate predictions. Results: Hypothesizing that popular methods would perform well without hand-tuning, we used well-characterized algorithms to produce verifiably untweaked results. We find that most state-of-the-art machine learning methods obtain gold standard performance as measured in critical assessments in defined tasks. Across a broad range of tests, we see close alignment in algorithm performances after controlling for the underlying data being used. We find that algorithm aggrega-tion provides only modest benefits, with a 17% increase in area under the ROC (AUROC) above the mean AUROC. In contrast, data aggregation gains are enormous with an 88% improvement in mean AUROC. Altogether, we find substantial evidence to support the view that additional algorithm development has little to offer for gene function prediction. Availability and implementation: The supplementary information contains a description of the algorithms, the network data parsed from different biological data resources and a guide to the source code (available at: http://gillislab.cshl.edu/supplements/).
IntroductionHigh-throughput genomic data often relies on computational methods for functional inference and interpretation. Improving our knowledge of gene function in otherwise uncharacterized genes is one major task to which these computational methods are put (). While this is often called gene function prediction when being treated as a machine learning problem, essentially the same methods underlie a variety of important biomedical applications, such as candidate disease gene prioritization (). Like many methods in machine learning, a major concern for function inference methods is the degree to which their performance is robust and generalizes from benchmark tasks to novel data (). To some extent, systematic problems with generalizing past gene function prediction to future performance have been recognized by the field. These have led to critical assessments of function prediction, e.g. the critical assessment of Mus musculus gene function prediction (MouseFunc) () and the critical assessment of function annotation (CAFA) (), where groups compete to predict gene function. These assessments are intended to provide field-wide benchmarks for comparative performance and thereby help determine which research directions may be more fruitful. Generally speaking, gene function prediction methods rely on (i) prior function assignments for genes, (ii) data to characterize genes and (iii) an algorithm which learns the data features associated with the previous function assignments. Novel genefunction mappings are then predicted based on the learned data features; this can be done either in a gene-centric or function-centricBecause individual laboratories approach this problem in quite different ways, characterizing the field overall in critical assessment has been difficult; it is hard to know what factors in one laboratory's implementation drives results. Indeed, even in critical assessments, the same laboratory may submit slightly altered versions of the same software and find their performance changes dramatically (). We solve the over-training problem addressed by critical assessments in an alternate way; by using verifiable implementations it can be checked that there are no adjustments or tweaks. Normally, 'fine-tuning' algorithms and data to work appropriately are necessary to obtain reasonable performance, but this is precisely what may contribute to poor generalization. Based on performance trends in previous critical assessments, we hypothesized that welldeveloped machine learning approaches would perform at a high level if using gold-standard data resources. That the performance assessment is not tuned to obtain artificially high performance can be ensured by using pre-existing and verifiable tests, i.e. the critical assessments. By having an in-house representative of the field as a whole, we are able to conduct additional experiments with greater control. One of our principal interests is in exploring how aggregation improves performance. That it generally does so has been a finding common to previous assessments of function prediction and network inference methods and is a frequent expectation for machine learning in general (). However, the factors central to this effect have not been well characterized, likely because most laboratories will have an individualized approach which makes well-controlled comparison difficult. To overcome this, we will first benchmark a set of machine learning algorithms based upon data resources available from MouseFunc to establish they are representative samples of high-performing methods. Then, we will systematically apply these algorithms on various types of Saccharomyces cerevisiae data as a sample of 'field-wide' properties and investigate how their combination improves performance and on what factors this depends.
DiscussionThere is a simple and consistent trend in our results: even algorithms of wholly distinct conception and design give quite similar results (in all ways) on the same data. This may seem an obvious findingone cannot get blood from a stonebut it is, in fact, contrary to much of the published literature. Summarizing our points of departure from the previous literature, the most similar previous analysis to ours concluded that 'it's the machine that matters', even where similarly using default implementations (). Similarly, it is the consensus that algorithm aggregation alone can provide substantial benefits on the closely related task of network inference (), which would normally suggest that algorithms vary enough to benefit from aggregation. Supported by these general findings, more focused assessments have also concluded that simply aggregating algorithms on the same data offers enormous benefits, again, on tasks closely related to but not identical to function prediction, e.g. mutation prioritization (). While we draw superficially contrary conclusion, our results are actually surprisingly consistent with these claims. Within the range of performance of our algorithms on a given dataset, aggregation has a very large influence. Thus, in any competition in which data is held constant, we too, would suggest aggregation as a potentially useful strategy. It is only by examining variation across data resources that we see how comparatively modest these benefits are. In support of this finding, in critical assessments where developers may alter their data choice, high-performing methods have tended to use large and diverse datasets, as in the top-performing CAFA method, where data integration was one of the chief areas of novelty (). This suggests that comparative assessment of data (or even its integration) may be more fruitful than the current algorithmic focus. In fact, it would then be important to hold methods somewhat constant. Fortunately, our analysis suggests this is rather easy and that even where methods differ in basic design and conception, they are quite similar in performance (relative to variation caused by data). Because of this, downstream results can be assumed to be the property of the data being studied, rather than the inference method being used to study it. While it is true that good experimental design can ensure over-training does not occur, if a custom method is necessary to get a dataset to work or the custom method works unusually well, we suggest there is a heavy onus to show it does not reflect accidental over-training in some way. Of course, another reason for our performance trends is that all our algorithms performed well. If some performed badly, then the machine could be said to matter more. We believe there may be a subtle selection bias at play here which is difficult to evaluate formally. Our function inference task was more computationally challenging than what might be considered a typical assessment for a targeted function or multiple functions in a targeted dataset. While our algorithms are not customized, many methods we initially tried simply did not operate readily in the large data and prediction space we wished to assess. This may well have selected for general-purpose