Motivation: Automatically quantifying semantic similarity and relatedness between clinical terms is an important aspect of text mining from electronic health records, which are increasingly recognized as valuable sources of phenotypic information for clinical genomics and bioinformatics research. A key obstacle to development of semantic relatedness measures is the limited availability of large quantities of clinical text to researchers and developers outside of major medical centers. Text from general English and biomedical literature are freely available; however, their validity as a substitute for clinical domain to represent semantics of clinical terms remains to be demonstrated. Results: We constructed neural network representations of clinical terms found in a publicly available benchmark dataset manually labeled for semantic similarity and relatedness. Similarity and related-ness measures computed from text corpora in three domains (Clinical Notes, PubMed Central articles , and Wikipedia) were compared using the benchmark as reference. We found that measures computed from full text of biomedical articles in PubMed Central repository (rho = 0.62 for similarity and 0.58 for relatedness) are on par with measures computed from clinical reports (rho = 0.60 for similarity and 0.57 for relatedness). We also evaluated the use of neural network based relatedness measures for query expansion in a clinical document retrieval task and a biomedical term word sense disambiguation task. We found that, with some limitations, biomedical articles may be used in lieu of clinical reports to represent the semantics of clinical terms and that distributional semantic methods are useful for clinical and biomedical natural language processing applications.
IntroductionAutomated approaches for representing the semantic content of terms and similarity and relatedness between them have been widely used in a number of Natural Language Processing (NLP) applications in both general English (knowledge discovery, among many other (seefor a comprehensive review). In the general English domain, distributional semantic approaches to measuring semantic similarity and relatedness have been quite successful, achieving correlations in the 70's and 80's with human judgments (). In the biomedical domain, the problem of representing lexical semantics of medical terms in such a way as to match human judgments of semantic similarity and relatedness between them has proven to be more challenging. State of the art approaches developed so far for computing semantic similarity and relatedness achieve only modest agreement with human judgments. This applies to both distributional methods (i.e., knowledge-free) and those based on relations in manually constructed ontologies such as WordNet and the Unified Medical Language System (i.e., knowledge-based). For example, Garla and Brant (2012) reported on a large systematic investigation of a wide range of knowledge-based and knowledge-free approaches to computing semantic relatedness and similarity and found that knowledge-based approaches augmented with information content obtained from corpora of text (e.g.,) outperformed distributional approaches based on first and second order semantic vectors (e.g., Lin (1998) and Patwardhan and). The authors used a number of publicly available benchmarks including the University of Minnesota Semantic Relatedness Standard () containing the largest number and diversity of medical term pairs to date. In terms of agreement with human ratings and the UMNSRS benchmark, the best correlations range between 0.30 and 0.46 (Spearman rank correlation).) reported similar correlations between the output of the second-order context vectors () and a subset of the UMNSRS benchmark.Another study bythat also included the UMNSRS benchmark, among others, confirmed Garla and Brant's (2012) findings demonstrating that their knowledge-based method based on the Wikipedia network (HITS-sim) achieved significantly higher correlations with human ratings than distributional methods. The Spearman rank correlations for the UMNSRS benchmark were 0.51 and 0.58 for semantic relatedness and semantic similarity judgments, respectively. The distributional semantic methods evaluated inincluded word2vec, a neural network based mechanism for semantic representation based on word embeddings that was originally proposed bytrained a skip-gram vector representation of medical terms using word2vec on the OHSUMED corpus (a collection of 348,566 biomedical research articles). This approach achieved a correlation of 0.39 on both relatedness and similarity human judgments with the UMNSRS benchmark. A similar study byalso examined word2vec semantic representations derived from the PubMed Central Open Access (PMC) corpus. Similarly to the study bytested their approach on the UMNSRS benchmark and reported Spearman rank correlations of 0.45 and 0.52 for semantic relatedness and semantic similarity judgments, respectively. These previous studies targeted overall performance of distributional semantic and other approaches and did not examine the performance on subsets of the UMNSRS dataset consisting of pairs of different semantic types. We hypothesize that one of the reasons the UMNSRS benchmark has been difficult to approximate with automated approaches (particularly with knowledge-free distributional approaches) is because it consists of a large number of clinical concepts from a variety of semantic types (disorders, symptoms, and drugs). Thus, in order to model human judgments of semantic relatedness and similarity of this benchmark using distributional methods one may need to use very large amounts of textual data from a corpus that closely matches the domain of clinical language represented by this benchmark. Previous studies tended to rely on relatively small corpora that are from a closely related but not perfectly matched domains (e.g., biomedical articles). One exception to this was a study bythat used Mayo Clinic clinical notes as a source of training data; however, while that study had a good domain match, the size of the corpus was relatively modest by current standards (~232M tokens). The objective of the current study is to examine the effect of corpus size and sublanguage domain match on the agreement between relatedness and similarity measures produced by the popular and highly efficient distributional semantic method (word2vec) and human judgments in the UMNSRS benchmark dataset.
DiscussionIn the current study, we performed a large-scale evaluation of a popular neural network learning approach (word2vec) to representing the meaning of words in the medical domain and measuring the strength of association between them. Our results are overall slightly better than the best results reported so far on the UMNSRS benchmark bythat used a graph-based approach (HITS-sim) to represent word semantics that leveraged Wikipedia as a network (rho 0.58 vs. 0.51 for relatedness and 0.62 vs. 0.58 for similarity). However, these differences are probably not significant from a practical standpoint due to reasons having to do with inter-rater agreement explained later in the discussion.For comparison,also used the word2vec approach (with skip-gram representation of word contexts) trained on OHSUMED corpus of 348,566 MEDLINE citations and the text of the Unified Medical Language System terms but obtained much lower correlations with human ratings (rho = 0.39). The citations included in the OHSUMED corpus are truncated at 250 words 2 ; therefore, the total size of the OHSUMED collection is at most 87M tokens. The main differences of our study's use of word2vec from the one reported byare that we used the bag-of-words representation of contexts instead of the skip-gram representation, and a much larger corpus of text (over 4B tokens) for training consisting of entire articles rather than just the MEDLINE citations containing only the abstract and title of the articles. We believe that that the improved performance of word2vec approach observed in our study was mostly due to the latter two differences  larger corpus and inclusion of entire articles; however, we did not directly test these assumptions in the current study. At the outset of the current study, we intuitively expected to find domain and corpus size effects. We expected that, since the UMNSRS reference standard was created based on ratings by clinicians, semantic representations derived from the CLINICAL domain would agree better with clinicians' ratings than representations derived from PMC or WIKIPEDIA. The findings of this negative and positive directions. The detailed comparisons are shown in.Another interesting finding of the current study is that increasing the size of the corpus beyond a certain size does not seem to provide an additional advantage. In the current study, the performance on both similarity and relatedness benchmarks plateaued between 10M and 100M tokens, which is consistent with the findings of another previous study byin which we found that the performance of another distributional semantics corpus-based approach to computing semantic relatedness plateaued after the training corpus reached 300,000 clinical notes (~ 66M tokens). These findings may be interpreted as providing additional evidence to show that the size of the corpus used for distributional semantic representations of medical terms does not matter beyond a certain point (e.g. 100M tokens); however, an alternative explanation is that the corpus size does matter but the plateauing of the correlations with human ratings is a function of the test data rather than the training data and has more to do with the inter-rater agreement. One possible way to test this hypothesis in future work is to apply corpus-based semantic relatedness measures in a secondary evaluation paradigm in which measures derived from corpora of different size would be used for another task such as word sense ambiguity resolution, spelling correction, or query expansion for information retrieval.In addition to cross-domain comparisons of the word2vec based semantic relatedness measures on the UMNSRS reference standard, we also compared the use of semantically related phrases derived from clinical and biomedical domains on two tasks directly relevant to medical NLP. Automatically derived phrases for expanding text queries to identify patients with heart failure presented in the current study are consistent with the terms defined by experts in cardiovascular research that were used in prior studies examining the utility of NLP for identification of patients with heart failure from the unstructured text of EHR (). These manually defined terms consisted of " cardiomyopathy, " " heart failure, " " congestive heart failure, " " pulmonary edema, " " decompensated heart failure, " " volume/fluid overload. " The sets of top 100 automatically derived terms related to " heart failure " from both the clinical (CLINCIAL-ALL) and biomedical (PMC) corpora in the current study contain all but one ( " pulmonary edema " ) of these terms manually determined to be good search terms for heart failure cases.Our results also show that expanding text search queries with semantically related terms based on word embeddings can significantly improve recall of the queries. This is an important finding in the context of using NLP to identify potential candidates for clinical research studies such as clinical trials and cohort studies. Improved recall is particularly important for cohort studies in which the completeness of ascertaining cases with a condition of interest is critical to minimizing potential bias. Using structured billing codes (a.k.a. claims data) for case ascertainment has been shown to have limitations in terms of accuracy and completeness, particularly in community-based samples (). Furthermore, a number of conditions (e.g., symptoms and physical examination findings) may not have a diagnostic code entered as part of the medical record. Using NLP to search unstructured text of EHR can complement the use of billing codes for improved recall of potential candidates for prospective and retrospective studies. Improved recall is also relevant for clinical trials from a more practical standpoint. Being able to identify any number of additional potential candidates for a clinical trial can improve recruitment rates and consequently shorten the duration of the trial resulting in faster delivery of new therapeutic interventions to the bedside.In the current study, we used the ICD-9 code 428.x to define the reference standard (with the understanding of the limitations inherent in this approach) for comparing query expansions to each other and to the baseline query. In order to minimize uncontrolled variability in these comparisons, we did not use advanced NLP tools (e.g., negation and family history detection) to make the text queries more precise. Therefore, the precision of the queries reported in this study does not reflect the actual precision that can be expected from text queries enhanced with NLP. In prior work, we showed that using negation detection can improve precision of text queries to approximately 50% (), and this number is likely to be higher for more sophisticated and customized NLP systems. In the current study, we focused on changes in precision between semantically expanded queries and the baseline. Our findings indicate that queries expanded with top 5 semantically related phrases from the CLINICAL-ALL and PMC corpora perform better than the baseline query and are comparable to each other. The performance of these queries diverges on larger sets of expansions (10, 20, and 40) with a large decrease in precision on queries derived from the PMC corpus. This drop in precision can be attributed to the terms " cardiac " and " hypertension " that are relatively high on the list of PMC derived phrases. Queries with these two terms would capture the majority of patients with heart disease (with or without heart failure) and thus negatively affect precision. However, despite the lower precision, these results indicate that the public PMC corpus can potentially be used as an alternative source of semantic relatedness information for expanding queries when searching clinical texts. This is particularly important in settings where the query expansion is not fully automated but is curated by the end user that can manually select more specific terms from the set of semantically related suggestions. While these findings are encouraging, clearly, further investigation is necessary to test their generalizability. Similarly to the evaluation on the clinical document retrieval task, the evaluation of using word embeddings on a biomedical term WSD task showed that this type of representation improves WSD accuracy over basic co-occurrence based approaches. The accuracy of ~78% that we achieved on this dataset with word embeddings derived from the PMC corpus is comparable to other previously reported machine readable dictionary based results on the same dataset (; B. T.). The improvement in WSD accuracy with word embeddings is likely due to the fact that even extended sense definitions are bound to be sparse, compared to occurrences in a text corpus of any reasonable size. Word embeddings may offer the advantage of smoothing over the gaps in representations based on sparse sense definitions. The fact that better WSD accuracy is achieved with methods trained on PMC data is not surprising because the NLM WSD dataset is derived from the same domain. What is interesting is that the accuracy achieved with word embeddings derived from the clinical domain is not that much lower than the accuracy achieved with in-domain training data, which constitutes additional evidence that the clinical and biomedical sources of text are similar with respect to semantic representations that can be derived from them.