Motivation: Next-generation sequencing (NGS) has revolutionized biomedical research in the past decade and led to a continuous stream of developments in bioinformatics, addressing the need for fast and space-efficient solutions for analyzing NGS data. Often researchers need to analyze a set of genomic sequences that stem from closely related species or are indeed individuals of the same species. Hence, the analyzed sequences are similar. For analyses where local changes in the examined sequence induce only local changes in the results, it is obviously desirable to examine identical or similar regions not repeatedly. Results: In this work, we provide a datatype that exploits data paral-lelism inherent in a set of similar sequences by analyzing shared regions only once. In real-world experiments, we show that algorithms that otherwise would scan each reference sequentially can be speeded up by a factor of 115. Availability: The data structure and associated tools are publicly available at http://www.seqan.de/projects/jst and are part of SeqAn, the C++ template library for sequence analysis.
INTRODUCTIONNext-generation sequencing (NGS) has revolutionized biomedical research in the past decade and led to a continuous stream of developments in bioinformatics, addressing the need for fast and space-efficient solutions for analyzing NGS data. Especially since the sequencing efficiency of modern NGS technologies outpaced the improvement of storage capacities, which directly leads to a growing economical issue, as storing and sharing the generated information is now bounded by the available storage and network resources (). The same technology led to the generation of comprehensive catalogs for genetic variations of the human () and other organisms as well (e.g.). Moreover, the rapid advances in NGS made ambitious sequencing endeavors like the 1000 Genomes Project (), systematic studies of 425 000 cancerous genomes (The International Cancer Genome) or most eagerly the announced goal of the Personal Genome Project to sequence 100 000 human genomes () possible. Those resources then provide detailed information about the genetic diversity of entire populations, which will be important to the societal health sector to acquire better understandings of the correlation between clinical conditions and phenotypes and their corresponding genotypes. As a result, two major challenges need to be tackled. The first challenge clearly is to compress the available sequence data to relieve disk and network resources. Owing to the high redundancy of sequences originating from the same or related organism, referential sequence compression has been proven to be especially efficient for these kinds of data (e.g.). More recentlyexploited cross-sequence correlations to achieve profitable compression ratios for the data of the 1000 Genomes Project. The second challenge, however, is to devise algorithms and data structures that can handle the massive amounts of available data to incorporate those information in existing analyzing pipelines. Clearly, one solution would be to apply the algorithms sequentially to each sequence contained in the database, but the runtimes scale linearly to the number of sequences included. Thus, it is desirable to analyze the data in succinct form to archive runtimes proportional to the compressed size. This paradigm is also known as compressive genomics (). The FM-index () and the compressed suffix array (), for example, are succinct representations of indices that can be searched efficiently. Yet, indexing thousands of genomes with these data structures would still exceed currently available memory capacities by far. In the past 5 years, this problem was subject in several publications (). Here, the focus was moved from considering each sequence individually to exploiting similarities among the sequences to reduce the overall memory footprint, and to gain substantial speedups opposed to the sequential case.presented compression-accelerated BLAST and BLAT, both tools to search patterns in a non-redundant sequence library approximatively. These methods, however, require the compressed sequence library to be generated in a computationally intensive preprocessing phase.used as input a more general format consisting of a reference sequence and a set of variants for a collection of sequences, which is a common representation of the data produced by large sequencing endeavors such as the 1000 Genomes Project. Subsequently they built an index over the reference set exploiting the high *To whom correspondence should be addressed.  The Author 2014. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com similarities. Yet, the read-mapper BWBBLE () was the first practical tool capable of mapping reads against thousand genomes simultaneously. In their approach, they indexed a multi-genome reference sequence with the FM-index and adapted the Burrows-Wheeler Aligner (BWA) () to work on the indexed multi-genome. To construct the multi-genome, they needed a context size to determine the sequence context left and right of genomic regions harboring insertions or deletions; thus, the entire multi-genome and the accompanied index must be reconstructed on changes of the context size. However, there exists a plethora of algorithms for sequence analysis that are sequential in nature. That means they scan the sequences to be analyzed from left to right and perform some computation, e.g. compute an approximate matching or alignment of a query sequence to a reference sequence, or scan sequences in the context of a hidden Markov model (HMM) (e.g. De). There are many more applications such as filtering and verification algorithms in read mappers () or searching with position-specific scoring matrices (). After reading a character of the string, the algorithm will change its internal state and possibly report some results.gave an algorithm to solve the pattern matching problem with Hamming distance for a set of extremely similar sequences. In their model, they assumed that each sequence differs in around 10 positions to every other sequence within the set. To find all occurrences of a pattern within all sequences they searched first the reference sequence and used then some auxiliary tables to check if at the reported positions the pattern can be found for the other sequences too. Besides the impractical error model for real biological data, they did not provide any experiments to evaluate their method. Thus, to the best of our knowledge, we provide for the first time a general solution to generically speed up a large class of algorithms when working on sets of similar strings. We will mostly address the reduction of execution time and space of such algorithms by providing a data type, called journaled string tree (JST) that can be traversed similarly to a simple for-loop over all sequences while exploiting the high similarity. The algorithm must simply be able to store its state when asked, continue its computation from a stored state when presented with a new character and work locally, i.e. when scanning a sequence its current state solely depends on a fixed-size window of last seen characters, also called context. Our approach is based on a reference-based compression of shared sequence parts with some additional bookkeeping. For example, if lets say 1 Mb of a set of 100 genomes is shared and we want to compute a semiglobal alignment on all sequences, we will execute the alignment only once on this stretch of 1 Mb. For regions that exhibit differences, a corresponding sequence context is constructed on-thefly and then examined. We can show that our approach exhibits speedups of 4100 times when analyzing a set of 2185 sequences of chromosome 1 [two haplotypes of 1092 sequences from the 1000 genomes project () and a reference sequence] compared with running the algorithms sequentially while using only $3.8 GB of space. This speedup includes all overheads. The speedup in searching alone is up to a factor of 570.