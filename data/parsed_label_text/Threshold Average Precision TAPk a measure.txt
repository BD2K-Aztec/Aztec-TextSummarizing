Motivation: Since database retrieval is a fundamental operation, the measurement of retrieval efficacy is critical to progress in bioinformatics. This article points out some issues with current methods of measuring retrieval efficacy and suggests some improvements. In particular, many studies have used the pooled receiver operating characteristic for n irrelevant records (ROC n) score, the area under the ROC curve (AUC) of a pooled ROC curve, truncated at n irrelevant records. Unfortunately, the pooled ROC n score does not faithfully reflect actual usage of retrieval algorithms. Additionally, a pooled ROC n score can be very sensitive to retrieval results from as little as a single query. Methods: To replace the pooled ROC n score, we propose the Threshold Average Precision (TAP-k), a measure closely related to the well-known average precision in information retrieval, but reflecting the usage of E-values in bioinformatics. Furthermore, in addition to conditions previously given in the literature, we introduce three new criteria that an ideal measure of retrieval efficacy should satisfy. Results: PSI-BLAST, GLOBAL, HMMER and RPS-BLAST provided examples of using the TAP-k and pooled ROC n scores to evaluate sequence retrieval algorithms. In particular, compelling examples using real data highlight the drawbacks of the pooled ROC n score, showing that it can produce evaluations skewing far from intuitive expectations. In contrast, the TAP-k satisfies most of the criteria desired in an ideal measure of retrieval efficacy. Availability and Implementation: The TAP-k web server and downloadable Perl script are freely available at
INTRODUCTIONIn bioinformatics, retrieval from databases is a fundamental operation. Therefore, progress depends on being able to recognize superior retrieval algorithms, so the measurement of retrieval efficacy is critical in bioinformatics.stated that an ideal * To whom correspondence should be addressed. measure of retrieval efficacy (or more simply, a 'retrieval measure') should satisfy four conditions:(1) It should concern itself solely with the effectiveness of separating the relevant from the non-relevantand not with the efficiency of resource use.(2) It should not be dependent on athreshold, but should measure the potential output of the method.(3) It should be a single number.(4) It should have absolute significance as a measure of a single method and should readily allow comparisons of different methods to decide which is best.To fix our terms, when a user queries a database of R records, a retrieval algorithm typically lists up to R records, ranked by some score S indicating the probability that the corresponding record has relevance to the query. In text retrieval (e.g. Google or PubMed results), retrieval lists do not indicate the scores producing their list orders. In a significant break with the traditions of information retrieval, however, bioinformatics retrieval often explicitly presents an E-value with the score, so users are free to choose an E-value threshold E 0 and then ignore the retrieval list beyond E 0. For concreteness, we discuss only E-values, but the methods in this article apply to any score S. (Note that E-values and the retrieval ranks increase together.) In accord with the motivation behind E-values,modified Swets':(2 ) It should be characterized by athreshold, but should reflect the quality of retrieval at every rank down to that threshold.Wilbur's modification implicitly respects an overarching principle governing retrieval measures, which we call 'the Principle of Fidelity': a retrieval measure should faithfully reflect the actual usage of the retrieval list. If not, the measure might be 'ideal' in some abstract sense, but would lack a practical meaning. The Principle of Fidelity supports Wilbur's Condition (2 ) in bioinformatics, because an E-value threshold E 0 influences the actual usage of a retrieval algorithm. Since a user rarely examines a retrieval list far beyond the E-value threshold E 0 , any practical measure of database retrieval in bioinformatics should reflect the user's E 0. The rest of this article, therefore, disregards Swets' Condition (2) in favor of Wilbur's Condition (2 ), referring to the result as the 'SwetsWilbur Conditions'.Page: 1709 17081713
DISCUSSIONThis article is not the first to question the pertinence of ROC analysis to information retrieval (). In fact, many other researchers have pointed out the superiority of PR curves over ROC curves in information retrieval.advises that, 'PR graphs are commonly used where " the number ofis many orders of magnitude greater than [the number of relevant records] " ', the common case for database retrieval and most of bioinformatics. Likewise, Liu and Shriberg (2007) suggest that for 'an imbalanced dataset, PR curves generally provide better visualization than do ROC curves, for viewing differences among different algorithms.' Similarly, Davis and Goadrich (2006) warn that 'with highly skewed datasets, PR curves give a more informative picture of an algorithm's performance'and that, 'by comparing false positives to true positives rather than true negatives,captures the effect of the large number of negative examples on the algorithm's performance.'argue that ROC analysis effectively ignores the 'minority class' of relevant records. Several bioinformatics studies have relied on ROC analysis as their figure of merit for automatic improvement of database retrieval algorithms. As a figure of merit, however, the pooled ROC n score suffers from defects so obvious that other bioinformatics studies (wisely, in our opinion) have gone so far as to defend conclusions drawn from the pooled ROC n score by checking individual retrieval lists (). Clearly, if the defects of the pooled ROC n score require human intervention, it is inadequate to the task of automated improvement of retrieval algorithms. To provide explicit logical foundations for the discussion about the inadequacies of retrieval measures, this article also articulated a Principle of Fidelity: a retrieval measure should faithfully reflect the actual usage of the retrieval list. In harmony with the Principle of Fidelity, we suggested adding Conditions (5)(7) to the SwetsWilbur Conditions for an ideal retrieval measure. The Section 3 demonstrates that the pooled ROC n can violate Conditions (5) and (6). Condition (6) is common sense, so its failure inis particularly disturbing. On the other hand, since the TAP-k is an average over all queries, Conditions (5) and (6) both follow as rigorous mathematical truths. Moreover, consideration of the geometry of a ROC curve shows that the ROC n always increases with the EPQ (or equivalently, with the E-value threshold), in violation of Condition (7). On the other hand, the TAP-k sometimes does satisfy Condition (7):shows its eventual decrease against an increasing E-value threshold E 0. Interestingly, the peak values inoccur at values of E 0 *(A) not entirely outside acceptable ranges of the E-value thresholds for the corresponding algorithms, perhaps leading to the hope that even the selection of threshold E-values E 0 might be automated. Unfortunately, the Supplementary Material gives an example of retrieval lists where the TAP-k increases monotonically with the E-value threshold. Although the TAP-k has many properties desirable to optimizing retrieval algorithms automatically, it is currently unable to serve as a basis for automated determination of a best E-value threshold E 0 *(A). For concreteness, this article has discussed E-values, but in fact it is pertinent to any score S, not just to E-values. The E-value is really just a type of score that retains theoretical meaning across different queries as a surrogate for a record's probability of relevance. Like other scores, however, E-values do not predict the relevancy of records with complete accuracy, and the accuracy depends very much on the application (). Thus, if a particular algorithm A produces a retrieval list, a user willing to tolerate about a median k EPQ must apparently learn the corresponding