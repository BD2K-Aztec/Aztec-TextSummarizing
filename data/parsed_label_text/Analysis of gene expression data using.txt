Motivation: Gene expression data exhibit common information over the genome. This article shows how data can be analysed from an efficient whole-genome perspective. Further, the methods have been developed so that users with limited expertise in bioinformatics and statistical computing techniques could use and modify this procedure to their own needs. The method outlined first uses a large-scale linear mixed model for the expression data genome-wide, and then uses finite mixture models to separate differentially expressed (DE) from non-DE transcripts. These methods are illustrated through application to an exceptional UK Brain Expression Consortium involving 12 human frozen post-mortem brain regions. Results: Fitting linear mixed models has allowed variation in gene expression between different biological states (e.g. brain regions, gender, age) to be investigated. The model can be extended to allow for differing levels of variation between different biological states. Predicted values of the random effects show the effects of each transcript in a particular biological state. Using the UK Brain Expression Consortium data, this approach yielded striking patterns of co-regional gene expression. Fitting the finite mixture model to the effects within each state provides a convenient method to filter transcripts that are DE: these DE transcripts can then be extracted for advanced functional analysis.
INTRODUCTIONResearch efforts in molecular and cellular biology are vital to develop our understanding of, for example, the human central nervous system (CNS) function, and to dissect the functional complexity and the progress of diseases. These rapid advancements were mainly determined by the genomic, transcriptomic and diagnostic technological innovations (). These technologies, such as expression arrays and RNA sequencing, are applicable at the genome-wide scale, and their application results in more insightful information being produced, and this can lead towards more informed investigations and in turn through to clinical applications. However, the analyses and the volume of the data that are generated from these complex techniques are computationally challenging. To date, there is no standard protocol available to normalize and process the raw data from these experiments into a manageable format that can then be further interrogated on a personal computer and to align and assemble the data to a reference genome by scientists with no great experience in bioinformatics (). With expression arrays and RNA sequencing increasing in coverage and reducing in cost, they are now widely used with increasingly large sample sizes. Consequently, there is an ongoing need to develop and assess computationally efficient means of analysing 'big data'. Although there is no 'universal' approach to gene expression analysis, the conventional approach is to consider a model for one gene at a time, and consider if this particular gene is differentially expressed (DE) between any of the biological states considered, for example, as implemented in the limma package of R (). Naturally, this results in an extreme multiple testing situation, which is usually overcome by using an appropriate false discovery rate (FDR) control, such as the q-value procedure. The alternative is to consider a global model for all the expression data and replace the many thousands of null and alternative hypotheses by more global tests to dissect causes of variation in gene expression. Such an approach would have two advantages. First, transcripts are not independent identities, and there is much common information across the genome, so combining this would be expected to increase power of signal detection. Second, although a more philosophical point, we question whether it is best practice to consider many thousands of separate hypotheses: as students, we were taught the importance of writing out the null and alternative hypotheses *To whom correspondence should be addressed.for every statistical test, and this pragmatic approach would seem to fail here. So a global or genome-wide approach can be achieved by fitting linear mixed models (LMMs) to all the data, for example, as has been done by, with the variation between transcripts as another source of variation.suggested a similar process using hierarchical models. One potential limitation of the approach of using an LMM fitted to all the expression data is that it may not allow for differences in variability in expression across the biological states of the system under study. For example, in the human brain, some regions of the brain show greater levels of variation in gene expression than in other regions, However, improvements in LMM software, for example, as described inand Pinheiro and Bates (2000), allowing flexible variance structure modelling, make it worthwhile again considering the use of large-scale LMMs in expression data analysis. Fitting LMMs by themselves does not lead to the determination of which individual transcripts are DE, nor do the LMMs identify clusters of transcripts with similar expression profiles. However, fitting of finite mixture models has been used successfully in this endeavour, to separate out potential DE from nonDE transcripts (). The following article demonstrates how these two approaches can be adopted in the analysis of a large gene expression dataset. The methods are described to allow easy implementation in other situations. While some of this approach has been described in previous papers (), the present article provides additional detail and extends these methods. To illustrate the methods, one of the most comprehensive gene expression databases related to human brain tissue, namely, the UK Brain Expression Consortium (UKBEC), is used, highlighting different expression patterns across the brain regions ().
APPLICATION