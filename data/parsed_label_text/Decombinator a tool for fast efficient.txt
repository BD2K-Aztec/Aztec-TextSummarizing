High-throughput sequencing provides an opportunity to analyse the repertoire of antigen-specific receptors with an unprecedented breadth and depth. However, the quantity of raw data produced by this technology requires efficient ways to categorize and store the output for subsequent analysis. To this end, we have defined a simple five-item identifier that uniquely and unambiguously defines each TcR sequence. We then describe a novel application of finite-state automaton to map Illumina short-read sequence data for individual TcRs to their respective identifier. An extension of the standard algorithm is also described, which allows for the presence of single-base pair mismatches arising from sequencing error. The software package, named Decombinator, is tested first on a set of artificial in silico sequences and then on a set of published human TcR-b sequences. Decombinator assigned sequences at a rate more than two orders of magnitude faster than that achieved by classical pairwise alignment algorithms, and with a high degree of accuracy (488%), even after introducing up to 1% error rates in the in silico sequences. Analysis of the published sequence dataset highlighted the strong V and J usage bias observed in the human peripheral blood repertoire, which seems to be unconnected to antigen exposure. The analysis also highlighted the enormous size of the available repertoire and the challenge of obtaining a comprehensive description for it. The Decombinator package will be a valuable tool for further in-depth analysis of the T-cell repertoire. Availability and implementation: The Decombinator package is implemented in Python (v2.6) and is freely available at https://github. com/uclinfectionimmunity/Decombinator along with full documentation and examples of typical usage. Contact
INTRODUCTIONThe power of vertebrate adaptive immunity lies in its ability to synthesize and deploy an enormously diverse repertoire of antigen-specific receptors, by a unique process of imprecise recombination of DNA segments within the lymphocyte germline. Estimates for the number of possible B-and T-cell receptors that can be generated by this mechanism are in excess of 10 10 (). Because of this diversity, global analysis of immune repertoires and responses has lagged behind the structural understanding of receptor/antigen interactions. Advances in high-throughput sequencing (HTS) now offer the possibility of probing, cataloguing and analysing immune responses with unprecedented breadth and depth (). So far, this approach has been applied to only a handful of examples. However, with the rapid increase in highquality read length that can be achieved, and the fall in cost, the number of such datasets generated in the coming years is likely to increase rapidly. The ability to extract the maximum possible useful information from such datasets, whether to answer basic scientific questions or for more translational applications in diagnosis and disease stratification, will depend on simple and efficient bioinformatic pipelines with which to process and analyse the raw data. An individual T-cell receptor (we focus here on T-cell receptor analysis, although a similar approach is equally applicable to B cells) is made up of a heterodimeric b chain ($95% of T cells) or chain (5%). Each chain is made up of a variable and constant region, which is encoded by separate open reading frames and spliced together after transcription. The DNA sequence encoding the variable region is itself made up of two [variable (V) and joining (J) for ] or three [V, diversity (D) and J for b) minigenes. The-chain locus contains 45 V and 50 J gene segments (not including a number of pseudogenes) (). The b-locus contains 48 V, 2D and 13 J gene segments. During T-cell development in the thymus, one V, (D) and J minigene are recombined to give a contiguous open reading frame. Additional diversity is achieved by random deletion of germline nucleotides and addition of non-template nucleotides at the VJ junction ( and chains) or VD and DJ junctions (b and chains). To unambiguously define an individual TcR chain, therefore, it is necessary to specify the V, (D) J gene which is being used, and both deletions and additions occurring at each relevant junction. In this study, we focus on the TcR-b chain, as HTS data are publicly available (). As the two D region sequences are short and rather similar, it is difficult to unambiguously assign D gene usage of a specific b sequence. We, therefore, define each TcR-b sequence in terms of a unique five-part identifier. The first two parts identify the V and J *To whom correspondence should be addressed. regions used. The third part gives the number of 3 0 deletions of the V region. The fourth part gives the number of 5 0 deletions of the J region. The final part is the sequence found between V and J, which includes added nucleotides at the VD and DJ junctions, as well as any remaining nucleotides from the D region itself (). To generate identifiers from a large number (potentially 100 million from a single experiment using Illumina HiSeq technology) of short-read sequences rapidly and efficiently, we implement the algorithm described by Aho and Corasick (). Existing algorithms to analyse TcR and Ig sequences include IMGT/HighV-QUEST () that can only analyse up to 150 000 sequences per batch, SoDA (), which uses dynamic programming to analyse Ig sequences, typically taking up to 25 s/sequence to perform such analysis () and expectation maximization () to determine the most likely distributions over recombination variables (e.g. V and J gene usage and V and J deletions). Other alternatives include classic pairwise alignment implemented in R (), which can deal with larger batches of sequences than IMGT/HighV-Quest (), but it still lacks the efficiency to deal with the prodigious increase in sequence volume now obtainable from HTS. The AhoCorasick algorithm was originally developed for exact pattern set matching within a text string, and it has been widely used (). It constructs a finite-state machine that resembles a trie with additional links between the various internal nodes. The trie for any specific set of query texts need only be built once in advance; thus, the preprocessing time is O(n), where n is the sum of the lengths of all keywords to search for. The algorithm then works in O (n  m  x), where x is the number of keywords found in the query text, and m is the length of the text being queried. We first implement this algorithm using a set of unique identifying short-tag sequences from each known TcR-b V and J sequences as the set of query texts. The algorithm is then tested, both on artificial sets of sequences created in silico and on real sets of TcR sequences (). The standard form of the AhoCorasick algorithm requires exact matches between query and target. To accommodate realistic estimates of sequencing error using current HTS technology, we develop an extension of the basic algorithm. In comparison with pairwise alignment, the AhoCorasick modified algorithm increased speed of execution by over two orders of magnitude while still unambiguously assigning identifiers to 488% of sequences tested.. Pseudocode outlining our modification to the classic AhoCorasick algorithm, whereby a search is first adopted using the full-length V keywords, and the modification is used if no full-length V keyword is found. The classic AhoCorasick approach outputs all keywords found, along with their position within the sequence. An identical approach is adopted in searching for J keywords sequence of symbols, a keyword as a desired string to be found and a target as a string in which one searches for a keyword. The framework of Aho and Corasick allows one to search within a text string T of length m for the occurrence of all keywords within a pattern set P of length j, where P  P 1 , P 2 , :::, P j   1 If n  P j i1 P i j j, then the algorithm works in O (n  m  x) time, where x is the number of keywords in P that were found in T. This can be compared with complexity of order O(tm) for Smith Waterman pairwise alignment for each keyword of length t. A pattern-matching machine comprises a set of states, which are traversed by making a series of comparisons with characters of the target string. At each step, the new state reached depends on the next character seen in the target string. The pattern-matching machine's output is derived from goto, failure and output functions. The goto function is based on a keyword trie, such that all keywords are contained within the trie and listed from the root of the trie, and it maps a given state and an observed input character to a new state. The failure function maps one state to another, and it is used when a given state and an observed input character are not defined for a particular state. The failure function for each state is given by the longest suffix y already matched, such that y is the prefix of another keyword. Finally, the output function is defined for those states that, once reached, give one of the keywords in P. Thus, the beauty of this approach is that it makes only one pass through the target string to find all keywords present within it. The algorithm was implemented using Acora (implemented as a C extension embedded in Python using Cython) and BioPython. This combines the speed of C with the user-friendly interface of Python. This provides a fast efficient tool capable of large-scale HTS analyses, combined with a user-friendly high-level programming language appealing to those with limited programming experience. The keyword trie (i.e. the set of patterns to be identified) represents the set of V or J functional, prototypic alleles, of which there are 48Vb, 13Jb, 2Db, 45V and 50J in the human genome (). We consider only the b chain of the TcR for the purpose of testing our software on real sequences, as it is the only one for which HTS sequence data have been published (), although our software is tested on both TcR and TcRb in silico sequences. In line with many other studies (), we found that the two alternative Db genes are often too short (12 or 16 bp) and too similar to reliably assign in most sequences [$75% (after germline deletions have occurred; therefore, these are considered to be part of the additional non-template nucleotides at the VJ junction. Rather than searching for a whole V or J gene, each V or J region was represented by a short sequence (20 bp), which we call a 'tag', which unambiguously identifies one, and only one, gene segment. The 'tags' corresponding to each V or J segment were found by a simple exhaustive search, where each V (or J) region was split into a set containing all possible substrings, and then searched for in all other V (or J) regions.
DISCUSSIONWe have presented a novel application of an FSA developed by Aho and Corasick () to the problem of gene assignment and characterization of T-cell receptor sequences. By comparison with previous methods, such as spectratyping () and Amplicot (), HTS provides an unprecedented opportunity to analyse the TcR repertoire in depth. However, the quantity of raw data produced by this technology requires efficient ways to categorize and store the output for subsequent analysis. To this end, we have defined a simple five-item identifier that uniquely and unambiguously defines each TcR sequence. Fields one to four each contain a two-digit integer, which defines the V and J gene segment and the number of N-terminal V and C-terminal J deletions, respectively. The fifth field is a categorical variable, consisting of the string of contiguous nucleotides (A, T, C or G) from 3 0 V to 5 0 J. The length of the fifth field varies between zero and $40 bp (A, T, C and G). The identifier is, therefore, an economical way of storing all the required information about each sequence, and it is readily incorporated into a potentially large TcR database structure that can be searched and processed efficiently using established database management tools. It provides a simple framework to mitigate for sequencing errors while simultaneously allowing interrogation of such fundamental. Frequency distribution of V (left) and J (right) gene usage in distinct sequences obtained from three individuals. Non-uniform usage is clearly apparent for both sets of gene segments, whereas the overall pattern of usage seems to be largely conserved across multiple individuals. The distributions of usage are based on sequences obtained from three individuals, from two separate blood draws (), using only distinct sequences to avoid biases associated with the analysis of clonally expanded populations of cellsThe implementation was tested on the same set of 10 6 in silico and b sequences with 0, 0.1 and 1% sequencing error. Without modification of the classic Aho Corasick method, an extra 10% of sequences is lost (see also). This modification does not significantly affect the algorithm's efficiency. features as V and J region use and the distribution of germline deletions. Moreover, in using Decombinator to process raw sequence reads, the resultant file of identifiers produced makes further downstream analyses far easier to conduct for the uninitiated, taking away the challenging step of dealing with unprocessed data and translating it to a form which is accessible to even the most inexperienced programmer. Having defined the five-part identifier, the main aim of this study was, therefore, to develop an efficient way of mapping raw HTS sequence data to the identifier. Decombinator is an FSA based on a modified keyword trie, incorporating goto, failure and output functions that allow fast pattern matching by using information from characters that have already been matched. This approach is, therefore, completely different from previous studies using HTS to determine TcR repertoire diversity, which have relied on common methods, such as pairwise alignment () and BLAST-like alignment tool (BLAT) (). These methods (or variants thereof) have been successful in the context of large-scale genome sequencing studies, where the objective is to assign a series of. Overlap of shared sequences between Male1, Male2 and Female from sequences obtained from the study described in (). In each Venn diagram, the blood sample from Day 1 is shown on the left and the sample from Day 8 on the right. The numbers in each Venn diagram represent (lr) the number of distinct sequences found in the respective sample taken on Day 1, the number of distinct sequences common to both samples and the number of distinct sequences found in the respective sample taken on Day 8. Two separate samples, even from the same individual (leading diagonal), show only partial overlap, but display a greater proportion of shared sequences than samples taken from two different individuals. Distribution of the number of nucleotides found between 3 0 V and 5 0 J. Sequences were obtained from Male 1 from two separate blood draws, using only distinct sequences to avoid biases associated with clonally expanded populations. The region between 3 0 V and 5 0 J includes any remnants of the D region that remains after germline deletions. The distribution is quasi-normal, centred on a mean insert length of $12 or 13 bp. Distribution of germline V deletions. Non-uniform distribution of V deletions is apparent, in broad agreement with previously published data (). The distribution is based on sequences obtained from Male 1 from two separate blood draws, using only distinct sequences to avoid biases associated with the analysis of clonally expanded populations of cells. Distribution of germline J deletions. As with germline V deletions, a non-uniform distribution of J deletions is apparent, in broad agreement with previously published data (). The distribution is based on sequences obtained from Male 1 from two separate blood draws, using only distinct sequences to avoid biases associated with the analysis of clonally expanded populations of cells short reads to a large and diverse target (the wholegenome, for example). In contrast, the problem which is tackled here is distinguishing between a limited but highly overlapping series of queries in the most efficient manner possible. FSA matching is likely to perform well under these conditions, as it focuses on short regions of exact matches, and it can simultaneously search for several similar tags. Even tools that have been developed with TcR and Ig sequences in mind () still struggle to deal with the vast number of sequences obtained from HTS. In practice, Decombinator demonstrated remarkable efficiency, improving on equivalent pairwise matching algorithms by several orders of magnitude while retaining a high degree of accuracy. Although it is possible that fine tuning the parameters of pairwise algorithms specifically in the context of TcR data could improve their efficiency somewhat, it seems unlikely that they would achieve anything like comparable speed on the current datasets. However, a weakness of the original AhoCorasick FSA was that it required an exact match between query and target, and indeed many attempts have been made to extend the strategy to accommodate error or uncertainty in the query (). The major causes of mismatch are sequencing errors, whose frequency is well known (). In theory, polymerase chain reaction error can also introduce mismatches, although in practice the rate of polymerase chain reaction error using high-fidelity polymerases is much lower than sequencing error and can be largely ignored. To accommodate a realistic degree of sequencing error without compromising too much on efficiency, we developed a straightforward modification of the FSA, which identified sequences even if the location of the tags contained a single error. The algorithm delivered a significant increase in the proportion of sequences that could be assigned, even at error rates of 1%, which lies at the upper bound of that observed experimentally. Further modifications that could accommodate two or even more mismatches are unlikely to generate major benefits, as the chances of having two sequence errors within a typical 20 bp tag are low. However, we are exploring whether slower methods, including pairwise alignment or hidden Markov models might be useful in characterizing the small proportion of sequences which cannot be assigned by Decombinator. The major objective of this study was to produce a tool that can efficiently assign large number of T-cell short-read sequences with high accuracy. The strengths and limitations of the tool we have developed are discussed earlier in the text. However, it is also worth commenting on some features of the output generated from the published set of sequences we analyse in the article (). One striking feature is the non-uniform nature of the distribution of the different indices of the identifier. Even after restricting the analysis to distinct TcRs (i.e. counting each different identifier only once), so as to remove the potential effects of clonal expansion, both V and J usage is strikingly non-uniform. V20  1, for example, is found at a much higher frequency, whereas V15 or V16 are rarely present. This pattern is, at least in part, conserved across three unrelated individuals, making it extremely unlikely that it reflects any exposure to specific antigen. The pattern observed is similar to that described in several previous studies, (), suggesting that it is not an effect of bias introduced either by experimental or computational methodology. A recent HTS study of mouse V and J usage found similar bias, which was attributed to constraints imposed by physical features of the chromosome structure (). Non-uniform distributions of the number of deletions and insertions has also been observed previously (), and it presumably reflects molecular features of the recombinase machinery. Overall, learning the underlying distributions of each facet of the overall recombination process from this type of sequence data will be an important objective for future work, and it will allow us to build more realistic computational models for repertoire generation. An initial step in this direction, in which the experimentally observed V and J distributions were incorporated into the in silico generation algorithm is described in the article. The second feature that emerges immediately from analysis of the sequence data is the enormous diversity of TcRs which exist, which is reflected in the small overlaps observed between samples. The limited overlap between repeat samples from the same individual presents an obvious potential drawback, as each sample only captures a small proportion of the sequences present in that individual. Only sequences that occur at relatively high frequency are, therefore, likely to be sampled consistently by this approach. However, as the sequences at higher frequency are in fact likely to be those associated with specific immune responses, this may not prove to be a major limitation in studies focusing on events associated with antigen-specific challenges. The overlap between samples of different individuals is even smaller, reflecting not only differences in repertoire generation (e.g. the effects of major histocompatibility complex polymorphism and different histories of antigen exposure) but also the larger possible pool of all possible TcR sequences. Given the enormous number of possible sequences, it is in fact rather surprising that there are any sequences in common at all between any two randomly chosen individuals. The sequences found are often present at rather low frequency, suggesting that they may not reflect responses to antigen. Instead, such common sequences may correspond to 'public' sequences () described previously. Like preferential V and J region usage, these sequences found at unexpectedly high frequencies within the population may reflect some specific feature of the mechanisms for generation of diversity. Further work to investigate the characteristics of these common sequences is in progress.Comparing two separate samples from the same individual yields Jaccard index values of O 10 2   , whereas samples from two different individuals yields Jaccard index values of O 10 4   , indicating far greater overlap of sequences observed from samples from the same individual (see).
CONCLUSIONIn conclusion, we describe a five-part identifier that uniquely classifies all TcR sequences, and a computational tool that maps HTS reads to this identifier efficiently and accurately. The computational tool is based on the classic approach of Aho and Corasick to pattern matching, but it crucially includes a novel modification to correct for sequencing error. These tools, and the increasing application of HTS technology to lymphocyte antigen receptor analysis, will lead to a better understanding of the rules that regulate the TcR repertoire. The Decombinator package is implemented in Python (v2.6) and is freely available at https://github.com/uclinfectionimmunity/Decombinator along with full-documentation and examples of typical usage.