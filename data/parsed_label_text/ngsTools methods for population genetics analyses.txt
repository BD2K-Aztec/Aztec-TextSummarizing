Next-generation sequencing technologies produce short reads that are either de novo assembled or mapped to a reference genome. Genotypes and/or single-nucleotide polymorphisms are then determined from the read composition at each site, which become the basis for many downstream analyses. However, for low sequencing depths, e.g. 510Ã‚, there is considerable statistical uncertainty in the assignment of genotypes because of random sampling of homologous base pairs in heterozygotes and sequencing or alignment errors. Recently, several probabilistic methods have been proposed to account for this uncertainty and make accurate inferences from low quality and/or coverage sequencing data. We present ngsTools, a collection of programs to perform population genetics analyses from next-generation sequencing data. The methods implemented in these programs do not rely on single-nucleotide polymorphism or genotype calling and are particularly suitable for low sequencing depth data.
INTRODUCTIONNext-generation sequencing (NGS) technologies have revolutionized population genetics research by enabling unparalleled data collection from genomes or subsets of genomes from many individuals. Current technologies produce short fragments of sequenced DNA called 'reads' that are either de novo assembled or mapped to a pre-existing reference genome. This leads to chromosomal positions being sequenced a variable number of times across the genome, usually referred to as the sequencing depth. Individual genotypes are then inferred from the proportion of nucleotide bases covering each site after the reads have been aligned. Low sequencing depth, along with high error rates stemming from base calling and mapping errors, causes single-nucleotide polymorphism and genotype calling from NGS data to be associated with considerable statistical uncertainty. Recently, probabilistic models that take these errors into account have been proposed to accurately assign genotypes and estimate allele frequencies (e.g.). We present ngsTools, a collection of programs for population genetics analyses that use methods which account for the statistical uncertainty of NGS data. The implemented methods are specially tailored for low-depth sequencing datasets with multiple individuals and populations, and can incorporate deviations from HardyWeinberg equilibrium. The inputs for these programs are the files generated by ANGSD, a software for reading and handling NGS data (popgen.dk/angsd).
CONCLUSIONAlthough sequencing costs are decreasing, NGS of large samples is still expensive causing many researchers to focus on low-depth samples. This is particularly true for nonhuman nonmodel organisms for which research funding typically does not provide for deep sequencing of many individuals. Analyses of data from such species are particularly challenging because imputationbased methods used in human genomics are not available and because they may suffer from high levels of inbreeding. This beckons for new and efficient computational methods that directly address the problem of genotyping uncertainty on NGS data. The methods provided by ngsTools are designed with this problem in mind. ngsTools provides tools to accurately estimate genetic variation in case of low-coverage sequencing data. The individual methods have been previously tested providing extensive documentation of their statistical and computational properties. We here report on the availability of an integrated open source computer package facilitating access to the methods for the broader research community. ngsTools is available on a public repository for shared development so that additional methods can be developed under this framework and integrated into the software package.