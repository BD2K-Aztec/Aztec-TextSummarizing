Motivation: A basic issue for translational genomics is to model gene interaction via gene regulatory networks (GRNs) and thereby provide an informatics environment to study the effects of intervention (say, via drugs) and to derive effective intervention strategies. Taking the view that the phenotype is characterized by the long-run behavior (steady-state distribution) of the network, we desire interventions to optimally move the probability mass from undesirable to desirable states Heretofore, two external control approaches have been taken to shift the steady-state mass of a GRN: (i) use a user-defined cost function for which desirable shift of the steady-state mass is a by-product and (ii) use heuristics to design a greedy algorithm. Neither approach provides an optimal control policy relative to long-run behavior. Results: We use a linear programming approach to optimally shift the steady-state mass from undesirable to desirable states, i.e. optimization is directly based on the amount of shift and therefore must out-perform previously proposed methods. Moreover, the same basic linear programming structure is used for both unconstrained and constrained optimization, where in the latter case, constraints on the optimization limit the amount of mass that may be shifted to ambiguous states, these being states that are not directly undesirable relative to the pathology of interest but which bear some perceived risk. We apply the method to probabilistic Boolean networks, but the theory applies to any Markovian GRN. Availability: Supplementary materials, including the simulation results, MATLAB source code and description of suboptimal methods
INTRODUCTIONGenetic regulatory networks (GRNs) refer to a class of models describing the multivariate functional relationships among a cohort of genes or their products. From a graphical perspective, genes are nodes in this network, and edges describe regulatory relationships between genes. These networks aim to model cellular control and how abnormal cell functions result from one or several breakdowns in the regulatory mechanisms. Thus, GRNs are an essential part of translational medicine, whose ultimate goal is to develop therapies based on the disruption or mitigation of aberrant gene function contributing to the pathology of a disease (). Therapies usually involve some procedure and several drug candidates acting on various gene products with the aim of mitigating undesirable gene functions. Developing therapeutic methods in the context of GRNs involves designing intervention strategies to alter the dynamics of the gene activity profiles (GAPs) of the network in some desired manner, thereby identifying potential drug targets (). Modeling GRNs via Markovian dynamical networks has received much attention because they capture uncertainty intrinsic to the interactions among genes or gene products at different levels. Furthermore, one can use the rich theory of Markov decision processes (MDPs) to formulate optimal intervention problems. In the present article, we choose probabilistic Boolean networks (PBNs) () as our reference model for GRNs. The transition probabilities of PBNs are characterized by their associated Markov chains, and PBNs have played the major role in intervention studies. Assuming that the collection of all possible GAPs constitutes the state space, we can partition it into desirable and undesirable cellular states according to the expression values of a given set of genes. An undesirable state can be associated with a phenotype representative of a cancerous state; e.g. cell growth in the absence of growth factors is undesirable. Although there might exist some states in the desirable set that do not represent healthy conditions, they can be classified as 'desirable' because they are not associated with the particular pathology defining the undesirable set (). Once a model for the underlying GRN is assumed and desirable and undesirable sets are recognized, the objective is to find an optimal therapeutic strategy (control policy) with respect to a predefined objective function to drive the dynamics of the network from undesirable states to the desirable ones. This problem has been extensively studied within two basic intervention approaches in the context of PBNs, external control and structural intervention. External control is generally based on flipping (or not flipping) the value of a specific gene (or possibly more than one), called the control gene. As the dynamic behavior of a PBN can be represented by a finite-state Markov chain, the first proposed intervention approach was to determine an optimal single-gene perturbation to the network based on mean first-passage times in Markov chains (). Attention, later, *To whom correspondence should be addressed. turned to dynamic programming-based finite-horizon () and infinite-horizon external control in which the steady-state distribution (SSD) is altered (). These works were followed by several articles developing approaches to external intervention that take into account practical issues that arise from therapeutic constraints (), biological complexity () and computational limitations (). Structural intervention, on the other hand, involves a one-time change in the network regulatory structure (wiring) so that the long-run behavior of the network is altered in a desired manner. Given a collection of potential structural changes, the problem is to find an optimal structural intervention resulting in a maximum alteration of the SSD toward the direction of desirable states and away from undesirable states (). In the main, optimal infinite-horizon intervention for GRNs has involved the specification of a cost function based on the current state of the system and the desirability of potential transitions. Thus, the long-run effect of the optimal policy on the network dynamics becomes a by-product of this cost functionbased optimization problem. As phenotype is associated with steady-state behavior, as in the case of attractor cycles in BNs and PBNs (), from a practical perspective, it would be better to determine optimality directly in terms of long-run behavior. Direct optimization has been addressed to some extent in, where the authors propose three classes of greedy stationary intervention policies that bypass the need for a user-defined cost function and directly use long-run behavior as the optimization criterion to reduce the mass of the SSD corresponding to undesirable states and increase the mass corresponding to desirable states (). In this article, we rigorously formulate this intervention problem and provide an optimal intervention policy with a computational complexity equivalent to solving a linear program (LP) optimization problem in Section 2.4 for a general cost function and in Section 2.5 for maximal phenotype alteration. We also consider a variant of this optimization problem where one might constrain the steady-state probability mass of some 'ambiguous' states in the network while finding the optimal intervention policy. This is especially important in therapeutic methods because it is prudent to avoid introducing new probability mass to states associated with unknown phenotypes (). We demonstrate the performance of optimal policies using synthetically generated networks and two real networks derived from the metastatic melanoma and mammalian cell cycle.