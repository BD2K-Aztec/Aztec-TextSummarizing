Motivation: Markov models are very popular for analyzing complex sequences such as protein sequences, whose sources are unknown, or whose underlying statistical characteristics are not well understood. A major problem is the computational complexity involved with using Markov models, especially the exponential growth of their size with the order of the model. The probabilistic suffix tree (PST) and its improved variant sparse probabilistic suffix tree (SPST) have been proposed to address some of the key problems with Markov models. The use of the suffix tree, however, implies that the space requirement for the PST/SPST could still be high. Results: We present the probabilistic suffix array (PSA), a data structure for representing information in variable length Markov chains. The PSA essentially encodes information in a Markov model by providing a time and space-efficient alternative to the PST/SPST. Given a sequence of length N, construction and learning in the PSA is done in O(N) time and space, independent of the Markov order. Prediction using the PSA is performed in O(mlog N ||) time, where m is the pattern length, and is the symbol alphabet. In terms of modeling and prediction accuracy, using protein families from Pfam 25.0, SPST and PSA produced similar results (SPST 89.82%, PSA 89.56%), but slightly lower than HMMER3 (92.55%). A modified algorithm for PSA prediction improved the performance to 91.7%, or just 0.79% from HMMER3 results. The average (maximum) practical construction space for the protein families tested was 21.58±6.32N (41.11N) bytes using the PSA, 27.55±13.16N (63.01N) bytes using SPST and 47±24.95N (140.3N) bytes for HMMER3. The PSA was 255 times faster to construct than the SPST, and 11 times faster than HMMER3.
INTRODUCTIONMarkov models are often used for modeling complex sequences, such as protein sequences, whose underlying statistical characteristics are not well understood. This is especially the * To whom correspondence should be addressed. case when the sequences exhibit short-term memory. For a shortterm memory of length, say L, the sequences can be modeled using Markov models of order L, or using Hidden Markov Models (. The models provide efficient mechanisms to compute the required conditional probabilities, and also for generating sequences from the models. The problem is that the size of Markov models increases exponentially with increasing memory length L. Thus, they are practical only for low-order models with short-memory lengths. This leads to another problem: such low-order Markov models often provide a poor approximation of the true sequence being modeled. It is known that learning and inference with HMMs is computationally very challenging (). Probabilistic suffix models such as probabilistic suffix trees (PSTs) have been proposed byto address some of the key problems with Markov models. They showed the equivalence between PSTs and a subclass of probabilistic finite automata (PFA) called probabilistic suffix automata (PFA/PSA): for a given PST, there is an algorithm to construct a PFA/PSA whose size is the same as that of the PST within a constant factor. Further, the distribution generated by a PST is guaranteed to be within a small distance from that generated using the PSF/PSA, as measured by the KullbackLeibler divergence. Their probabilistic suffix models, however, require O(LN 2 ) time and space to construct, where N is the sequence length. The algorithm to construct the PFA/PSA from the PST also runs in O(LN 2 ) time. Later, () showed how the PST can be constructed in O(N) time, independent of the order L, using traditional suffix links used in constructing suffix trees (STs), and the notion of reverse suffix links. PSTs and probabilistic suffix automatons are related to contextbased models which are extensively used in sequence prediction and data compression. The use of these context models as a surrogate for variable length Markov models with applications in sequence prediction are reviewed in (). Earlier, the PST was used to model DNA sequences in (). Modeling and prediction of protein families using the PST was reported in (). Importantly, using the PST, prediction was performed without any prior alignment of the protein sequences. () proposed an improved variation of the PST, called sparse probabilistic suffix tree (SPST) for use in protein family prediction. Unlike the usual PST, the SPST allows some contexts to be grouped together to form an equivalent class. Essentially, this grouping results in a form of pruning of the PST, however, the node depths are not fixed, but could vary depending
DiscussionThe proposed data structure is based on the standard SA, and hence can be implemented using compressed index structures such as compressed suffix arrays (CSAs;).showed that for a general alphabet, , with ||  2, the CSA can be constructed in O(N log || N) processing time and stored in (1+ 1 2 loglog || N)N log||+5N +O N loglogN bits, such that each lookup operation can be performed in O loglog || N time. Similarly, in theory, the PST or SPST can be implemented using compressed suffix trees (CSTs). () showed that a CST with full functionality (including suffix links) can be constructed in O(N) time, and represented using O(N log||) bits of storage space. He showed that the CST can be implemented using (NH h + 6N +o(N)) bits, where H h is the order-h entropy of the original sequence. For the CSA, the storage requirement can be reduced to (NH h +O N loglogN log || N bits. These will no doubt result in a lower memory requirement when compared with our current result using standard SAs. However, apart from the usual slowdown in processing speed using CSAs or CSTs, some types of traversals and new attributes required for the PSA (or PST/SPST) could be quite difficult to implement on a standard CSA (or CST). This type of improvement will be an interesting direction for further work on space-efficient probabilistic suffix structures. The proposed methodIndicated results are true positive rates in percentage (%). For PSA, w = 10 using average probability, and w = 80 using maximum probability. is not directly affected by the computer word length (32 bit or 64 bit), so far as the data structure including input data can fit into main memory. For very large datasets where this may not be the case, methods will need to be considered for more efficient I/O operations (Ferragina, 2010). In terms of prediction accuracy, a potential approach to improve the performance will be to consider prediction based on short fragments of the test sequence. These will provide more locality in the analysis, and hence may be able to improve the ability for detecting distant homology. One way to use the predictions from different fragments will be to choose the one with the family with the maximum predicted probability over all the fragments as the predicted family. However, other approaches are also possible, for instance by combining the predictions using some well defined protocol. We tested this approach, using overlapping windows of various sizes, w. The best results were obtained at w = 10 (86.80%) using average of window probabilities, and at w = 80 (91.76%) using the maximum probability.provides a summary of the results, showing that with the improvement, the average PSA prediction accuracy is now 0.8% below that of HMMER3. Detailed results are in Supplementary Table S1. Further, as can be seen from the results (Supplementary Tables S1 and S2), there are cases where the PSA performed signifcantly better than HMM, e.g. by as much as 40% for FAD_binding_3 or 59% (for FA_hydroxylase). Thus, a potential improvement could be obtained by combining prediction results from the PSA with those from non-PST-based methods, such as the profile HMM, for instance, using an approriate classifier fusion scheme (). We acknowledge some basic problems with using VLMMs in general (whether PST, SPST or PSA). One problem is that these tend to provide a global characterization of the sequence, and hence may have problems when similarity between sequences is localized within a smaller region of the proteins, or is interspersed between islands of non-similar regions. This will thus affect the ability to detect distant homology, or the ability to handle a previously unseen mutation in a protein sequence that is being analyzed. This last problem is akin to the zero-frequency-problem, and various methods have been proposed, for instance, using a default probability distribution, or a Poisson process model (). A related problem with the current PSA implementation is that the conditioning context for the symbol probability reverts to the longest observed suffix, meaning that the conditional probability would be based on a small number of counts. This may not always correspond to the true probability. One solution would be to use some kind of
CONCLUSIONWe have presented the PSA, a data structure for representing information in VLMMs. The PSA provides the same functionality as the PST, but at a significantly reduced time and space requirement. Given a sequence of length N, construction and learning in the PSA is done in O(N) time and O(N) space, independent of the Markov order. Prediction using the PSA is performed in O mlog N || time, where m is the pattern length, and is the symbol alphabet. We have shown practical results on the comparative performance of PSA, SPST and HMMER3 using protein families from Pfam 25.0. In terms of modeling and prediction, SPST and PSA produce equivalent results (SPST 89.82%, PSA 89.56%). These were slightly lower than the 92.55% obtained using HMMER3 on the same dataset. An improved version of PSA prediction (predicting families using fragments from the test sequence) improved PSA perfomance to 91.7%, thus reducing the performance gap with HMMER3 to just 0.79%. The average practical construction space for the protein families tested was 21.586.32N bytes using the PSA, 27.5513.16N bytes using SPST and 4724.95N bytes for HMMER3. The maximum practical space needed was 42.11N for PSA, 63.01N for SPST and 140.3N for HMMER3. With respect to construction time, the PSA was 255 times faster than SPST, and 11 times faster than HMMER3. Our results show that, the PSA provides an accuracy similar to that of PST/SPST and HMMER3 in protein family prediction, but at a significantly lower time and space requirement.