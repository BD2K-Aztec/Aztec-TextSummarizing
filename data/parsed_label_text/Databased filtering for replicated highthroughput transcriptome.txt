
INTRODUCTIONDuring the past 5 years, next-generation high-throughput sequencing (HTS) technology has become an essential tool for genomic and transcriptomic studies. In particular, the use of HTS technology to directly sequence the transcriptome, known as RNA sequencing (RNA-seq), has revolutionized the study of gene expression by opening the door to a wide range of novel applications. Unlike microarray data, which are continuous, RNA-seq data represent highly heterogeneous counts for genomic regions of interest (typically genes) and often exhibit zero-inflation and a large amount of overdispersion among biological replicates; as such, a great deal of methodological research (e.g.) has recently focused on appropriate normalization and analysis techniques that are adapted to the characteristics of RNA-seq data; seefor a review of RNA-seq technology and analysis procedures. As with data arising from previous technologies, such as microarrays or serial analysis of gene expression, HTS data are often used to conduct differential analyses. In recent years, several approaches for gene-by-gene tests using gene-level HTS data have been proposed, with the most popular making use of Poisson (), overdispersed Poisson () or negative binomial distributions (). Because a large number of hypothesis tests are performed for gene-by-gene differential analyses, the obtained P-values must be adjusted to address the fact that many truly null hypotheses will produce small P-values simply by chance; to address this multiple testing problem, several well-established procedures have been proposed to adjust P-values to control various measures of experiment-wide false positives, such as the false-discovery rate. Although such procedures may be used to control the number of false positives that are detected, they are often at the expense of the power of an experiment to detect truly differentially expressed (DE) genes, particularly as the number of genes in a typical HTS dataset may be in the thousands or tens of thousands. To reduce this impact, several authors in the microarray literature have suggested the use of data filters to identify and remove genes that appear to generate an uninformative signal () and have no or little chance of showing significant evidence of differential expression; only hypotheses corresponding to genes that pass the filter are subsequently tested, which in turn tempers the correction needed to adjust for multiple testing. In recent work,advocate for the use of independent data filtering, in which the filter and subsequent test statistic pairs are marginally independent under the null hypothesis, and the dependence structure among tests remains largely unchanged pre-and post-filter, ensuring that post-filter P-values are true P-values. For such an independent filter to be effective, it must be positively correlated with the test statistic under the alternative hypothesis; indeed, it is this correlation that leads to an increase in detection power after filtering. In addition, Bourgon et al. demonstrate that non-independent filters for which dependence exists between the filter and test statistic (e.g. making use of condition labels to filter genes with average expression in at least one condition less than a given threshold), can in some cases lead to a loss of control of experiment-wide error rates. Several ad hoc data filters for RNA-seq data have been used in recent years, including filtering genes with a total read count smaller than a given threshold () and filtering genes with at least one zero count in each experimental condition (); however, selecting an arbitrary threshold *To whom correspondence should be addressed.  The Author 2013. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com value to filter genes in this way does not account for the overall sequencing depth or variability of a given experiment. One exception to these ad hoc filters is the work of, in which a comparison between expression levels of exonic and intergenic regions was used to find a threshold for detectable expression above background in various human and mouse tissues, where expression was estimated as Reads Per Kilobase per Million mapped reads (RPKM) (). The threshold of 0.3 RPKM identified in this work has in turn been applied to several other studies (e.g. Canovas). However, to our knowledge, although filters for read counts are routinely used in practice, little attention has been paid to the choice of the type of filter or threshold used or its impact on the downstream analysis. In this article, we propose a novel data-based procedure to choose an appropriate filtering threshold based on the calculation of a similarity index among biological replicates for read counts arising from replicated high-throughput transcriptome sequencing data. This technique provides an intuitive datadriven way to filter RNA-seq data and to effectively remove genes with low constant expression levels. Our proposed filtering threshold may be useful in a variety of applications for RNA-seq data, including differential expression analyses, clustering and co-expression analyses, and network inference.