Motivation: Graphical models are often employed to interpret patterns of correlations observed in data through a network of interactions between the variables. Recently, Ising/Potts models, also known as Markov random fields, have been productively applied to diverse problems in biology, including the prediction of structural contacts from protein sequence data and the description of neural activity patterns. However, inference of such models is a challenging computational problem that cannot be solved exactly. Here, we describe the adaptive cluster expansion (ACE) method to quickly and accurately infer Ising or Potts models based on correlation data. ACE avoids overfit-ting by constructing a sparse network of interactions sufficient to reproduce the observed correlation data within the statistical error expected due to finite sampling. When convergence of the ACE algorithm is slow, we combine it with a Boltzmann Machine Learning algorithm (BML). We illustrate this method on a variety of biological and artificial datasets and compare it to state-of-the-art approximate methods such as Gaussian and pseudo-likelihood inference. Results: We show that ACE accurately reproduces the true parameters of the underlying model when they are known, and yields accurate statistical descriptions of both biological and artificial data. Models inferred by ACE more accurately describe the statistics of the data, including both the constrained low-order correlations and unconstrained higher-order correlations, compared to those obtained by faster Gaussian and pseudo-likelihood methods. These alternative approaches can recover the structure of the interaction network but typically not the correct strength of interactions , resulting in less accurate generative models. Availability and implementation: The ACE source code, user manual and tutorials with the example data and filtered correlations described herein are freely available on GitHub at https://github.com/kkrizanovic/NanoMark.
IntroductionInterpreting patterns of correlations in data is a fundamental problem across scientific disciplines. A common approach to this problem is to infer a simple graphical model that explains the statistics of the data through a network of effective interactions between the variables, which may then be used to generate new predictions (). The goal of this approach is to disentangle the direct interactions between variables from their correlations, which arise through a combination of direct and indirect effects. Here, we focus on a particular family of undirected graphical models, referred to as Potts models in the language of statistical physics, which have recently been applied to study a wide variety of biological systems. Applications include inference of the effective connectivity of populations of neurons, and their patterns of firing activity, based on data from multi-electrode recordings (), and the prediction of protein contact residues () and the fitness effects of mutations () based on the analysis of multiple sequence alignments (MSAs). Unfortunately, the inference of Potts models from data is challenging. The computational time required for naive Potts inference algorithms scales exponentially with the system size, rendering the problem intractable for realistic systems of interest. Various approximations have been employed to combat this problem, including Gaussian and mean-field inference (), perturbative expansions () and pseudo-likelihood methods (). These approximate methods can successfully capture the general structure of the network of interactions, recovering, in particular, contact residues in the threedimensional structure of protein families (), but the resulting models typically give a less accurate statistical description of the data (). Alternately, algorithms based on iterative rounds of Monte Carlo simulation () are capable of inferring models that accurately reproduce the observed correlations, but they are typically slow to converge. Here, we describe an extension of the adaptive cluster expansion (ACE) method, originally devised for binary (Ising) variables (), to more general (Potts) variables taking multiple categorical values. We also describe new computational methods for faster inference, including a Monte Carlo learning procedure and the optional incorporation of prior knowledge about the structure of the interaction graph. The algorithm has been successfully applied to real data with as many as several hundred variables, including studies of neural activity in the retina and prefrontal cortex (), human immunodeficiency virus (HIV) fitness based on protein MSA data (), and lattice protein models (). Below we illustrate the application of this method to both real and artificial datasets. We show that models inferred by ACE give an excellent reconstruction of the statistics of the data. They also accurately recover, considering sampling limitations, true underlying model parameters when they are known, and can achieve comparable performance to state-of-the-art methods for predicting structural contacts in protein family data. We compare these results to those obtained using other approximate inference methods, focusing in particular on pseudolikelihood methods.
DiscussionPotts models have been successfully applied to study a variety of biological systems. However, the computational difficulty of the inverse Potts problem, i.e. the inference of a Potts model from correlation data, has presented a barrier to their use. Here, we presented ACE, a flexible, easy-to-use method for solving the inverse Potts problem, which can be applied to analyze a wide variety of real and synthetic data. We also provide tools for automatically generating correlation data from multiple sequence alignments (MSA), making the analysis of this type of data even more accessible. Here, we have adapted the complexity of the inferred Potts models to the level of the sampling in the data. This is achieved by regrouping less frequently observed Potts states into a unique state (according to a threshold on entropy or frequency), then by a sparse inference procedure that omits interactions that are unnecessary for reproducing the statistics of the data to within the error bounds due to finite sampling. On artificial data we verified that compression of the number of Potts states allows for a faster and more precise inference of the uncompressed model parameters while reducing overfitting. The methods of compression that we describe here can also be applied to other inference methods (including, for example, the DCA and plmDCA approaches discussed above), a topic of future study. In addition, as described above ACE yields sparser models when sampling is poor, leading to more robust inference. This method allows for the simple construction of models from various types of data, which can then be used to predict the evolution of experimental systems and their response to perturbations. Previous work has demonstrated promising applications of such models in a variety of different biological contexts. In neuroscience, the analysis of multi-electrode recordings has led to models that identify cell assemblies, which are thought of as basic units of neural computation and memory ().Here, we show the top 100 predicted contacts, with true predictions in orange and false predictions in blue. Other contact residues in the crystal structure are shown in gray. For true positives and other contact residues, close contacts (6 ) are darkly shaded and further contacts (8 ) are lightly shaded. The upper and lower triangular parts of the contact map give predictions for the inferred model with strong regularization/no compression (c  1) and weak regularization/ high compression (c  2=B), respectively. (b) Precision (number of true predictions divided by the total number of predictions) as a function of the number of contact predictions for close contact residues that are widely separated on the protein backbone (i  j  4). Results using ACE compare favorably with those from DCA () and are competitive with those from plmDCA ()