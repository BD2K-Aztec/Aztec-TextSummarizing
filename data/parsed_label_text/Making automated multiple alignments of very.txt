Motivation: Recent developments in sequence alignment software have made possible multiple sequence alignments (MSAs) of 4100 000 sequences in reasonable times. At present, there are no systematic analyses concerning the scalability of the alignment quality as the number of aligned sequences is increased. Results: We benchmarked a wide range of widely used MSA packages using a selection of protein families with some known structures and found that the accuracy of such alignments decreases markedly as the number of sequences grows. This is more or less true of all packages and protein families. The phenomenon is mostly due to the accumulation of alignment errors, rather than problems in guide-tree construction. This is partly alleviated by using iterative refinement or selectively adding sequences. The average accuracy of progressive methods by comparison with structure-based benchmarks can be improved by incorporating information derived from high-quality structural alignments of sequences with solved structures. This suggests that the availability of high quality curated alignments will have to complement algorithmic and/or software developments in the long-term. Availability and implementation: Benchmark data used in this study are available at http://www.clustal.org/omega/homfam-20110613-25. tar.gz and http://www.clustal.org/omega/bali3fam-26.tar.gz.
INTRODUCTIONMultiple sequence alignments (MSAs) of many thousands of protein sequences are becoming commonplace. The biggest families in Pfam () have 4100 000 sequences or domains and will expand greatly as new genome sequences are being sequenced. MSAs are an integral part of how Pfam and other domain databases are used and maintained. Metagenomics research routinely involves the use of alignments of tens of thousands of sequences, and almost all phylogenetic analysis involves generating an MSA as a starting point. However, the generation of the MSA can be computationally too intensive for very large scale phylogenetic analysis. Recent work on predicting protein structure from sequence alignments is based on having very high quality alignments of many thousands of sequences (e.g.). Alignments of thousands of sequences have also been used in the area of virus classification () and epistasis (). Only a few of the standard MSA packages are capable of aligning tens of thousands of sequences. In a recent article, we described a new package called Clustal Omega (), which makes it practical to align 4100 000 protein sequences on a desktop computer and is as accurate as some of the most computationally demanding methods that can only align a few hundred sequences. The PartTree program () of the MAFFT package () and Kalign () can also make alignments of this size, although with a lower accuracy. Some studies (;) suggest that the quality of an alignment may increase as more sequences are added. This is only true if the newly added sequences are few and carefully chosen. As of yet, no systematic analyses have been conducted for large numbers of homologous sequences. In this article, we look at some of the issues that occur when making alignments of 10050 000 sequences using standard automatic MSA packages. For small alignments, we confirm a limited increase in accuracy, however, only if the added sequences are carefully chosen. We find a universal trend towards marked decrease in alignment accuracy as large numbers of sequences are added indiscriminately. We explore strategies that attenuate this deterioration, but they are useful only in certain cases. The strategy that best preserves alignment accuracy with very large datasets is to use a very high quality alignment of a small subset of the sequences to help guide the alignment. This suggests that very large alignments of high quality may be possible, but only if very high quality alignments such as those from structure superpositions or expert curated alignments are available.
DISCUSSIONAll of the standard automatic MSA packages behave very similarly, when the number of sequences to be aligned is increased into the thousands. Although few families exhibit a marked improvement in accuracy, the average accuracyas measured on structure-based benchmarksdecreases steadily. This raises two obvious questions: what is the reason for the fall off and how can it be fixed? The simplest explanation for the fall off in accuracy is attrition owing to the accumulation of noise and/or alignment errors as sequences are added. All of the widely used algorithms are based directly or indirectly on 'progressive alignment', which aligns the sequences according to the branching order in a 'guide-tree'. This requires a series of alignment steps, at any of which alignment errors can be made. These errors cannot be reversed, except by iteration of the alignment process. Such alignment errors occur less frequently with programs such as T-Coffee that use consistency (), but such programs cannot easily cope with 41000 sequences. The presence of fragments,. Change in HomFam TC score for different algorithms and different sampling schemes. (a) Clustal Omega, (b) Kalign, (c) MAFFT LINS-i, (d) Probalign. Random sampling with bullets and thicker lines, sampling of sequences of high similarity with circles, of low similarity with crosses and of in-between similarity with diamonds and boxes frameshifts, swapped domains and very large insertions or deletions will aggravate this situation. With small numbers of sequences, the algorithms have proved to be very robust for general use. With very large numbers of sequences, however, the number of opportunities for irreversible alignment errors increases steadily. Even if during progressive alignment, only one sequence alignment in a thousand introduces a serious error, in a dataset of 100 000, this will occur 100 times. In large datasets, the scope for errors is simply very great. By fixing the guide-tree topology, we were able to separate out the effects of possible errors in guide-tree construction from alignment errors. Guide-tree construction certainly has an effect on alignment accuracy, but it is not the main source of error here. Iteration does help to delay the fall off in accuracy to an extent. We tested various combinations of iteration of guide-tree construction and alignment. For small-to-medium-sized datasets, the effects are noticeable, but the fall off in accuracy inevitably follows. Either the iteration strategy needs to be changed or it needs to be done more intensively. This would have the effect of greatly increasing alignment times. Carefully choosing the sequences to be aligned certainly has a beneficial effect, again, for modest increases in dataset size. We observe the best results when sequences of intermediate similarity are added.clearly demonstrates that sequences that are very similar did not improve accuracy. Perhaps, if huge alignments are desired, new sequences to be added to the dataset must be selected carefully. Using progressive alignment packages is not the only way to make very large alignments, however. In the Pfam database, HMMER is used in a simple process, to add sequences one at a time to a smaller seed alignment. The accuracy of such alignment schemes has not been tested much, and it has probably been assumed that the accuracy is low. The full Pfam alignments are not intended as high-accuracy alignments. In Clustal Omega, there is a facility to use a pre-existing HMM to help the alignment of a new set of sequences, in a process called EPA. In the long-term, the most obvious solution to the issue of how to make very large alignments may be to use smaller high-quality alignments as seeds or 'external profiles' and algorithms for extending alignments such as Pagan (), PaPaRa () or as explained in (). Progressive alignment alone can make the alignments, but the accuracy will be a serious issue without new algorithms or strategies being developed.