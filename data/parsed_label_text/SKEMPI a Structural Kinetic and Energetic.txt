Motivation: Empirical models for the prediction of how changes in sequence alter protein–protein binding kinetics and thermodynamics can garner insights into many aspects of molecular biology. However, such models require empirical training data and proper validation before they can be widely applied. Previous databases contained few stabilizing mutations and no discussion of their inherent biases or how this impacts model construction or validation. Results: We present SKEMPI, a database of 3047 binding free energy changes upon mutation assembled from the scientific literature, for protein–protein heterodimeric complexes with experimentally determined structures. This represents over four times more data than previously collected. Changes in 713 association and dissociation rates and 127 enthalpies and entropies were also recorded. The existence of biases towards specific mutations, residues, interfaces, proteins and protein families is discussed in the context of how the data can be used to construct predictive models. Finally, a cross-validation scheme is presented which is capable of estimating the efficacy of derived models on future data in which these biases are not present. Availability: The database is available online at
INTRODUCTIONAscertaining how the alteration of protein sequence influences the thermodynamics and kinetics of binding is of fundamental importance to many disparate areas of biomedical research. Among others, its application is central to antibody engineering (), interaction design (), locating binding hotspots (), determining the energetics of specific moiety contacts (for instance,), uncovering binding mechanisms (), characterizing transitions states (for instance,), determining the diversity of affinities and specificities of extant proteins () as well as ascertaining the functional consequences of potentially pathological mutations (for instance,). Should it become possible to estimate how single or multiple mutations influence the kinetics and thermodynamics of protein binding on a much greater scale, many further investigations would materialize. For instance, determining the consequences of polymorphisms and mutations from initiatives such as the 1000 genomes project (), the Cancer Genome Project (http://www.sanger .ac.uk/genetics/CGP/) or the Cancer Genome Atlas (http:// cancergenome.nih.gov/) promises to yield significant insights into the mechanisms of disease on a systemic level. Yet, the amount of data being produced is staggering. Applied to genetic data obtained in a clinical setting, such methods could find use in personalized medicine. Used to characterize sequence-function landscapes, these methods could test theoretical models regarding the relationship between mutational robustness and evolvability (). Applied to orthologs and paralogs, this would indicate how affinities and specificities vary across phylogenetic trees, giving insights in how organisms have adapted to their niches at the interaction network level. Furthermore, a more widespread application of de novo interface design could be used to rewire interaction networks. Incisive and large-scale experimental studies have illuminated many aspects of protein binding (), whereby the binding of mutants is typically characterized using surface plasmon resonance, isothermal titration calorimetry or spectroscopic techniques. However, these approaches are resource intensive, which prohibits their use in the high-throughput characterization of sequence-function landscapes in many systems. Recourse can be made in combinatorial methods that circumvent the need for direct physical measurement (). However, these methods can suffer from challenges arising from library generation, display, selection and sequencing (). Currently, it is unlikely that any of these methods alone will be able to keep up with the abundance of data generated by modern genomics initiatives. In light of this and ever increasing computational resources, computational models are an attractive option where structural data are available. Methods such as MM-PBSA and MM-GBSA can be used to derive free energies from structural ensembles generated using Monte Carlo sampling or molecular dynamics simulation. *To whom correspondence should be addressed. However, errors can arise from force field approximations and insufficient conformational sampling, and the production of trajectories can be costly (). Promisingly, it is possible to produce empirically parameterized functions for the physical modelling or statistical inference of G () or even H, S, k on and k off. Such empirical functions have the potential to be highly efficient and, where the role of conformational flexibility is limited, highly accurate. These methods are limited by the availability of training data; with greater data, finer inferences can be made and a broader range of phenomena investigated (). However, special care must be taken to avoid subtle ways of overfitting or deriving overly optimistic estimates of the generalisation error. In this article, we present SKEMPI: Structural database of Kinetics and Energetics of Mutant Protein Interactions. SKEMPI is a large, manually curated database of experimentally measured changes in binding free energy, entropy, enthalpy and rate constants, upon mutation. The data may be used to investigate the structural principles governing the influence of mutations upon binding, for the evaluation of theoretical or experimental techniques, as well as for the training of empirical functions. As the kinetic and thermodynamic changes reported in the literature are not systematic and reflect the interests of the experimentalists who determine them, the data have specific biases towards certain residues, types of mutation, spatial locations, binding sites, proteins and protein families. These biases are reviewed and discussed in the context of how they can be accounted for in the construction and evaluation of empirical models. Specifically, a cross-validation framework is presented within which empirically trained models of arbitrary complexity can be evaluated, such that the resultant cross-validation error is not lowered by these biases and can thus be used to give an indication of how well the model is likely to perform when applied to mutations that are not necessarily of the same type as those overrepresented in the training set.
DISCUSSIONOne of the motivating forces behind assembling this dataset is that a major obstacle to gaining a deep understanding of the binding process and to efficiently engineering protein interfaces is the lack of structural and affinity data (). Such affinity data can be used to train appropriate G models, ideally with some form of overfitting avoidance bias, complexity commensurate to the number of training examples and accounting for the biases in the training set. A number of G functions have been developed previously (), although with very few stabilizing mutations present in the data used for parameterization. Furthermore, a large proportion of the data have come from alanine scanning experiments. As such, models have been trained to estimate the influence of clashes or the removal of stabilising contacts. The most common goal of interface engineering, however, is affinity optimization. The data presented here include 303 mutations that stabilize the interaction by at least 0.5 kcal mol 1 , and up to 12.4 kcal mol 1. This is, in part, due to a number of crystal structures that have been solved for lower affinity mutant complexes (notably BPTI/trypsin (), SGB/OMTKY3 (), Colicin E9/IM9 (), barnase/barstar (), hemagglutinin/ IgG1 (), subtilisin BNP'/CI2 (), cytochrome C/peroxidase (), HEW lysozyme/antibody (),-lactamase/BLIP (), cyclophilinthe specific residues or complexes in the dataset. Thus, instead of using leave-one-out cross-validation, leave-residue-out or better still leave-complex-out cross-validation should be used to estimate the efficacy of the model when applied to residues or proteins other than those in the database. However, even doing this neglects biases towards certain protein families or interfaces. This can sometimes be desirable; the large number of antibody/antigen complexes would be favourable for those looking to apply models to antibody affinity optimization. We may wish to estimate the performance of a model specifically on this class of mutations and leave-complex-out cross-validation would suffice. However, to avoid overestimating the general predictive power of a model on any future unspecified protein complex, this must be accounted for in the validation process, for instance by removing entire classes of complexes together when performing cross-validation. However, in order to do this, one must carefully specify how classes are defined, as we have attempted to do below. Care should be made not to aggregate complexes that need not be put together, as this will reduce the number of cases in the cross-validation training sets, and can lead to overestimation of the generalisation error.When defining classes to be simultaneously held out during cross-validation, we wanted to hold out pairs of interactions where at least one protein in each pair was using the same, or a homologous, binding site. We did not wish to simultaneously hold out interactions at sites distal to each other, such as the factor VIIa/tissue factor site, where binding is antipodal to the inhibitor binding site. Furthermore, we wanted to capture common binding modes that may not be reflected by sequence homology. In serine protease inhibitor interactions, which are highly represented in the benchmark, inhibition is achieved by a loop that adopts a canonical conformation. The convergent evolution of this local loop structure in unrelated protein families of different global structure and sequence prevents some complexes involving this loop from being classified together using structural or sequence alignment. However, the change in binding energy of mutations within this loop is often independent of the inhibitor family, as shown by interscaffolding additivity cycles (). In light of this, we propose a scheme based on the grouping together of interactions based on cross-reactive and homologous binding sites. Supplementaryshows the homologies andinteractions between the proteins in the database. For all pairs of proteins with a shared binding partner or homologous binding partners, we checked whether the interactions took place at the same binding site (see Section 2).shows pairs of proteins that bind to the same binding site on a third protein (red), or that bind to the same binding site on two homologous proteins (blue). For more than half of the proteins, binding is to a unique site. For many of the rest, the site is shared in one or two other interactions (for instance, the binding site on the-lactamase inhibitory protein BLIP is shared by both TEM-1 and SHV-1-lactamase), and thus these interactions should be simultaneously held out during cross-validation. The remainder fall into three groups. The first is the protease inhibitors, including the inhibitory antibodies E2 and S4, which share protease binding sites. The second is the antigens, which bind to the same region of their respective antibodies. The third group is the proteases, which share a common binding site on the inhibitors. Thus, all protease-inhibitor interactions should also be simultaneously held out, as should all the antibodyantigen interactions. For the membrane-type serine protease, the active site is shared by both protease inhibitors and the S4 and E2 inhibitory antibodies. Thus, interactions involving this protein should be either ignored during evaluation or both classes of interaction should be held out. The classes, as well as all the complexes which should be omitted, are outlined in the SKEMPI database. This scheme allows the evaluation of the generalization error with the minimal amount of grouping required to account for, to the authors' knowledge, all biases arising from overrepresentation of specific types of mutation in the SKEMPI database.