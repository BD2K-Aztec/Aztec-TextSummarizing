Motivation: Current informatic techniques for processing raw chromatography/mass spectrometry data break down under several common, non-ideal conditions. Importantly, hydrophilic liquid interaction chromatography (a key separation technology for metabolomics) produces data which are especially challenging to process. We identify three critical points of failure in current infor-matic workflows: compound specific drift, integration region variance, and naive missing value im-putation. We implement the Warpgroup algorithm to address these challenges. Results: Warpgroup adds peak subregion detection, consensus integration bound detection, and intelligent missing value imputation steps to the conventional informatic workflow. When compared with the conventional workflow, Warpgroup made major improvements to the processed data. The coefficient of variation for peaks detected in replicate injections of a complex Escherichia Coli extract were halved (a reduction of 19%). Integration regions across samples were much more robust. Additionally, many signals lost by the conventional workflow were rescued by the Warpgroup refinement, thereby resulting in greater analyte coverage in the processed data. Availability and implementation: Warpgroup is an open source R package available on GitHub at github.com/nathaniel-mahieu/warpgroup. The package includes example data and XCMS compatibility wrappers for ease of use.
IntroductionOmics-scale separation/mass spectrometry approaches (e.g. LC/MS, GC/MS, CE/MS, etc.) generate large, 3D data sets consisting of elution time (rt), mass-to-charge ratio (m/z), and signal intensity information (). Analytes are separated by their chemical characteristics prior to being introduced into the mass spectrometer (yielding rt). The mass spectrometer acts as a second dimension of separation and a detector, providing information on the accurate mass (m/z) and amount of each analyte (signal intensity). Each sample run can generate gigabytes of data representing tens of thousands of distinct analytes (K ll and). The processing of raw data is a significant challenge and the conventional workflow consists of several steps. These steps include mass trace detection, chromatographic feature detection, inter-sample retention time drift correction, inter-sample grouping of common features (correspondence determination), and statistical analysis of feature groups (). A feature in this context refers to signal which displays a peak shape in both m/z and rt domains. The result of this data processing is quantification of all unique analytes detected across multiple sample runs. Historically, most chromatography/mass spectrometry experiments have been performed with reversed-phase chromatography.This well-established separation technique commonly generates Gaussian peak shapes and exhibits highly reproducible retention times. A simple retention mechanism based primarily on compound polarity also minimizes compound specific drift (). One drawback to reversed-phase separation is a lack of retention for the highly polar compounds such as sugars and organic acids commonly of interest in metabolomic studies. As a result, many new separation chemistries have emerged under the umbrella term hydrophilic interaction liquid chromatography (HILIC), which aim to achieve separation of polar molecules (). Unfortunately, analytes measured by HILIC separation exhibit a wide range of non-Gaussian peak shapes as well as larger, compound-specific retention time drift (). Current informatic approaches for metabolomics were primarily developed by using reversed-phase C18 chromatography, and even today most new advances are benchmarked solely on reversed-phase datasets (). Thus, the performance of these algorithms degrades when applied to HILIC datasets. Detection of features and selection of integration regions is an initial and critical step of the informatic workflow (). In cases where peak shapes are simple and peaks exhibit large signal-to-noise ratios, the detection and integration of peaks is reproducible. Complex metabolomic datasets, however, contain a high proportion of poorly resolved and low-abundance peaks (). Additionally, the non-Gaussian peak shapes exhibited by a large portion of HILIC features impede the robust selection of integration bounds. These factors complicate peak detection and result in undetected features as well as integration bounds which describe different regions of a peak in each sample (and B). The second major informatic step is determination of correspondence. Current feature grouping techniques rely on the reproducible elution of compounds across multiple experimental runs. The elution time of m/z-rt pairs (i.e. features) is the key information used to associate the same compound detected in different runs (). In practice, elution times vary from sample to sample due to many factors (). This necessitates correction of retention time drift prior to grouping. Most techniques assume that drift is a function of retention time alone and thus generate a global correction curve f(rt A )  rt B (). This critical assumption is overly simplistic. In practice, retention time drift is compound dependent (Supplementary Figs S2 and S3) (). Additionally, residual drift becomes greater when using more vagarious separation strategies such as HILIC, as larger groups of samples are aligned, and as research studies begin to incorporate inter-laboratory comparisons (). Given the global correction assumption, most alignment techniques minimize only the average drift between samples considering all analytes equally () (Supplementarypost correction is an optimistic example of retention drift for technical replicates run over the course of nine hours). As such, the inherent compound-specific drift results in many peaks remaining unaligned after correctionmoreover many compounds move even further out of alignment upon global correction (Supplementary). This poor feature alignment causes major challenges for current peak grouping algorithms. The density method employed by XCMS, e.g. can only group peaks if their maximum residual drift is less than the distance to the nearest group (is an example of this failure) (). Further complexity is added by samples in which a feature is undetected or when spurious noise is detected as a feature, both common occurrences in current workflows. These failings of the current informatic workflow motivated our development of the Warpgroup algorithm. Warpgroup is an algorithm which utilizes dynamic time warping (DTW) and network graph decomposition. Herein we achieve five goals: (i) accurate grouping of features between samples even in the case of deviation from the global retention time drift, (ii) splitting of peak subregions into distinct groups, (iii) determination of consensus integration bounds within each group such that each group represents a similar chromatographic region, (iv) detection of the appropriate integration region in samples where no peak was detected, and (v) reporting of several parameters which allow filtering of noise groups. The Warpgroup algorithm establishes a correspondence between the time domains of each feature's extracted ion chromatogram (EIC) trace, utilizing DTW by default (). Based on this correspondence, Warpgroup evaluates whether all supplied peak bounds represent a similar chromatographic region using graph community detection (). Subsequently, it determines 'consensus integration regions' for each sample and selects the appropriate integration region for samples with no detected peak. During the time warping and graph analysis, several descriptors of each group are generated and reported for use in filtering unreliable and noise-containing groups.Increased precision of metabolomic data processing
DiscussionThe exact use cases of Warpgroup are dependent on the data and the problem at hand. We imagine four distinct goals in the next section and summarize appropriate inputs and expected outputs. Warpgroup operates optimally after inter-sample peak correspondence has been established. Though it is possible to supply ungrouped data to the Warpgroup algorithm, there are several drawbacks to this approach. First, processing time for the DTW algorithm scales with the square of the input length. Second, if a feature is present in one sample but missing from the others, this dissimilar topography can result in incorrect alignments. Finally, establishing correspondence is a complex challenge for which many more sophisticated solutions have been suggested (). These should be used in conjunction with the Warpgroup refinements. Although Warpgroup is not intended to determine peak correspondence, it does make a less restrictive assumption for peak alignment. Current algorithms assume that peak elution order remains monotonic across all masses. Warpgroup assumes only that peaks of indistinguishable mass retain their elution order. This more relaxed assumption allows for rudimentary correspondence to be established in more complex cases such as that shown inand is a major improvement to the XCMS-based workflow ().
ConclusionIn summary, we have found Warpgroup to be an important refinement step for current integration and correspondence methods. With Warpgroup refinement in place, data processing results remain robust across a wide range of experimental conditions. Major advantages have been noted in coverage as well as quantitation, especially in lowabundance signals. Further, Warpgroup output includes additional descriptors which can be used to filter noise and unreliable groups from the final datasets. Overall, we expect the addition of a Warpgrouping step to the informatic workflow to improve the quality and reliability of untargeted metabolomic analyses.