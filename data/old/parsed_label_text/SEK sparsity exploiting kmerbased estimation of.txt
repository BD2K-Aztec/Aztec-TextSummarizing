Motivation: Estimation of bacterial community composition from a high-throughput sequenced sample is an important task in metage-nomics applications. As the sample sequence data typically harbors reads of variable lengths and different levels of biological and technical noise, accurate statistical analysis of such data is challenging. Currently popular estimation methods are typically time-consuming in a desktop computing environment. Results: Using sparsity enforcing methods from the general sparse signal processing field (such as compressed sensing), we derive a solution to the community composition estimation problem by a simultaneous assignment of all sample reads to a pre-processed reference database. A general statistical model based on kernel density estimation techniques is introduced for the assignment task, and the model solution is obtained using convex optimization tools. Further, we design a greedy algorithm solution for a fast solution. Our approach offers a reasonably fast community composition estimation method, which is shown to be more robust to input data variation than a recently introduced related method. Availability and implementation: A platform-independent Matlab implementation of the method is freely available at http://www.ee.kth.se/ ctsoftware; source code that does not require access to Matlab is currently being tested and will be made available later through the above Web site.
INTRODUCTIONHigh-throughput sequencing technologies have recently enabled detection of bacterial community composition at an unprecedented level of detail. The high-throughput approach focuses on producing for each sample a large number of reads covering certain variable part of the 16S rRNA gene, which enables an identification and comparison of the relative frequencies of different taxonomic units present across samples. Depending on the characteristics of the samples, the bacteria involved and the quality of the acquired sequences, the taxonomic units may correspond to species, genera or even higher levels of hierarchical classification of the variation existing in the bacterial kingdom. However, at the same time, the rapidly increasing sizes of read sets produced per sample in a typical project call for fast inference methods to assign meaningful labels to the sequence data, a problem that has attracted considerable attention (). Many approaches to the bacterial community composition estimation problem use 16S rRNA amplicon sequencing where thousands to hundreds of thousands of moderate length (around 250500 bp) reads are produced from each sample and then either clustered or classified to obtain estimates of the prevalence of any particular taxonomic unit. In the clustering approach, the reads are grouped into taxonomic units by either distance-based or probabilistic methods (), such that the actual taxonomic labels are assigned to the clusters afterward by matching their consensus sequences to a reference database. Recently, the Bayesian BeBAC method () was shown to provide high biological fidelity in clustering. However, this accuracy comes with a substantial computational cost such that a running time of several days in a computing-cluster environment may be required for large read sets. In contrast to the clustering methods, the classification approach is based on using a reference database directly to assign reads to meaningful units representing biological variations. Methods for the classification of reads have been based either on homology using sequence similarity or on genomic signatures in terms of oligonucleotide composition. Examples of homology-based methods include MEGAN () and phylogenetic analysis (von). A popular approach is the Ribosomal Database Project's (RDP) classifier, which is based on a na ve Bayesian classifier (NBC) that assigns a label explicitly to each read produced for a particular sample (). Despite the computational simplicity of NBC, the RDP classifier *To whom correspondence should be addressed.  The Author 2014. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com may still require several days to process a dataset in a desktop environment. Given this challenge, considerably faster methods based on different convex optimization strategies have been recently proposed (). In particular, sparsity-based techniques, mainly compressive sensing-based algorithms (), are used for estimation of bacterial community composition in (). However, () used sparsity-promoting algorithms to analyze mixtures of dye-terminator reads resulting from Sanger sequencing, with the sparsity assumption that each bacterial community comprises a small subset of known bacterial species, the scope of the work thus being different from methods intended for high-throughput sequence data. The Quikr method of () uses a k-mer-based approach on 16S rRNA sequence reads and has a considerable similarity to the method (SEK: Sparsity Exploiting K-mers-based algorithm) introduced here. Explained briefly, the Quikr setup is based on the following core theoretical formulation: given a reference database D=fd 1 ;. .. ; d M g of sequences and a set S=fs 1 ;. .. ; s t g of sample sequences (the reads to be classified), it is assumed that there exists a unique d j for each s l , such that s l = d j. In general, all reference databases and sample sets consist of sequences with highly variable lengths. In particular, the lengths of reference sequences and samples reads are often different. Violation of the assumption leads to sensitivity in Quikr performance according to our experiments. Another example of fast estimation is called Taxy (), which addresses the effect of varying sequence lengths (). Taxy uses a mixture model for the system setting and convex optimization for a solution. The method referred to as COMPASS () is another convex optimization approach, similar to the Quikr method, that uses large k-mers and a divide-andconquer technique to handle large resulting training matrices. The currently available version of the Matlab-based COMPASS software does not allow for training with custom databases, so a direct comparison with SEK is not yet possible. To enable fast estimation, we adopt an approach where the estimation of the bacterial community composition is performed jointly, in contrast to the read-by-read analysis used in the RDP classifier. Our model is based on kernel density estimators and mixture density models (), and it leads to solving an under-determined system of linear equations under a particular sparsity assumption. In summary, the SEK approach is implemented in three separate steps: off-line computation of k-mers using a reference database of 16S rRNA genes with known taxonomic classification, online computation of k-mers for a given sample and then final online estimation of the relative frequencies of taxonomic units in the sample by solving an under-determined system of linear equations.