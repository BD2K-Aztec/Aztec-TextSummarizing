
INTRODUCTIONrecently published an interesting letter to the editors of Bioinformatics outlining the importance of validation of new proposed algorithms through a thorough comparison with existing approaches. We completely agree with the necessity of more comparison studies in general to help end users to make an informed choice based on objective criteria. We also agree that 'The practical result [of the lack of comparison studies] is that practitioners stop short of exhaustively evaluating all the possible options and choose based on some other criteria (e.g. popularity, ease of use or familiarity)', and that this may harm both algorithm developers and usersas also claimed in one of our previous publications on this topic (). In a few words, we fully comply with the claim of the authors that everybody (users and algorithm makers) suffers from the trend observed in bioinformatics toward the publication of many new algorithms without proper comparison between these algorithms. We also agree withthat articles suggesting new algorithms should always include a comparison with existing methods. Here we guess that the authors implicitly mean comparisons based on real datasetsas opposed to simulated data that are often investigated in other fields related to data analysis such as statistics. However, our opinion diverges when it comes to defining the goals of such comparison studies and their interpretation. Our major argument is that comparison studies conducted as a part of an article suggesting a new algorithm rarely reach the level of representativity and objectivity required to be used as guidance by other researchers when choosing their algorithm. The two underlying main ideas behind this claim are that (i) comparison studies comparing a new algorithm with existing algorithms are often severely biased for different reasons, as documented in our empirical study based on the example of supervised classification with high-dimensional data (); simply, when reading articles on new methods, most of us think 'well, of course they say their method is better, but. . .'; and (ii) as stressed by, an extensive and fair comparison study requires a lot of work, care and attention in itself (, b) and can hardly be conducted within a study focused on another problemthe development of a new algorithm. To specify our point of view, we suggest a taxonomy of comparison studies based on real datasets while referring to the ideas presented by Smith et al. (2013).
CONCLUSIONIn conclusion, we agree that comparisons of new algorithms to existing algorithms are important. As, we make a plea for more comparison studies on real datasets in bioinformatics literature. Suggesting an algorithm without even running it on a real dataset is just unacceptable. Going back to our parallel between medical and computational sciences, it would be as if a physician described a new therapy without even showing that it was successful on a few patients. The application of the new algorithm to, say, at least two distinct example datasets with different characteristics should be a minimum nonnegotiable requirement for publication. Generally, we also think that in research articles applying bioinformatics algorithms to obtain substantive results, the results should not be based on a new novel algorithm that is scarcely described, applied only to the dataset of interest and not compared with any other method. We also believe that both approaches discussed in this letter illustrative and representativemay make sense, but that they are completely different and should be reported differently. Each approach implies a different way to report the results and a different way to design the experiment. Reporting and design should be consistent. In practice, the most frequent violation of this principle is when algorithm developers 'feel' that their new method might be better than existing methods based on an underpowered comparison study and report their results as if the comparison was representative. The design of a representative comparison study is so difficult and time consuming and the risk of bias in favor of the new method is in practice so important that most applications presented in articles on new methods should probably be seen as illustrative applications. In our letter, we discussed a few conditions that a good representative application should in our view fulfill to be really representative. However, we believe that much more effort is needed to define precise criteria and guidelines. In particular, the neutrality issue addressed inshould be carefully taken into account. Referring to the sentence 'these evaluations ought to be primarily provided in the novel algorithm publications themselves' (), we again point out that representative comparisons (as defined in our letter) can hardly be performed within an article on a new method, especially because of the well-known optimistic bias in favor of the new method (see also the editorial by). Addressing this general issue related to scientific methodology and publication practice will probably need a long time and much coordinated effort from all partiesresearchers, editors and reviewers. This problem is also connected with publication bias and publication of negative research findings (). In the meantime, we believe that authors and readers should interpret illustrative comparisons as suchwithout implicitly assuming that they give information about what would happen on other datasets, and that journals might give more attention to 'neutral' comparison studies entirely devoted to the comparison task itself. This would give a motivation to potential authors of such comparison studies: if they know that their work will not only provide useful information to other scientists but also have good chance to be published in a high-ranking journal, they will be more likely to conduct such a study. Motivating potential authors of comparison studies by publishing more of these studies is certainly easier than motivating overloaded reviewers to spend hours investigating the possible bias of a comparison study included in an article on a new method. To conclude, in this letter we tried to clarify some aspects left unaddressed by. The exact definition of requirements for the publication of new algorithms, however, cannot be formulated within this modest framework. Such guidelines should be the result of coordinated efforts of consortia involving a large number of scientists, similarly to the teams working on reporting guidelines in medical sciences such as the EQUATOR network ().