Motivation: Reliable estimation of the mean fragment length for next-generation short-read sequencing data is an important step in next-generation sequencing analysis pipelines, most notably because of its impact on the accuracy of the enriched regions identified by peak-calling algorithms. Although many peak-calling algorithms include a fragment-length estimation subroutine, the problem has not been adequately solved, as demonstrated by the variability of the estimates returned by different algorithms. Results: In this article, we investigate the use of strand cross-correlation to estimate mean fragment length of single-end data and show that traditional estimation approaches have mixed reliability. We observe that the mappability of different parts of the genome can introduce an artificial bias into cross-correlation computations, resulting in incorrect fragment-length estimates. We propose a new approach , called mappability-sensitive cross-correlation (MaSC), which removes this bias and allows for accurate and reliable fragment-length estimation. We analyze the computational complexity of this approach , and evaluate its performance on a test suite of NGS datasets, demonstrating its superiority to traditional cross-correlation analysis. Availability: An open-source Perl implementation of our approach is available at
INTRODUCTIONNext-generation sequencing (NGS) technologies have revolutionized molecular biology with their unprecedented capacity for genome-wide measurement of proteinDNA interactions, chromatin state changes and transcription levels (). Although NGS technologies differ in their details, most of the common platforms work by sequencing large numbers of shortDNA fragments. These fragments may originate, for example, from simple extraction of DNA from a sample of cells, selective extraction based on a chromatin-immunoprecipitation pulldown or reverse transcription of RNA into DNA. When dealing with DNA from an organism that lacks a canonical genome assembly, the sequences can be assembled to create a de novo estimate of the genome or transcriptome (). When the organism does have a canonical genome, the DNA fragment sequences are typically mapped back to the canonical genome, so that their distribution, and especially sites of enrichment, may be studied (). NGS technologies usually do not sequence each DNA fragment in its entirety. Indeed, depending on the size of the fragments, this is typically impossible and is not the intended use of the technology. The best practical alternative offered by typical current technologies is sequencing the fragments starting from both ends. However, most experiments do not take advantage of this option for cost reasons and, instead, choose to sequence only one end of each fragment. Thus, despite having a canonical genome assembly to which one end of each fragment can be mapped, most NGS experiments lack information on the other, unsequenced end of each fragment. A fundamental step in many NGS analysis pipelines is to estimate mean fragment length, so that we can have at least some idea of the genomic locations of the unsequenced ends. First, this helps in the visualization of the NGS dataset in a genome browser. Each read can be 'extended' to the average fragment length and shown as an interval in the browser, giving a more accurate impression of the regions of the genome represented by the DNA sample. Secondly, fragment-length estimation is important for peak-calling algorithmsmethods for automatically detecting the genomic regions that are enriched in the sample of DNA fragments (). In most such algorithms, either the reads are extended to an average fragment length (e.g.), or the positive-and negative-strand reads are shifted towards each other by half the estimated fragment length (e.g.). Indeed, many peak-calling algorithms include a fragment-length estimation subroutine. However, many of these have not been rigorously validated, and different algorithms often produce different fragment-length estimates. Partly, as a result, the enriched regions identified by different peak-calling algorithms can have poor overlap () (also see Supplementary Section S1). Yet other peak-calling algorithms require the fragment length as input (e.g.). For all of these reasons, reliable fragment-length estimation is an important problem that has not yet been adequately solved. Mean fragment length can be estimated in the wetlab, and this is often a part of DNA sample preparation protocols. Fragment *To whom correspondence should be addressed.  The Author 2013. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. length is commonly controlled by the aggressiveness of DNA fragmentation (e.g. duration of DNA sonication) and/or by gel-based size selection. However, such procedures vary by laboratory and even by experimenter, and they often go unreported for public NGS datasets. Moreover, such procedures are typically not highly quantitative, and only result in rough estimated ranges for fragment length (e.g. 200300 bp), which are not satisfactory. In this article, we investigate the use of cross-correlation of positive-and negative-strand reads for estimating mean fragment length. By applying this method to a small number of available paired-end datasets, for which mean fragment length can be computed exactly, we show that cross-correlation usually produces an accurate fragment-length estimate. Occasionally, however, estimates are clearly wrong, returning a value near the read length rather than the fragment length. Our key insight is that the mappability of different parts of the genome can introduce an artificial bias into the cross-correlation function, which sometimes results in incorrect fragment-length estimates. Based on this insight, we propose a new approach, called mappability-sensitive cross-correlation (MaSC), which removes this bias, allowing for much more accurate fragment-length estimation. We analyze the computational complexity of this algorithm and evaluate its performance on a test suite of NGS datasets, demonstrating its superiority to traditional cross-correlation analysis.
DISCUSSIONWe have demonstrated that mappability can introduce a strong bias into genome-wide cross-correlation computations of positive-and negative-strand read densities. When those computations are carried out to estimate fragment length, and when the bias is strong enough, a dramatically wrong fragment-length estimate can result. When used for peak calling, such incorrect estimates can have adverse effects on the set of peaks returned. Crucially, we have shown that the mappability-induced bias can be corrected for using our MaSC algorithm. Tests using a variety of public NGS datasets demonstrated the effectiveness of MaSC. We also showed that smoothing the correlation signal helps in obtaining an unambiguous summit location. We do recommend checking the plots of the smoothed and the unsmoothed signals provided by our software to ensure a reasonable agreement between the summit locations. The computational complexity of MaSC is comparable with the traditional cross-correlation computation. The only serious caveat to the MaSC computation is that it requires a mappability map to have been established for the target genome and for the read length under consideration. Such maps are not always available and are non-trivial to compute (). However, when such mappability information is available, we see no reason not to use the MaSC computation instead of the traditional, biased crosscorrelation computation.The first five rows correspond to data from the ENCODE project. SISSRs, Kharchenko-correlation and coverage output estimates by chromosome. Hence, these columns have means AE 2  standard errors.The pervasiveness of the mappability problem in correlation analysis is unclear. In some datasets we obtained from GEO, we found the problem, whereas other datasets did not show the problem. Generally, we expect datasets with shorter reads to be more susceptible because a greater fraction of the genome is unmappable with shorter reads. However, mappability also varies significantly by organism. The problem seems to be present both in older datasets and new datasets. We are presently planning a comprehensive examination of the large number of ENCODE ChIP-Seq datasets to get a better assessment of the issue. Although we have emphasized positive-versus negative-strand cross-correlation and the fragment-length estimation problem, our approach to eliminating mappability bias is relevant to other correlative-type analysis of short-read data. For instance, if one were to compute autocorrelation functions as a measure of the spatial structure of the genomic signal being assessed, a similar mappability correction could be performed. The concept could also be relevant to correlating different datasetsfor instance, relating a transcription factor binding signal to a histone-modification signal. This would especially be true if the different datasets used different read lengths, and thus were differentially susceptible to mappability problems. Finally, based on the cross-correlation function, we have begun exploring the possibility of estimating not just the mean fragment length, but the variance as well, or, even more generally, the entire fragmentlength distribution.