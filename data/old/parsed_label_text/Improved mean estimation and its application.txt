Motivation: High-dimensional data such as microarrays have created new challenges to traditional statistical methods. One such example is on class prediction with high-dimension, low-sample size data. Due to the small sample size, the sample mean estimates are usually unreliable. As a consequence, the performance of the class prediction methods using the sample mean may also be unsatisfactory. To obtain more accurate estimation of parameters some statistical methods, such as regularizations through shrinkage, are often desired. Results: In this article, we investigate the family of shrinkage estimators for the mean value under the quadratic loss function. The optimal shrinkage parameter is proposed under the scenario when the sample size is fixed and the dimension is large. We then construct a shrinkage-based diagonal discriminant rule by replacing the sample mean by the proposed shrinkage mean. Finally, we demonstrate via simulation studies and real data analysis that the proposed shrinkage-based rule outperforms its original competitor in a wide range of settings.
INTRODUCTIONWith the advent of high-throughput technologies, learning highdimensional complex models is critical in many disciplines such as biology, genetics, epidemiology, geology, ecology, neurology and engineering. One such example is microarray data, where the expression levels of thousands of genes are measured simultaneously from each sample. Due to the cost and/or other experimental difficulties such as the availabilities of biological materials, it is common that high-throughput data are collected only in a limited number of samples. They are referred to as high-dimensional data with small sample size, or 'large G small n' data, where G is the number of dimensions and n is the sample size. Highdimensional data pose many challenges to traditional statistics methods. Specifically, due to the small n, there are more uncertainties associated with standard estimations of parameters such as the mean and variance estimations. As a consequence, statistical analyses based on such parameter estimation are usually unreliable. To obtain * To whom correspondence should be addressed. more accurate estimation of parameters some statistical methods, such as regularizations through shrinkage, may yield better results. Shrinkage-based methods have been proposed in recent years to improve the variance estimation for 'large G small n' data. See for example,, Storey and, Wright and Simon (2003), Smyth (2004),, Tong and, Opgen-Rhein and Strimmer (2007) and, among many others. In contrast to the advances on variance estimation, little attention has been paid to improving the mean estimation for high-dimensional data until recently (). In this article, we investigate the family of shrinkage estimators for the mean value which is tailored to the high-dimensional data such as microarrays. Specifically, we will propose the optimal shrinkage parameter under the quadratic loss function for the data when G tends to be infinite. Class prediction with high-dimensional data has been recognized as a very important problem and received much attention in different fields such as genomics, proteomics, brain images, medicine and machine learning. For high-dimensional data with small sample sizes, it is known that the traditional classification methods such as the linear discriminant analysis are not applicable as the sample covariance is going to singular when G is greater than n. To overcome the singularity problem,introduced two diagonalized discriminant rules, the diagonal linear discriminant analysis (DLDA) and the diagonal quadratic discriminant analysis (DQDA). When the sample size is small, DLDA performed remarkably well compared with more sophisticated classifiers in terms of both accuracy and stability (). In addition, DLDA is easy to implement and is not sensitive to the number of predictor variables. Though DLDA performed well for high-dimensional small sample size data, there is still room to improve it (). In particular, we notice that the mean estimation (the sample mean) in DLDA will be unreliable when the sample size is not sufficiently large. As a consequence, the performance of DLDA may also be unsatisfactory. With this insight, we propose in this article an improved version of DLDA, which replaces the sample mean by the optimal shrinkage estimator. We expect that the proposed shrinkage-based DLDA will improve the classification accuracy in practice. The remainder of the article is organized as follows. In Section 2, we investigate the family of shrinkage estimators for the mean value under the quadratic loss function. With the nature of highdimensional data, we assume that the variances are unequal and unknown. Under regularity conditions, we discuss the choices of thePage: 532 531537
DISCUSSIONIn this article, we proposed an optimal shrinkage estimator for the mean value under the 'large G small n' scenario. We then applied the proposed shrinkage estimator to high-dimensional classification problem by constructing a shrinkage-based diagonal discriminant rule. Its improvement over the original competitor was demonstrated through both simulations and real data analysis. Though the independence assumption in this article is popular in the literature (), it is unlikely to be true in practice and so certain remedy might be necessary for a further improvement when additional information is available. Langaas Page: 536 531537 T.suggested that the clumpy dependence is a likely form of dependence, where the clumpy dependence means that the genes are dependent within groups and independent among groups. Inspired by that, one natural extension would be to propose new shrinkage estimators for the mean value under the clumpy dependence structure. To avoid the singularity problem, we might need to assume that the largest group size is not larger than the number of samples. Another future work is to examine if the proposed optimal shrinkage estimator has any good in its own right, or if it can be further improved by its positive-part estimator. Finally, we note that the proposed SmDLDA in this article is a shrinkage-mean-only-based DLDA, whereas inthe authors proposed a shrinkage-variance-only-based DLDA. As both the mean and variance estimations are crucial in the statistical analysis, further research might be needed to develop new classification rules that shrink both the mean value and the variance. Possible approaches can be either by plugging-in the existing shrinkage estimators, respectively, or by proposing new shrinkage estimators for the mean value and variances simultaneously.Finally, by minimizing the above quantity, we have