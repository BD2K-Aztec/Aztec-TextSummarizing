Motivation: Gene prioritization aims at identifying the most promising candidate genes among a large pool of candidatesâ€”so as to maximize the yield and biological relevance of further downstream validation experiments and functional studies. During the past few years, several gene prioritization tools have been defined, and some of them have been implemented and made available through freely available web tools. In this study, we aim at comparing the predictive performance of eight publicly available prioritization tools on novel data. We have performed an analysis in which 42 recently reported disease-gene associations from literature are used to benchmark these tools before the underlying databases are updated. Results: Cross-validation on retrospective data provides performance estimate likely to be overoptimistic because some of the data sources are contaminated with knowledge from disease-gene association. Our approach mimics a novel discovery more closely and thus provides more realistic performance estimates. There are, however, marked differences, and tools that rely on more advanced data integration schemes appear more powerful.
INTRODUCTIONA major challenge in human genetics is to discover novel diseasecausing genes, both for Mendelian and complex disorders. Identifying disease genes is a crucial first step in unraveling molecular networks underlying diseases, and thus understanding disease mechanisms, also toward the development of effective therapies. The discovery of a novel disease gene often starts with a cytogenetic study, a linkage analysis, a high-throughput omics experiment or a genome-wide association study (GWAS). However, these studies do not always pinpoint the disease gene uniquely, but often result in large lists of candidate genes that are potentially relevant (). Moreover, recent advances in next-generation sequencing offer promising opportunities to explore the genomic alterations of patients (). However, thousands of mutations in hundreds of genes are often detected, among which only a few are in fact linked to the genetic condition of interest (). The experimental validation of these candidate genes, for instance, through resequencing, pathway or expression analysis, is still expensive and time consuming. An efficient way to reduce the validation cost is to narrow down the large list of candidate genes to a small and manageable set of highly promising genes, a process called gene prioritization. Prioritization in the past was achieved manually by geneticists and biologists and was mainly based on their own expertise. Nowadays, biologists and geneticists can use computational approaches that can handle and analyze the large amount of genomic data currently available. In the past few years, many gene prioritization methods have been proposed, some of which have been implemented into publicly available tools that users can freely access and use (). Information about these tools is summarized in our Gene Prioritization Portal (http://www.esat.kuleuven.be/gpp) that currently describes 33 prioritization tools. This web site has been designed to help researchers to carefully select the tools that best correspond to their needs. For instance, only few tools can prioritize the whole genome, which can be necessary when no positive regions can be identified beforehand, or when selecting candidates for a medium-throughput screen (instead of low-throughput validation). Another example is the study of a poorly characterized disorder for which a prioritization tool not relying on a set of known disease genes might be more suited. Recently, several studies have demonstrated that gene prioritization tools can help geneticists to discover novel disease genes (). For instance, a KIF1A mutation was discovered in hereditary spastic paraparesis patients after KIF1A was predicted to be the best candidate gene from the locus using multiple prioritization tools (). Another study discovered homozygous mutations in the PTRF-CAVIN gene in patients with congenital generalized lipodystrophy with muscle rippling after PTRF-CAVIN was predicted as the most probable candidate gene for high expression in muscle and adipose tissue (). A third study identified the HHEX gene to be associated with Type *To whom correspondence should be addressed. yThe authors wish it to be known that, in their opinion, the first three authors should be regarded as joint First Authors. 2 diabetes (T2D) in a Dutch cohort after investigating the T2Dsusceptibility loci using candidate gene prioritization (). However, beyond these conceptual differences, one essential parameter to consider when selecting gene prioritization tools is their respective performancethat is, their ability to identify the true positive genes as promising candidate genes to maximize the yield of the follow-up experimental validation. A common standard in bioinformatics is to estimate the performance with a benchmark analysis. Several publications that introduce a novel prioritization approach also describe a comparative benchmark with several existing methods (). However, these benchmarks are most of the time cross-validations of gold-standard disease datasets (e.g. known data). Therefore, the estimation of the performance is likely an overestimate of the real performance (i.e. on novel data). Because different types of data are dependent on each other (e.g. Gene Ontology (GO) annotation, Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway membership and MEDLINE  abstracts), it becomes impossible to remove all cross-talk effects between data sources (e.g. removing MEDLINE data does not remove all information from the biomedical literature since much of it is present in GO and KEGG) to prevent contamination of the prediction of the disease gene by actual retrospective knowledge of this association. This makes it challenging to create benchmarks on retrospective data that are indicative of the performance of the method in an actual research setting. Next to benchmarking, some studies use several prioritization methods to analyze disease-associated loci, mostly for Type 2 diabetes and obesity (). However, the results have not been experimentally validated, which means that it is not possible to identify which methods made better predictions. Also, a few studies combine computational and experimental analysis: in silico-generated hypothesis are then validated in vivo. We have, for instance, performed a computationally supported genetic screen in Drosophila that led to the identification of 12 novel atonal genetic interactors (). Although useful, such studies often rely on the use of a single tool and therefore cannot be used to compare different approaches. They also give no indication of the performance of the method in general, but only illustrate it on a single well-validated case. In this study, we aim at comparing the performance of several freely accessible web-based gene prioritization tools on novel data, which, to our knowledge, has never been performed before. To this aim, we selected recently reported disease-gene associations from literature and use several gene prioritization tools to make predictions immediately after publication (typically within 2 days). Our approach relies on the fact that, when the prioritization tools are used, the novel disease-gene association of interest is not yet included in the databases that underlie these tools. As a consequence, our approach mimics a novel discovery, and therefore, the estimation of the performance is more accurate. It has to be mentioned that we compare tools and not the underlying algorithms (we see a tool as an algorithm plus some data sources), because this is what is most relevant to geneticists.
DISCUSSIONWe aim at assessing the usefulness of eight gene prioritization tools that are freely available via web applications. We have built a validation based on 42 recently discovered disease-gene associations from literature containing novel genes for both monogenic conditions and complex disorders. We have selected novel disease-gene associations regardless of their strength and of the underlying methodology. To mimic a real discovery, we have run the tools as soon as the article appeared online so that all databases used for gene prioritization are still not contaminated by the knowledge of the novel disease-gene association. This also means that we had to exclude tools that query MEDLINE online because their results would be biased. We want to compare the performance of the tools even if the inputs are different (genes versus keywords, genome-wide versus candidate set). Among the eight gene prioritization tools that we have analyzed in this study, only Endeavour, Candid and Pinta have been used for genome-wide prioritization. The input data for Endeavour and Pinta are training genes, whereas Candid requires keywords. The gene prioritization tools that we have used to prioritize candidate genes within a region of interest are Suspects, ToppGene, GeneWanderer, Posmed, GeneDistiller, and again Endeavour and Pinta. Suspects and Posmed are trained with keywords, and the other tools require training genes. We have extensively searched through literature and dedicated databases to identify as many reliable training genes as possible for the disease of interest as well as a set of appropriate keywords to derive fair and meaningful comparisons. However, different, and possibly better, results might be obtained by refining the inputs. Our validation is too small to claim that the differences among tools are significant. However, a trend can still be observed, GeneDistiller and Endeavour-CS consistently appear as the best tools when looking at all performance measures. It is interesting to notice that the best results are, in general, obtained with tools that use many data types in conjunction (up to eight for Endeavour, when compared with the three data sources used by Posmed), but there is no perfect correlation. This is in agreement with the conclusion of the recent review by, who indicate that successful computational applications will be facilitated by improved data integration. All tools except Posmed have a high response rate ranging from 88 to 100%, meaning that at least 37 of the 42 novel disease genes are prioritized (or 24 of 27 for Suspects). However, the response rates for Posmed-KS and Posmed-DN are 47.6% and 50%, respectively, which can be explained by the fact that Posmed also acts as a filter on the candidate genes to obtain a reduced list of genes in the end. There are therefore cases for which the novel disease gene has been removed by the filter. This is different from the other tools for which missing genes basically correspond to genes that are not recognized by the tool (it happens most of the time with poorly characterized genes, such as C20orf54). Another special case is Suspects that went offline during the validation and therefore could only be validated with the first 27 associations. We therefore calculated the response rate only on the first 27 associations. Two types of tools can be distinguished, the ones that are trained with already-known genes and the ones that are trained with descriptive keywords. It appears that gene-based tools seem to work better than keyword-based tools (the average of medians is 17.2 for gene-based tools and 27 for keyword based tools; similar results are obtained with the other measures, see Supplementary). This could be because we use, ingeneral, more genes than keywords for training (18.8 genes on an average for six keywords). This also indicates that more keywords might be needed to model a disease and that a small text (such as an OMIM entry) might even be necessary (van). There is in general an agreement between the five performance measures we use throughout our study. One notable exception exists for ToppGene, whose AUC is 66%, and corresponds to rank 10th (out of the 12 prioritization tools). In contrast, its associated TPR in top 10% is 42.9%, which corresponds to rank second. This apparent contradiction can be explained by observing, in which the ROC curve exhibits a non-convex shape. This is because ToppGene either ranks the novel disease gene on top or at the bottom (i.e. the disease genes are rarely ranked in the middle). Therefore, the TPR in top 10% will be high because it only takes into account the top of the list, while the AUC will be lower because it basically behaves like an average over all cases. Another important point is that our observations are in line with the 'no free lunch' theorem. Indeed, each tool can perform better than all the others for some cases, or, in other words, none of the tools outperforms another on the complete dataset (if we do not consider the special case of Posmed that also acts as a filter). Posmed-KS has been trained with the complete keyword set, whereas Posmed-DN has been trained only with the disease name. The median rank ratio is 31.44 when the complete keyword set is used and drops to 45.45 when only the disease name is inputted. If we only compare the results over the 19 associations for which both tools are able to prioritize the novel disease gene, the difference becomes even larger (29.6 and 50, respectively, for Posmed-KS and Posmed-DN). Altogether, these results indicate that Posmed does not rely on the use of the single disease name and that the extra keywords are indeed important. It can be observed that the performance measures for Posmed are worse than for the other tools in our benchmark study. However, when looking at the individual ranks, it can be observed that Posmed returns far fewer genes than the other tools because it also acts as a filter. As a result, the rank ratios are in general larger and the performance measures are therefore worse. As such, it becomes difficult to fairly compare Posmed with the other tools because our measures of performance naturally penalize the fact that Posmed returns prioritizations for a limited set of candidates. Changing our performance measures to counterbalance this effect would then give an unfair advantage to Posmed because it returns prioritizations only for the 'safer bets'. GeneWanderer has also been run twice with different network algorithms: random walk and diffusion kernel. The respective performances are very similar although the random walk approach is performing a little bit better than the diffusion kernel albeit non-significant (22.1122.97 for median rank ratio  similar differences are observed with the other measures). The heat map indicates a strong correlation (40.9, see Supplementary) between the two modes, which was expected since applying diffusion to a kernel can be interpreted as equivalent to applying a random walk on the underlying network. Altogether, this indicates that these two algorithms are similar. Endeavour and Pinta are used to prioritize both the whole genome (Endeavour-GW and Pinta-GW) and the defined chromosomal region (Endeavour-CS and Pinta-CS), allowing us to identify the influence of the size of the gene list to prioritize. The median rank ratio is better for Endeavour-CS (11.16) than for Endeavour-GW (15.49) in our benchmark. The difference remains, albeit smaller, when considering the AUC and the TPR in top 10 and 30%. The same training genes are used, and therefore the observed difference is only caused by extending the small candidate gene set to the whole genome. This confirms previous findings that prioritizing the whole genome is more difficult than prioritizing a rather small positive locus. The heat map indicates that the two Endeavour modes are strongly correlated as expected since the core algorithm is the same in both modes (40.9, see Supplementary). In contrary, the results for both Pinta modes are similar (correlation of 0.99) and seem to indicate that the size of the candidate set does not influence this algorithm. In this study, we consider the tools as off-the-shelf solutions and use them as recommended by the developers without fine-tuning of the parameters. However, an important feature that might influence the results is the date of the last data update. The latest genomic data (still prior to discoveries considered in this study) is likely to give the best results because it will model more accurately what is currently known, when compared with data that are 2 years old. In our setup, we have no control over the genomic data used and cannot identify whether variation in performance among tools can be explained by this. In addition, the quality of both the data sources and the integration methodologies are also influencing the outcome of the prioritization process. However, we aim at estimating the usefulness of some prioritization tools for geneticists. Therefore an in-depth comparison of the implementation of the tools is beyond the scope of this study. It is important to notice that the 42 novel disease-gene associations do not represent a homogeneous set. Indeed, the median of the rank ratios over the tools show that some associations seem to be easier to predict than others. This also explains why all tools are moderately correlated on the heat map (40.4). A plausible explanation is the disparity in the available data between the novel disease genes. Since only little data can be gathered for poorly characterized genes, such as C20orf54, they are more difficult to prioritize. However, we also hypothesize that the nature of the underlying genetic disorder as well as the quality of the reported association might influence the ability of the tools to correctly predict that association. We have therefore divided the associations between confirmed, intermediate and unconfirmed. Among the 42 associations, 23 are confirmed, 8 are intermediate and 11 are unconfirmed (see Supplementary). We hypothesize that this might influence our validation since some unconfirmed associations might in fact be spurious. We observe that Suspects and ToppGene perform better for the 23 confirmed associations than for the 19 unconfirmed ones (see Supplementary Tables S4 and S5). However, this trend is not always shared as the situation is opposite for GeneDistiller and GeneWanderer. Although informative, these comparisons are not significant due to the small number of associations. In our validation dataset, there are 17 monogenic diseases and 25 multifactorial disorders (see Supplementary Tables S6 and S7). It has been shown that it is more difficult to make predictions for multifactorial diseases than for monogenic diseases (). Our results however seem to indicate that not all tools are influenced by the intrinsic complexity of multifactorial diseases. For instance, Endeavour and ToppGene seem to perform better for monogenic conditions while GeneWanderer and Suspects perform better for complex disorders. However, the size of our validation dataset does not allow for a complete statistical analysis. Larger validation datasets and real predictive studies will be pursued to complement our preliminary study. We are aware of the limited coverage of available literature in human genetics in our study that report novel disease-gene associations. However, we aimed at estimating the real performance of gene prioritization tools and therefore have decided to keep under strict control all the factors that could potentially bias the benchmark. We were further interested in finding novel disease-gene associations for defining a proper benchmark, and there is no guarantee that these associations are uniformly distributed over the whole literature. We have used journals about genetic disorders, in general, and favor journals that report novel associations and have avoided specialized journals that focus on few diseases to avoid introducing bias toward one disease class. Our choice of the six selected journals may not be perfect, but they allowed us to cover most disease types and most situations. Several studies have shown that combining predictions of several tools lead to even better predictions (). However, no performance criteria were used to select the tools to be combined. With this comparison of tools, we ease the selection of the most efficient tools, whose combination may lead to more accurate predictions. In addition, we report that the meta-predictors that integrate the predictions made by several tools perform better than the best individual tools as already reported (). Our results indicate that cross-validation based benchmarks tend to overestimate the real predictive performance. Indeed, all the tools for which such a benchmark exists have lower AUC than anticipated using our dataset (see Supplementary). We therefore believe that developers should take extra care when benchmarking their tools as to avoid these pitfalls. Also, some hard constraints have made this study small enough not to reach significance (e.g. only few tools have a programmatically queryable interface). As already discussed in, this field needs to consolidate through improved benchmarking efforts due to the lack of a ground truth for evaluating the performance of prioritization methods. Therefore, we see a need for a large-scale community effort to compare multiple tools across common prospective benchmarks. We hope our work represents the first step toward a collaborative effort to tackle this problem at a larger scale.