Motivation: Prior to applying genomic predictors to clinical samples, the genomic data must be properly normalized to ensure that the test set data are comparable to the data upon which the pre-dictor was trained. The most effective normalization methods depend on data from multiple patients. From a biomedical perspective, this implies that predictions for a single patient may change depending on which other patient samples they are normalized with. This test set bias will occur when any cross-sample normalization is used before clinical prediction. Results: We demonstrate that results from existing gene signatures which rely on normalizing test data may be irreproducible when the patient population changes composition or size using a set of curated, publicly available breast cancer microarray experiments. As an alternative, we examine the use of gene signatures that rely on ranks from the data and show why signatures using rank-based features can avoid test set bias while maintaining highly accurate classification, even across platforms. Availability and implementation: The code, data and instructions necessary to reproduce our entire analysis is available at https://github.com/
IntroductionOne of the most common barriers to the development and translation of genomic signatures is cross-sample variation in technology, normalization and laboratories (). Technology, batch and sampling artifacts have been responsible for the failure of genomic signatures (), irreproducibility of genomic results () and retraction of papers reporting genomic signatures (). Even highly successful signatures such as Mammaprint (van't) have required platformspecific retraining before they could be translated to clinical use (). An under-appreciated source of bias in genomic signatures is test set bias (). Test set bias occurs when the predictions for any single patient depend on the data for other patients in the test set. For example, suppose that the gene expression data for a single patient is normalized by subtracting the mean expression and dividing by the standard deviation of the expression across all patients in the test set. Then the normalized value for any specific gene for that patient depends on the values for all the patients they are normalized with. The result is that a patient may get two different predictions using the same data and the same prediction algorithm, depending on the other patients used to normalize the test set data (). There are many scenarios under which a patient's classification ought to change: if new information updates or alters the prediction algorithm or if the raw, biological patient data itself changes.The case we would like to explore is when the gene signature and prediction algorithm are 'locked down' and when there is no biological variation in the patient data. We are concerned with how much data transformation due to pre-processing and normalization affects classification. It is our assertion that steps taken to transform patient data for the purposes of applying a prediction algorithm should not alter the patient's eventual classification. Some normalization methods () and some batch correction methods () have addressed this issue by normalizing each sample against a fixed, or 'frozen', set of representative samples. Unfortunately, these approaches can be applied only to specific platforms where large numbers of representative samples have been collected. This is especially relevant when custom chips are designed, as is the case in many clinical applications. There remain a large range of platforms for measuring gene expression in use by researchers (), and single sample normalization methods are not currently available for many of these platforms. Additionally, methods such as quantile normalization and other forms of data scaling and transformation have become well known in the field and are often applied as standard steps in a data processing pipeline. Even if single sample normalization methods were universally available, public measures of gene expression are frequently preprocessed using a range of methods for cleaning, normalization and analysis, resulting in a range of expression values for the same gene across different platforms (). A more tractable solution is to build gene signatures that do not rely on raw gene expression values. We propose using the ranks of genes instead of their raw expression values under the assumption that any transformation applied to the data is rank-preserving.As a concrete example, we focus on the PAM50 signature for breast cancer subtyping (), which is used to assign patients with breast cancer to one of five molecular subtypes: Basal, Luminal A, Luminal B, Her2 and Normal. We show that when the number of patients in the test set changes, the predictions for a single patient may change dramatically. We also show that variation in patient populations being predicted upon leads to test set bias. Interestingly, PAM50 can be easily modified into a rank-based signature. We show that predictions from rank-based PAM50 are comparable to those from standard PAM50 and that predictions from rank-based PAM50 are invariant to test set bias. Test set bias is a failure of reproducibility of a genomic signature. In other words, the same patient, with the same data and classification algorithm, may be assigned to different clinical groups. A similar failing resulted in the cancellation of clinical trials that used an irreproducible genomic signature to make chemotherapy decisions (The Cancer). The implications of a patient's classification changing due to test set bias may be important clinically, financially and legally. In the example of PAM50, a patient's classification could affect a treatment or therapy decision. In other cases, an estimation of the patient's probability of survival may be too optimistic or pessimistic. The fundamental issue is that the patient's predicted quantity should be fully determined by the patient's genomic information, and the bias we will explore here is induced completely due to technical steps.