Motivation: Feature selection, identifying a subset of variables that are relevant for predicting a response , is an important and challenging component of many methods in statistics and machine learning. Feature selection is especially difficult and computationally intensive when the number of variables approaches or exceeds the number of samples, as is often the case for many genomic datasets. Results: Here, we introduce a new approach—the Bayesian Ising Approximation (BIA)—to rapidly calculate posterior probabilities for feature relevance in L2 penalized linear regression. In the regime where the regression problem is strongly regularized by the prior, we show that computing the marginal posterior probabilities for features is equivalent to computing the magnetizations of an Ising model with weak couplings. Using a mean field approximation, we show it is possible to rapidly compute the feature selection path described by the posterior probabilities as a function of the L2 penalty. We present simulations and analytical results illustrating the accuracy of the BIA on some simple regression problems. Finally, we demonstrate the applicability of the BIA to high-dimensional regression by analyzing a gene expression dataset with nearly 30 000 features. These results also highlight the impact of correlations between features on Bayesian feature selection. Availability and implementation: An implementation of the BIA in Cþþ, along with data for reproducing our gene expression analyses, are freely available at
IntroductionLinear regression is one of the most broadly and frequently used statistical tools. Despite hundreds of years of research on the subject (), modern applications of linear regression to large datasets present a number of new challenges. Modern applications of linear regression, such as Genome Wide Association Studies (GWAS), often consider datasets that have at least as many potential variables (or features) as there are data points (). Applying linear regression to high-dimensional datasets often involves selecting a subset of relevant features, a problem known as feature selection in the literature on statistics and machine learning (). Even for classical least-squares linear regression, it turns out that the associated feature selection problem is quite difficult (). The difficulties associated with feature selection are especially pronounced in genomics and GWAS. In general, the goal of many genomics studies is to identify a relationship between a small number of genes and a phenotype of interest, such as height or body V C The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com mass index (). For example, many GWAS seek to identify specific genetic mutations (called single nucleotide polymorphismsSNPs) that best explain the variation of a quantitative trait, such as height or body mass index, in a population (). Using various techniques, the trait is regressed against binary variables representing the presence or absence of the SNPs in order to find a subset of SNPs that are highly explanatory for the trait (). Although the number of individuals genotyped in such a study may be in the thousands or even tens of thousands, this pales in comparison to the number of potential SNPs which can be in the millions (). Moreover, the presence or absence of various SNPs tends to be correlated due to chromosome structure and genetic processes that induce the so-called linkage disequilibrium (). As a result, selecting the best subset of SNPs for the regression involves a search for the global minimum of a landscape that is both high dimensional (due to the large number of SNPs) and rugged (due to correlations between SNPs). The obstacles that make feature selection difficult in GWAS also occur in many other applications of linear regression to big datasets. In fact, the task of finding the optimal subset of features is proven, in general, to be NP-hard (). Therefore, it is usually computationally prohibitive to search over all possible subsets of features and one has to resort to other methods of feature selection. For example, forward (or backward) selection adds (or eliminates) one feature at a time to the regression in a greedy manner (). Alternatively, one may use heuristic methods such as Sure Independence Screening (SIS) (), which selects features independently based on their correlation with the response, or Minimum Redundancy Maximum Relevance (), which penalizes features that are correlated with each other. The most popular approaches to feature selection for linear regression, however, are penalized least-squares methods () that introduce a function that penalizes large regression coefficients. Common choices for the penalty function include an L2 penalty, called 'Ridge' regression (), and an L1 penalty, commonly referred to as LASSO regression (). Penalized methods for linear regression typically have natural interpretations as Bayesian approaches with appropriately chosen prior distributions. For example, L2 penalized regression can be derived by maximizing the posterior distribution obtained with a Gaussian prior on the regression coefficients. Similarly, L1 penalized regression can be derived by maximizing the posterior distribution obtained with a Laplace (i.e. double-exponential) prior on the regression coefficients. While penalized regression methods essentially aim to find the features that maximize a posterior distribution they do not allow one to actually compute posterior probabilities, which provide information about confidence in a Bayesian framework. Calculating these posterior probabilities generally requires Monte Carlo methods, which can be very computationally demanding in high dimensions (). Thus, in order to apply Bayesian approaches to feature selection to highdimensional problems it is necessary to develop approximate methods for computing posterior probabilities that bypass the need for extensive sampling from the posterior distribution. Inspired by the success of statistical physics approaches to hard problems in computer science () and statistics (), we study high-dimensional regression with 'strongly regularizing' prior distributions. A strongly regularizing prior distribution is one that exerts a significant influence on the posterior distribution even when the sample size goes to infinity. The definition will be made more precise later. In this strongly regularized regime, we show that the marginal posterior probabilities of feature relevance for L2 penalized regression are well-approximated by the magnetizations of an appropriately chosen Ising modela widely studied model from physics used to describe magnetic materials (). For this reason, we call our approach the Bayesian Ising Approximation (BIA) of the posterior distribution. Using the BIA, the posterior probabilities can be computed without resorting to Monte Carlo simulation using an efficient mean field approximation that facilitates the analysis of very high-dimensional datasets. We envision the BIA as part of a two-stage procedure where the BIA is applied to rapidly screen irrelevant variables, i.e. those that have low rank in posterior probability, before applying a more computationally intensive cross-validation procedure to infer the regression coefficients for the reduced feature set. This study is especially well suited to modern feature selection problems where the number of features, p, is often larger than the sample size, n. Our approach differs significantly from previous methods for feature selection. Traditionally, penalized regression and related Bayesian approaches have focused on the 'weakly regularized regime' where the effect of the prior is assumed to be negligible as the sample size tends to infinity. The underlying intuition for considering the weak-regularization regime is that as long as the prior (i.e. the penalty parameter) is strong enough to regularize the inference problem, a less influential prior distribution should be better suited for feature selection and prediction tasks because it 'allows the data to speak for themselves' (). In the machine learning literature, the penalty parameter is usually chosen using cross validation to maximize out-of-sample predictive ability (). A similar esthetic is also reflected in the abundant literature on 'objective' priors for Bayesian inference (). As expected, these weakly regularizing approaches perform well when the sample size exceeds the number of features n ) p. However, very strong priors may be required for high-dimensional inference where the number of features can greatly exceed the sample size p ) n. Our BIA approach exploits the large penalty parameter in this strongly regularized regime to efficiently calculate marginal posterior probabilities using methods from statistical physics. The article is organized as follows: in Section 2.1, we review Bayesian linear regression; in Section 2.2, we derive the BIA using a series expansion of the posterior distribution and describe the associated algorithm for variable selection; and in Section 3.1, we present analytical results and simulations on the performance of the BIA using features with a constant correlation, in Section 3.2 we analyze a real dataset for predicting bodyfat percentage from 12 different body measurements and in Section 3.3 we analyze a real dataset for predicting a quantitative phenotypic trait from data on the expression of 28 395 genes in soybeans.
DiscussionTo summarize, we have shown that Bayesian feature selection for L2 penalized regression, in the strongly regularized regime, corresponds to an Ising model, which we call the BIA. Mapping the posterior distribution to an Ising model that has simple expressions for the local fields and couplings using a controlled approximation opens the door to analytical studies of Bayesian feature selection using the vast number of techniques developed in physics for studying the Ising model. It will be interesting to see if our analyses can be generalized to study Bayesian feature selection for many statistical techniques other than linear regression, as well as other prior distributions. From a practical standpoint, the BIA provides an algorithm to efficiently compute Bayesian feature selection paths for L2 penalized regression. Using our approach, it is possible to compute posterior probabilities of feature relevance for very high-dimensional datasets such as those typically found in genomic studies. Unlike most previous work of feature selection, the BIA is ideally suited for large genomic datasets where the number of features can be much greater than the sample size, p ) n. The underlying reason for this is that we work in strongly regularized regime where the prior always has a large influence on the posterior probabilities. This is in contrast to previous works on penalized regression and related Bayesian approaches that have focused on the 'weakly regularized regime' where the effect of the prior is assumed to be small. Moreover, we have identified a sharp threshold for the regularization parameter k   n1  pr where the BIA is expected to break down. This threshold depends on the sample size, n, number of features, p, and root-mean-squared correlation between features, r. The threshold at which the BIA breaks down occurs precisely at the transition from the strongly regularized to the weakly regularized regimes where the prior and the likelihood have a comparable influence on the posterior distribution. This study also highlights the importance of accounting for correlations between features when assessing statistical significance in large datasets. When the number of features is large, even small correlations can cause a huge reduction in the posterior probabilities of features. For example, our analysis of a dataset including the expression of 28 395 genes demonstrates that the resulting posterior probabilities of gene relevance may be very close to value representing random chance P k s j jy  1=2 when p ) n and the genes are moderately correlated, e.g. r % 0:29. This is likely to have important implications for assessing the results of GWAS studies where such correlations are often ignored. Moreover, we suggest that it is generally not reasonable to choose a posterior probability threshold for judging significance on very high-dimensional problems. Instead, the BIA can be used as part of a two-stage procedure, where the BIA is applied to rapidly screen irrelevant variables, i.e. those that have low rank in posterior probability, before applying a more computationally intensive crossvalidation procedure to infer the regression coefficients. The computational efficiency of the BIA and the existence of a natural threshold for the penalty parameter where the BIA works make this procedure ideally suited for such two-stage procedures.. True positive rates for feature selection with (a) correlated and (b) uncorrelated genes. Features were selected by taking the q genes with highest posterior probability at k  0:5k . The uncorrelated genes were created by randomly shuffling the correlated genes. The root mean squared correlation among the correlated genes was r  0.28 compared with r  0.7 for the uncorrelated genes