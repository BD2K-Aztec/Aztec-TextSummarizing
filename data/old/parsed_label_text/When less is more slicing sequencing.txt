Motivation: As the invention of DNA sequencing in the 70s, computational biologists have had to deal with the problem of de novo genome assembly with limited (or insufficient) depth of sequenc-ing. In this work, we investigate the opposite problem, that is, the challenge of dealing with excessive depth of sequencing. Results: We explore the effect of ultra-deep sequencing data in two domains: (i) the problem of decoding reads to bacterial artificial chromosome (BAC) clones (in the context of the combinatorial pooling design we have recently proposed), and (ii) the problem of de novo assembly of BAC clones. Using real ultra-deep sequencing data, we show that when the depth of sequencing increases over a certain threshold, sequencing errors make these two problems harder and harder (instead of easier, as one would expect with error-free data), and as a consequence the quality of the solution degrades with more and more data. For the first problem, we propose an effective solution based on divide and conquer: we slice a large dataset into smaller samples of optimal size, decode each slice independently, and then merge the results. Experimental results on over 15 000 barley BACs and over 4000 cowpea BACs demonstrate a significant improvement in the quality of the decoding and the final assembly. For the second problem, we show for the first time that modern de novo assemblers cannot take advantage of ultra-deep sequencing data. Availability and implementation: Python scripts to process slices and resolve decoding conflicts are available from
IntroductionWe have recently introduced in () a novel protocol for clone-by-clone de novo genome sequencing that leverages recent advances in combinatorial pooling design (also known as group testing). In our sequencing protocol, subsets of non-redundant genome-tiling bacterial artificial chromosomes (BACs) are chosen to form intersecting pools, then groups of pools are sequenced on an Illumina sequencing instrument via low-multiplex (DNAbarcoding). Sequenced reads can be assigned/decoded to specific BACs by relying on the combinatorial structure of the pooling design: since the identity of each BAC is encoded within the pooling pattern, the identity of each read is similarly encoded within the pattern of pools in which it occurs. Finally, BACs are assembled individually, simplifying the problem of resolving genome-wide repetitive sequences. In (), we reported preliminary assembly statistics on the performance of our protocol in four barley (Hordeum vulgare) BAC sets (Hv3Hv6). Further analysis on additional barley BAC sets and two genome-wide BAC sets for cowpea (Vigna unguiculata) revealed that the raw sequence data for some datasets was of significantly lower quality (i.e. higher sequencing error rate) than others. We realized that our decoding strategy, solely based on the software HASHFILTER (), was insufficient to deal with the amount of noise in poor quality datasets. We attempted to (i) trim/clean the reads more aggressively or with different methods, (ii) identify low quality tiles on the flow cell and remove the corresponding reads (e.g. tiles on the 'bottom middle swath'), (iii) identify positions in the reads possibly affected by sequencing 'bubbles' and (iv) post-process the reads using available error-correction software tools (e.g. QUAKE, REPTILE). Unfortunately, none of these steps accomplished a dramatic increase in the percentage of reads that could be assigned to BACs, indicating that the quality of the dataset did not improve very much. These attempts to improve the outcome led however, to a serendipitous discovery: we noticed that when HASHFILTER processed only a portion of the dataset, the proportion of assigned/decoded reads increased. This observation initially seemed counterintuitive: we expected that feeding less data into our algorithm meant that we had less information to work with, thus decrease the decoding performance. Instead, the explanation is that when data is corrupted, more (noisy) data is not better, but worse. The study reported here directly addresses the observation that when dealing with large quantities of imperfect sequencing data, 'less' can be 'more'. More specifically, we report (i) an extensive analysis of the trade off between the size of the datasets and the ability of decoding reads to individual BACs; (ii) a method based on 'slicing' datasets that significantly improves the number of decoded reads and the quality of the resulting BAC assemblies; (iii) an analysis of BAC assembly quality as a function of the depth of sequencing, for both real and synthetic data. Our algorithmic solution relies on a divide-and-conquer approach, as illustrated in.
DiscussionBecause the introduction of DNA sequencing in the 70s, scientists had to come up with clever solutions to deal with the problem ofde novo genome assembly with limited depth of sequencing. As the cost of sequencing keeps decreasing, one can expect that computational biologists will have to deal with the opposite problem: excessive amount of sequencing data. The Lander-Waterman-Roach theory () has been the theoretical foundation to estimate gap and contig lengths as a function of the depth of sequencing. We do not have a theory that would explain why the quality of the assembly starts degrading when the depth is too high. Possible factors include the presence (in real data) of chimeric reads, sequencing errors, and read duplications, or their combination thereof. In this study, we report on the de novo assembly of BAC clones, which are relatively short DNA fragments (100150 kbp). With current sequencing technology it is very easy to reach depth of sequencing in the range of 100010 000 and study how the assembly quality changes as the amount of sequencing data increases. Our experiments show that when the depth of sequencing exceeds a threshold the overall quality of the assembly starts degrading (). This appears to be a common problem for several de novo assemblers (). The same behavior is observed for the problem of decoding reads to their source BAC (), which is the main focus of this article. The important question is how to deal with the problem of excessive sequencing depth. For the decoding problem we have presented an effective 'divide and conquer' solution: we 'slice' the data in subsamples, decode each slice independently, then merge the results. In order to handle conflicts in the BAC assignments (i.e. reads that appear in multiple slices that are decoded to different sets of BACs), we devised a simple set of voting rules. The question that is still open is what to do for the assembly problem: one could assemble slices of the data independently, but it is not clear how to merge the resulting assemblies. In general, we believe that the problem of de novo sequence assembly must be revisited from the ground up under the assumption of ultra-deep coverage.