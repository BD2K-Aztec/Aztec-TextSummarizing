Motivation: Whole-genome low-coverage sequencing has been combined with linkage-disequilibrium (LD)-based genotype refinement to accurately and cost-effectively infer genotypes in large cohorts of individuals. Most genotype refinement methods are based on hidden Markov models, which are accurate but computationally expensive. We introduce an algorithm that models LD using a simple multivari-ate Gaussian distribution. The key feature of our algorithm is its speed. Results: Our method is hundreds of times faster than other methods on the same data set and its scaling behaviour is linear in the number of samples. We demonstrate the performance of the method on both low-and high-coverage samples. Availability and implementation: The source code is available at https://
IntroductionThe 1000 Genomes Project (1000GP) has pioneered the approach of combining low-coverage whole-genome sequencing (LCWGS) with linkage disequilibrium (LD)-based genotype refinement to successfully build large panels of accurately genotyped individuals (). This has provided a cost-effective alternative to sequencing many individuals at high-coverage. However, genotype refinement has a large computational burden. For example,quote around 32 compute years to perform haplotype estimation on 1092 LCWGS individuals using the 1000GP haplotype estimation pipeline. This figure measures the cost of haplotype phasing (which our method does not address) as well as genotype refinement. Given increasing sample sizes, decreasing sequencing costs and the typically super-linear scaling of refinement algorithms, we are fast approaching a point where computation will account for a substantial proportion of the cost of such analyses. Low-coverage genotyping typically proceeds by calculating genotype likelihoods (GLs) at a fixed set of variants (SNPs and small indels) from read alignments, the variant list being created at an earlier variant discovery step. These GLs reflect the likelihood of the read data conditional on each of the three possible genotypes (assuming a bi-allelic site). These uncertain GLs are then refined into genotypes by exploiting LD, the correlation between physically close variants across individuals. This final step is often referred to as genotype refinement and involves one (or more) phasing and imputation algorithms. The most accurate phasing and imputation techniques typically employ hidden Markov models (HMMs) which are computationally demanding, examples include Beagle (), Thunder () and SHAPEIT (). The final genotypes of 1000GP were created using a combination of SHAPEIT and Beagle; starting haplotypes were generated with the faster Beagle method and then were further refined using the slower, and more accurate, SHAPEIT (). A closely related problem is the imputation of variants into study samples assayed on DNA microarrays from reference panels of sequenced individuals (). Several very fast methods have recently emerged for this scenario (). These rely on theavailability of phased haplotypes for both study and reference data and it is not clear such algorithms will generalize to the LCWGS use case. An alternative to HMM-based imputation is simply to predict genotypes as linear combinations of other genotypes at physically close flanking markers, modelling the correlation between variants as a multivariate normal (MVN) distribution. This idea was first introduced by Wen and Stephens (2010), where it was used in the more traditional setting of imputing genotypes into DNA microarray samples from a reference panel. Menelaou andintroduced a related approach, MVNcall, that performs imputation on LCWGS data for which the individual has also been assayed on a DNA microarray, exploiting the 'backbone' of confident microarray genotypes to improve genotypes at non-microarray sites. We introduce a new technique based on MVN representations of LD that extends these ideas to the LCWGS-only imputation scenario. The method exploits various efficient linear algebra operations, making it hundreds of times faster than the fastest HMM method. This speed comes with a decrease in accuracy compared with HMMs, but is still substantially more accurate than genotype calls made using no LD information. In the 'Methods' section, we outline the model and its implementation. In our 'Results' section, we contrast the speed and accuracy of our technique with Beagle on 2535 samples from 1000GP Phase 3 (LCWGS) and 3781 samples taken from the UK10K project (UK10K). Finally, we demonstrate the applicability of LD-based genotype refinement in the high-coverage WGS setting, something that has not been investigated to date. The method is implemented in a software package called MarViN (MultiVariate Normal imputation) and is freely available under the GPLv3 license.
DiscussionThe algorithm presented in this article is at least two orders of magnitude faster than Beagle on the UK10K cohort. Although this speed does come with a decrease in accuracy (particularly for rare variants), our method still makes nearly 10-fold fewer errors than a genotyping routine that does not take LD into account. The rapidly growing size of reference panels may soon preclude the use of super-linear complexity techniques such as Beagle, since computation will become too expensive. For example, the Haplotype Reference Consortium () has collected 32 488 LCWGS samples to create a reference panel for imputation. Extrapolating from, it seems unlikely it would be tractable to run Beagle on a cohort of this size. One possible use of MarViN would be to quickly generate an initial estimate of genotypes, which could then be supplied as starting values to a more sophisticated routine, reducing the number of iterations the latter needs to perform. MarViN might also be an ideal routine for intermediate coverage (%15) projects. The reduced accuracy of MarViN compared to Beagle at lower frequency variation is likely due to the limitations of modelling the population using one vector of allele frequencies and one covariance matrix. This simplistic model may not capture more subtle population substructure. Notably MarViN performs better on the more homogeneous UK10K cohort than on the 1000GP cohort which has far more population structure (although also has a smaller sample size). One possible way to improve this situation would be to add more flexibility to the MarViN model by using an MVN mixture distribution, but we leave this for future work. We have also demonstrated the efficacy of genotype refinement in the high-coverage scenario, the first such investigation to our knowledge. A modest gain in recall for SNPs was achieved at a cost of a negligible decrease in precision. We also attempted refining indels with this approach, gains in recall were indeed observed but were accompanied by unacceptable increases in the false-discovery rate (FDR). This may be due to a higher FDR in the 1000GP indels and could perhaps be solved via aggressive filtering. Although the improvements seen on high-coverage data are modest, we nevertheless believe it noteworthy that results achieved from high-coverage data can be improved at all by this method. Moreover the efficiency of our method means it adds little additional overhead to processing pipelines for WGS data, whereas genotype refinement using existing HMM-based methods would be a considerable computational undertaking.