Motivation: Computational methods are essential to extract action-able information from raw sequencing data, and to thus fulfill the promise of next-generation sequencing technology. Unfortunately, computational tools developed to call variants from human sequen-cing data disagree on many of their predictions, and current methods to evaluate accuracy and computational performance are ad hoc and incomplete. Agreement on benchmarking variant calling methods would stimulate development of genomic processing tools and facilitate communication among researchers. Results: We propose SMASH, a benchmarking methodology for evaluating germline variant calling algorithms. We generate synthetic datasets, organize and interpret a wide range of existing benchmark-ing data for real genomes and propose a set of accuracy and computational performance metrics for evaluating variant calling methods on these benchmarking data. Moreover, we illustrate the utility of SMASH to evaluate the performance of some leading single-nucleotide polymorphism, indel and structural variant calling algorithms. Availability and implementation: We provide free and open access online to the SMASH tool kit, along with detailed documentation, at
INTRODUCTIONNext-generation sequencing is revolutionizing biological and clinical research. Long hampered by the difficulty and expense of obtaining genomic data, life scientists now face the opposite problem: faster, cheaper technologies are beginning to generate massive amounts of new sequencing data that are overwhelming our technological capacity to conduct genomic analyses (). Computational processing will soon become the bottleneck in genome sequencing research, and as a result, computational biologists are actively developing new tools to more efficiently and accurately process human genomes and call variants, e.g. SAMTools (), GATK (), Platypus (http://www.well.ox.ac.uk/platypus), BreakDancer (), Pindel () and Dindel (). Unfortunately, single-nucleotide polymorphism (SNP) callers disagree as much as 20% of the time (), and there is even less consensus in the outputs of structural variant algorithms (). Moreover, reproducibility, interpretability and ease of setup and use of existing software are pressing issues currently hindering clinical adoption (). Indeed, reliable benchmarks are required to measure accuracy, computational performance and software robustness, and thereby improve them. In an ideal world, benchmarking data to evaluate variant calling algorithms would consist of several fully sequenced, perfectly known human genomes. However, ideal validation data do not exist in practice. Technical limitations, such as the difficulty in accurately sequencing low-complexity regions, along with budget constraints, such as the cost to generate high-coverage Sanger reads, limit the quality and scope of validation data. Nonetheless, significant resources have already been devoted to generate subsets of benchmarking data that are substantial enough to drive algorithmic innovation. Alas, the existing data are not curated, thus making it extremely difficult to access, interpret and ultimately use for benchmarking purposes. Owing to the lack of curated ground truth data, current benchmarking efforts with sequenced human genomes are lacking. The majority of benchmarking today relies on either simulated data or a limited set of validation data associated with real-world datasets. Simulated data are valuable but do not tell the full story, as variant calling is often substantially easier using synthetic reads generated via simple generative models. Sampled data, as mentioned earlier, are not well curated, resulting in benchmarking efforts, such as the Genome in a Bottle Consortium () and the Comparison and Analytic Testing resource (GCAT) (http://www.bioplanet. com/gcat), that rely on a single dataset with a limited quantity of validation data. Rigorously evaluating predictions against a validation dataset presents several additional challenges. Consensus-based evaluation approaches, used in various benchmarking efforts (), may be misleading. Indeed, *To whom correspondence should be addressed.
DISCUSSIONHundreds of variant calling algorithms have been proposed, and the majority of these algorithms have been benchmarked in some form (see detailed discussion in Section A in the Supplementary Material). To the best of our knowledge, none of these existing benchmarking methodologies accounts for noise in validation data, ambiguity in variant representation or computational efficiency of variant calling methods in a consistent and principled fashion. Given the rapid growth of next-generation sequencing data, the need for a robust and standardized methodology has never been greater.The IT industry serves as an illuminating case study in the context of benchmarking. Similar to next-generation sequencing technology, the IT industry has benefited greatly from the additional hardware resources provided by Moore's Law. However, the industry's rapid progress also hinged on the agreement on proper metrics to measure performance as well as consensus regarding the best benchmarks to run to fairly evaluate competing systems (). Prior to this industry-wide agreement, each company invented its own metrics and ran its own set of benchmarking evaluations, making the results incomparable and customers suspicious of them. Even worse, engineers at competing companies were unable to determine the usefulness of their competitors' innovations, and so the competition to improve performance occurred only within companies rather than between them. Once the IT industry agreed on a fair playing field, progress accelerated, as engineers could see which ideas worked well (and which did not), and new techniques were developed to build on promising approaches. Similarly, we believe that SMASH could help accelerate progress in the field of genomic variant calling. We have compiled a rich collection of datasets and developed a principled set of evaluation metrics that together allows for quantitative evaluation of variant calling algorithms in terms of accuracy, computational efficiency and robustness/ease-of-use (via ability to run on AWS). Moreover, although SMASH currently focuses on benchmarking variant calling algorithms for normal human genomes, we believe that the motivating ideas behind SMASH, along with the tools developed as part of SMASH, will be useful in devising analogous variant calling benchmarking toolkits for human cancer genomes and for the genomes of other organisms. Finally, we view SMASH as a work in progress, as the contents of SMASH reflect (and are limited by) existing technologies. SMASH currently has limited ground truth data for human genomes, and the validation data across datasets are enriched in 'easier' non-repetitive regions owing to underlying sequencing and chip biases. Like any benchmarking suite, SMASH must evolve over time to stay relevant. As new sources of validation data become available, e.g. the NA12878 knowledge base (M.DePristo, 2013, personal communication) or curated variants from Illumina's Platinum Genome, these datasets should be incorporated into SMASH. Existing datasets should also be updated to keep them fresh and prevent algorithms from 'overfitting' to stale benchmarks, and benchmarking datasets should be deprecated as new data sources obviate their utility.