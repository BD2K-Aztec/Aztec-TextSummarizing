Motivation: The pre-estimate of the proportion of null hypotheses (π 0) plays a critical role in controlling false discovery rate (FDR) in multiple hypothesis testing. However, hidden complex dependence structures of many genomics datasets distort the distribution of p-values, rendering existing π 0 estimators less effective. Results: From the basic non-linear model of the q-value method, we developed a simple linear algorithm to probe local dependence blocks. We uncovered a non-static relationship between tests p-values and their corresponding q-values that is influenced by data structure and π 0. Using an optimization framework, these findings were exploited to devise a Sliding Linear Model (SLIM) to more reliably estimate π 0 under dependence. When tested on a number of simulation datasets with varying data dependence structures and on microarray data, SLIM was found to be robust in estimating π 0 against dependence. The accuracy of its π 0 estimation suggests that SLIM can be used as a stand-alone tool for prediction of significant tests. Availability: The R code of the proposed method is available at
INTRODUCTIONExperiments in the 'omics' fields often involve hundreds to thousands of dependent variables, such as those encountered in genetic linkage (), gene expression () and metabolic profiling (). Multiple testing correction is necessary in order to identify statistically significant variables for subsequent analyses, without a flood of false positives called by chance. The false discovery rate (FDR) approach () and its many variants, including the positive FDR (pFDR)-based q-value statistic (), have been widely used for false discovery control in multiple hypothesis testing. The q-value represents the minimum pFDR that can occur for any possible  greater than or equal to a p-value point. Given a set of p-values ranked in an increasing order, p i ,i = 1,2,...,m (m is the total number of tests), * To whom correspondence should be addressed. the q-value is calculated as:This formula indicates that  0 is the only unknown parameter to be pre-estimated. The accuracy of the  0 estimate directly affects the q-value calculation and the optimal control of FDR. A reliable  0 estimate also provides a simple prediction of the number of genes that are differentially expressed under the experimental conditions in gene expression analysis. A widely used method is the -estimator (), i.e.where   (0,1) is a pre-chosen cutoff and #{p i } is the number of p-values greater than . Considering the non-linear relationship between  0 () and , we refer to this estimator as the non-linear model. The underlying assumption is that the largest p-values are most likely to come from a uniform distribution of null features in the range (0,1). In practice, there is a bias versus variance tradeoff for choosing an optimal  for the estimation of  0 (). To balance this trade-off, Storey and Tibshirani (2003) used a natural cubic spline smoothing (CSS) method to fit the nonlinear relationship across a range of , as implemented in the QVALUE software.proposed an average estimate (AE) method, which takes advantage of multiple non-linear estimators to reduce the  0 estimation variance. Markitsis and Lai (2010) recently proposed a censored beta mixture model (CBMM), based upon the beta-uniform mixture (BUM) method of Pounds and, to approximate the p-value distribution. Both the original and the q-value-based FDR procedures were developed for independent test statistics. Although these methods can be applied to weakly dependent data (), they are unreliable for datasets with inherent multiplicity and complex dependence (). The BUM-based methods also cannot effectively handle irregular p-value distribution (). To specifically handle multiple testing under dependence, Efron (2007a) developed an empirical Bayesian framework, called Locfdr, to remedy the effects of data correlation. However, Locfdr is only applicable to data with a large (0.9)  0 (). We have developed a linear  0 estimator from the non-linear method of Storey (2002) to explore local properties of p-value distributions as a means to better capture data dependence. When applied to data with a uniform null p-value distribution, the slope and intercept of the linear model reflect the proportions of null
DISCUSSIONAn important issue in multiple hypothesis testing is how to deal with the dependencies hidden among thousands of tests. Efron (2007b) has shown that correlation among variables considerably changes the theoretical null distribution patterns. We also observed that dependence structures lead to distorted distributions of null p-values, and this likely underlies the relatively large  0 estimation errors by methods developed under the assumption of data independence. The Locfdr approach () was specifically designed to handle data containing dependence structures, but it is only applicable when  0 is large. CBMM uses a censored beta-uniform mixture model to fit the distorted p-value distribution, alleviating to some degree the difficulty caused by dependence. SLIM is based on a linear model transformed from the non-linear  estimator (). The superior performance of SLIM can be ascribed to its data partitioning and optimization schemes. SLIM uses a sliding linear model to partition data into local dependence blocks. This reduces data complexity, while enabling SLIM to utilize information from a broader range of p-value distribution for  0 estimation. Using simulated data, we uncovered a non-static relationship between p-values and q-values of a given set of tests that is influenced by data structure and  0 scenarios. SLIM employs an optimization scheme to explicitly exploit this relationship by minimizing the difference (L) between the fractions of tests called significant by the p-value and q-value methods. The optimization scheme is particularly important to balance between positive and negative errors, thereby achieving FDR control. Thus, SLIM effectively handles hidden dependence without the need to empirically adjust the null p-value distributions. The selection of a proper q-value cutoff in multiple hypothesis testing is not trivial, especially given the dependence of the q-value calculation on the  0 estimation. Recalling that the number of significant tests in a given experiment is simply  1 m, we argue that an accurate estimation of  0 can serve as an alternative to q-value-based significance testing. Using simulated data, SLIM was shown to outperform the other methods, achieving the lowest FDR overall, accompanied by the highest degree of accuracy in declaring significant tests. This suggests that SLIM can be used as a stand-alone tool in multiple testing for determination of significant tests. In summary, SLIM is a robust estimator especially suited for datasets with non-uniform p-value distribution patterns due to data dependence. SLIM is computationally efficient and easy to implement. It requires four user-selected parameters, n,  1 , p max and B (the latter two are proxies of the quantile parameter ). We recommend n = 10,  1 = 0.1, B = 100 and p max = 0.05 as the default settings. Users may wish to test a higher B to ensure sufficient granularity, and a range of p max for optimal  selection and  0 estimate. In addition to microarray analysis, SLIM has been applied to metabolite profiling analysis in our laboratory, and should be applicable to a wide range of experiments