Motivation: High-throughput single-cell quantitative real-time poly-merase chain reaction (qPCR) is a promising technique allowing for new insights in complex cellular processes. However, the PCR reaction can be detected only up to a certain detection limit, whereas failed reactions could be due to low or absent expression, and the true expression level is unknown. Because this censoring can occur for high proportions of the data, it is one of the main challenges when dealing with single-cell qPCR data. Principal component analysis (PCA) is an important tool for visualizing the structure of high-dimensional data as well as for identifying subpopulations of cells. However, to date it is not clear how to perform a PCA of censored data. We present a probabil-istic approach that accounts for the censoring and evaluate it for two typical datasets containing single-cell qPCR data. Results: We use the Gaussian process latent variable model framework to account for censoring by introducing an appropriate noise model and allowing a different kernel for each dimension. We evaluate this new approach for two typical qPCR datasets (of mouse embryonic stem cells and blood stem/progenitor cells, respectively) by performing linear and non-linear probabilistic PCA. Taking the censoring into account results in a 2D representation of the data, which better reflects its known structure: in both datasets, our new approach results in a better separation of known cell types and is able to reveal subpopulations in one dataset that could not be resolved using standard PCA. Availability and implementation: The implementation was based on the existing Gaussian process latent variable model toolbox (https:// github.com/SheffieldML/GPmat); extensions for noise models and kernels accounting for censoring are available at http://icb.helmholtz
INTRODUCTION
DISCUSSIONConventional approaches for PCA of censored data where values beyond the detection limit are substituted with the detection limit can yield strongly biased results. We have proposed a novel approach for performing dual PCA of censored data. Our new approach resulted in a mapping between low-dimensional and high-dimensional space such that more censored data points were mapped correctly to values greater than the detection limit. It was previously shown that for single-cell qPCR data, it is crucial to explicitly model the population of non-detects when performing a statistical test of univariate differential expression (). To date no approaches for dealing with this issue for multivariate analyses such as PCA have been proposed. We evaluated our new approach for two different real-world datasets comprising measurements of single-cell qPCR data. For both datasets, the PCA representations better reflected the known structure of the data when the censoring was explicitly considered. We evaluated using a linear as well as a non-linear kernel, and for both datasets accounting for non-linearities resulted in better visualizations. In contrast to using a linear kernel (i.e. PCA), this comes at the price of losing interpretabilityalthough in the linear case loadings can be easily visualized in a bi-plot, in the non-linear case this is more difficult because loadings change across the 2D plot. Whether trading off interpretability for complexity is beneficial depends highly on the dataset under consideration and any non-linearities present. In the context of single-cell qPCR data, our analyses suggest that a non-linear kernel is necessary to capture the typical complex dependency structure of such data. For linear kernels (corresponding to standard PCA) as well as for non-linear kernels (allowing for interactions), our new approach yielded considerably lower nearest neighbour error rates with reductions of up to 29% in the linear case. Furthermore, in the case of mESC data, the structure of subpopulations was reflected better in the case when censoring was taken into account in the non-linear case: in contrast to nonlinear probabilistic PCA with the substitution approach, two subpopulations corresponding to cells from the 16-cell stage with high Id2 expression and cells in the ICM with high Fgf4 expression could be identified. These known subpopulations were previously identified in a univariate analysis of cells from the same cell stage. However, this standard approach has several drawbacks because it can become unfeasible when too many genes are measured simultaneously. Furthermore, only univariate patterns can be identified, whereas important information may lie in multivariate patterns, which could be defined by the differential expression of several genes. Finally, when analysing univariate distributions or correlations between two genes for cells from the same cell stage, the identified subpopulations cannot be put in context with other cells from other cell stages. In contrast, when performing a probabilistic (kernel) PCA of all cell stages, it is possible to identify complex multivariate subpopulations, and by simultaneously displaying all cells, the PCA plot provides an intuitive illustration of the relation between all cell populations. This was achieved by implementing a GPLVM with different kernels for each dimension. Censoring was accounted for by a probit noise level. The steepness parameter of the probit function was learnt together with other kernel parameters, resulting in a parameter-free approach for PCA of censored data. Although our approach was designed for accounting for uncertainties in single-cell qPCR data, related issues can be found in single-cell RNA-Seq data. In contrast to single-cell qPCR, however, high levels of technical noise are present in all commonly used protocols for single-cell RNA-Seq (). This technical noise is particularly strong for low levels of expression and dominates all other uncertainties (like censoring). Although these uncertainties are inherently different from the censoring found in single-cell qPCR, the flexible framework of Gaussian processes allows us to account for these uncertainties in a straightforward manner by using an additional term in the (Gaussian) noise model reflecting the technical noise, which can be estimated using the approach suggested by. Although single-cell RNA-Seq data are generated in the form of read counts, it is crucial to perform normalization steps accounting for different cell sizes, different sequencing depth and, depending on the protocol, different transcript lengths (). Such normalization can be performed by calculating reads per kilo base per million (RPKM) and fragments per kilobase of exon per million fragments mapped (FPKM) or using DEseq-inspired normalization procedures (). After normalization, gene expression is measured on a continuous scale such thatafter an appropriate variancestabilizing transformation (e.g. log-transformation)GPLVM can be applied without modification. Because efficient implementations allow fast processing of datasets with tens of thousands of genes and hundreds of cells without overfitting, it is a promising tool for analysing such datasets. The main drawback of our proposed approach is that it scales cubically with the number of cells, which may be prohibitive when the number of analysed cells is very large (4 410 4 ). Although standard GPLVMs are time-consuming, too, significant speed-ups can be achieved because of sharing the kernel across all dimensions and using a spherical noise model. However, if necessary, approximations resulting in sparse covariance matrices commonly used in Gaussian process literature could be applied for our framework, too. For the application to single-cell qPCR data, we found that this was not necessary because computation times were in the order of only a few hours on a standard laptop. We acknowledge that this is a considerable increase of time compared with standard PCA, which can be performed when using the substitution approach to deal with censored data. In applications with only a small fraction of, taking censoring into account with probit noise model and fixed (E) and probit noise model with learnt from data (F). (GJ) GPLVM with RBF kernel for blood data. Standard GPLVM with substitution approach (G), taking censoring into account with probit noise model and fixed (H) and probit noise model with learnt from data (I). The background intensity indicates the relative uncertainty of the mapping with black pixels corresponding to the highest uncertainty of the mapping 1874censored data points, this rather large increase in runtime may result in only minor changes of the PCA representation, and simpler approaches such as the substitution approach or treating the data as missing may be a valid alternative if runtime is an issue. However, in the case of single-cell qPCR data, we have shown that taking censoring into account avoids a potential bias in low-dimensional representations due to the censoring. This in turn can result in better biological insights: first, our approach can yield a better separation of different cell types; second, it may even reveal new biologically meaningful subpopulations that may be obscured because of a bias introduced by the censoring. When designing single-cell qPCR experiments, the quantification of heterogeneities and the reliable identification of new subpopulations are often key goals. That is why we believe that our approach will be of interest for many practitioners working with censored data, especially in the field of high-throughput singlecell qPCR.
CONCLUSIONWe have presented a new approach for performing probabilistic PCA for censored data within the framework of GPLVMs. Therefore, we implemented an appropriate noise model and allowed different kernels for each dimension. We showed that for single-cell qPCR data with a high fraction of censored data points, the resulting probabilistic (kernel) PCA representations reflected the true structure of the data better than conventional approaches. In two real-world datasets, known cell types could be better separated when censoring was taken into account, and in one dataset several distinct subpopulations could be revealed that could not be resolved with standard PCA.