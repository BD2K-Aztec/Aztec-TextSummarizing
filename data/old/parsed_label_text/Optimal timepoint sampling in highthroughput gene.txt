Motivation: Determining the best sampling rates (which maximize information yield and minimize cost) for time-series high-throughput gene expression experiments is a challenging optimization problem. Although existing approaches provide insight into the design of optimal sampling rates, our ability to utilize existing differential gene expression data to discover optimal timepoints is compelling. Results: We present a new data-integrative model, Optimal Timepoint Selection (OTS), to address the sampling rate problem. Three experiments were run on two different datasets in order to test the performance of OTS, including iterative-online and a top-up sampling approaches. In all of the experiments, OTS outperformed the best existing timepoint selection approaches, suggesting that it can optimize the distribution of a limited number of timepoints, potentially leading to better biological insights about the resulting gene expression patterns.
INTRODUCTIONTime-series high-throughput gene expression experiments can measure the expression levels of tens of thousands of genes in a biological sample over time and provide dynamic information which can be used to construct regulatory networks and infer regulatory relationships among genes (). Although there are several thousand time-series microarray and RNA-seq datasets on the Gene Expression Omnibus (GEO) database (), as of June 2012, most of these contain very few timepoints.shows that 475% of these datasets (in which 'time' has been set as a subset variable type) in GEO contain five or fewer timepoints. Given that researchers are often limited to being able to sample very few timepoints, it is extremely important to choose the most appropriate timepoints for observing strong target gene expression pattern changes. With a fixed number of samples, researchers can choose between (i) a very densely sampled short time-series experiment, in which important gene regulation events that do not occur quickly may be missed or (ii) a sparsely sampled long time-series experiment, where improperly positioned timepoints can lead to missing rapid but important regulation events and can also lead to temporal aggregation bias (which reduces the ability to infer actual regulatory relationships;). Determining the best sampling timepoints for sparsely sampled time-series high-throughput experiments is a challenging optimization problem that is frequently discussed in the biological literature (). An active learning algorithm has been developed for iteratively choosing timepoints to sample, using the uncertainty in the interpolation of the currently estimated time-dependent curve as the objective function (). The performance evaluation in this study showed that this algorithm can find optimal timepoints such that majority cycling yeast genes can be identified. However, to capture the differential gene expression patterns, the interpolation step requires a minimum of five timepoints (according to their online documentation), so it would not have been applicable for 75% of the existing datasets in GEO and would have only been able to predict very few timepoints in almost all of the existing datasets. Furthermore, active learning is based only on the differential gene expression data in the dataset to which a new timepoint will be added, and existing time-series datasets using similar treatments (which may be high resolution and contain useful differential gene expression information) cannot be applied in the algorithm. Although other advanced gene expression prediction or interpolation methods can utilize sequence information () and 'biologically plausible' constraints () on gene expression estimates, these approaches do not address the complicated issue of timepoint selection among large groups of genes and also cannot utilize existing data. In this article, we present a new model called Optimal Timepoint Selection (OTS) to identify optimal sampling timepoints for new microarray and RNA-seq experiments, based *To whom correspondence should be addressed. on gene expression data in existing datasets. We build OTS based on three observations: (1) Gene expression experiments can be sampled in an online fashion; i.e. samples can be treated and collected at a high rate and then stored at a relatively low cost, and particular samples can be measured at a later time, after deciding which timepoint will be optimal (). (2) A researcher is usually interested in capturing the expression patterns of a subset of genes (which may be grouped into several clusters with similar expression patterns) associated with a given treatment/condition. and (3) Differential gene expression patterns from previous experiments performed under similar treatments/conditions can provide information valuable for defining an optimal timepoint for sampling, even if the sampling rates are different from the new experiment. Based on these observations, a straightforward approach to choosing the best timepoint is to find unsampled timepoints at which there are significant upregulation or downregulation events for the genes of interest in the existing datasets. This approach is based on the assumption that the differential expression patterns for the genes of interest in existing datasets are similar to each other and are similar to the dataset to which a timepoint will be added. However, in practice, this assumption may be violated in many cases due to (1) large differences in the dynamic ranges between platforms (e.g. RNA-seq technology has a dynamic range several orders of magnitude higher than microarray technology;; (2) inconsistency among different datasets, either due to different growing conditions, different treatments or 'lab signatures', which result in differences in differential gene expression patterns among different laboratories, even after attempts to reproduce conditions exactly (); (3) high noise rates in expression values, particularly for microarray datasets () and (4) sparse sampling rates in existing data. To address these data integration problems, we have developed OTS, which includes a novel method of combining differential gene expressions from existing datasets ('training' datasets) based on their similarity to the experiment to which timepoints will be added ('current' dataset). OTS is novel in the following ways:(1) Projection of differential gene expression to threshold space: In contrast to existing differential gene expression prediction algorithms (;), the goal of our method is to predict the best timepoints to add to a high-throughput experiment. Therefore, rather than focusing on specific expression patterns, we are instead interested in how many genes are significantly differentially expressed at each timepoint, and how significant the overall expression values are (in a categorized fashion). Consequently, we project the differential gene expression values to threshold space to better capture important regulatory timepoints (explained in Section 2.3).(2) Data normalization and scaling: Instead of averaging or pooling all of the training data together, we first weight each training data's contribution to the overall result based on their similarity to the current dataset. Then, we adjust the weighted-average values with a shifting function for local fitting (explained in Section 2.4).(3) Timepoint selection with multi-objective optimization (MOO): We adopt a MOO model to select the overall optimal timepoint for all of the clusters (). MOO is superior to the sampling voting method because timepoints chosen by MOO benefit all (or the majority) of clusters, while the sampling voting method may be biased to one or a few clusters (explained in Section 2.5).The overall experimental approach for OTS is shown in Supplementary Figure S1 and Section S1. First, a biological experiment is performed, and samples are preserved at dense timepoints. A subset of timepoints (including at least the last timepoint in the range of interest and one other timepoint) is sampled. Then, time-series training datasets are collected. It is not necessary for the training datasets to be collected using the same technology (i.e. PCR, microarray or RNA-seq experiments), but they should use treatments or conditions that are expected to affect target treatmentresponse genes in the same way as in the current dataset. OTS produces a ranked list of the optimal timepoints to be selected next. The optimal timepoint(s) can then be sampled and added to the current dataset for the identification of the next optimal timepoint. This process can then be continued iteratively until all of the samples or all of the resources available for sampling are used up. This online-sampling approach is advantageous when studying organisms for which the sample collection step is significantly less expensive than the gene expression measurement step. For difficult or costly experiments (including clinical experiments), it is more logical to measure the gene expression in every available sample (). In the performance experiments in this study, OTS was applied using high-throughput time-series datasets for two different organisms (yeast and Arabidopsis) utilizing different platforms (microarray and RNA-seq). Noisy, sparsely sampled and poorly matched datasets were used as training. In all the experiments, OTS clearly outperforms the existing approaches.
CONCLUSIONWe have demonstrated that OTS can out-perform existing algorithms at finding optimal timepoints for defining true differential gene expression patterns for large groups of target genes. We have shown that this algorithm is robust to the use of sparsely sampled, poorly matched and cross-platform data, as well as to noise in the datasets. Because it utilizes existing data effectively, OTS can be applied on datasets starting with as few as two timepoints, in contrast to the active learning algorithm which requires a minimum of five timepoints as input (). As high-throughput gene expression measurement technologies continue to be developed, high-resolution sampling may eventually become cost-effective. For example, 'nanostrings' are a recently developed medium-throughput gene expression measurement technology capable of measuring up to 800 genes at once at a relatively low cost (). However, using this technology, not all of the genes in the organism can currently be sampled, and the gene list needs to be pre-defined. Since OTS simply uses gene differential expression values as input, it would be possible and very advantageous to use the results from nanostring or real-time PCR experiments as training data for OTS, to select optimal timepoints. For RNA-seq technology, highly multiplex sampling is becoming increasingly accurate, allowing for denser timepoint sampling with a moderate increase in cost (). As more time-series datasets are produced due to these advances in technology, more and better training datasets for OTS will be produced, and the demand for better knowledge-based timepoint selection methods will increase. In this article, OTS was tested only using differential gene expression values, but it could also be extended to use other types of data, including raw transcript number counts, relative protein quantities or any type of measurement that can be sampled in an online fashion. Overall, OTS can be used to significantly improve the results from biological experiments by allowing researchers to optimize the distribution of timepoints when there is a limit on the number of samples that can be measured across a time-series dataset. The estimation power of extrapolation of time-series gene expression data is much less than for interpolation, particularly for relatively simple linear extrapolation methods (). For this reason, OTS is currently limited to selecting timepoints within the time range available in training datasets. Eventually, more sophisticated extrapolation methods such as the non-linear differential equation models outlined in Haye et al. (2012) may be integrated to improve the predictive power of OTS.