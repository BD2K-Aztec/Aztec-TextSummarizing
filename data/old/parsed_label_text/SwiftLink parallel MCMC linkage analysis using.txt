Motivation: Linkage analysis remains an important tool in elucidating the genetic component of disease and has become even more important with the advent of whole exome sequencing, enabling the user to focus on only those genomic regions co-segregating with Mendelian traits. Unfortunately, methods to perform multipoint linkage analysis scale poorly with either the number of markers or with the size of the pedigree. Large pedigrees with many markers can only be evaluated with Markov chain Monte Carlo (MCMC) methods that are slow to converge and, as no attempts have been made to exploit parallelism, massively underuse available processing power. Here, we describe SWIFTLINK, a novel application that performs MCMC linkage analysis by spreading the computational burden between multiple processor cores and a graphics processing unit (GPU) simultaneously. SWIFTLINK was designed around the concept of explicitly matching the characteristics of an algorithm with the underlying computer architecture to maximize performance. Results: We implement our approach using existing Gibbs samplers redesigned for parallel hardware. We applied SWIFTLINK to a real-world dataset, performing parametric multipoint linkage analysis on a highly consanguineous pedigree with EAST syndrome, containing 28 members, where a subset of individuals were genotyped with single nucleotide polymorphisms (SNPs). In our experiments with a four core CPU and GPU, SWIFTLINK achieves a 8.5Â speed-up over the single-threaded version and a 109Â speed-up over the popular linkage analysis program SIMWALK. Availability: SWIFTLINK is available at https://github.com/ajm/ swiftlink. All source code is licensed under GPLv3.
INTRODUCTIONExact linkage analysis algorithms scale exponentially with the size of the input. Beyond a critical point, the amount of work that needs to be done exceeds both available time and memory. The ElstonStewart algorithm () calculates successive conditional likelihoods between family members, permitting the algorithm to scale linearly with the size of the pedigree (more precisely, the number of meioses), but can only analyse a handful of markers jointly. The LanderGreen hidden Markov model (HMM) approach (), on the other hand, can analyse many markers, but only in pedigrees of limited size. In between these two extremes, approaches using Bayesian networks, for example SUPERLINK (), scale to greater numbers of markers than the ElstonStewart algorithm and to larger pedigrees than the LanderGreen algorithm, but still cannot handle both large pedigrees and many markers. In the situation where we have a large, perhaps consanguineous, pedigree typed with many markers, we are forced to either abbreviate the input or else use an approximation like Markov chain Monte Carlo (MCMC). Although MCMC helps make the problem tractable, it can take a long time to converge. The problem of slow running time is compounded by the fact that existing software is single-threaded and, therefore, not designed to take advantage of all the processing power available in even a single modern PC. This underuse will become more acute as multicore processors feature increasing numbers of cores and graphics processing units (GPUs) become more general purpose. Parallelism has previously been applied to accelerate exact linkage algorithms in the applications FASTLINK, across networks of workstations (), PARALLEL-GENEHUNTER, on computer clusters () and SUPERLINK-ONLINE, on grid middleware (). Although parallel implementations of exact linkage algorithms perform analyses faster and can scale to larger problems owing to accessing memory in multiple machines, they still have the same fundamental scaling properties as their underlying algorithms. Problems outside of the scope of exact algorithms must be analysed with MCMC-based linkage programs such as *To whom correspondence should be addressed.  The Author 2012. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com SIMWALK2 () and MORGAN (), which have both been shown, where possible, to produce results of comparable accuracy to exact algorithms (). To the best of our knowledge, there have been no published attempts to parallelize MCMC-based linkage analysis. In general, parallelizing MCMC is considered a hard problem because the states of the Markov chain form a strict sequential ordering. Successful parallel MCMC implementations tend to focus either on parallelizing expensive likelihood calculations (for example,) or on multiple chain schemes (for example,). The approach we took with SWIFTLINK maximizes the use of hardware resources by combining both previous approaches, allowing different parts of the calculation to be distributed across both CPU and GPU. The Markov chain is run in parallel on the CPU, where each step in the chain is performed by one of two multithreaded block Gibbs samplers based on the locus sampler () and the meiosis sampler (). Expensive likelihood calculations required to estimate LOD (logarithm of odds) scores are performed on the GPU, if one is available. High performance is achieved on the GPU by maximizing hardware use, requiring us to identify substantial independent calculations. This article is organized as follows: we elaborate on the details of the Gibbs samplers used to form the Markov chain, what aspects of each sampler were changed to facilitate execution on parallel hardware and how SWIFTLINK orchestrates these actions to maximize hardware use. We report experimental results comparing SWIFTLINK with other MCMC-based linkage analysis software on a highly consanguineous pedigree and end with discussion and an outline of future work.
DISCUSSIONThe methods presented in this article produce almost an order of magnitude speed-up of MCMC linkage analysis compared with an optimized single-threaded implementation on fairly minimal hardware. Compared with existing MCMC linkage software, SWIFTLINK achieves up to two orders of magnitude speed-up when using four CPU cores and a GPU concurrently. We believe SWIFTLINK will be especially useful to researchers in, for example, clinical research settings, where access and/or experience with computer clusters is limited. In addition, SWIFTLINK has been specifically designed to fit into existing workflows by supporting the standard 'LINKAGE' file formats (ped, map and data files) as other popular mulitpoint linkage analysis programs, e.g. Allegro and Genehunter. A majority of linkage projects include only small-to-medium size pedigrees and can therefore be analysed using exact algorithms. Many large pedigrees can be successfully abbreviated to run using an exact algorithm; however, for cases where there are few genotyped individuals or in cases like EAST syndrome (), where one of the branches only contains a single genotyped individual, any abbreviation will dramatically reduce the power of the study. This is a clear niche where SWIFTLINK can be used to great effect. Unlike many other articles detailing GPU applications, we did not show results for both 32-and 64-bit. As performance on the GPU is affected so strongly by register usage and therefore occupancy, 32-bit code tends to run faster as pointers take up half the space and more blocks can be run concurrently. However, CPU code in general performs faster in 64-bit thanks to wider instructions. In the current version of SWIFTLINK, the GPU is underused because the CPU is a bottleneck, therefore it only makes sense for us to run in 64-bit mode. In the future, we will be aiming to improve SWIFTLINK by removing the bottlenecks from the slower execution of the Markov chain on the CPU. Additional speed-ups could be made by using SIMD instructions as well as multithreading or by offloading some of the excess work to the GPU. It is not clear how to balance the load dynamically between both platforms and whether the additional complexity is warranted for a speed-up that can be provided more simply with additional CPUs, e.g. with more cores locally or in a computer cluster.