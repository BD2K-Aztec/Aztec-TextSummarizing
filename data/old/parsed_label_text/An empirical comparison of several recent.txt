Motivation: Many new methods have recently been proposed for detecting epistatic interactions in GWAS data. There is, however, no in-depth independent comparison of these methods yet. Results: Five recent methods—TEAM, BOOST, SNPHarvester, SNPRuler and Screen and Clean (SC)—are evaluated here in terms of power, type-1 error rate, scalability and completeness. In terms of power, TEAM performs best on data with main effect and BOOST performs best on data without main effect. In terms of type-1 error rate, TEAM and BOOST have higher type-1 error rates than SNPRuler and SNPHarvester. SC does not control type-1 error rate well. In terms of scalability, we tested the five methods using a dataset with 100 000 SNPs on a 64 bit Ubuntu system, with Intel (R) Xeon(R) CPU 2.66 GHz, 16 GB memory. TEAM takes ∼36 days to finish and SNPRuler reports heap allocation problems. BOOST scales up to 100 000 SNPs and the cost is much lower than that of TEAM. SC and SNPHarvester are the most scalable. In terms of completeness, we study how frequently the pruning techniques employed by these methods incorrectly prune away the most significant epistatic interactions. We find that, on average, 20% of datasets without main effect and 60% of datasets with main effect are pruned incorrectly by BOOST, SNPRuler and SNPHarvester. Availability: The software for the five methods tested are available from the URLs below.
INTRODUCTIONA genome-wide association study (GWAS) examines the association between phenotypes and genotypes in a study group. The first exciting finding was on age-related macular degeneration (AMD) (), which uncovers a disease allele (tyrosinehistidine polymorphism) with an effect size of 4.6 in 100 000 single nucleotide polymorphisms (SNPs). Since then, over 600 GWASs have been conducted for 150 diseases and traits; * To whom correspondence should be addressed. and 800 associated SNPs have been reported. The methodologies of these studies are similar: a quality control criteria is first defined to filter the genotype data; then the remaining genotypes are each tested for association with the disease phenotypes. Finally, the significant SNPs are reported after multiple testing correction. Most of these GWASs could only identify disease alleles with moderate effect size. Thus, single SNP association studies could explain very limited heritability of these diseases (). Consequently, researchers have started exploring multi-SNP interactions in the hope of discovering more significant associations. Multi-SNP interactions are also called 'epistatic interactions'. This term originated from Bateson's definition of epistasis 100 ago (). It was defined as the change of segregation ratio and the interaction of genes. However, in the current literature, there is a debate on the exact definition of epistasis (). Our article focuses on evaluating epistatic interaction detection methods in their computational aspect and all the experiments are based on simulation data. Thus, we consider epistatic interactions as the statistically significant associations of k-SNP interaction (k  2) with phenotypes. There are mainly two types of epistatic interaction detection methods: model-based methods and model-free methods. In general, model-based methods () predefine a statistical model between phenotypes and genotypes; then they fit the data to the model; and finally they output the significant SNPs. They work well for only a small number of important and filtered candidate SNPs; but they often fail when the number of SNPs grows to hundreds of thousands. To make model-based methods more efficient, researchers have proposed a variety of heuristic and filtering techniques. For example,(2010a) develop an upper bound of the likelihood ratio test statistic for two-locus epistatic interaction to prune the search space and a Boolean transformation of data to make collection of contingency table information faster. As another example,devise a two-stage analysis so that the overall analysis is more efficient. As a third example,use a stochastic search to identify only 4050 (set by the user) groups of candidate epistatic interactions for follow-up model-fitting analysis. In contrast, model-free methods () have no prior assumption on the data and the model. Given the genotype data, these methods only examine the test statistic of each possible epistatic interaction with phenotypes.propose a minimum spanning tree (MST) structure to represent the data; by traversing this MST, exhaustivecompare only approaches based on neural networks while our selected methods cover both data mining and statistical methods. Second,evaluate multifactor dimensionality reduction (MDR) (), grammatical evolution neural networks (GENN) (), focused interaction testing framework (FITF) (), random forests (RF) () and logistic regression (LR) () methods. They show that MDR is superior in all settings. After 2 years of advancement, most methods selected in this article have demonstrated that their performance is better than that of MDR; we therefore omit discussing methods mentioned in Motsingercompare AMBIENCE (with MDR, restricted partitioning method (RPM) () and logistic regression. They conclude that the performance of AMBIENCE is equivalent to that of logistic regression for two-locus models and better than that of RPM and MDR. However, according to, the performance of BOOST is better than that of PLINK (), which uses a pure logistic regression model. Therefore, we omit the evaluation of AMBIENCE and RPM in our study. Lastly,have shown that their overall performance is much better than that of BEAM (). We thus omit BEAM. In this article, we give an independent empirical comparison of five methods for detecting epistatic interactionsnamely, TEAM (), BOOST (), SNPRuler (), SNPHarvester () and Screen and Clean ()to help users better understand which method is more suitable for their data, which method is good for detecting epistatic interactions with and without main effect and which method is scalable to larger datasets. We also analyze why combining several of these methods cannot enhance power. Their basic characteristics are given in. The organization of this article is as follows. We first formulation the problem in Section 2. Then we briefly introduce each of the five methods in Section 3. We describe how the evaluation data is simulated in Section 4 and the detailed setting of each experiment in Section 5. After that, we present the results under each setting in Section 6. Finally, we discuss the performance of each method and provide advice to users in Section 7.
DISCUSSIONThe five methods all demonstrate respective utilities through the experiments results above. No single method is simultaneously the most powerful, the most scalable and has the lowest type-1 error rate in every setting. When users want powerful results and are not concerned with computation cost, we recommend using TEAM and BOOST. Compared with TEAM, BOOST uses a model-fitting procedure. If the data fits the model well, the result is usually good;Page: 2942 29362943the four methods on data with and without main effect. In (a), there are in total 1800 datasets for 18 settings of the simulated datasets, which corresponds to 1800 ground-truth. Among these ground-truth, only 800 of them can be detected by at least one of the four methods, while the best methodTEAMidentifies 787 ground-truth out of 800. This explains why using ensemble methods cannot outperform TEAM. Similar observation is illustrated in (b). otherwise, a model-free method may be the alternative choice. When users expect moderate running time and power, we recommend using SNPRuler. Its pruning technique helps reduce running time albeit at the risk of losing power. If users are conscious of computation cost and have to run very large datasets, we recommend using SNPHarvester because it only identifies a small number (4050) of groups for the model-fitting procedure. Our evaluations are based on simulation results. In a real study, users usually have no idea of the 'ground-truth' in the dataset. Hence, it may not be sufficient to rely only on one method to obtain results. We suggest that, if time and computation resources permit, users try both the recommended model-free (i.e. TEAM) and model-fitting (i.e. BOOST) methods. It is tempting to consider taking a 'majority vote' of the results of two or more methods. For example, let every algorithm report their top three predictions. An SNP pair receives k votes if it is reported by k methods. We select the one with the highest vote as the final prediction. When there is a tie, we choose the one with the lowest P-value. Unfortunately, for both types of data tested, we find that an ensemble using such a strategy cannot increase power over using solely BOOST or TEAM. In, we see that for data without main effect, BOOST's ground-truth predictions highly overlap with the other three methods, so any ensemble cannot contribute a significant number of new ground-truth predictions. Specifically, the proportion of BOOST's ground-truth predictions that are not predicted by the other three methods is 4.1%, while the proportion of the other methods' ground-truth predictions not predicted by BOOST is 0.2%. Similarly, for data with main effect, no ensemble can outperform TEAM. Our evaluations above only focus on two-locus epistatic interaction. Recently,provide a general model that can be extended to n-locus epistasis. They also provide mathematical details of dissecting the  2 test into different epistatic components. For example, two-way epistatic interaction can be partitioned into four epistatic components: additive  additive, additive  dominant, dominant  additive and dominant  dominant. This helps characterize epistatic interactions in a more specific way and provides more physiological insights.