Motivation: The advent of high throughput data has led to a massive increase in the number of hypothesis tests conducted in many types of biological studies and a concomitant increase in stringency of significance thresholds. Filtering methods, which use independent information to eliminate less promising tests and thus reduce multiple testing, have been widely and successfully applied. However, key questions remain about how to best apply them: When is filtering beneficial and when is it detrimental? How good does the independent information need to be in order for filtering to be effective? How should one choose the filter cutoff that separates tests that pass the filter from those that dont? Result: We quantify the effect of the quality of the filter information, the filter cutoff and other factors on the effectiveness of the filter and show a number of results: If the filter has a high probability (e.g. 70%) of ranking true positive features highly (e.g. top 10%), then filtering can lead to dramatic increase (e.g. 10-fold) in discovery probability when there is high redundancy in information between hypothesis tests. Filtering is less effective when there is low redundancy between hypothesis tests and its benefit decreases rapidly as the quality of the filter information decreases. Furthermore, the outcome is highly dependent on the choice of filter cutoff. Choosing the cutoff without reference to the data will often lead to a large loss in discovery probability. However, naı¨venaı¨ve optimization of the cutoff using the data will lead to inflated type I error. We introduce a data-based method for choosing the cutoff that maintains control of the family-wise error rate via a correction factor to the significance threshold. Application of this approach offers as much as a several-fold advantage in discovery probability relative to no filtering, while maintaining type I error control. We also introduce a closely related method of P-value weighting that further improves performance. Availability and implementation: R code for calculating the correction factor is available at http:// www.stat.uga.edu/people/faculty/paul-schliekelman.
IntroductionA dominant trend in biology in recent years has been the development of high throughput techniques and the dramatic increase in the resolution of available data. However, most of the information gained is not relevant for any particular question at hand and comes at the cost of more hypothesis tests and thus more stringent statistical thresholds. There is often high redundancy between tests and the gain in information may be slower than the increase in resolution. Thus, higher resolution will not always lead to higher probability of discovery. Given the realities of multiple testing, it is unlikely that a mere increase in throughput and resolution will greatly increase discoveries. Rather, it will be necessary to combine high throughput data with other sources of information in order to better target investigations. The problems of multiple testing are well understood and many methods have been proposed for using external informationto filter for the most promising features of the data. Such methods typically have two stages: first, some filtering criterion is used to select the most promising features. Then, only those features are tested for the effect of interest. These include methods for microarrays (), RNASeq (), genome-wide association studies () and epistasis (). Despite the popularity of filtering methods, key questions remain about their general statistical properties.discussed the conditions sufficient for maintaining Type I error control and showed that the key requirement is that the null hypothesis distribution of the test statistics after filtering should be the same as the null hypothesis distribution before filtering. They showed that some filtering techniques in use can violate this requirement. Their focus was on the conditions for filtering to be valid. Little is known about the conditions required for filtering to be successful in significantly increasing discovery probabilities, and our focus is on this question. In this paper, we address two major issues. First, we evaluate the usefulness of filtering and determine major factors affecting its behavior. We quantify the effect of the filtering statistic in terms of its probability of ranking true effects highly. We show that a strong filter that has a high probability of ranking true positives in e.g. the top 10% can greatly increase discovery probability (as much as 20-fold) when there is high redundancy between features and when a good cutoff is known in advance. Even a random filter can increase power when the cutoff point is well chosen. On the other hand, filtering is less effective when there is low redundancy between features and if the filter statistic is not able to reliably rank true positive features highly. Second, most applications of filtering methods have used ad-hoc approaches to choosing the filter cutoff point. The filter cutoff point refers to the value of the filtering criterion which separates features that will be included in the second stage and those which will not. We show that the filter cutoff has a large effect on the performance of filtering methods. A good choice can make a several-fold difference in discovery probability relative to a poor choice. Furthermore, inappropriate ad-hoc methods can greatly inflate false positive rates. We introduce a general and rigorous method for choosing the filter cutoff and show that this approach can increase discovery probabilities by several-fold relative to no filtering. We also introduce a simple and intuitive method for weighting p-values that is closely related to our filtering method and improves the performance further.