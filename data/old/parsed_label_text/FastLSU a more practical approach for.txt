Motivation: We address a common problem in large-scale data analysis, and especially the field of genetics, the huge-scale testing problem, where millions to billions of hypotheses are tested together creating a computational challenge to control the inflation of the false discovery rate. As a solution we propose an alternative algorithm for the famous Linear Step Up procedure of Benjamini and Hochberg. Results: Our algorithm requires linear time and does not require any P-value ordering. It permits separating huge-scale testing problems arbitrarily into computationally feasible sets or chunks. Results from the chunks are combined by our algorithm to produce the same results as the controlling procedure on the entire set of tests, thus controlling the global false discovery rate even when P-values are arbitrarily divided. The practical memory usage may also be determined arbitrarily by the size of available memory. Availability and implementation: R code is provided in the supplementary material.
IntroductionIn many fields the substantially increased scale of data available has resulted in a significant increase in the size of multiple hypotheses testing problems. In genetics, in particular, typical GWAS studies consist of 10 5  10 6 SNPs () while eQTL studies (), newly advanced methylation studies (), and imaging studies () usually start with 10 9 tests. These testing problems are hugescale as opposed to large-scale used byto describe studies consisting of hundreds to thousands of hypotheses. It is preferable to control the false discovery proportion rather than the number of false positives for a huge-scale testing problem. Therefore the FDR or the pFDR approaches are favored and both tend to offer larger, more powerful sets of results than those yielded by the conservative FWER control. These huge-scale multiple hypotheses testing problems create numerous computational challenges when many tests, say of the order 10 6 , are performed with all of the P-values of more or less equal importance. As a result some simpler testing procedures such as rigid P-value thresholds may be used that sacrifice power and correctness. Alternatively tests may be separated or chunked into smaller sets or chunks that are more computationally feasible.notes that the problem of separating hypotheses tests has not received great attention and warns of some pitfalls in chunking P-values, but focuses on grouping tests that share a biological property rather than arbitrary, computationally feasible chunks. Cai and Sun (2009) and later () propose alternative solutions to Efron's grouping problem but do not address the problem of arbitrary, computationally feasible chunking. We confront the computationally feasible chunking problem for theBenjaminiHochberg false discovery rate (). We show on data from Stranger's HapMap study () that if results from separate tests are not combined correctly, there is considerable inflation of type I error, offer an explication for this occurrence, and propose our algorithm as a solution. Consider a huge-scale testing problem of size m where our goal is to select exactly R ! 0 significant tests. Of the R significant discoveries, exactly V ! 0 tests will be false discoveries (i.e. truly nonsignificant tests that are declared significant). A common approach in multiple hypotheses testing problems is to control the family-wise error rate, FWER  PrV ! 1, the probability of selecting at least one false discovery. For huge-scale testing a more favorable alternative is to control the false discovery proportion, FDP  V=maxR; 1, the proportion of truly false tests among the significant R. Some will prefer to control the positive FDR, pFDR  EFDPjR  0, the expectation of the FDP when significant tests are selected, while others will opt to control the false discovery rate, FDR, the expectation of the FDP, E(FDP). The FDR is always of a potentially smaller magnitude than the FWER and of the pFDR (FDR  pFDR  PrR  0). Yet, in reality for huge-scale testing, FWER   FDR and sometimes, pFDR % FDR. Therefore both FDR and pFDR control approaches tend to offer larger, more powerful sets of results than those that might be offered by the conservative FWER control. For a further discussion about FWER, FDR and pFDR refer to Farcomeni (2004). In huge-scale testing when the m P-values are partitioned into chunks, it is challenging to control the FWER, pFDR or FDR over the entire collection of m P-values. Controlling these error rates on a per chunk basis, if not done correctly, may interfere with the overall results by introducing more false discoveries. Although the same difficulty arises for the FDR and pFDR control (), it is easier to illustrate this for the FWER. Consider, for instance, an example of FWER control using the Bonferroni approach by collecting all P-values less than a=m. Assuming that the number m of P-values happen to be very large so that m should be divided into k chunks, each of of size m i so that m  P k i1 m i. Applying Bonferroni in chunks of size m i will tend to select more significant results than applying it over the entire set of m  P m i P-values since a=m is less than a=m i. In the case of FWER control, using a fixed bound of a=m for all the chunks is theoretically preferred but often yields no significant results. A stricter constant cut-off on all sets of tests as suggested by Dudbridge and Gusnanto (2008) for GWAS was developed based on results from simulated GWAS. However such an ad-hoc approach eliminates from the entire multiple hypotheses testing problem any knowledge of the actual significance level a used.