Modern sequencing instruments have the capability to produce millions of short reads every day. The large number of reads produced in conjunction with variations between reads and reference genomic sequences caused both by legitimate differences, such as single-nucleotide polymorphisms and insertions/deletions (indels), and by sequencer errors make alignment a difficult and computationally expensive task, and many reads cannot be aligned. Here, we introduce a new alignment tool, SRmapper, which in tests using real data can align 10s of billions of base pairs from short reads to the human genome per computer processor day. SRmapper tolerates a higher number of mismatches than current programs based on Burrows– Wheeler transform and finds about the same number of alignments in 2–8Â less time depending on read length (with higher performance gain for longer read length). The current version of SRmapper aligns both single and pair-end reads in base space fastq format and outputs alignments in Sequence Alignment/Map format. SRmapper uses a probabilistic approach to set a default number of mismatches allowed and determines alignment quality. SRmappers memory footprint ($2.5 GB) is small enough that it can be run on a computer with 4 GB of random access memory for a genome the size of a human. Finally, SRmapper is designed so that its function can be extended to finding small indels as well as long deletions and chromosomal trans-locations in future versions.
INTRODUCTIONWith the advent of next-generation sequencing (NGS) instruments, the amount of raw genetic sequence information has exponentially increased during the past few years, and it is expected to continue to grow at a high rate as sequencing cost continue to decrease. Instruments such as the HiSeq2000 (Illumina), GS FLX titanium (Roche) and SOLiD 4 (ABI) can generate billions of base pairs (gigabases or Gb) of data per day with increasingly high accuracies, 498.5% for Illumina and 499.5% for Roche and ABI, respectively, and with costs that have decreased to $$10 000 per human genome (). Newer instruments, such as the Ion Proton (Life Technologies), can produce even higher amounts of data and are approaching the goal of a $1000 genome (). With the speed and cost factors making whole-genome sequencing practical, researchers are sequencing and analysing large numbers of genomes in hopes of finding genetic origins of many diseases, such as cancers. For example, one recent study sequenced 457 human genomes searching for rare mutations involved in ovarian cancer (). With the dramatically increased amount of raw data, analysis has become more challenging. This is because of two factors: the sheer amount of data gathered and the relatively short lengths of reads produced by current NGS instruments (). For example, in the study aforementioned, 412 terabases of sequence data would be gathered for a 10 coverage of the 457 genomes. The short length of the reads, typically between 30 and 100 bp for Illumina and ABI and $400 bp for Roche, complicates the building of a genome de novo (). For example, a genome may contain repetitive regions. It is difficult to reconstruct these regions and their flanking sequences if the length of the reads is much shorter than that of the repeating units. NGS instruments can now perform pair-end sequencing, in which the sequence of the two ends of longer fragments are determined, which has helped to resolve these problems (). Nevertheless, de novo assembly remains challenging and is memory intensive and, therefore, difficult to perform on genomes larger than those of bacteria (). A popular alternative to de novo assembly is reference assembly, in which reads are aligned against a pre-existing reference genome. There are three main classes of alignment tools: read-hashing tools, reference-hashing tools and Burrows Wheeler transform (BWT) tools. Examples of reference-hashing tools include BFAST (), SHRiMP-2 () and WHAM (). Tools that use the BWT include BurrowsWheeler Aligner (BWA) (), bowtie () and SOAP2 (). Most genome-hashing algorithms require large memory that they must be run on expensive large-memory machines. On the other hand, many BWT methods carry a small memory footprint that they can be run on computers with 4 GB of random access memory (RAM), accessible by many users possessing only desktop machines. Among the genome-hashing methods listed previously, only BFAST can be run on a computer with 4 GB of RAM. For methods based on BWT, both BWA and bowtie can be run on computers with 4 GB of RAM. Among these three, bowtie is the most restrictive in terms of allowing variants between the reference and short read, allowing a maximum of three mismatches and no insertions or deletions *To whom correspondence should be addressed.(indels), making it a less attractive option for sequencing longer reads, which would be expected to have a higher number of mismatches and errors. BWA and BFAST both allow indels and mismatches. Among the three, bowtie is slightly faster than BWA, and both are significantly faster than BFAST. However, BFAST has been shown to be more sensitive than the BWT-based methods for most datasets evaluated (in). Here, we introduce a new genome-hashing alignment tool, SRmapper. The original design goal for SRmapper was to build an alignment tool that was not restrictively slow, had as high sensitivity as other widely used alignment tools and was capable of finding long deletions and other more complicated genomic alternations, such as chromosomal translocations from short-read sequences. The current version of SRmapper already achieves the first two goals and uses a novel approach to determine the initial number of mismatches allowed and calculates alignment scores. In evaluations on real data, SRmapper is 28 faster than BWA on datasets of length !51 bp while aligning comparable number of reads as BWA. For short reads, 32 bp, SRmapper was 640 faster than BWA, but somewhat less sensitive. This article explains the approach taken by SRmapper, compares its performance against the popular BWA package for multiple datasets of different read length, describes how we envision this first version of SRmapper being used and discusses future extensions and improvements.