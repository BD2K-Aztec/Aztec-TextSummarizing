Motivation: High-throughput metagenomic sequencing has revolutionized our view on the structure and metabolic potential of microbial communities. However, analysis of metagenomic composition is often complicated by the high complexity of the community and the lack of related reference genomic sequences. As a start point for comparative metagenomic analysis, the researchers require efficient means for assessing pairwise similarity of the metagenomes (beta-diversity). A number of approaches were used to address this task, however, most of them have inherent disadvantages that limit their scope of applicability. For instance, the reference-based methods poorly perform on metagenomes from previously unstudied niches, while composition-based methods appear to be too abstract for straightforward interpretation and do not allow to identify the differentially abundant features. Results: We developed MetaFast, an approach that allows to represent a shotgun metagenome from an arbitrary environment as a modified de Bruijn graph consisting of simplified components. For multiple metagenomes, the resulting representation is used to obtain a pairwise similarity matrix. The dimensional structure of the metagenomic components preserved in our algorithm reflects the inherent subspecies-level diversity of microbiota. The method is computationally efficient and especially promising for an analysis of metagenomes from novel environmental niches. Availability and Implementation: Source code and binaries are freely available for download at https://github.com/ctlab/metafast. The code is written in Java and is platform independent (tested on Linux and Windows x86_64).
IntroductionRecently computational life scientists have witnessed an astounding increase in the volume of available shotgun metagenomic datasets. The challenge of reducing dimensionality of the data analysis is of primary demand for statistical analysis of metagenomes. This includes taxonomic and functional profiling, assessing richness and similarity. Technological advances and cost reduction of high-throughput sequencing allow to examine microbiota from previously unexplored ecological niches. The degree of detail has increased in an unprecedented way: the average coverage depth has increased by several orders of magnitude since the first metagenomic studies (). Vast genomic data obtained in studies of microbial isolates served as a cornerstone for developing reference-based approaches. Subsequently, there was a boom of such methods based on more elaborate techniques than direct alignment of each read with a reference genome, e.g. Kraken (), CLARK (), FOCUS (), MetaPhlAn2 (). However, a real challenge for reference-based methods is represented by the communities from novel unexplored niches that contain a large fraction of uncultured bacteria. Accordingly, there is a lack of representative genomes for many clades of microbes and viruses that could serve as a reference. This problem is of significant weight even for the environments that have been thoroughly studied for decades: e.g. human gut microbiota where unknown genomes form a lion's share of the total DNA reads (). One of the approaches for measuring metagenomic similarity that was developed in order to cope with the rapidly accumulating volume of data is based on an adaptive subsampling (). Another one is an alignment-free approach that appears to be attractive to metagenomic researchers due to the sparseness of available reference genome sets. Among such methods there are abstract composition-based methods (k-mer spectrum analysis (), neural networks (), Markov models () that are computationally efficient and can be run in parallel sections. However, there are certain limitations of these methods: the differentially abundant features between two or more groups of metagenomes turn out to be concealed within the method or provide little information. An alternative idea for assessing similarity is a de novo assembly of the metagenomes (similar to the process applied to individual genomes) followed by an analysis of the yielded contigs (classification, differential abundance analysis based on coverage depth). Here each individual feature is meaningful; however, the assembly is complicated due to a wide range of typical abundance of bacterial species and significant intra-species genomic variability. Special algorithms intended for metagenomic assembly have been developed that address these issues (). Particularly, combined assembly of metagenomic reads was proposed for estimating pairwise similarity (crAss) (). However, complete assembly from reads to contigs is computationally-and memory-intense, especially due to the rapid increase of publicly available metagenomic data. We have developed the MetaFast algorithm for compact representation of metagenomes using an adaptive segmentation of metagenomic de Bruijn graph, essentially based on a simplified metagenomic de novo assembly. Our method lies between the k-mer spectrum analysis and assembly and combines the best of these two alignment-free approaches: the speed of the former with the precision of the latter. Its independence of the reference allows to perform efficiently for both extensively studied and novel microbiota types. The performance of MetaFast was compared with several taxonomic profilers (Kraken, CLARK, FOCUS, MetaPhlAn2), as well as with a cross-assembly-based algorithm crAss on simulated data and real metagenomes of gut microbiota, New-York subway and viruses of lake water. Comparative analysis showed that MetaFast is highly efficient and the results of its work are in agreement with the existing methods.
DiscussionDramatic rates of accumulation of publicly available metagenomic data can be illustrated by a 45 times increase in the number of. Spearman correlation between the pairwise dissimilarity matrices obtained using MetaFast and other algorithms. The correlation values were obtained using Mantel test (P  0.001). The label 'MetaFast sp' denotes a modified version of MetaFast analysis when the step of pseudo-assembly is replaced with the assembly using SPAdes, 'MetaFast nb' when it was replaced with the assembly using Newbler datasets in some of the largest online metagenomic resources during the last two years; the sequencing depth also grows rapidly, reaching more than a hundred of terabasepairs per sample (). Such explosion of metagenomic Big Data makes comparative metagenomic studies an even more challenging area demanding new efficient algorithms () and visualization approaches (). Issues of speed, accuracy and memory efficiency become the key factors for novel approaches in metagenomic data processing, especially when the methods are intended to be applied to the broad spectrum of environmental datasets available. Bearing in mind the special nature of metagenomic datasets, we developed a hybrid algorithm that combines the principles of de novo pseudo-assembly with the k-mer spectrum analysis allowing to perform computationally-and memory-efficient accurate analysis for a large number of metagenomes in a reference-independent way. The domain of 'shotgun' metagenomic sequences currently contains two large niches. The first one encompasses projects that target moderate coverage for a large number of monotypic samples, e.g. human gut microbiota consortia (). The second category includes the projects mostly oriented towards de novo assembly of novel genomic sequences basing on high-coverage sequencing of a small number of samplestypically coming from various unique environments (e.g.). Most metagenomic datasets can be placed in between of these two extremesby containing a fraction of data from unknown organisms and a fraction of genetic material already contained in the public reference databases. A researcher chooses the approach basing on the understanding of the proportions between those two fractions in the current dataset. Interestingly, even in a seemingly well understood metagenomes, the combination of the accumulated data even from multiple projects allows for the discovery of novel species (). The two described groups of datasets imply the choice of considerably different types of algorithms: alignment against a reference base is a fast and easily parallelized algorithm, while assembly algorithms are hardware-demanding. Typical reference-based approaches imply the selection of a similarity criterion to create a non-redundant cataloguenormally selecting a sequence (of a gene or a genome) from a group having a high percent identity over a high percent of length. This step creates further biases in mapping: the more the difference between the analyzed sequence and the reference, the fewer reads are mapped (). The assembly-based approaches for metagenomic datasets are using the same paradigm producing long reference sequences. For estimating the relative abundance of the sequences in a metagenome, the mapping is used againthis time against the sequences assembled from metagenomes that are considered to be more appropriate for mapping. Although one uses several metagenomic samples to increase the coverage, the intrinsic presence of mutations in bacterial genomes leads to selection of only the one variant of a path in the graph out of all available variants. Therefore, the variants are underrepresented in the final obtained sequencethe genomic diversity is thus 'flattened'. The presented algorithm MetaFast is an intermediate solution allowing to work with the speed of mapping and at the same time to gain benefits of de novo assemblyparticularly, yielding novel genetic sequences. Moreover, one of the ideas implemented in the feature representation allows to avoid the mapping bias and to use 'unflattened' references. Technically, MetaFast has several differences from the traditional metagenomic assembly approaches. Firstly, MetaFast does not perform a complete de novo assembly (e.g. used in the global human microbiome catalog construction (), but rather an incomplete version of assembly (pseudo-assembly). This greatly improves the performance of the algorithm in terms of speed. To assess whether the precision is affected negatively, we replaced the pseudo-assembly step with a conventional assemblerthe results were highly similar. Second, while Qin et al. performed the pairwise alignment of the combined pool of sequences and select a single representative per each cluster basing on high percent identity and alignment length, we do not drop the other similar variants but rather connect all of them into a single subgraph. This allows to capture the genomic details of each metagenome individually and then combine the individual results together for the comparison task. Thirdly, we do not perform the final 'flattening' step, instead preserving the branching nature of the graph components (MetaFast features). Thus, information about the variation of the same species between different samples is not lost and can be used for further analysis of gene variations. This also allows to avoid the mapping biases, as the calculation of feature representation in each separate metagenome is performed via k-mer counting for a branched Metafast feature. Finally, in the spirit of metagenome-based research, MetaFast allows to identify the features differentiating the groups and concentrate the researcher's attention on them. Advantages of our approach in terms of speed, accuracy, memory efficiency and independence of reference base were demonstrated using both simulated and real metagenomes; the performance was compared with a variety of tools widely used in metagenomics. Simulations showed high correlation with the results of read mapping confirming basic accuracy of MetaFast. However, it should be kept in mind that the provided simulated datasets are a primitive model of real data; particularly, genomic polymorphisms and gene content variations are ignored in these simulations. A high degree of correlation was shown in experiments with real data, where the results of MetaFast were compared with two versions of mappingto a reference genome and gene catalog. High correlation with the results obtained via mapping to a genome catalog reflects the correct functioning of the algorithm: it means that the most part of genetic information is mappable to genomes and is also assembled to yield the features. Noteworthy, the comparison of MetaFast with the combined assembly demonstrated memory efficiency of MetaFast. While in conventional assembly the memory usage increases approximately linearly with the number of the samples, in MetaFast it tends to achieve saturation at a certain number of samples (e.g. around 20 for gut metagenomes, see Supplementary) and does not increase further. This fact is likely associated with the effect of ComponentCutter module (see Methods) that limits the size of the total graph: as soon as the number of metagenomes is sufficiently high to encompass the major diversity of community structures, an addition of new metagenomes does not increase the size of the graph thus does not demand extra memory. Overall, our approach provides a more economic memory usage accompanied with significant speed improvement at the expense of only slight decrease in accuracy. Applicability of the approach for a wide range of problems was demonstrated for MetaFast. It has shown good correlation with the adopted methods in human gut microbiome datasets and comparative accuracy in case of novel microbiota types and reference based approaches. Interestingly, the only tool comparable to MetaFast in the terms of applicability to a set of metagenomes without welldescribed reference set is the crAss algorithm designed specifically to assemble species from multiple metagenomes. The results of the comparative viral profiling showed that MetaFast is an adequate tool for dissimilarity analysis of novel microbiota metagenomes due to the independence of the reference base that might also contain poorly annotated sequences (unclassified at various levels of taxonomy). Our results showed that on viral datasets it outreaches crAss in the terms of both speed and memory consumption, allowing to work with hundreds of metagenomes. While obviously even in well studied datasets there is a chance to find novel genes, we suggest MetaFast is a very useful tool for exploratory data analysis. The features and their quantification across metagenomes can be obtained rapidly and the technical implementation allows to work with a high number of metagenomes in a robust manner. The settings help to orient the algorithm towards obtaining the features of desired lengthcomparable to the typical microbial gene lengthmaking it convenient for primary analysis.