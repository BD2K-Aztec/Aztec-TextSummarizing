Motivation: Genomic repositories are rapidly growing, as witnessed by the 1000 Genomes or the UK10K projects. Hence, compression of multiple genomes of the same species has become an active research area in the past years. The well-known large redundancy in human sequences is not easy to exploit because of huge memory requirements from traditional compression algorithms. Results: We show how to obtain several times higher compression ratio than of the best reported results, on two large genome collections (1092 human and 775 plant genomes). Our inputs are variant call format files restricted to their essential fields. More precisely, our novel Ziv-Lempel-style compression algorithm squeezes a single human genome to $400 KB. The key to high compression is to look for similarities across the whole collection, not just against one reference sequence, what is typical for existing solutions.
INTRODUCTIONThe DNA sequencing technology has become so affordable that there are several large-scale projects in which at least hundreds of individuals of some species are sequenced. From many perspectives, including the advent of personalized medicine, the Homo sapiens data belong to the most interesting, and this is the reason why large projects like the 1000 Genomes Project (1000GP) () and the UK10K Project (http://www.uk10k.org), with thousands of human genomes sequenced so far, were initiated. Among such projects, the most ambitious perhaps is the Personal Genome Project (PGP) (), with genomes of 100 000 individuals as the anticipated outcome. Large repositories are built not only for human genomes, to mention 1001 Genomes Project (1001GP), with Arabidopsis thaliana genetic variation (http://www. 1001genomes.org/about.html). It is a well-known fact that two organisms of the same species are highly similar; it was estimated that the genomes of two persons are identical in 99.5% (). The huge amount of data obtained in the large-scale projects demands efficient ways of storing them. Taking into account the high similarity of organisms, it becomes obvious that some compression method may be effectively applied. Compression of single genomic sequences is hardly efficient, as the best obtained compression ratios achieve a factor of 4 or 5 () only. When realized that instead of the complete genomic sequence, storing only differences between it and some referential sequence is enough, the task became easier. In their seminal paper,showed how to store the description of variations between James Watson's (JW) genome and a referential genome in only 4.1 MB. However, the authors' prior knowledge was not only the reference sequence but also the single nucleotide polymorphism (SNP) map. As the input, they took the information about the SNPs and insertion or deletion (indels) variations between JW genome and referential genome, and compressed these data using some clever, but simple, techniques. Comparing with $3.1 Gbases of human genome, this means $750-fold compression. In the following years, a number of articles on relative compression of genomes were published (). In all the articles, the input sequences were complete genomes, not differences between genomes and a reference genome. This complicates the compression problem, as it is necessary to find the differences between genomes without any prior knowledge and without a database of variants (i.e. SNP and indel database). The most successful of the algorithms seems to be GDC (), which differentially compressed a collection of 69 human genomes from Complete Genomics Inc. to 215 MB (3.1 MB per individual). It is based on the ZivLempel () paradigm and finds approximate matches between the genome sequences. Recently,showed how to improve the technique from, compressing the JW genome to 2.5 MB, with very similar average results on multiple 1000GP genomes. The introduced novelties are partly biologically inspired, e.g. making use of tag SNPs characterizing haplotypes. Another line of research concerns indexing genomic collections (or more generally, repetitive sequences), i.e. building data structures enabling fast pattern search in the genomes (). Such indexes are efficient when the index resides in the main computer memory, which is challenging considering the sheer volume of indexed data. Some of the listed works are rather theoretical and their *To whom correspondence should be addressed. implementations are not yet available, whereas the works that have been implemented () are tested on relatively small collections, not exceeding $1 GB. In this article, we try to answer the question how well a collection of genomes of the same species can be compressed, when knowledge of the possible variants is given. The cited works ofare so far the only attempts to compress a (single) genome sequence with a variant database. In this work, we take two large collections of genomes (H.sapiens and A.thaliana) and try to exploit cross-sequence correlations in the variant loci. Our solution is a specialized Ziv-Lempel-style compressor, where the input sequences are basically formed by binary flags denoting if successive variants from the database were found in the individuals. This approach appears highly successful, allowing to store the human collection in 432 MB (395 KB per individual) and the plant collection in 110 MB (142 KB per individual). We point out that the general idea of exploiting common features for improved compression is known for some other NGS tasks, including compression of (both mapped and unmapped) reads (). In the next section, we present the input data and the general idea of our approach. Then, we show some details of the proposed compression algorithm. Finally, we evaluate the compressor. The last section concludes the article.
DISCUSSIONWe examined the possibility of obtaining much better compression ratios of genomic collections than from existing tools, when additional knowledge is given. The knowledge was the information about the possible variants in genomes and the occurrence of these variants in specific genomes. This helps a lot in compression of genomic sequences, as all input sequences are perfectly aligned and the task of finding repetitions in data (usually the most important and time-consuming task handled by data compression algorithms) becomes rather simple. We should mention that in theory such perfect alignments can be found by compression algorithms, but the computational burden would be enormous. Thus, compression tools usually make some heuristic decisions when comparing the sequences in the hope that they do not lose too much. The success of our algorithm was possible not only because of the variant database, but also because we searched for crosscorrelations between individuals. In other words, for each individual, similarities to any other previously processed individual (i.e. runs of repeating variants) can be found. In principle, the available memory may be a limiting factor but processing the collection on the chromosome level resulted in 52.5 GB memory use for the larger (human) of the tested datasets. In the future, when much more genomes are available, we may need to re-address the memory issue though, possibly via working on blocks smaller than whole chromosomes, or trying to re-order the sequences in a way to maximize local similarities. In the compression method design, we sometimes traded compression ratio for reduced memory requirements, e.g. some (rather minor) improvements in compression would be possible owing to higher-order contextual modeling. Probably, a more practical approach is to make use of more biological knowledge; the very recent work ofgives new insight, which might be possible to use in our scheme, but we leave it for future work. Why such experiments can be interesting? Although accurate and efficient analyses of such huge (several terabytes in raw format) genomic collections remain a major challenge, we believe that the mere compressibility of human genomes (e.g. as a 'lower bound' for memory requirements of future algorithms and tools) is a question worth investigating. For example, our compressed collection takes $430 MB, so including also a compressed reference genome (at most 700 MB) requires $1.1 GB of space, which seems quite modest. Naturally, running efficient queries over such data is another matter (clearly with some overhead in space use), but our results suggest this is not impossible. The information kept in VCF or genome variation format (GVF) files is often more detailed (e.g. may include quality scores) than what our tool preserves. Although clearly efficient compression methods for such data are also needed, we do not anticipate a possibility to obtain similar compression ratios to TGC, unless a (strongly) lossy mode is used. Unfortunately, we cannot see a way to easily adapt our compression techniques for such data. TGC allows extracting an arbitrary chromosome (or a whole genome) from the compressed collection, yet this solution is simple and rather slow. Making this extraction faster, or (even better) allowing for quick access to position-restricted arbitrary snippets of the genomes in the collection, is an important task left for future work. Clearly, there must be some space-time tradeoffs for such functionalities. A somewhat related functionality will be to add or remove an individual genome to/from the collection. Currently, changing the archive content requires recompressing the collection from scratch. The performed experiments showed that even the best genomic sequence compressor, GDC-ultra, is significantly (up to seven times) poorer in compression ratio than what can be obtained with extra knowledge. The main conclusions from our work are:Note: VCFmin means a simplified VCF with SNP calls only that spends only 4 bytes for each genotype. All sizes are in megabytes. The values marked in bold indicate best compression.