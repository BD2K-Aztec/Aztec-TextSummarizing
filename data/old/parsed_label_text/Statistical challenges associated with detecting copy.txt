Motivation: Analysing next-generation sequencing (NGS) data for copy number variations (CNVs) detection is a relatively new and challenging field, with no accepted standard protocols or quality control measures so far. There are by now several algorithms developed for each of the four broad methods for CNV detection using NGS, namely the depth of coverage (DOC), read-pair, split-read and assembly-based methods. However, because of the complexity of the genome and the short read lengths from NGS technology, there are still many challenges associated with the analysis of NGS data for CNVs, no matter which method or algorithm is used. Results: In this review, we describe and discuss areas of potential biases in CNV detection for each of the four methods. In particular, we focus on issues pertaining to (i) mappability, (ii) GC-content bias, (iii) quality control measures of reads and (iv) difficulty in identifying duplications. To gain insights to some of the issues discussed, we also download real data from the 1000 Genomes Project and analyse its DOC data. We show examples of how reads in repeated regions can affect CNV detection, demonstrate current GC-correction algorithms, investigate sensitivity of DOC algorithm before and after quality control of reads and discuss reasons for which duplications are harder to detect than deletions.
INTRODUCTIONCopy number variations (CNVs) are an important and abundant source of variation in the human genome, encompassing a greater proportion of the genome, as compared with singlenucleotide polymorphisms (SNPs); an estimated 1.2% of a single genome differs from the reference human genome when considering CNVs, as compared with 0.1% by). In the past several years, SNP arrays and array comparative hybridization (aCGH) are commonly used for detection of CNVs, albeit with relatively low resolution, especially in terms of breakpoint determination. Sanger sequencing of paired reads, often seen as the gold standard for CNV detection, is able to detect CNVs with higher accuracy and resolution, to detect balanced rearrangements such as inversions and translocations and to detect CNVs in regions where probe density of other platforms, such as SNP arrays, is low. However, the technique is not feasible for a large number of genomes because of time and budget constraints. Next-generation sequencing (NGS) or also known as high-throughput sequencing attempts to combine the benefits of array technology and sequencing. The biggest advantage of NGS over traditional Sanger sequencing is the ability to sequence millions of reads in a single run at a comparatively inexpensive cost (). However, because of the complexity of the genome and the short read lengths (usually 35400 bp) from NGS technology, there are still many challenges associated with the analysis of NGS data for CNVs, no matter which method or algorithm is used. The growing popularity and success of NGS are evident from large-scale projects such as the 1000 Genomes Project (http:// www.1000genomes.org/), which aims to sequence at least 1000 individuals from different populations around the world to construct a detailed map of genetic variations in the human genome (). Thus far, in its pilot phase, the project has identified $15 million SNPs, 1 million short indels and420 000 structural variations (SVs), most of which were previously unreported ($61% of deletions and 89% of duplications are novel). The average SV size detected by the study was 8 kb, approximately four times smaller than a recent SV detection study using tiling CGH array (). SVs include dosage-altering variants such as CNVs (usually defined as deletions and insertions larger than 1 kb) and shorter indels, as well as dosage-neutral variants such as inversions and translocations. Nevertheless, current analytical methodologies to analyse NGS data for CNVs are not yet mature, and there are no well-established pipelines/protocols/quality control measures. Broadly, there are four methods for CNV detection using NGS data, namely (i) depth of coverage [DOC, or also known as read-depth (RD) methods], (ii) paired-end mapping (PEM, or also known as read-pair methods), (iii) split-read-(SR) and (iv) assembly-based (AS) methods (). The different methods are usually complementary to one another, as the underlying concepts excel at detecting certain types of variants, and a large proportion of discovered variants remain unique to a particular approach (). For example, in the 1000 Genomes Project CNV analysis, the group applied various variations of the four methods, with a total of 36 call sets with *To whom correspondence should be addressed. vastly varying degrees of false discovery rates (FDR51089%), as well as notable differences in terms of genomic regions ascertained, CNV size range and breakpoint precision among the different methods (). This review article highlights and investigates the challenges encountered when analysing NGS data for CNVs. In particular, we focus on issues pertaining to (i) mappability, (ii) GC-content bias, (iii) quality control measures of reads and (iv) difficulty in identifying duplications. As the characteristics of CNVs in germline and tumour cells are different, we caution that this review focuses largely on CNVs in the germline, and issues unique to tumour CNVs (also known as copy number alterations) are not discussed.
DISCUSSIONNGS, with its ability to perform massive parallel sequencing in a single run, is becoming increasingly popular. This brings with it an unprecedented opportunity to sequence many genomes at a relatively inexpensive cost (as compared with using Sanger sequencing). However, with billions of reads generated per individual, the sheer and exponentially increasing amount of data demands for better bioinformatics support and computers with larger storage and higher computing powers. No less important than the production of the data is the information technology infrastructure, and bioinformatics team needed to analyse it, with speculations that the costs associated with handling, storing and analysis of the data could be more than the production of the data. Analysing NGS data for structural variants is a relatively new and challenging field, with no standard protocols or quality control measures. The four methods of CNV detection are complementary. Comparing DOC, PEM and SR methods used in the 1000 Genomes Project, each approach uniquely discovered 30 60% of the CNVs (). These three methods require first mapping the sequenced reads to a reference genome. As the mapped reads are used in all downstream analysis, this first step of alignment is extremely crucial. As has been shown in the article, how the aligner or subsequent algorithm deals with reads in repeat regions is important for detecting variants that lie in these regions. Currently, the problem of CNV detection in repeated regions is still not completely solved. Using real data from the 1000 Genomes Project, this article highlights and investigates challenges associated with current methodologies and areas of potential biases encountered when analysing NGS data for CNVs. In particular, we focus on issues pertaining to (i) mappability, (ii) GC-content bias, (iii) quality control measures of reads and (iv) difficulty in identifying duplications. We feel this is a timely critical review that would aid researchers in a much needed well-validated pipeline for the analysis of structural variants.