
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bioimage informatics DP2: Distributed 3D image segmentation using micro-labor workforce</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013">10 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Richard</forename>
								<forename type="middle">J</forename>
								<surname>Giuly</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">National Center for Microscopy and Imaging Research</orgName>
								<orgName type="department" key="dep2">Center for Research in Biological Systems</orgName>
								<orgName type="department" key="dep3">Department of Neurosciences</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego, La Jolla</addrLine>
									<postCode>92093</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Keun-Young</forename>
								<surname>Kim</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">National Center for Microscopy and Imaging Research</orgName>
								<orgName type="department" key="dep2">Center for Research in Biological Systems</orgName>
								<orgName type="department" key="dep3">Department of Neurosciences</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego, La Jolla</addrLine>
									<postCode>92093</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Mark</forename>
								<forename type="middle">H</forename>
								<surname>Ellisman</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">National Center for Microscopy and Imaging Research</orgName>
								<orgName type="department" key="dep2">Center for Research in Biological Systems</orgName>
								<orgName type="department" key="dep3">Department of Neurosciences</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego, La Jolla</addrLine>
									<postCode>92093</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bioimage informatics DP2: Distributed 3D image segmentation using micro-labor workforce</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">29</biblScope>
							<biblScope unit="page" from="1359" to="1360"/>
							<date type="published" when="2013">10 2013</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btt154</idno>
					<note type="submission">Received on October 15, 2012; revised and accepted on March 26, 2013</note>
					<note>Associate Editor: Jonathan Wren Contact: rgiuly@ucsd.edu Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This application note describes a new scalable semi-automatic approach, the Dual Point Decision Process, for segmentation of 3D structures contained in 3D microscopy. The segmentation problem is distributed to many individual workers such that each receives only simple questions regarding whether two points in an image are placed on the same object. A large pool of micro-labor workers available through Amazon&apos;s Mechanical Turk system provides the labor in a scalable manner. Availability and implementation: Python-based code for non-commercial use and test data are available in the source archive at https://sites.google.com/site/imagecrowdseg/.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The improved resolution and amount of detail afforded by emerging electron microscopy (EM) techniques, such as serial block scanning EM (SBEM) (<ref type="bibr" target="#b4">Denk and Horstmann, 2004;</ref><ref type="bibr" target="#b10">Leighton, 1981</ref>), is enabling researchers to explore scientific questions pertaining to morphology and network connectivity that were previously impossible (<ref type="bibr" target="#b5">Eisenstein, 2009</ref>). SBEM techniques, coupled to new staining protocols (<ref type="bibr" target="#b3">Deerinck et al., 2010</ref>), are able to reveal cell boundaries, sites such as synapses, and many intracellular components, such as synaptic vesicles and mitochondria. Manual segmentation represents a well-recognized bottleneck in cellular imaging. In a typical scenario, segmentation involves a single trained expert using automated algorithms or manual methods to examine and mark up each individual slice and trace contours around the structures of interest using a program such as TrakEM2 (<ref type="bibr" target="#b1">Cardona, 2006</ref>) or other specialized software programs. Dual Point Decision Process (DP2) streamlines and parallelizes the process by distributing it to a large number of workers. Amazon's Mechanical Turk system enables rapid completion of jobs as a consequence of the large number of workers continuously available and attracted to this resource (In our tests, tens of thousands of decisions were accomplished in 51 day.). Average cost was 1.2 dollars per cubic micron for Dataset 1, a mouse optic nerve sample, and 56 dollars per cubic micron for Dataset 2, a mouse cerebellar neuropil sample (shown in Supplementary Information). The tests were performed with a payment of one cent (US Dollars) per decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Although progress has been made to develop automatic segmentation techniques appropriate for cells (<ref type="bibr" target="#b0">Andres et al., 2008;</ref><ref type="bibr" target="#b6">Jain et al., 2007;</ref><ref type="bibr" target="#b8">Jurrus, 2010;</ref><ref type="bibr" target="#b9">Kaynig et al., 2010;</ref><ref type="bibr" target="#b11">Mishchenko, 2009</ref>), there remains a need for more accurate, rapid and robust techniques to delineate cells in SBEM data. Jeong and Chklovskii use a combination of 3D visualization and semi-automatic segmentation to address the segmentation challenge (<ref type="bibr" target="#b2">Chklovskii et al., 2010;</ref><ref type="bibr" target="#b7">Jeong et al., 2009</ref>). Our method differs from these in that we use micro-labor workers with a simple web interface rather than a trained user with a more complex interface. This makes our method scalable in that a large number of workers are readily available for a given job. Roberts describes a method using sparse scribbles from users (<ref type="bibr" target="#b12">Roberts et al., 2011</ref>) to semi-automatically trace neural processes. Additionally, the Eyewire project led by Sebastian Seung uses volunteers who learn to perform 3D semi-automatic segmentation with a specialized web viewer (<ref type="bibr" target="#b13">Wiecek, 2012</ref>). Our method differs from these in that user decisions are completely binary based on images of points within superpixels. Additionally, our application uses the micro-labor environment of Amazon's Mechanical Turk so that work can be accomplished practically by the existing pool of visitors who survey Mechanical Turk offerings. To make our process appropriate for such a system, users are given a simple presentation of dots automatically rendered on an image and only asked to answer yes or no decisions. Also, in contrast to Eyewire, we test with membranes and intracellular components stained, whereas Eyewire currently addresses data where only extracellular regions are stained. Finally, although some other methods such as (<ref type="bibr" target="#b6">Jain et al., 2007</ref>) use machine learning for automatic segmentation, our method does not; therefore, we do not need a large amount of human-labeled training data or a large of amount of training time for machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head><p>The overall DP2 process consists of an initial over-segmentation followed by a manual merging process to create a segmentation. The 3D dataset is represented as a stack of 2D images. To pre-process the data, we first smooth the original images with a 2D Gaussian filter. Then a 2D watershed operation from the Insight Toolkit (<ref type="bibr" target="#b14">Yoo et al., 2002</ref>) is applied on the gradient magnitude of the *To whom correspondence should be addressed. ß The Author 2013. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. Gaussian-filtered image to create superpixels. Each region of the watershed operation is a superpixel, and the watershed operation is tuned to favor over-segmentation rather than under-segmentation. This superpixelization process is applied to every slice of the 3D volume to produce an initial over-segmentation. To begin the merge decision process, a graph is created to represent the superpixels and possible merge operations. Each of the superpixels is a node of the graph, and each pair of adjacent superpixels represents a possible edge in the graph. Any two neighboring superpixels in a 2D image are considered adjacent (Case 1). Additionally, any two superpixels in neighboring planes that touch are considered adjacent (Case 2). The goal of the workers is to decide what adjacent superpixels should be merged to transform the over-segmentation into a more accurate segmentation. Therefore, if users judge that adjacent superpixels should be connected, the corresponding edge in the graph is added. Two positive votes from independent users are required for the merge to be accepted. After graph edges are added, the final step consists of computing connected components in the graph. Each component represents a single object of the image, such as an axon belonging to a nerve cell. Careful presentation of decisions to the workers in the merging decision process is important. To keep presentation clear and elegant, they are not shown the outline of superpixels explicitly. Rather, for each pair of adjacent superpixels, an image is generated (as shown in<ref type="figure" target="#fig_0">Fig. 1B</ref>) that shows two dots superimposed on the original image data placed at the two superpixel centers (If the center of a superpixel lies outside of the superpixel, a random point inside of the superpixel region is used instead.). The images are cropped so that the field of view only covers the two superpixels relevant to the decision. For each merge decision, the job of the user is to decide whether the two dots lie on the same object. If both dots lie in the same object, then the superpixels should be merged. The implementation of the merging process is demonstrated using Mechanical Turk. Two types of tasks are assigned to users. One is to merge two adjacent superpixels in one image plane (Case 1), and the other is to connect superpixels in adjacent planes (Case 2). For Case 1, the user makes merge decisions based on whether two dots on a single image corresponding to two adjacent superpixels are within the same cell. For Case 2, the worker must identify superpixels that should be connected from one image slice to the next. To accomplish this, a two-frame animated image is presented that repetitively switches between the two image slices, showing a dot on each the two superpixels that are being evaluated. In this case, the user must judge whether the dot is staying inside the same cell as frames alternate.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. (A) Image of axons and myelin in optic nerve. (B) Red dots placed on two different superpixels. (C) Two superpixels associated with two dots presented to the user. (D) All superpixels in the view, highlighted in different colors. (E) Segmentation after all superpixels merge decisions have been made. (F) 3D rendering showing two axons in red and blue</figDesc></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">R.Giuly et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors thank David Lee and Eric Bushong for useful advice.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Segmentation of SBFSEM volume data of neural tissue by hierarchical classification</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Andres</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th DAGM symposium on Pattern Recognition</title>
		<editor>Rigoll,G.</editor>
		<meeting>the 30th DAGM symposium on Pattern Recognition<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="142" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">TrakEM2: an ImageJ-based program for morphological data mining and 3d modeling</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Cardona</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ImageJ User and Developer Conference</title>
		<meeting>the ImageJ User and Developer Conference<address><addrLine>Luxembourg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-03-18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-automated reconstruction of neural circuits using electron microscopy</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">B</forename>
				<surname>Chklovskii</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Opin. Neurobiol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="667" to="675" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Enhancing serial block-face scanning electron microscopy to enable high resolution 3-D nanohistology of cells and tissues</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">J</forename>
				<surname>Deerinck</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Microsc. Microanal</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1138" to="1139" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Serial block-face scanning electron microscopy to reconstruct three-dimensional tissue nanostructure</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Denk</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Horstmann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Biol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">329</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural circuits: putting neurons on the map</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Eisenstein</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">461</biblScope>
			<biblScope unit="page" from="1149" to="1152" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Supervised learning of image restoration with convolutional networks</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Jain</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on Computer Vision</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Scalable and interactive segmentation and visualization of neural processes in EM datasets</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">K</forename>
				<surname>Jeong</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1505" to="1514" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Detection of neuron membranes in electron microscopy images using a serial neural network architecture</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Jurrus</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="770" to="783" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Neuron geometry extraction by perceptual grouping in ssTEM images</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Kaynig</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2902" to="2909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">SEM images of block faces, cut by a miniature microtome within the SEM—A technical note</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">B</forename>
				<surname>Leighton</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scan. Electron Microsc</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Automation of 3D reconstruction of neural tissue from large volume of conventional serial section transmission electron micrographs</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Mishchenko</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci. Methods</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="page" from="276" to="289" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural process reconstruction from sparse user scribbles</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Roberts</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Comput. Comput. Assist. Interv</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="621" to="628" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Pt. 1</note>
</biblStruct>

<biblStruct   xml:id="b13">
	<monogr>
		<title level="m" type="main">Crowdsourcing microscopic analysis. Biotechniques. http://www. biotechniques.com/news/Crowdsourcing-Microscopic-Analysis/biotechniques331740.html?</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">S</forename>
				<surname>Wiecek</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2012-04-18" />
			<biblScope unit="page">314861</biblScope>
		</imprint>
	</monogr>
	<note>date. last accessed</note>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Engineering and algorithm design for an image processing Api: a technical report on ITK-the Insight Toolkit. Stud</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">S</forename>
				<surname>Yoo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Health Technol. Inform</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="586" to="592" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>