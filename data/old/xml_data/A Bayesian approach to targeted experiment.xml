
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Systems biology A Bayesian approach to targeted experiment design</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012">2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">J</forename>
								<surname>Vanlier</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of BioMedical Engineering</orgName>
								<orgName type="institution">Eindhoven University of Technology</orgName>
								<address>
									<postCode>5612 AZ</postCode>
									<settlement>Eindhoven</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Netherlands Consortium for Systems Biology</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<postCode>1098</postCode>
									<settlement>Amsterdam, XH</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">C</forename>
								<forename type="middle">A</forename>
								<surname>Tiemann</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of BioMedical Engineering</orgName>
								<orgName type="institution">Eindhoven University of Technology</orgName>
								<address>
									<postCode>5612 AZ</postCode>
									<settlement>Eindhoven</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Netherlands Consortium for Systems Biology</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<postCode>1098</postCode>
									<settlement>Amsterdam, XH</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">P</forename>
								<forename type="middle">A J</forename>
								<surname>Hilbers</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of BioMedical Engineering</orgName>
								<orgName type="institution">Eindhoven University of Technology</orgName>
								<address>
									<postCode>5612 AZ</postCode>
									<settlement>Eindhoven</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Netherlands Consortium for Systems Biology</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<postCode>1098</postCode>
									<settlement>Amsterdam, XH</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">N</forename>
								<forename type="middle">A W</forename>
								<surname>Van Riel</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of BioMedical Engineering</orgName>
								<orgName type="institution">Eindhoven University of Technology</orgName>
								<address>
									<postCode>5612 AZ</postCode>
									<settlement>Eindhoven</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Netherlands Consortium for Systems Biology</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<postCode>1098</postCode>
									<settlement>Amsterdam, XH</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Systems biology A Bayesian approach to targeted experiment design</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">28</biblScope>
							<biblScope unit="issue">8</biblScope>
							<biblScope unit="page" from="1136" to="1142"/>
							<date type="published" when="2012">2012</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/bts092</idno>
					<note type="submission">Received on October 28, 2011; revised on January 31, 2012; accepted on February 17, 2012</note>
					<note>Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:57 27/3/2012 Bioinformatics-bts092.tex] Page: 1136 1136–1142 Associate Editor: Martin Bishop Contact: j.vanlier@tue.nl; N.A.W.v.Riel@tue.nl Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Systems biology employs mathematical modelling to further our understanding of biochemical pathways. Since the amount of experimental data on which the models are parameterized is often limited, these models exhibit large uncertainty in both parameters and predictions. Statistical methods can be used to select experiments that will reduce such uncertainty in an optimal manner. However, existing methods for optimal experiment design (OED) rely on assumptions that are inappropriate when data are scarce considering model complexity. Results: We have developed a novel method to perform OED for models that cope with large parameter uncertainty. We employ a Bayesian approach involving importance sampling of the posterior predictive distribution to predict the efficacy of a new measurement at reducing the uncertainty of a selected prediction. We demonstrate the method by applying it to a case where we show that specific combinations of experiments result in more precise predictions. Availability and implementation: Source code is available at:</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Computational models can be used to predict (un)measured behaviour or system responses and formalize hypotheses in a testable manner. To be able to make predictions parameters are required. Despite the development of new quantitative experimental techniques, data are often relatively scarce. Consequently the modeller is faced with a situation where large regions of parameter space can describe the measured data to an acceptable degree (<ref type="bibr" target="#b0">Brännmark et al., 2010;</ref><ref type="bibr" target="#b2">Calderhead and Girolami, 2011</ref>;<ref type="bibr" target="#b11">Girolami and Calderhead, 2011;</ref><ref type="bibr" target="#b14">Hasenauer et al., 2010;</ref><ref type="bibr" target="#b23">Raue et al., 2009</ref>). This is not a problem when the predictions required for testing the hypothesis (which we shall refer to as predictions of interest) are well constrained (<ref type="bibr" target="#b6">Cedersund and Roll, 2009;</ref><ref type="bibr" target="#b12">Gomez-Cabrero et al., 2011;</ref><ref type="bibr" target="#b13">Gutenkunst et al., 2007;</ref><ref type="bibr" target="#b19">Kreutz et al., 2011;</ref><ref type="bibr" target="#b27">Tiemann et al., 2011</ref>). When this is not the case more data will be required. Optimal experiment design (OED) methods can be used to determine which experiments would be most useful in order to perform statistical inference. Classical design criteria are often based on linearization around a best fit parameter set (<ref type="bibr" target="#b17">Kreutz and Timmer, 2009</ref>) and * To whom correspondence should be addressed. pertain to effectively constraining the parameters (<ref type="bibr" target="#b9">Faller et al., 2003;</ref><ref type="bibr" target="#b24">Rodriguez-Fernandez et al., 2006</ref>) or predictions (<ref type="bibr" target="#b3">Casey et al., 2007</ref>). However, when data are scarce considering the model complexity or the model is strongly non-linear, such methods are not appropriate (<ref type="bibr" target="#b19">Kreutz et al., 2011</ref>). This makes investigating the role of parameter uncertainty in OED a relevant topic to explore. We propose a method for experimental design that overcomes these issues by adopting a probabilistic approach which incorporates prediction uncertainty. Our method enables the modeller to target experimental efforts in order to selectively reduce the uncertainty of predictions of interest. Using our approach, multiple experiments can be designed simultaneously revealing potential benefits that might arise from specific combinations of experiments. We focus on biochemical networks that can be modelled using a system of ordinary differential equations. These models comprise of equations f ( x(t), u(t), p) which contain parameters p (constant in time), inputs u(t) and state variables x(t). Given a set of parameters, inputs and initial conditions x(0) these equations can subsequently be simulated. Measurements y(t) are performed on a subset and/or a combination of the total number of states in the model. Measurements are hampered by measurement noise ξ while many techniques used in biology (e.g. western blotting) necessitate the use of scaling and offset parameters q (<ref type="bibr" target="#b18">Kreutz et al., 2007</ref>). We define θ as θ ={ p, q, x 0 }, which lists all the required variables to simulate the model.</p><formula>˙ x(t) = f ( x(t), u(t), p) (1) y(t) = g( x(t), q)+ ξ(t)</formula><formula>(2) x(0) = = x 0 (3)</formula><p>In order to perform inference and experiment design an error model is required. For ease of notation we shall demonstrate our method using a Gaussian error model. If we consider M time series of length N 1 , N 2 , ..., N M hampered by such noise, we obtain Equation (4) for the probability density function of the output data. Here y t is the true system with true parameters θ t , where σ i,j indicates the SD of a specific data point and K serves as a normalization constant.</p><formula>P(y| θ t ) = M i=1 N i j=1 P(y i (t j ), θ t ) (4) = Ke − M i=1 N i j=1 y i (t j )−y t i (t j , θt ) √ 2σ i,j 2 (5)</formula><p>Using Bayes' theorem, we obtain an expression for the posterior probability distribution over the parameters (<ref type="bibr" target="#b16">Klinke, 2009</ref>). The posterior probability distribution is given by normalizing © The Author(s) 2012. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Targeted Experiment Design</head><p>Equation (4) multiplied with the prior to a unit area. Here P(y| θ t ) corresponds to the probability of observing dataset y given the true parameters θ t. Both computational as well as methodological advances have made Markov Chain Monte Carlo (MCMC) an attractive option for obtaining samples from such a distribution (<ref type="bibr" target="#b10">Geyer, 1992;</ref><ref type="bibr" target="#b11">Girolami and Calderhead, 2011;</ref><ref type="bibr" target="#b16">Klinke, 2009</ref>). Given a sample of the posterior parameter distribution, predictions can be made by simulating the model for each of the parameter sets. The distribution of such predictions shall be referred to as the posterior predictive distribution (PPD) and reflects their uncertainty. Since all of these predictions are linked via the parameter distributions, the relations between the different predictions can be exploited for experimental design. By considering the effects of a new measurement on the PPD, it is possible to predict how useful an experiment would be. Our approach consists of a number of steps. First, we briefly mention how to compute the PPD. Subsequently we detail how to compute the efficacy of the new measurement. In a third step this measure is used for experimental design. We conclude by demonstrating the method by applying it to a case study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">APPROACH</head><p>In order to overcome the limitations of existing OED methods, a samplingbased approach for experimental design is proposed. This approach consists of four consecutive steps which we shall outline below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Step 1. Computation of the posterior parameter distribution</head><p>The first step in the analysis is computing the posterior parameter distribution of the model based on the available data:</p><formula>P( θ|y D ) ∝ P(y D | θ)P( θ) (6)</formula><p>Here probability reflects a degree of belief and prior knowledge regarding the parameters is included in the form of priors, and P(y D | θ) denotes the conditional probability of the data given the model parameters. The unnormalized form of this function is often referred to as the likelihood function. Furthermore, P( θ) refers to the prior distribution of the parameters.</p><p>In order to sample from the posterior distribution we employ a MCMC method known as the Metropolis–Hastings algorithm. This algorithm performs a random walk through parameter space where each subsequent step is based on a proposal distribution (centred on the current step) and an acceptance criterion based on the proposal and probability densities at the sampled points. In brief, after an initial burn-in period (which is discarded), MCMC methods generate samples from probability distributions whose probability densities are known up to a normalizing factor. The Metropolis–Hastings algorithm proceeds as follows:</p><p>(1) Generate a sample θ n+1 by sampling from a proposal distribution based on the current state</p><formula>θ n .</formula><p>(2) Compute the likelihood of the data L(y D | θ n+1 ) and calculate</p><formula>P( θ n+1 |y D ) = L(y D | θ n+1 )P( θ n+1 ), where P( θ n+1</formula><p>) refers to the prior density function.</p><p>(3) Draw a random number γ from a uniform distribution between 0 and 1 and accept the new step if γ&lt;min</p><formula>P( θ n+1 |y D )Q( θ n+1 → θn) P( θn|y D )Q( θn→ θ n+1 ) ,1 .</formula><p>Here Q(θ 1 → θ 2 ) refers to the proposal density from current parameter set θ 1 to θ 2. The ratio of Q ensures detailed balance, a sufficient condition for the Markov chain to converge to the equilibrium distribution (<ref type="bibr" target="#b22">Neal, 1996</ref>). It corrects for sampling biases resulting from non-symmetric proposal distributions and is defined as the ratio between the proposal densities associated with going from n to n+1 and n+1 to n. We employ an adaptive Gaussian proposal distribution whose covariance matrix is based on a quadratic approximation to the posterior probability at the current sample point (<ref type="bibr" target="#b13">Gutenkunst et al., 2007</ref>). Further details regarding the implementation can be found in the Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Step 2. Determine PPDs for all candidate experiments</head><p>A PPD is a distribution of predictions conditioned on the available data as shown in Equation (7). A PPD is obtained by simulating the model (including the addition of measurement noise) for a sample of parameter sets from the posterior parameter distribution. We simulate a PPD for each candidate experiment. These PPDs link the parameters to the predictions and via the parameters also link predictions (across different experiments) to each other. The model and data constrain the dynamics of the system and hereby implicitly impose non-trivial relations between the different predictions. Therefore, the observables of candidate experiments are related to our prediction of interest. The next step is to exploit the relations within these distributions for experimental design.</p><formula>P(y|y D , u (t) ) = P(y| θ, u (t) )P( θ|y D )d θ (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Step 3. Predict EVR based on PPDs and measurement accuracies</head><p>To be able to perform experiment design a measure of expected measurement efficacy is required. For this purpose, we introduce the expected variance reduction (EVR). Consider an independent new measurement of a specific prediction (observable). This new measurement is associated with an error model G which reflects a certain degree of uncertainty associated with the new experiment. If this new experiment were to be performed and a value is obtained, then the subsequent step would be to incorporate the new data point (and its associated error model) in the likelihood function and re-perform the MCMC. This new data would subsequently constrain the posterior parameter distribution, hence also affecting the prediction of interest (which cannot be measured directly). This process is illustrated in<ref type="figure" target="#fig_1">Figure 1</ref>. The outcome of the experiment is known only after the experiment has been performed. Therefore the measured value in the error model is not known a priori. However, the PPD provides us with a predicted distribution of this value (shown in grey in<ref type="figure" target="#fig_1">Fig. 1</ref>) which reflects the uncertainty associated with this value. Samples from the PPD can subsequently be substituted as 'true' values in the error model. By repeating this process for every Rth point of our MCMC chain and averaging the result, we weight by the probability distribution of this predicted value. Considering that a single MCMC is often already computationally demanding, such a nested MCMC is likely not tractable. Here we propose an alternative approach. Consider the unnormalized densities P(y|</p><formula>θ), P(y n | θ) and P( θ)</formula><p>, respectively, corresponding to the density model of the data used to determine the initial posterior distribution, the density model for the new data point, and the parameter prior. Assuming that the new data point is independent of the existing data points we can state that</p><formula>P N ( θ|y,y n ) ∝ P(y| θ) P(y n | θ) P( θ)</formula><p>in order to obtain the following equation for the new normalized posterior (8). In this equation, Z 1 and Z 2 denote the normalization constants of the old and new posterior, respectively.</p><formula>P N ( θ|y,y n ) = P(y| θ) P(y n | θ) P( θ) P(y| θ) P(y n | θ) P( θ)d θ (8) = P(y| θ) P( θ) P(y| θ) P( θ)d θ P(y| θ) P(y n | θ) P( θ) P(y| θ) P( θ) P(y| θ) P( θ)d θ P(y| θ) P(y n | θ) P( θ)d θ</formula><formula>(9) = P( θ|y) P(y n | θ) P(y| θ) P( θ)d θ P(y| θ) P(y n | θ) P( θ)d θ = P( θ|y) P(y n | θ) Z 1 Z 2 (10)</formula><p>This relation between the two posteriors can be exploited in order to compute expected values by re-weighting samples from the old posteriorappropriately. Rather than running a new MCMC for every sample, we can use self normalized importance sampling on the predictions of the output in order to compute expected values. This is shown in Equation (11), where samples θ i and θ j are taken from the old posterior distribution, T indicates the number of MCMC samples included in the analysis and z( θ) indicates our quantity of interest.</p><formula>E[z|y,y n ]= P N ( θ|y,y n )z( θ)d θ = P( θ|y) P(y n | θ) Z 1 Z 2 z( θ)d θ ≈ T i=1 P(y n | θ i ) T j=1 P(y n | θ j ) z( θ i ) (1 1)</formula><p>As mentioned before, the value of y n is not known a priori. Therefore, we subsequently compute such an expected value for each parameter set in the PPD with y n set to the predicted output value from the original PPD. Hence this provides a distribution of expected values considering possible outcomes of the experiment. The mean of these expected values then provides us with a prediction of the quantity of interest. The entire approach can then be succinctly summarized in Equation (12). Here the expected value of z is computed, G corresponds to the error model and θ i refers to the i-th parameter vector of the chain. Assuming a Gaussian error model with SD σ for a new measurement on the output y, probability model G is given by Equation</p><p>(13). Note that both input as well as output can be any quantity of interest (prediction or parameter) indicating the flexibility of the approach.</p><formula>E[z]= 1 T T r=1 T i=1 G(t, u(t), θ i , θ r ) T k=1 G(t, u(t), θ k , θ r ) z(t, u(t), θ i ) (12) G(t, u(t), θ i , θ r ) = e − y(t, u(t), θ i )−y(t, u(t), θr ) 2 2σ 2 (13)</formula><p>Since the variance of a variable of interest can be computed by</p><p>Equation (14), we can use the aforementioned method to estimate this quantity. The variance reduction can then be computed as shown in<ref type="bibr">Equation (15)</ref>where σ 2 old corresponds to the posterior variance without the new measurement and σ 2 new corresponds to the expected posterior variance with the new measurements taken into account. In other words, one obtains the mean variance reduction considering the prediction uncertainty. The variance reduction computed by this sampling method is referred to as the sampled variance reduction (SVR).</p><formula>Var[z]=E[z 2 ]− E[z] 2 (14) VarR = 1−E σ 2 new σ 2 old (15)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Linear variance reduction:</head><p>When the measurement error models and PPD can reasonably be assumed Gaussian, one can approximate the variance reduction by approximating the PPD between the output and the measurements of interest with a multivariate Gaussian distribution. First the PD covariance matrix (16) is computed, where z denotes the output of interest and x a b the b-th MCMC sample of the a-th measurable state (without measurement noise), with Q and T the number of measured points and samples, respectively.</p><formula>posterior = cov ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ z 1 x 1 1 ... x Q 1 z 2 x 1 2 ... x Q 2 . . . . . . .. . . . . z T x 1 T ... x Q T ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ (16)</formula><p>After performing the new measurements with given SDs σ b the covariance matrix is updated according to Equation (18). The resulting variance of the prediction of interest z can then be obtained as new (1,1). We shall refer to the approximated variance reduction as the linear variance reduction (LVR).</p><formula>noise = ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ 0 0 ... 0 0 σ 2 1 ... 0 . . . . . . .. . . . . 0 0 ... σ 2 Q ⎤ ⎥ ⎥ ⎥ ⎥ ⎦</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Step 4. Determine measurement points for optimal variance reduction</head><p>The probability density model can be obtained by multiplying the error models for each candidate measurement. Subsequently, the space of all candidate measurements is sampled using Monte Carlo sampling. The efficacy of a specific combination of measurements is evaluated by computing the variance reduction, which is defined as Equation (15). During this sampling stage, additional constraints which arise because of practical considerations can be imposed on the experimental design (simply by rejecting such samples). An example of this could be the inability to measure certain states simultaneously. The optimal experiment is then obtained by determining the combination of measurements that yields the maximal predicted variance reduction.using finite differencing (H ≈ J T J). All available priors were subsequently included in the Hessian approximation. After convergence, the chain was thinned to 10 000 samples. The SVR was computed in parallel using OpenCL on the GPU using a compiled MEX file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>To illustrate our method, we apply it to a model of the JAK-STAT signalling pathway (<ref type="bibr" target="#b23">Raue et al., 2009;</ref><ref type="bibr" target="#b28">Toni et al., 2009</ref>). The model is based on a number of hypothesized steps (See<ref type="figure">Figure 2</ref>). The first reaction describes the activation of the erythropoietin receptor which subsequently phosphorylates cytoplasmic STAT (x 1 ). Then phosphorylated STAT (x 2 ) dimerises (x 3 ) and is imported into the nucleus (x 4 ). Here dissociation and dephosphorylation occurs which are associated with a time delay. Similar to the implementation given in the original paper, the driving input function was approximated by a spline interpolant, while the delay was approximated using a linear chain approximation (x 5 ,...,x 13 ).</p><formula>˙ x 1 = 2 V nucleus V cyto p 4 x 13 −p 1 x 1 u 1 ˙ x 8 = p 4 x 7 −p 4 x 8 ˙ x 2 = p 1 x 1 u 1 −2p 2 x 2 2 ˙ x 9 = p 4 x 8 −p 4 x 9 ˙ x 3 = p 2 x 2 2 −p 3 x 3 ˙ x 10 = p 4 x 9 −p 4 x 10 (19) ˙ x 4 = V cyto V nucleus p 3 x 3 −p 4 x 4 ˙ x 11 = p 4 x 10 −p 4 x 11 ˙ x 5 = p 4 x 4 −p 4 x 5 ˙ x 12 = p 4 x 11 −p 4 x 12 ˙ x 6 = p 4 x 5 −p 4 x 6 ˙ x 13 = p 4 x 12 −p 4 x 13 ˙ x 7 = p 4 x 6 −p 4 x 7</formula><p>In order to infer the posterior distribution data from the paper by Swameye et al.<ref type="bibr">[</ref>both reported in arbitrary units (which necessitates two scaling parameters). The initial cytoplasmic concentration of STAT is unknown while all other forms of STAT are assumed zero at the start of the simulation. Given the data, not all parameters are identifiable (<ref type="bibr" target="#b23">Raue et al., 2009</ref>). We used uniform priors in logspace for the kinetic parameters and a Gaussian (μ = 200 nM, σ = 20 nM) for the initial condition. Parameter two was bounded between ranges, since this parameter was non-identifiable from the data (<ref type="bibr" target="#b23">Raue et al., 2009</ref>). We simulated two chains starting at different initial values up to one million parameter sets and assessed convergence by visually inspecting differences between batches of samples. The uncertainty in model parameters propagates as an uncertainty in the predicted responses of the state variables. PPDs were simulated for all states as well as the summations of states already measured. To simulate the PPDs the chain of parameter sets was thinned to 10 000 samples using equidistant thinning. Since the error model in this case is additive Gaussian noise, there is no need to explicitly simulate measurement noise. This can be taken into account by multiplying the SD of the measurement by √ 2 (see Supplementary Materials for more information). An example is shown in<ref type="figure" target="#fig_4">Figure 3</ref>revealing the relation between two predictions at different time points. For a complete overview of the PPDs for all states, see the Supplementary Materials. The relation between the PPDs of different states was explored. This relation between two states at the indicated time points is shown in more detail in both scatter plot and 2D histogram form in<ref type="figure" target="#fig_4">Figure 3</ref>. The former shows the actual samples from the PPD for one point in time. Here each dot represents a simulated value for one parameter set from the MCMC chain. As shown in the figure, these different states are often non-linearly related at specific points in time. The associated 2D histogram corresponds to the same information interpreted as probability density. Considering state 3 as observable and state 4 as prediction, while assuming a measurement accuracy of σ = 10/ √ 2 for x 3 , it can be observed that a significant decrease in variance can be attained during the rise of state 3. Measuring state 3 at the peak value however, results in a smaller variance reduction. A few things can be observed. In order for the measurement to be useful, there should be a correlation between the measurement and the prediction of interest. Additionally, the uncertainty in both should be large enough. Since all predictions of state 3 start with an initial condition of zero, this implies that the uncertainty at this point is low. Therefore, an additional measurement at t = 0 would not yield appreciable variance reduction which is also reflected by the fact that the SVR starts at a value of zero. In order to demonstrate the flexibility of our method it was decided to perform OED for a quantity that depends on the model predictions in a highly non-linear fashion, namely the time to peak for the concentration of dimerized STAT in the nucleus (state 4). The time to peak was computed for the state 4 prediction for each parameter set from the posterior parameter distribution. We assumed that all states except state 4 are measurable with an accuracy of σ = 10/ √ 2. As potential measurements we also included the two sums of states as measured in earlier experiments. The experiment space was sampled using a Monte Carlo approach, uniformly sampling the experiment space. This sampling is shown in<ref type="figure" target="#fig_5">Figure 4</ref>where the SVR is shown for several combinations of two measurements. In this figure each axis corresponds to a potential measurement. Different model outputs (potential measurements) are separated using grid lines, while. The relation between the two states at the indicated time points is shown in both scatter plot and 2D histogram form. The former shows the actual samples from the PPD for one point in time. Here the dots represent simulated values belonging to different parameter sets from the MCMC chain. In the histogram the colour indicates the number of samples in a particular region which is proportional to the probability density.where the different model outputs are numbered. Numbers 1 to 3 correspond to the first three states whereas 4 and 5 correspond to the sums of states on which the original PPD was parametrized. Note that each block on each axis corresponds to an entire time series. The block corresponding to experiments involving state 1 is shown enlarged in (B). Variance reduction is computed using the importance sampling method. the interval between each pair of lines corresponds to an entire time series. The colour value indicates the SVR for that specific experiment. Recall that the original dataset contained measurements of two sums of model states. These two observables correspond to outputs 5 and 6 in<ref type="figure" target="#fig_5">Figure 4</ref>, which indicates that additional measurements on these would provide very little additional variance reduction. Interestingly, performing the experimental design for two measurements revealed that the largest reduction in variance could be obtained by measuring state one at an early and late time point. This result underlines the benefit of being able to combine multiple measurements in the OED. Furthermore, the analysis clearly revealed that the timing of this first time point is crucial. However, if accurate timing is not possible in the experiment one could consider measuring state three and one instead. Here smaller reductions are attained but the timing accuracy required for a A B<ref type="figure">Fig. 5</ref>. Comparison of two methods for calculating the variance reduction. Variance reduction of the peak time of dimerized STAT (x 4 ) with respect to two new measurements. (A) LVR. (B) Difference between the variance reduction computed by means of LVR and importance sampling (shown in<ref type="figure" target="#fig_5">Fig. 4</ref>). reasonable reduction is less stringent. Additionally, we investigated how the bounds of the priors on the non-identifiable kinetic rates affected our experimental design by widening them. This revealed that the EVRs obtained when measuring state 2 or 3 in combination with state 1 were more robust (for more information see the Supplementary Materials). Since both error models in this case are Gaussian, the same analysis can be performed using the LVR (which for T = 1000 samples is about 100-fold faster). The resulting sampling is shown in<ref type="figure">Figure 5</ref>. Qualitatively, the results agree well with those in<ref type="figure" target="#fig_5">Figure 4</ref>revealing its applicability as an initial sampling step. Information gained from an initial LVR sweep can subsequently be used to sample only relevant regions of the experiment design space. Another example can be found in the Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION AND CONCLUDING REMARKS</head><p>In this article, we have outlined a flexible method to perform experimental design. Here a Gaussian probability density function was used to model the uncertainties. Note, however, that our method is not restricted to such error models. In statistical parameter inference it is important to determine which error model to use for each experiment as this will define the appropriate likelihood function. If the likelihood function cannot be computed explicitly then approximate Bayesian methods can provide a solution (<ref type="bibr" target="#b28">Toni et al., 2009</ref>). In the OED the timing of the new measurement is assumed instantaneous (infinitely accurate). It remains an open but relevant challenge to incorporate temporal inaccuracies in the current framework. It is expected that when timing is more error prone and explicitly accounted for, experiments that are only effective during brief time intervals will be marked as less beneficial for variance reduction. In our method, we base the experimental design on the expected value of a distribution of variance reductions. However, since the entire distribution of possible variance reductions has been computed one could also consider incorporating information regarding the accuracy of this estimate into the selection process. Finding a sensible trade off between EVR and its inaccuracy considering the prediction uncertainty remains an open topic for further research. In order to obtain the posterior distribution, the parameters are required to be either identifiable or restricted by means of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Targeted Experiment Design</head><p>a finite prior distribution. Even for a small model, identifiability can be problematic but easily tested (<ref type="bibr" target="#b23">Raue et al., 2009</ref>). Given a sufficient amount of data, the posterior distribution should be relatively insensitive to the assumed priors. It is important to verify this a posteriori. One option to investigate prior dependence is to vary the priors or determine the effect of a measurement on the assumed prior. Note though that the latter strongly depends on the initial prior, which should be chosen sufficiently wide to cover all potential parameter regimes. The number of samples required in order to get a reliable estimate is highly problem specific. MCMC convergence is hard to assess and only non-convergence can be diagnosed (<ref type="bibr" target="#b2">Calderhead and Girolami, 2011</ref>; Cowles and Carlin,<ref type="bibr">1996</ref>). Once convergence has been attained, one should verify that the model sufficiently describes the acquired data as EVRs will be based on model predictions. The method is not limited to a specific family of distributions for the parameters and model predictions. However, strongly tailed distributions (such as high variance logarithmic distributions) can be problematic. The reason for this is that in such cases variance estimates from a small sample of the tail are quite unreliable and give a poor description of the distribution. Therefore it is sensible to a posteriori visually inspect the distribution at the time point determined optimal. In the case of heavy tailed distributions, it can be beneficial to perform a transformation of the PPD before performing the experimental design. Consider performing a new measurement as illustrated earlier in<ref type="figure" target="#fig_1">Figure 1</ref>. The estimation of the measurement efficacy involves multiplying samples of the old posterior with new weights in order to estimate quantities for the situation after the experiment has been performed. When computing such a weighted average it is important to keep track of the quality of the estimation. When the posterior before and after a new experiment is very different, many of these sample weights will be very low and a large fraction of the samples will contribute only negligibly to the estimation of the new variance. It follows that such an estimate will be poor. We monitor this degeneracy by estimating the effective sample size (ESS) defined below (<ref type="bibr" target="#b8">Del Moral et al., 2006</ref>).</p><formula>ESS r = N k=1 G(t, u(t), θ k , θ r ) 2 N k=1 G(t, u(t), θ i , θ r ) 2 (20)</formula><p>We compute a distribution of ESS values (one for each incorporated sample) which we characterize by its median value. This ESS gives a measure for the quality of the sampling. In the case that the importance sampling distribution agrees well with the new posterior, it should scale linearly with the number of included samples. When the values for the ESS are very low then values obtained for the variance reduction can be inaccurate. It also implies that such a measurement would be very informative from an inferential perspective. This stems from the fact that the updated probability distribution would be much narrower. In such a case, it would be beneficial to perform the experiment and subsequently redo the MCMC step in such cases (for more information, see the Supplementary Materials). Obtaining the PPDs as well as performing the experiment design is computationally expensive. For the former, model simulation time is a primary concern which can be significantly reduced by using compiled simulation code [see COPASI (<ref type="bibr" target="#b15">Hoops et al., 2006</ref>); ABC-SysBio (<ref type="bibr" target="#b20">Liepe et al., 2010</ref>); Potters Wheel (Maiwald</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:57 27/3/2012 Bioinformatics-bts092.tex] Page: 1138 1136–1142 J.Vanlier et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.1.</head><figDesc>Fig. 1. Illustration of the effect of adding a new data point on the PPD. Shown on the top right is the PPD at one specific time point for two predictions with a subset of the samples of the chain indicated with white points. The square denotes the location of the 'new measurement'. Prediction A refers to a prediction of which a new measurement can be performed (observable), whereas B denotes the prediction of interest. Here the grey distribution corresponds to the PPD before the new measurement, whereas the white Gaussian corresponds to the error model of the new measurement. Due to additional constraints imposed by this new measurement in combination with the old data and the model, the distribution on the hypothesis side is also updated in light of the new data point and shown in white.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><figDesc>2003; http://webber.physik.uni-freiburg.de/~jeti /PNAS_Swameye_Data/ (dataset 1)] was used. Measured quantities were the total concentration of STAT (x 1 +x 2 +2x 3 ) and the total concentration of phosphorylated STAT in the cytoplasm (x 2 +2x 3 ),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><figDesc>Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:57 27/3/2012 Bioinformatics-bts092.tex] Page: 1140 1136–1142 J.Vanlier et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.3.</head><figDesc>Fig. 3. Top left: one simulated time course of state 3 superimposed on the PPD. Two time points are indicated with circles. Bottom left: correlation coefficient between states 3 and 4 and SVR of state 4 based on a measurement of state 3 (SVR). The relation between the two states at the indicated time points is shown in both scatter plot and 2D histogram form. The former shows the actual samples from the PPD for one point in time. Here the dots represent simulated values belonging to different parameter sets from the MCMC chain. In the histogram the colour indicates the number of samples in a particular region which is proportional to the probability density.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.4.</head><figDesc>Fig. 4. Variance reduction of the peak time of dimerized STAT (x 4 ) with respect to two new measurements. (A) Each axis represents an experiment, where the different model outputs are numbered. Numbers 1 to 3 correspond to the first three states whereas 4 and 5 correspond to the sums of states on which the original PPD was parametrized. Note that each block on each axis corresponds to an entire time series. The block corresponding to experiments involving state 1 is shown enlarged in (B). Variance reduction is computed using the importance sampling method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><figDesc>Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:57 27/3/2012 Bioinformatics-bts092.tex] Page: 1141 1136–1142</figDesc></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="3"> COMPUTATIONAL METHODS All of the discussed algorithms were implemented in Matlab (Natick, MA, USA). Numerical integration of the differential equations was performed with compiled MEX files using numerical integrators from the SUNDIALS CVode package (Lawrence Livermore National Laboratory, Livermore, CA, USA). Absolute and relative tolerances were set to 10 −8 and 10 −9 , respectively. The Gaussian proposal distribution for the MCMC was based on an approximation to the Hessian computed using a Jacobian obtained 1138 at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">and Timmer, 2008); Sloppy Cell (Brown and Sethna, 2003)]. Additionally more efficient sampling methods for obtaining such posteriors in high dimensional spaces are being developed (Girolami and Calderhead, 2011; Toni et al., 2009). For the experiment design part, the computational burden can be divided into two contributions. First is the sampling of the experiment space. Since each experiment constitutes a dimension in experiment space, densely sampling this space for a large number of experiments can become prohibitively time consuming. In this case, it may be required to resort to more sophisticated sampling techniques such as population MCMC or sequential Monte Carlo methods. One option we employ is to perform a fast initial sweep of the experiment space by sampling the LVR. Then in a subsequent step the actual SVR is computed for those samples that resulted in a large LVR. For a comparison of the LVR and SVR for one specific application, see the Supplementary Materials. Additionally, profiling the resampling step revealed that the distance calculations for the error model were most time consuming. Since this step exhibits a large degree of parallelism, the resampling step was also implemented to run on the GPU (using OpenCL), treating the resampling for each MCMC simultaneously. Even on a modest GPU (NVIDIA Quadro FX 580) this resulted in considerable speedup (see Supplementary Material). As a last remark we would like to point out that if the goal of the experiment is to discriminate between models, alternative approaches (Skanda and Lebiedz, 2010) could be relevant to explore. We proposed a flexible data-based strategy for OED. Where existing design criteria pertain to effectively constrain specific parameters or target the variance of predictions using model linearization (Casey et al., 2007; Faller et al., 2003; RodriguezFernandez et al., 2006), this method is not limited to any specific error model or assumption regarding the parameter distribution. It enables the modeller to select specific predictions of interest that require decreased uncertainty thereby focus the experimental efforts in order to save time and resources. Furthermore, it allows the prediction of interest to be any quantity that can be obtained from simulations. An additional strength of the method is that multiple different measurements can be included in the design simultaneously in order to elucidate their combinatorial efficacy.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>The authors gratefully acknowledge the helpful comments of our two anonymous referees.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Mass and information feedbacks through receptor endocytosis govern insulin signaling as revealed using a parameter-free modeling framework</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Brännmark</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biol. Chem</title>
		<imprint>
			<biblScope unit="volume">285</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Statistical mechanical approaches to models with many poorly known parameters</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">S</forename>
				<surname>Brown</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">P</forename>
				<surname>Sethna</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="page" from="68" to="021904" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Statistical analysis of nonlinear dynamical systems using differential geometric sampling methods</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Calderhead</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Girolami</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Soc. Interface Focus</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="821" to="835" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimal experimental design in an epidermal growth factor receptor signalling and down-regulation model</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Casey</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Syst. Biol. IET</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="190" to="202" />
			<date type="published" when="2007-08-30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="m">oxfordjournals.org/ Downloaded from Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="57" to="84" />
		</imprint>
	</monogr>
	<note>bts092. .tex] Page</note>
</biblStruct>

<biblStruct   xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Vanlier</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Systems biology: model based evaluation and comparison of potential explanations for given biological data</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Cedersund</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Roll</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FEBS J</title>
		<imprint>
			<biblScope unit="volume">276</biblScope>
			<biblScope unit="page" from="903" to="922" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Markov chain Monte Carlo convergence diagnostics: a comparative review</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Cowles</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Carlin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="883" to="904" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Sequential monte carlo samplers</title>
		<author>
			<persName>
				<forename type="first">Del</forename>
				<surname>Moral</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="411" to="436" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Simulation methods for optimal experimental design in systems biology</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Faller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Simulation</title>
		<imprint>
			<biblScope unit="page" from="79" to="717" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Practical markov chain monte carlo</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Geyer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="473" to="483" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Riemann manifold Langevin and Hamiltonian Monte Carlo methods</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Girolami</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Calderhead</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="123" to="214" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Workflow for generating competing hypothesis from models with parameter uncertainty</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Gomez-Cabrero</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Soc. Interface Focus</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">438</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Universally sloppy parameter sensitivities in systems biology models</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">N</forename>
				<surname>Gutenkunst</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">189</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Parameter identification, experimental design and model falsification for biological network models using semidefinite programming</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Hasenauer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Syst. Biol. IET</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="119" to="130" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Copasia complex pathway simulator</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Hoops</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">3067</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">An empirical Bayesian approach for model-based inference of cellular signaling networks</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Klinke</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">371</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Systems biology: experimental design</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Kreutz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Timmer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FEBS J</title>
		<imprint>
			<biblScope unit="volume">276</biblScope>
			<biblScope unit="page" from="923" to="942" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">An error model for protein quantification</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Kreutz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">2747</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Likelihood based observability analysis and confidence intervals for predictions of dynamic models</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Kreutz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arxiv preprint</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">ABC-SysBio approximate Bayesian computation in Python with GPU support</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Liepe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">1797</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamical modeling and multi-experiment fitting with potterswheel</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Maiwald</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Timmer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2037" to="2043" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Sampling from multimodal distributions using tempered transitions</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Neal</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="353" to="366" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Structural and practical identifiability analysis of partially observed dynamical models by exploiting the profile likelihood</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Raue</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">A hybrid approach for efficient and robust parameter estimation in biochemical pathways</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Rodriguez-Fernandez</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biosystems</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="248" to="265" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">An optimal experimental design approach to model discrimination in dynamic biochemical systems</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Skanda</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Lebiedz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">939</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Identification of nucleocytoplasmic cycling as a remote sensor in cellular signaling by databased modeling</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Swameye</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci</title>
		<meeting>. Natl Acad. Sci</meeting>
		<imprint>
			<date type="published" when="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Parameter adaptations during phenotype transitions in progressive diseases</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Tiemann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Syst. Biol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">174</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Approximate bayesian computation scheme for parameter inference and model selection in dynamical systems</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Toni</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Soc. Interface</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="187" to="202" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>