
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Genetics and population analysis Learning phenotype densities conditional on many interacting predictors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014">2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">David</forename>
								<forename type="middle">C</forename>
								<surname>Kessler</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Advanced Analytics Division</orgName>
								<orgName type="institution">SAS Institute Inc</orgName>
								<address>
									<postCode>27513</postCode>
									<settlement>Cary</settlement>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Jack</forename>
								<forename type="middle">A</forename>
								<surname>Taylor</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Molecular and Genetic Epidemiology Section</orgName>
								<orgName type="laboratory">Epidemiology Branch and Laboratory of Molecular Carcinogenesis</orgName>
								<orgName type="institution">National Institute of Environmental Health Sciences</orgName>
								<address>
									<postCode>27709</postCode>
									<settlement>Research Triangle Park</settlement>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">David</forename>
								<forename type="middle">B</forename>
								<surname>Dunson</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Statistical Science</orgName>
								<orgName type="institution">Duke University</orgName>
								<address>
									<postCode>27708</postCode>
									<settlement>Durham</settlement>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Jeffrey</forename>
								<surname>Barrett</surname>
							</persName>
						</author>
						<title level="a" type="main">Genetics and population analysis Learning phenotype densities conditional on many interacting predictors</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">30</biblScope>
							<biblScope unit="issue">11</biblScope>
							<biblScope unit="page" from="1562" to="1568"/>
							<date type="published" when="2014">2014</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btu040</idno>
					<note type="submission">Received on July 19, 2013; revised on December 29, 2013; accepted on January 17, 2014</note>
					<note>BIOINFORMATICS ORIGINAL PAPER Associate Editor:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Estimating a phenotype distribution conditional on a set of discrete-valued predictors is a commonly encountered task. For example , interest may be in how the density of a quantitative trait varies with single nucleotide polymorphisms and patient characteristics. The subset of important predictors is not usually known in advance. This becomes more challenging with a high-dimensional predictor set when there is the possibility of interaction. Results: We demonstrate a novel non-parametric Bayes method based on a tensor factorization of predictor-dependent weights for Gaussian kernels. The method uses multistage predictor selection for dimension reduction, providing succinct models for the phenotype distribution. The resulting conditional density morphs flexibly with the selected predictors. In a simulation study and an application to molecular epidemiology data, we demonstrate advantages over commonly used methods. Availability and implementation: MATLAB code available at https:// googledrive.com/host/0Bw6KIFB-k4IOOWQ0dFJtSVZxNE0/ktdctf.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many areas of research are concerned with learning the distribution of a response conditional on numerous categorical (discrete) predictors. The important predictors for characterization of this distribution are not usually known in advance, and there may be hundreds or thousands of candidates. Methods that attempt to accommodate interactions among these predictors become mired in the enormous model space. For example, in a moderatedimensional case involving p ¼ 40 categorical predictors, each with d j ¼ 4 possible realizations, considering all possible levels of interaction leads to a space of 4 40 % 10 24 possible models. Parallelization and technical tricks may work for smaller examples, but data sparsity and the sheer volume of models force us to consider different approaches. The conditional density may vary in more than just location;<ref type="bibr" target="#b5">Chung and Dunson (2009)</ref>illustrated this in an application to the conditional density of blood glucose levels given insulin sensitivity and age. In the work that follows, we present a novel non-parametric Bayes (NPB) approach to learning conditional densities that makes use of a conditional tensor factorization to characterize the conditional distribution given the predictor set, allowing for complex interactions between the predictors. The particular form assumed for the conditional density gives rise to an attractive predictor selection procedure, providing support for distinct predictor selection steps. This addresses the challenges of high-dimensional data and produces conditional density estimates that allow assessment of tail risks and other complex quantities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">APPROACH</head><p>The primary goal of our work is to model the conditional density fðyjxÞ, where the form of this density for the response y changes flexibly with the predictor vector x. There is a large body of work devoted to this idea of density regression in settings involving x of dimension p 30, and such models have provided many options for that situation. We wish to develop techniques for problems involving much larger p, and ideally to scenarios where p41000. We want to provide a method that performs variable selection, assesses the probability of a predictors inclusion in the model and provides easily interpretable estimates of the impact of different predictors. This problem has been addressed with variations on the finite mixture model,</p><formula>fðyÞ ¼ X K h¼1 h Kðy; h Þ ð 1Þ</formula><p>This is the basic form of the hierarchical mixture of experts model [HME, Jordan and<ref type="bibr" target="#b16">Jacobs (1994)]</ref>. In this representation, K represents the number of contributing parametric kernels Kð; h Þ distinguished by parameters h. The h provides the weights in this convex combination of kernels, where P K h¼1 h ¼ 1 and ð 1 ,. .. , K Þ 2 S KÀ1 , the K – 1 probability simplex. The most straightforward forms rely on a pre-specified K and include the predictors x in a linear model for the mean. HME methods in the frequentist literature have often relied on expectation maximization (EM) (<ref type="bibr" target="#b8">Dempster et al., 1977</ref>) techniques, which can suffer from overfitting (<ref type="bibr" target="#b0">Bishop and Svense´nSvense´n, 2003</ref>). EM approaches in the Bayesian literature seek to avoid this;<ref type="bibr" target="#b30">Waterhouse et al. (1996)</ref>used EM to find maximum a posteriori estimates using the inherent Bayesian penalty against complexity to regulate those estimates. In addition, the Bayesian framework allows the quantification of uncertainty about the parameters in the model. *To whom correspondence should be addressed. NPB methods, such as the Dirichlet Process, prompted techniques like that in<ref type="bibr" target="#b21">Muller et al. (1996)</ref>,<ref type="bibr">(2011)</ref>presented an approach using mixtures of transformed Gaussian processes. These and other methods of Bayesian density regression have proven successful, but as datasets have grown in size and complexity, these approaches encounter difficulties. This is even more daunting when we consider interactions of discretely valued predictors because we must consider the factorial combinations of those levels. The associated challenges of variable selection and dimensionality reduction have been explored in Bayesian density regression. Dimensionality reduction has a goal similar to that of variable selection, that of finding a minimal set of predictors that account for variation in the response. The logistic Gaussian process approach of<ref type="bibr" target="#b28">Tokdar et al. (2010)</ref>includes a subspace projection method to reduce the dimension of the predictor space.<ref type="bibr" target="#b25">Reich et al. (2011)</ref>developed a technique for Bayesian sufficient dimensionality reduction based on a prior for a central subspace. Although all of these approaches have demonstrated their utility, they do not scale easily beyond p ¼ 30 predictors. There are also techniques like the random forest (<ref type="bibr" target="#b1">Breiman, 2001</ref>) that aim to find parsimonious models for density estimation involving a large number of predictors. One disadvantage to this type of 'black box' method is in interpreting the impact of specific predictors on the response. Bayesian additive regression trees (BART) (<ref type="bibr" target="#b2">Chipman et al., 2006</ref><ref type="bibr" target="#b3">Chipman et al., , 2010</ref>) focus on modeling the conditional mean and assume a common residual distribution. As previously noted, there are many questions that require learning about more than just the conditional mean of the response. Another flexible approach is the Bayes network (BN), which considers the predictors and the response on equal footing to develop a parsimonious network linking all variables (<ref type="bibr" target="#b23">Pearl, 1988;</ref><ref type="bibr" target="#b6">Cowell et al., 1999;</ref><ref type="bibr" target="#b18">Lauritzen, 1992</ref>). The conditional distribution of the response given the predictors can be derived from such a model, using developed BN techniques for mixed continuous and discrete data (<ref type="bibr" target="#b18">Lauritzen, 1992;</ref><ref type="bibr" target="#b20">Moral et al., 2001;</ref><ref type="bibr" target="#b17">Langseth et al., 2012</ref>). A BN does estimate a joint density for all of the predictors; the effort to estimate this very high-dimensional nuisance parameter is unattractive, if the conditional density is of primary interest. We propose an approach based on a conditional tensor factorization (CTF) for the mixing weights. As in the DDP and certain of the kernel stick-breaking methods, the predictors influence the mixing weights for this CTF model. The conditional tensor factorization facilitates borrowing of information across different profiles in a flexible representation of the unknown density. We focus our attention on situations involving continuous responses and categorical predictors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head><p>We consider a univariate response y and a vector of p categorical predictors x ¼ ðx 1 ,. .. , x p Þ, where the j th predictor x j can take values 1,. .. , d j. We would like a model that can flexibly accommodate conditional densities that change in complex ways with changes in the predictor vector. In addition, we must consider situations where p ) n; there may be no exemplars for certain predictor vectors. To address this, we propose a Tucker-style factorization with the following general model for the conditional density fðyjxÞ:</p><formula>fðyjxÞ ¼ X k1 h1¼1 Á Á Á X kp hp¼1 h1</formula><p>, ... , hp ðxÞðy; h1, ... , hp Þ where h1, ... , hp ðxÞ ¼ Y p j¼1 ðjÞ hj ðx j Þ: ð2Þ</p><p>This form uses the maps ðjÞ , j ¼ 1,. .. , p to associate the predictor vector x with a separate weight for each combination of the latent identifiers h 1 ,. .. , h p and thus with each of the k 1 Â Á Á Á Â k p kernels in the representation. The x th j row of ðjÞ is a vector of weights, one for each h j ¼ 1,. .. , k j. These weights ðjÞ 1 ðx j Þ,. .. , ðjÞ kj ðx j Þ are all in<ref type="bibr">[0,</ref><ref type="bibr">1]</ref>and P kj hj¼1 ðjÞ hj ðx j Þ ¼ 1. The number of latent predictors p, is the same as the number of observed predictors, but the form of the ðjÞ hj may mean that different predictor vectors x result in the same sets of weights 1, ... , 1 ðxÞ,. .. , k1, ... , kp ðxÞ. This provides the mechanism for dimension reduction that we will develop. An important distinction from the HME is in the treatment of the weights h1, ... , hp ðxÞ as a tensor factorization and the use of kernels ðy; h1, ... , hp Þ, which do not depend on the predictor vector x. This is similar in spirit to the classification approach proposed by Yang and Dunson, 2012, but we address the problem of estimating an infinite-dimensional conditional density rather than the finite-dimensional problem of a categorical response distribution. In addition, we make distinct improvements in predictor selection to allow the approach to scale to larger numbers of candidate predictors. Tucker decompositions (<ref type="bibr" target="#b29">Tucker, 1966</ref>) and other kinds of decompositions have appeared in the machine learning literature before.<ref type="bibr" target="#b32">Xu et al. (2012)</ref>developed an 'infinite' Tucker decomposition making use of latent Gaussian processes rather than explicit treatment of tensors and matrices; in comparison, the proposed method uses the Tucker decomposition to characterize the mapping of predictors into weights. Other factorizations have been used for similar problems;<ref type="bibr" target="#b14">Hoff (2011)</ref>presented a reducedrank approach for table data, but this approach focused on the development of estimates for the mean of a continuous response. Chu and<ref type="bibr" target="#b4">Ghahramani (2009</ref>) derive an approach for partially observed multiway data based on a Tucker decomposition; their objective is to learn about the latent factors driving observations rather than the characterization of the response distribution or variable selection. The collection across j ¼ 1,. .. , p forms a 'soft' clustering from the d 1 Â Á Á Á Â d p possible realizations of the x vector to each of the M ¼ k 1 Â Á Á Á Â k p possible latent vectors. This means that a predictor vector x is not exclusively associated with one of the M kernels, but has a weight for each kernel determined by the product in (2). This allows each observation to contribute some information about the influence of each of the p sites, and thus allows borrowing of information across different combinations of h 1 ,. .. , h p. In settings of extreme sparsity, where most of the possible predictor vectors are not represented, this is an attractive property. This uses many fewer parameters than a full factorial representation and is still flexible enough to represent complex conditional distributions. Finally, we assume normal kernels:</p><formula>fðy i jx i Þ ¼ X k1 h1¼1 Á Á Á X kp hp¼1 Nðy i ; h1, ... , hp , À1 h1, ... , hp Þ Â Y p j¼1 ðjÞ hj ðx ij Þ ( ) ð3Þ</formula><p>This resembles other mixture-based approaches to density estimation as originally specified in (1), but the proposed model for the weights provides the desired support for sparsity and information borrowing as previously discussed. In addition, the kernel-specific means h1, ... , hp and precisions h1, ... , hp are not functions of the predictor vector.<ref type="figure" target="#fig_1">Figure 1</ref>shows a conditional dependence graph for the model parameters and the observed data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Predictor selection</head><p>The first task in learning the conditional distribution is to identify those predictors that provide the most information about the response; the second task is to learn the form of the conditional distribution given this set of informative predictors. The k 1 ,. .. , k p parameters indicate the number of latent levels for each predictor. Because each k j can take on the values 1,. .. , d j , the possible combinations of different values for k 1 ,. .. , k p can be immense, and including these as parameters in an Markov chain Monte Carlo (MCMC) sampler is not an attractive option. In the notation of (3), predictors exclusion is equivalent to identifying those sites j such that k j ¼ 1. Consequently, predictor vectors that differ only at the j th position will have the same conditional density, and the j th predictor can be excluded from the model. To identify those j such that k j ¼ 1, we use a predictor selection step based on a special form of the ðjÞ. For each j ¼ 1,. .. , p and each x j ¼ 1,. .. , d j , we specify the ðjÞ so that ðjÞ hj ðx j Þ ¼ 1 for exactly one h j and ðjÞ hk ðx j Þ ¼ 0 for all h k 6 ¼ h j. This form for the ðjÞ associates each predictor vector x with exactly one of the M ¼ k 1 Â Á Á Á Â k p kernels by giving that particular kernel a weight of one. That is, if the set of maps ðjÞ , j ¼ 1,. .. , p is such that ð1Þ h1 ðx i1 Þ ¼ 1,. .. , ðpÞ hp ðx ip Þ ¼ 1 for values h 1 ,. .. , h p , then only the kernel indexed by h 1 ,. .. , h p will have non-zero weight. For computational convenience, we use conjugate priors and make the simplifying assumption that the prior precision of each kernel mean h1, ... , hp is the same as the kernel precision h1, ... , hp for each h 1 ,. .. , h p , so that h1, ... , hp j h1, ... , hp $ Nð0, À1 h1, ... , hp Þ and h1, ... , hp $ Gammað t =2, t =2Þ. Because the proposed form for ð1Þ ,. .. , ðpÞ maps each predictor vector to exactly one of the M groups, we can collect the observations that map to each of the M groups and compute a marginal likelihood for each group. Given the prior structure, the simplifying assumptions and the clusterings defined by the ð1Þ ,. .. , ðpÞ , the log marginal likelihood for the m th group is</p><formula>N m 2 logðÞ À 1 2 logðN m þ 1Þ þ log À N m þ t 2 À log À t 2 þ t 2 logð t Þ À 1 2 ðN m þ t Þ log Y T m Y m À ðY T m J Nm Þ 2 N m þ 1 þ t ,</formula><p>where Y m is the vector of responses, N m is the number of observations in group m and J Nm is a N m Â 1 vector of 1's. The sum of these M approximated log-marginal likelihoods gives a score for the particular levels of k 1 ,. .. , k p and the particular ð1Þ ,. .. , ðpÞ. Using these scores for different levels of k 1 ,. .. , k p and different hard-clustering forms of ð1Þ ,. .. , ðpÞ , we can find those predictors with influence on the conditional density. It is not generally feasible to evaluate every possible set of k 1 ,. .. , k p , even for moderately sized problems. Instead, we begin with the null model, where k 1 ¼ k 2 ¼. .. ¼ k p ¼ 1 and propose random changes to the different k j and the associated ðjÞ. The randomly proposed changes are of two types: 'split' and 'merge'. A 'split' change at position j means changing the ðjÞ map so that the distinct x ij map to more levels. For example, assume that the j th position has three observed levels (d j ¼ 3) and the current form of ðjÞ is such that all three observed levels of x j are mapped to the same level. In this case, k j ¼ 1 and ðjÞ ¼. Conversely, a 'merge' move will decrease the number of mapped levels by one; using the definitions above, one such merge move would be to replace ðjÞ Ã with ðjÞ. If site j already has k j ¼ d j , then only merge moves are considered. Likewise, if site j already has k j ¼ 1, then only split moves are considered. We use a Metropolis step to accept or reject the proposed change; the stochastic search proceeds as:</p><formula>(i) Set n j ¼ 0, k j ¼ 1; j ¼ 1,. .. , p; set ðjÞ ¼ J dj , j ¼ 1</formula><p>,. .. , p; compute the marginal likelihood ML c .</p><p>(ii) For j ¼ 1,. .. , p, draw from all possible split and merge moves with equal probability. For a split, propose k Ã j ¼ k j þ 1; for a merge, propose k Ã j ¼ k j À 1.</p><p>(iii) Compute ML Ã for the proposed configuration; accept the move with probability 1 ^ ML Ã ML c. If the new configuration is accepted, set</p><formula>k j ¼ k Ã j and ML c ¼ ML Ã ; if k j 41, set n j ¼ n j þ 1.</formula><p>(iv) After T iterations of steps 2-3, compute inclusion probabilities</p><formula>p j ¼ nj T for j ¼ 1,. .. , p.</formula><p>(v) Retain those predictors such that p j 4; using ¼ 0:5 is equivalent to choosing the median probability model. This stochastic search is similar to<ref type="bibr" target="#b11">George and McCulloch (1997</ref>). The approach we propose here is simple and appealing, but in our initial simulation studies we noticed a tendency for this search to choose overly complex models. Model selection was sensitive to the order in which the predictors were considered. When the important features were considered after many unimportant factors, randomly induced associations in the data and stochastic variation in the search led to complex models that were not improved by addition of the important predictors.To combat this tendency, we introduced a preliminary predictor identification step that considers each of the predictors in isolation. We can represent the entire stochastic search on the j th predictor with a d j Â d j discrete-time Markov transition matrix derived from the acceptance and move probabilities defined above. We can then compute the long-run proportion of time that the chain spends in states such that k j 41. This can be done in an embarrassingly parallel fashion, and the computation of each p j proceeds quickly. For the simulation case, where d j ¼ 4 for all j, computation of each p j took $0.3 s. At the conclusion of this single-predictor search step, we arrange the predictors in descending order of these p j , retaining only those predictors such that p j 4, and proceed with the full stochastic search to identify a final predictor set.</p><formula>μ h1, … , hp τ h1, … , hp π h1 (1) π h2 (2) … π hp (p) x i1 x i2 x ip y i ∏ j=1 p k j</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Estimation after predictor selection</head><p>To estimate the parameters in the model using the selected predictors, we introduce a prior precision 0 $ Gammað 0 =2, 0 =2Þ for each kernel mean h1, ... , hp $ Nð0, 0 Þ, a prior for each kernel precision h1, ... , hp $ Gammað t =2, t =2Þ and separate Dirichlet priors for each weight vector ðjÞ ðx j Þ $ Dirið 1 kj ,. .. , 1 kj Þ. To facilitate computation, we augment the model proposed in (3) with classification vectors z i that associates the i th observation with exactly one kernel and gives a complete-data likelihood that can be expressed as a product:</p><formula>Y N i¼1 Y k1 h1¼1 Á Á Á Y kp hp¼1 N y i ; h1, ... , hp , À1 h1, ... , hp Â Y p j¼1 ðjÞ hj ðx ij Þ ( ) 1½zi¼ðh1, ... , hpÞ ð4Þ</formula><p>The full conditional distributions are</p><p>(1) h1, ... , hp j Á Á Á $ Nð Ã h1, ... , hp , ð Ã h1, ... , hp Þ À1 Þ, where: Ã h1, ... , hp ¼ 0 þ h1, ... , hp P N i¼1 1½z i ¼ ðh 1 , Á Á Á , h p Þ Ã h1, ... , hp ¼ f h1, ... , hp P N i¼1 y i 1½z i ¼ ðh 1 , Á Á Á , h p Þg= Ã h1, .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.. , hp</head><p>(2) h1, ÁÁÁ, hp j Á Á Á $ Gammað Ã =2, Ã =2Þ, where:</p><formula>Ã ¼ t þ P N i¼1 1½z i ¼ ðh 1 ,. .. , h p Þ Ã ¼ t þ P N i¼1 1½z i ¼ ðh 1 ,. .. , h p Þðy i À h1, ... , hp Þ 2 (3) 0 j Á Á Á $ Gammað½ 0 þ M=2, ½ 0 þ f P k1 h1¼1 Á Á Á P kp hp¼1 2 h1, ... , hp g=2Þ (4) ð ðjÞ 1 ð'Þ,. .. , ðjÞ kj ð'ÞÞj Á Á Á $ Dirið 1 kj þ P N i¼1 1½x ij ¼ ' 1½z ij ¼ 1,. .. , 1 kj þ P N i¼1 1½x ij ¼ ' 1½z ij ¼ k j Þ for ' ¼ 1,. .. , d j and j ¼ 1,. .. , p (5) Pr½z i ¼ z Ã jm ðh 1 ,. .. , h jÀ1 , m, h jþ1 ,. .. , h p Þj Á Á Á / ½ðy i À z Ã jm Þ ffiffiffiffiffiffiffi z Ã jm p  Â ðjÞ m ðx ij Þ for m ¼ 1,. .. , k j within each j ¼ 1,. .. , p; ðÁÞ indicates the standard normal density.</formula><p>The updates for the h1, ... , hp , h1, ... , hp and ðjÞ can be done blockwise, and the z i can be updated blockwise at each position j. Using the final predictor set and the full conditionals, we produce a posterior sample for the model parameters. This posterior sample allows us to compute conditional densities and credible intervals around those estimates for various combinations of the predictors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simulation study</head><p>To assess the variable selection and prediction performance of the CTF, we conducted a simulation study, varying the number of training observations N 2 f300, 500, 1000, 1500g and using a consistent ground truth to produce simulated datasets with total number of predictors p ¼ 1000. In each case, the true model was based on three predictors at positions 30, 201 and 801, each with d j ¼ 4 levels and including three-way interactions among these predictors. The resulting marginal density is an equally weighted mixture of 64 Gaussians with different means and the same residual precision. In other words, an observation with ðx i, 30 , x i, 201 , x i, 801 Þ ¼ ð3, 2, 1Þ is drawn from Nð 3, 2, 1 , À1 Þ, and so forth for each of the 64 distinct predictor vectors. For each of 20 training sets, we produced selected predictor sets and posterior samples. We then made predictions for 20 validation sets drawn from the same underlying true distribution. As competitor methods, we used random forests (RF) and quantile regression random forests (QRF) (<ref type="bibr" target="#b19">Meinshausen, 2006</ref>); these are implemented in the randomForest and quantregForest packages in R. BART, as implemented in the BayesTree package, was unable to run to completion on any of the training sets, though we were able to use BART with the real data example in Section 4.2. RF and QRF include predictor selection directly, and QRF directly addresses the idea of coverage proportion. BART is another MCMC-based approach, but it does not directly address variable selection, allowing us to investigate the impact of the large predictor space. The implicit cost in estimating the joint distribution of predictors and response made Bayes networks unattractive. We computed mean square prediction error (MSPE) as the average squared difference between the response value predicted by the model for a predictor vector from the validation set and the actual response value for that observation. We defined coverage proportion as the proportion of times that the 95% prediction interval for an observation in the validation set included the actual response value, averaged over the intervals for each posterior sample. When comparing performance with that of the competitors, we attempted to give those competitors whatever advantages we could provide. In the case of RF, this meant that we did two passes over the training data. The first pass identified important variables using the importance method in the randomForest package. We used the 'mean decrease in accuracy' style of importance; this measurement is derived from the impact of permuting out-of-bag data for each tree in the forest. We then fed those variables identified as important as a preselected set into a second run of RF. This generally improved the MSPE performance of RF. An analogous method was not available for QRF, so we could not treat that method in the same manner. In each of the 20 cases for p ¼ 1000 and training N ¼ 500, the CTF outperformed RF on mean square prediction error and showed comparable 95% coverage proportions to those derived from QRF; this is summarized in<ref type="figure">Figure 2</ref>. The CTF and RF showed comparable accuracy in identifying important predictors, but RF tended to include many unimportant predictors. In contrast, the CTF produced no false-positive results, identifying the correct subset of predictors in each case. This performance is particularly attractive given the large number of possible interactions in the original predictor set. Both RF and QRF may have suffered because of the strong interactions present in these simulated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Molecular epidemiology application</head><p>We also consider an application to an epidemiology dataset, comparing CTF performance with that of the same competitor methods (RF, QRF and BART). The dataset concerns DNA damage to instances of different cell lines when exposed to environmental chemicals. The exposure types are hydrogen peroxide (H 2 O 2 ) and methyl methane sulfonate (MMS), and the remainder of the predictor set is genotype information on 49 428 single nucleotide polymorphisms (SNPs).<ref type="bibr" target="#b26">Rodriguez et al. (2009)</ref>provide extensive details on the original experiments. One hundred separate instances of each of 90 cell lines were exposed to each chemical and examined at each of three time points (before treatment, immediately after treatment and a longer time after treatment). The nature of the measurement is destructive; at the desired time interval, comet assay was performed on each cell and the Olive tail moment (OTM) (<ref type="bibr" target="#b22">Olive et al., 1991</ref>) was recorded; this assesses the amount of DNA damage in the cell, with higher measurements indicating more damage. The cells from each line are genetically identical, but the resulting distribution of OTM has a different shape for each cell line. In addition, these distributions are different at the separate time points; generally, OTMs are smallest (least damage) before exposure to the chemical, largest (most damage) immediately after exposure and somewhere in-between after a longer recovery time. We computed empirical quantiles of the OTM for each cell line at each of the three time points and then derived a singlenumber summary w ij to tie these three quantile vectors together for cell line i and exposure j. The summary measure w ij 2 ð0, 1Þ is the value that minimizes</p><formula>X 31 h¼17 jw ij Q ij, N, h þ ð1 À w ij ÞQ ij, L, h À Q ij, A, h j ð 5Þ</formula><p>Here, Q ij, N, h indicates the h=32 th quantile for the i th cell line's OTM distribution at the 'No treatment' time, with corresponding quantities for the 'Later' time point and the 'immediately After' time point for the j th exposure. The use of only the higher quantiles reflects our desire to learn more about the extremes of DNA repair. We used a logit transform to derive our final response y ij ¼ logð wij 1Àwij Þ; this is appropriate for the assumptions of the model. Negative values of the response indicate that the OTM distribution long after treatment is closer to the distribution right after treatment; positive values indicate that the 'long after' distribution is closer to the distribution before treatment. SNPs in genes thought to be associated with some aspect of DNA repair were genotyped, leading to data on 49 428 individual SNPs. Given the small number of cell lines and the fact that many individuals have two copies of the major allele for these SNPs, many of the SNP profiles were identical or had no individuals with two copies of the minor allele. We recoded the genotypes so that one indicated at most one copy of the major allele and two indicated two copies of the major allele. After recoding, we reduced the predictor set to those SNPs with distinct profiles, leaving 23 210 SNPs for analysis. We used leave-one-out cross-validation to assess the performance of CTF against the three competitors RF, QRF and BART. We ran the variable selection chain for 5000 burn-in iterations and computed inclusion probabilities from 10 000 samples. We ran the MCMC chain for 40 000 burn-in iterations and retained a sample of 20 000 iterations. Autocorrelation diagnostics indicated an effective sample size of 15 000. We used the same burn-in and posterior sample sizes for BART. As in the simulation study, we used the results from a first run of RF to seed a final run of RF. CTF showed consistent selection of the treatment (H 2 O 2 or MMS) as the most important predictor and selected a set of four SNPs (IGFBP5, TGFBR3, CHC1L and XPA) as predictors; information about these SNPS is summarized in<ref type="figure" target="#tab_1">Table 1</ref>. In contrast, RF chose the treatment variable in only 56 of the 180 crossvalidation scenarios and did not consistently identify any other predictors. Comparison with the competitor methods showed patterns similar to the simulation study;<ref type="figure" target="#tab_2">Table 2</ref>compares the results from each method. The interactions between the treatment and the various SNPs may be weak enough that they do not contribute to the same elevated MSPE that RF demonstrated in the simulation study. Even though the MSPE for RF was close to that for the CTF, the CTF was able to achieve lower MSPE while not sacrificing coverage performance. This improved performance offsets the CTF's higher computational time requirement.<ref type="figure" target="#fig_3">Figure 3</ref>shows estimated conditional densities with 95% credible intervals from the full dataset given varying levels of the treatment and of the IGFBP5 SNP while holding the other three SNPs atthe 'Zero/One Copy' level, and illustrates how the conditional density changes in more than the conditional mean when the predictor vector changes. In this case, the interaction between MMS treatment and two copies of the major allele for this IGFBP5 SNP tightens the density markedly, although it has a more muted impact on the conditional mean. The change is less dramatic under the exposure to H 2 O 2. Here, the shift in the mean response as treatment and genetic profile change is less interesting than the difference in conditional variance; under treatment with H 2 O 2 , the mean response is slightly different than under treatment with MMS, but the tail probabilities are noticeably different.<ref type="figure" target="#tab_3">Table 3</ref>summarizes these differences in conditional mean, conditional variance and conditional 90 th percentile for each scenario. As suggested in<ref type="figure" target="#fig_3">Figure 3</ref>, the medians of the conditional densities given the exposure (H 2 O 2 or MMS) are close, but in the tail of the distribution (the 90th percentile), the separation between the estimated quantile curves is larger. This varying shift in the 90th percentile reflects the interaction between the exposure and the level of the IGFBP5 SNP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have presented a novel method for flexible conditional density regression in the common case of a continuous response and categorical predictors. The simulation study and real data example suggest that this conditional tensor factorization method can have better performance than other modeling tools when there is substantial interaction between the predictors of interest. The CTF does have a higher computational time requirement than the competitor methods, but the improvement in prediction accuracy and coverage still make the CTF an attractive method. A particularly appealing aspect of the CTF is predictor selection, which finds low-dimensional structure in the high-dimensional predictor set. This reduction to more parsimonious models yields a succinct description of the ways in which the phenotype varies given exposure and SNPs. Finally, a distinct advantage of the CTF is its ability to produce conditional density estimates. This property of the CTF provides insight beyond a simple conditional expectation and makes it possible to answer more complex questions about the relationship between the response and the predictors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.1.</head><figDesc>Fig. 1. Conditional dependence graph showing the relationship between the model parameters and the observed data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><figDesc>Funding: This research was supported in part by the Intramural Research Program of the NIH, National Institute of Environmental Health Sciences, Z01 ES049032. David Kessler's work was partially supported by National Institute of Environmental Health Sciences training grant T32ES007018. David Dunson's work was supported by Award Numbers R01ES017240 and R01ES017436 from the National Institute of Environmental Health Sciences. Conflict of Interest: none declared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.3.</head><figDesc>Fig. 3. Selected conditional densities given different exposures and different number of copies of the dominant allele at the IGBP5 SNP, holding all other SNPs at the 0/1 level. Heavy black lines show the mean conditional density and gray lines show the 95% credible interval</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>Table 1.</figDesc><table>Details for SNPs included in the final CTF model for the mo-
lecular epidemiology data 

Gene 
SNP 
Position 

IGFBP5 
RS11575170 
217256085 
TGFBR3 
RS17880594 
92118885 
CHC1L 
RS9331997 
47986441 
XPA 
RS3176745 
99478631 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>Table 2.</figDesc><table>Comparison of MSPE, 95% coverage proportion and mean 
computation time for different methods applied to molecular epidemi-
ology data 

Metric 
CTF 
RF 
QRF 
BART 

MSPE 
0.263 
0.353 
– 
0.425 
95% Coverage 
0.961 
– 
0.928 
0.817 
Time (s) 
3317 
80 
88 
2343 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><figDesc>Table 3. Summary of conditional distribution characteristics</figDesc><table>Profile 
Mean 
Variance 
90 th %ILE 

H2O2, IGFBP5 ¼ 0/1 
0.226 
11.39 
2.65 
H2O2, IGFBP5 ¼ 2 
0.156 
7.31 
2.25 
MMS, IGFBP5 ¼ 0/1 
0.023 
9.76 
2.07 
MMS, IGFBP5 ¼ 2 
À0.023 
6.11 
1.88 </table></figure>

			<note place="foot">ß The Author 2014. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">Learning phenotype densities with the CTF at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">D.C.Kessler et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian hierarchical mixtures of experts</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Bishop</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Svense´nsvense´n</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Nineteenth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Breiman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Bayesian ensemble learning</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Chipman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">BART: Bayesian additive regression trees</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Chipman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="266" to="298" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Probabilistic models for incomplete multidimensional arrays</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Chu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Ghahramani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the 12th International Conference on Artificial Intelligence and Statistics (AISTATS)<address><addrLine>Clearwater Beach, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Nonparametric Bayes conditional distribution modeling with variable selection</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Chung</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Dunson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="1646" to="1660" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<monogr>
		<title level="m" type="main">Probabilistic Networks and Expert Systems</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Cowell</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Springer</publisher>
			<pubPlace>New York, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">An ANOVA model for dependent random measures</title>
		<author>
			<persName>
				<forename type="first">De</forename>
				<surname>Iorio</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="205" to="215" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">P</forename>
				<surname>Dempster</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Society B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Kernel stick-breaking processes</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Dunson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Park</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="307" to="323" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Nonparametric Bayes modeling of multivariate categorical data</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Dunson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Xing</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc. (Theory and Methods)</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="1042" to="1051" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Approaches for Bayesian variable selection</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>George</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Mcculloch</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Sin</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="339" to="373" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Order-based dependent Dirichlet processes</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Griffin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Steel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="179" to="194" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Dirichlet process mixtures of generalized linear models</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Hannah</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1923" to="1953" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical multilinear models for multiway data</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Hoff</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Stat. Data Anal</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="530" to="543" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">A class of mixtures of dependent tail-free processes</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Jara</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hanson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="553" to="566" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical mixtures of experts and the EM algorithm</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Jordan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Jacobs</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="181" to="214" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Inference in hybrid Bayesian networks with mixtures of truncated basis functions</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Langseth</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth European Workshop on Probabilistic Graphical Models</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Propagation of probabilities, means, and variances in mixed graphical association models</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Lauritzen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1098" to="1108" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Quantile regression forests</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Meinshausen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="983" to="999" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Mixtures of truncated exponentials in hybrid Bayesian networks</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Moral</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth European Conference on Symbolic and Quantitative Approaches to Reasoning Under Uncertainty</title>
		<meeting>the Sixth European Conference on Symbolic and Quantitative Approaches to Reasoning Under Uncertainty</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Bayesian curve fitting using multivariate normal mixtures</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Muller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="67" to="79" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">DNA double-strand breaks measured in individual cells subjected to gel electrophoresis</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Olive</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Res</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="4671" to="4676" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Pearl</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Francisco, California, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">A multivariate semiparametric Bayesian spatial modeling framework for hurricane surface wind fields</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Reich</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Fuentes</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="249" to="264" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Sufficient dimension reduction via Bayesian mixture modeling</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Reich</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="886" to="895" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Bayesian hierarchically weighted finite mixture models for samples of distributions</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Rodriguez</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="155" to="171" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Nonlinear models using Dirichlet process mixtures</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Shahbaba</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Neal</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1829" to="1850" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Bayesian density regression with logistic Gaussian process and subspace projection</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Tokdar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Anal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="319" to="344" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Some mathematical notes on 3-mode factor analysis</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Tucker</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="page" from="31" to="279" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Bayesian methods for mixtures of experts</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Waterhouse</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="351" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<monogr>
		<title level="m" type="main">Bayesian Conditional Tensor Factorizations for High-Dimensional Classification</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Yang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">B</forename>
				<surname>Dunson</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>Durham, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Infinite Tucker decomposition: nonparametric Bayesian models for multiway data analysis</title>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Xu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning<address><addrLine>Princeton, New Jersey, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>