
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining An ensemble correlation-based gene selection algorithm for cancer classification with gene expression data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012">. 24 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Yongjun</forename>
								<surname>Piao</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Chungbuk National University</orgName>
								<address>
									<settlement>Chungbuk</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Minghao</forename>
								<surname>Piao</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Kiejung</forename>
								<surname>Park</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Chungbuk National University</orgName>
								<address>
									<settlement>Chungbuk</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Division of Bio-Medical informatics</orgName>
								<orgName type="department" key="dep2">Center for Genome Science</orgName>
								<orgName type="institution">Korea National Institute of Health</orgName>
								<address>
									<settlement>Osong</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Keun</forename>
								<surname>Ho</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Chungbuk National University</orgName>
								<address>
									<settlement>Chungbuk</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Ryu</forename>
							</persName>
						</author>
						<title level="a" type="main">Data and text mining An ensemble correlation-based gene selection algorithm for cancer classification with gene expression data</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">28</biblScope>
							<biblScope unit="page" from="3306" to="3315"/>
							<date type="published" when="2012">. 24 2012</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/bts602</idno>
					<note type="submission">Received on May 29, 2012; revised on September 18, 2012; accepted on October 2, 2012</note>
					<note>BIOINFORMATICS ORIGINAL PAPER Associate Editor: Jonathan Wren Availability: By request from the author.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Gene selection for cancer classification is one of the most important topics in the biomedical field. However, microarray data pose a severe challenge for computational techniques. We need dimension reduction techniques that identify a small set of genes to achieve better learning performance. From the perspective of machine learning, the selection of genes can be considered to be a feature selection problem that aims to find a small subset of features that has the most discriminative information for the target. Results: In this article, we proposed an Ensemble Correlation-Based Gene Selection algorithm based on symmetrical uncertainty and Support Vector Machine. In our method, symmetrical uncertainty was used to analyze the relevance of the genes, the different starting points of the relevant subset were used to generate the gene subsets and the Support Vector Machine was used as an evaluation criterion of the wrapper. The efficiency and effectiveness of our method were demonstrated through comparisons with other feature selection techniques , and the results show that our method outperformed other methods published in the literature.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recently, there has been increasing interests in changing the emphasis of cancer classification from morphologic to molecular (<ref type="bibr" target="#b43">Xiong et al., 2001</ref>). Cancers are usually marked by a change in the expression levels of certain genes; thus, the selection of relevant genes for cancer classification is an important task in most cancer gene expression studies. These discriminative genes are very useful in clinical applications, such as in recognizing disease profiles (<ref type="bibr" target="#b44">Yang et al., 2006</ref>). However, microarray data pose a severe computational challenge because of its high dimensionality and small sample size (<ref type="bibr" target="#b49">Saeys et al., 2007</ref>). From the perspective of machine learning, the selection of genes is a feature selection problem that aims to find a small subset of features with the most discriminative information for the target. Feature selection is an important pre-processing step in eliminating irrelevant and redundant features for classification. The growing dimensionality of recorded data demands dimension reduction techniques that identify small sets of features that lead to better learning performance. The objective of feature selection is to provide faster and more effective models, and also to avoid overfitting and the curse of dimensionality. Feature selection methods can be broadly categorized into three types: filter, wrapper and hybrid (<ref type="bibr" target="#b34">Pok et al., 2010;</ref><ref type="bibr" target="#b39">Talavera, 2005</ref>). The filter methods use specific evaluation criteria that are independent of a learning algorithm to identify a feature subset from the original feature set. Filter techniques (<ref type="bibr" target="#b30">Liu and Setiono, 1996;</ref><ref type="bibr" target="#b29">Liu et al., 2002</ref>) are fast and scale easily to high-dimensional datasets, but they ignore interaction with the classifier. Wrapper methods (<ref type="bibr" target="#b23">Kim et al., 2000;</ref><ref type="bibr" target="#b24">Kohavi and John, 1997</ref>) use the classifier to evaluate the performance of each subset with a search algorithm. Wrapper methods tend to find the most suitable feature subset for the learning algorithm, but they are very computationally expensive. Hybrid methods (<ref type="bibr" target="#b22">Kannan et al., 2010;</ref><ref type="bibr" target="#b42">Xie and Wang, 2011</ref>) combine the advantages of filter and wrapper techniques. These algorithms aim to achieve the best learning performance with a predetermined learning algorithm and a similar time complexity to filter algorithms (<ref type="bibr" target="#b47">Yu and Liu, 2003</ref>). A feature selection procedure can usually be divided into two steps: subset generation and subset evaluation (<ref type="bibr" target="#b31">Liu and Yu, 2005</ref>). The most important issue in generating a feature subset is how to choose the search strategy and the starting point. Complete search, sequential search and random search are the typical search methods used for subset generation. Complete search methods consider every feature subset to be a potential candidate to guarantee finding the optimal result. However, the computational time is intractable when the dimensionality is high. Sequential search methods, such as Sequential Forward Selection (SFS) and Sequential Backward Selection (SBS), sacrifice completeness by applying the greedy hill-climbing approach (<ref type="bibr" target="#b16">Han and Fu, 1996</ref>), which adds or removes features one at a time. These algorithms are computationally simpler and faster than a complete search strategy, but they can still lead to local optima. Random search methods, such as random-start hill-climbing and simulated annealing (<ref type="bibr" target="#b10">Doak, 1992</ref>), start with a randomly selected subset, and these algorithms help to escape local optima in the search space. During the past few years, the Support Vector Machine (SVM) has become very popular because of its good performance on high-dimensional data. SVM was developed by<ref type="bibr" target="#b40">Vapnik (1995)</ref>to successfully solve the problems of handwritten digit recognition (<ref type="bibr" target="#b1">Adankon and Cheriet, 2009</ref>), object recognition (<ref type="bibr" target="#b17">Hanson and Halchenko, 2008</ref>), text classification (<ref type="bibr" target="#b50">Zaghloul et al., 2009</ref>), cancer diagnosis (<ref type="bibr" target="#b2">Akay, 2009</ref>) and bioinformatics (<ref type="bibr" target="#b51">Zhang et al., 2009</ref>).In this article, we proposed a hybrid feature selection algorithm named Ensemble Correlation-Based Gene Selection (ECBGS) based on symmetrical uncertainty (SU) and SVM for gene selection. Our proposed method combined a filter approach and a wrapper method to remove the redundant features and to find the relevant features from the original feature set. For the original feature set, SU was used as an evaluation criterion for the filter, using the different starting points as a subset generation strategy and SVM as the evaluation learning algorithm of a wrapper. It was observed that the classifier combined with our proposed feature selection method obtained promising classification accuracy with a small gene subset on six gene expression datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head><p>The hybrid model (<ref type="bibr" target="#b7">Deisy et al., 2007</ref>) attempts to combine the advantages of both filters and wrappers by exploiting their different evaluation criteria in different search strategies. Hybrid models have the advantage of including the interaction with a classification algorithm, while at the same time being far less computationally intensive than wrapper methods. Many hybrid feature selection algorithms have been proposed in the past few years. However, many search algorithms, such as SFS and SBS, ignore feature redundancy during the feature subset generation procedure. Along with irrelevant features, redundant features also affect the speed and accuracy of the classifiers (<ref type="bibr" target="#b47">Yu and Liu, 2003</ref>). Furthermore, many hybrid methods are still computationally expensive. Based on these observations, the proposed feature selection method uses SU as the evaluation measure in the filter step to select relevant genes. To use the SU value, all the features need to be discretized. Then, different genes are used as the starting point to generate multiple gene subsets, and the generated subsets are evaluated by the SVM. Finally, we use the best gene subset to train the SVM model. The outline of the classification procedure with ECBGS is shown in<ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Fast correlation-based filter</head><p>There are many filter approaches that have been developed, such as the chi-squared test, mutual information, Pearson correlation coefficients (<ref type="bibr" target="#b26">Li et al., 2011</ref>), Information gain, the Gain ratio and Relief (<ref type="bibr" target="#b13">Gheyas and Smith, 2010</ref>). These methods are fast but lack robustness against interactions among features.<ref type="bibr" target="#b48">Yu and Liu (2004)</ref>proposed a Fast Correlation-Based Filter (FCBF) approach to remove the redundant and irrelevant features. SU was used to measure the correlation: IGðXjYÞ ¼ HðXÞ À HðXjYÞ ð 1Þ SUðX, YÞ ¼ 2 Ã IGðXjYÞ=ðHðXÞ þ HðYÞÞ ð2Þ where IGðXjYÞ is the information gain of X after observing variable Y. HðXÞ and HðYÞ are the entropy of variables X and Y. Using SU as the correlation measure, the feature selection procedure can be done by considering the C-correlation and F-correlation.</p><p>Definition 1 (C-correlation): The correlation between any feature F i and the class C is called C-correlation, denoted by SU i,c .</p><p>Definition 2 (F-correlation): The correlation between any pair of features F i and F j (i 6 ¼ j) is called F-correlation, denoted by SU i,j. FCBF removes irrelevant features by ranking C-correlation. Redundant features could be defined using predominant features and the approximate Markov Blanket. A feature is predominant if it does not have any approximate Markov Blanket in the current set. For two relevant features, F i and F j (i6 ¼j), F j forms an approximate Markov Blanket for F i if SU j, c ! SU i, c and SU i, j ! SU i, c ð3Þ where SU i, c is the correlation between the feature and the class, and SU i, j is the correlation between feature i and feature j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ECBGS</head><p>In general, FCBF identifies a single feature subset for which the discriminative capability is limited for classification purposes (<ref type="bibr" target="#b28">Liu et al., 2010</ref>). To obtain multiple feature subsets, we use different starting points to remove redundant features in the search procedure, which allow each subset to have its own information and to avoid being trapped in local optima. We call the algorithm ECBGS, and the pseudocode of our algorithm is shown in<ref type="figure" target="#fig_1">Figure 2</ref>. As in<ref type="figure" target="#fig_1">Figure 2</ref>, given a dataset D that contains N features (F 1 , F 2 ,. .. , F N ) and a class C, the algorithm seeks several feature subsets, whereas each partial set does not include any redundant features. In the first part (lines 2–7), all of the features are sorted in descending order according to the SU i, c value, which is the correlation between the i-th feature and the class. A feature with a higher SU value indicates higher discrimination of this feature compared with other categories, and means that the feature contains useful information for classification. After calculating the SU values for all of the features, a threshold for the results is established. If the SU value of a feature is higher than the threshold, the feature is selected; if not, the feature is not selected. D rel is the selected relevant subset of the original features. In the second part (lines 9–32), a number of feature subsets are derived by splitting the redundant features in the relevant feature subset into several parts [Subset(1), Subset(2),. .. , Subset(i)]. During the redundancy analysis, if we remove the redundant features as FCBF does, the selected feature subset cannot guarantee the best prediction for the classification problem. This is because the features that are highly correlated with the class are also highly correlated with each other such that the removed feature subset in FCBF can lead to a more accurate result. Therefore, in our method, we choose the first element of theis evaluated by SVM, and the subset with the best classification accuracy will be selected for the final input of the<ref type="figure">Fig. 1</ref>. Procedure of the classification model with ECBGS classification (line 33). If two subsets have identical classification accuracy, the smaller subset will be the final input. Here, we illustrate the subset generation step of ECBGS using a heart disease dataset that is available in the UCI (University of California at Irvine) machine learning repository. The dataset consists of 13 features denoted as F 1 , F 2 ,. .. , F 13 and 294 instances with five classes. First, the algorithm sorts all of the features in descending order based on SU values between the feature and class.<ref type="figure" target="#tab_1">Table 1</ref>shows the SU value of each feature. Then high-ranking features with SU values that are40 are selected. Here, 0 is the predefined threshold. As a result, F 11 , F 9 , F 3 , F 10 , F 8 and F 2 are selected as the relevant subset. Next, we perform redundancy analysis for this relevant subset. Starting with the first element in the relevant subset, F 11 , we calculate the SU value between F 11 and the other features. If the SU value between a feature and F 11 is greater than the SU value between this feature and class, it will be moved into the removed subset; if not, it will remain in the current subset. As shown in<ref type="figure" target="#tab_2">Table 2</ref>, the SU value between F 11 and F 9 is 0.427, which is larger than the SU value between F 9 and the class. Thus, the feature F 9 is considered to be redundant and moved into the removed subset along with F 10 and F 8. Next, we choose the next element of F 11 in the relevant subset. The next element of F 11 is F 3 because the feature F 9 was already moved into the removed subset. We calculate the SU value between F 3 and the other features. From<ref type="figure" target="#tab_2">Table 2</ref>, we can easily see that there are no features that are redundant with F 3 such that no features will be removed, and the same scenario occurs for F 2. Consequently, subset {F 11 , F 3 , F 2 } is the subset generated by the first iteration of our algorithm, and subset {F 9 , F 10 , F 8 } is the removed subset in the first iteration. In the second iteration, we use the first feature in the removed subset, F 9, as the starting point to remove redundant features. In addition, we do not need to analyze redundancy for the whole relevant subset because the removed features are eliminated by the features that have rankings higher than the starting point during the previous iteration. Thus, we only analyze the redundancy of the features that have rankings lower than the starting point. In this example, subset {F 9 , F 3 , F 10 , F 8 , F 2 } is the analyzed subset in the second iteration. As shown in<ref type="figure" target="#tab_3">Table 3</ref></p><formula>, F 3 , F 2 }, {F 9 , F 2 }, {F 3 , F 10 , F 2 } and {F 8 , F 2 }</formula><p>. Finally, these four subsets are evaluated by SVM, and the subset that has the highest classification accuracy is selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>To evaluate the effectiveness of our method, we used six publicly available datasets. The datasets share common characteristics, such as a very low sample/dimension ratio.</p><p>(1) The Breast_B dataset comprises 49 samples and 1213 genes. Primary breast tumors from the Duke Breast Cancer SPORE frozen tissue bank were selected. Tumors were either positive or negative for both estrogen and progesterone receptors.</p><p>(2) The Central Nervous System (CNS) dataset (<ref type="bibr" target="#b33">Pomeroy et al., 2002</ref>) is derived from patient samples of embryonal tumors of the central nervous system. The dataset contains 60 samples, including 39 medulloblastoma survivors and 21 treatment failures, with expression profiles of 7129 genes.(3) The Leukemia dataset (<ref type="bibr" target="#b14">Golub et al., 1999</ref>) was produced in a study that was aimed at building a model to discriminate between acute myeloid leukemia and acute lymphoma leukemia tissues. Gene expression profiles have been constructed from 72 people who have either acute lymphoblastic leukemia or acute myeloid leukemia, and each sample is composed of 7129 gene expression profiles.</p><p>(4) The Lymphoma dataset (<ref type="bibr" target="#b3">Alizadeh et al., 2000</ref>) comes from a study on diffuse large B-cell lymphoma. The dataset consists of 62 samples and 4026 genes. There are three types of samples in the dataset, with 42 samples of diffuse large B-cell lymphoma, nine observations of follicular lymphoma and 11 cases of chronic lymphocytic leukemia.</p><p>(5) The MLL_leukemia dataset (<ref type="bibr" target="#b4">Armstrong et al., 2002</ref>) contains three types of leukemia samples compared with the binary-class leukemia dataset. This dataset contains a total of 72 samples in three classes, acute lymphoblastic leukemia, acute myeloid leukemia and mixed-lineage leukemia gene (MLL), which have 24, 28 and 20 samples, respectively. The number of genes is 12 582.</p><p>(6) The prostate dataset was first published by<ref type="bibr" target="#b35">Singh et al. (2002)</ref>; it is a two-class classification problem and contains 102 samples and 12 600 genes. One of the tasks addressed by the authors is to build a model that can discriminate between normal prostate and tumorous prostate tissue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parameter settings for the SVM</head><p>Selecting the kernel and appropriate parameters plays an important role in SVM classification performance. The radial basis function (RBF) kernel is a commonly used kernel for three reasons (<ref type="bibr" target="#b19">Hsu et al., 2010</ref>). First, the RBF kernel can handle nonlinear relationships between class labels and attributes. Second, it has fewer hyperparameters that influence the complexity of the model selection than the Polynomial kernel. Third, the RBF kernel has fewer numerical difficulties. The RBF kernel function is here:</p><formula>Kðx, x 0 Þ ¼ expðÀjjx À x 0 jj 2 = 2 Þ ð 4Þ</formula><p>In our experiments, we chose the RBF kernel function, and the parameters C and must be optimized for the RBF kernel for each dataset. To determine the best values of C and , we conducted a grid-search approach using 10-fold cross validation. A number of pairs of (C,&#x0D;) values were attempted, and the pair with the best accuracy was picked in the range of C 2 f2 À5 , 2 À3 , :::, 2 15 g and 2 f2 À15 , 2 À13 , :::, 2 3 g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Performance evaluation</head><p>To obtain a statistically reliable predictive measurement, we performed 10-fold cross validation on all the datasets. In 10-fold cross validation, each dataset was randomly partitioned into 10 parts. Nine parts were used as the training set, and the remaining one was used as the testing dataset. In cancer classification, it is important to assess both false-positive and false-negative errors, as these two types of errors usually have different consequences (<ref type="bibr" target="#b32">Ma and Huang, 2005</ref>). Therefore, we have used several measures to evaluate the effectiveness of our method:where true positives (TP) denote the correct classifications of positive examples, true negatives (TN) are the correct classification of negative examples, false positives (FP) denote the incorrect classification of negative examples into the positive class and false negatives (FN) represent the incorrect classification of positive examples into the negative class. In addition, some datasets such as Breast_B, Lymphoma and MLL_Leukemia have the multi-class problem, which refers to the input data being divided into more than two categories. To solve this problem, we used the one-against-one approach that constructs N(N-1)/2 binary classifiers for an N class dataset. The posterior probabilities provided by individual binary classifiers were combined using the pairwise coupling method (<ref type="bibr" target="#b18">Hastie and Tibshirani, 1998</ref>). As mentioned previously, the FCBF method cannot always identify the best feature subset for high-dimensional data, which was demonstrated by comparing the prediction accuracy of the FCBF feature selection algorithm with our method.<ref type="figure" target="#tab_4">Table 4</ref>exhibits the classification accuracy of the subsets that are selected from each iteration of our method on six datasets with a 60–40% training-test partition, and the best results in each row are shown in bold letters. The subset selected from the second iteration of our method is denoted as Set 2, the subset selected from the third iteration is denoted as Set 3 and so on. Because our method uses the same starting point as the FCBF initially, the subset selected by the first iteration of our method will be the same as the subset selected using FCBF. As shown in<ref type="figure" target="#tab_4">Table 4</ref>, although the FCBF algorithm has the same accuracy as other subsets on the MLL_Leukemia datasets, it has a lower accuracy than other subsets in most cases, which proves that our method outperforms the FCBF. Tables 5–10 summarize the results of the classification achieved by ECBGS on different values of the relevance threshold. The performance of the classifier was evaluated by precision, TP rate, FP rate and AUC. For each dataset, the number of selected genes is listed as the '#Genes' row. With the increasing of threshold, there are no genes to be selected in some datasets. In this case, we cannot measure the performance of the classifier, as there are no inputs for the classifier.The training-test data partition is 60–40%, and each subset was evaluated using SVM. Set 2, Set 3,. .. , Set 7 are the feature subsets generated in the second, third,. .. , seventh iteration of our algorithm, respectively. The bold values indicate the highest classification accuracy.The third row shows the number of selected genes corresponding to the relevance threshold. and MLL_Leukemia dataset, respectively). It is reasonable because the genes in the Lymphoma and MLL_Leukemia dataset are more relevant than others. Moreover, the number of relevant genes in the Lymphoma and MLL_Leukemia dataset is much bigger than other datasets. In addition, we compared the accuracies of SVM using the features selected by the Gain Ratio (GR), Information Gain (IG), ReliefF and our method. Furthermore, we used the SFS and SBS search strategies in the subset generation step for these three feature selection algorithms. These classification accuracies were obtained through 10-fold cross validation.<ref type="figure" target="#tab_1">Table 11</ref>shows classification accuracy of four feature selection algorithms on six datasets. In the majority of the datasets, the accuracy is higher than other methods, and the classifier with our proposed feature selection algorithm is found to result in the best prediction average accuracy, which was 95.71%. The other methods were found to be 89.97, 91.89, 89.54, 93.89, 89.46 and 92.66%. To catch the detailed characteristics of ECBGS, we also made the comparison of accuracy obtained from nine different partitions of training and test datasets.<ref type="figure" target="#fig_3">Figure 3</ref>presents the classification accuracy on different sizes of training and test data. From<ref type="figure" target="#fig_3">Figure 3</ref>, one can easily observe that the prediction performance of the classification model constructed from the feature subset that is generated using our method is better than other models in most cases. Moreover, even when the training data set is small, our method shows high prediction accuracy, whereas other methods primarily make poor predictions. It is also obvious that, at least for one training-test partition, the classification accuracy of our method is 100% on all of the tested datasets. For example, the classification accuracy is 100% when the training-test partition is 70–30 or 85– 15 on the Breast_B dataset (<ref type="figure" target="#fig_3">Fig. 3a</ref>), as well as when the training-test partition is 80–20 on the prostate dataset (<ref type="figure" target="#fig_3">Fig. 3f</ref>).<ref type="figure" target="#tab_1">Table 12</ref>shows the running time for each feature selection algorithm. For each method, the parameter of the filter part isThe third row shows the number of selected genes corresponding to the relevance threshold. set to 0 throughout the experiments. From<ref type="figure" target="#tab_1">Table 12</ref>, it is clear that ECBGS is significantly faster than the other three algorithms. Moreover, the running time of these three algorithms is extremely expensive because the number of selected features in the filter part is 42000 on the lymphoma, MLL_Leukemia and prostate datasets. This result verifies that ECBGS is suitable for high-dimensional microarray data analysis, saving a significant amount of time. In addition to these feature selection algorithms, we can also make a comparison with the results of other methods published in the literature.<ref type="figure" target="#tab_1">Table 13</ref>presents the best classification accuracy of other methods. From Table 13, we can see that the classification accuracy achieved by our method is higher than other methods and the number of selected genes is significantly smaller than other methods except on the Breast_B dataset. Comparing the results between the Breast_B and Prostate dataset, it is found that greater amount of genes and samples can result in smaller models than minor amount of genes and samples. This finding is not surprising, as our method is based on the information theory of entropy. Thus, for the dataset that has larger number of samples, the importance of the genes is easier to be captured. Recently, much research has been performed on analyzing gene expression data for cancer classification using various gene selection methods. The Lymphoma dataset has been cross validated by many authors. Dettling and Bu¨hlmann<ref type="bibr" target="#b9">Bu¨hlmann (2003)</ref>modified the boosting classifiers and applied Wilcoxon's two sample tests to select discriminative genes; comparing the results between our study and their proposed method, our best performing results are better than their results using about the same number of genes.<ref type="bibr" target="#b28">Liu et al. (2010)</ref>proposed an ensemble gene selection method to analyze the gene expression data. The classification results for the Lymphoma dataset are identical to ours. However, the performance on the CNS and Prostate dataset are not better than ours (though they used leave one out cross validation). Moreover, our method returns a much smaller set of genes on these three datasets, and our method does not need to predefine the number of genes that will be selected. For the Prostate dataset, Dıáz<ref type="bibr" target="#b8">Uriarteb et al. (2006)</ref>reported 0.061 error rates with 18 genes;<ref type="bibr" target="#b44">Yang et al. (2006)</ref>proposed two gene selection methods which were not affected by the unbalanced sample class sizes. Their results with K-nearest-neighbor and SVM for the MLL_Leukemia and Prostate dataset show lower performance and they used larger set of genes (56 for the MLL_Leukemia and 8 for the Prostate dataset). The Breast_B dataset was firstly analyzed by<ref type="bibr" target="#b41">West et al. (2001)</ref>using Bayesian approach. Dettling and Bu¨hlmannBu¨hlmann (2002) also used Breast_B dataset and reported 4.08% of test error rates with 10 genes, which was better than<ref type="bibr" target="#b41">West et al. (2001)</ref>. In comparison with our method, although their method used a smaller subset of genes, our method made prediction more accurately. Finally,<ref type="bibr" target="#b20">Hsu et al. (2011)</ref>introduced a hybrid feature selection method based on Information Gain and F-score. They achieved 98.6% of accuracy using SVM with 70 features on the Leukemia dataset. However, this method seems to be difficult to be used for gene selection because they select too much genes. Therefore, our proposed method is an ideal candidate for gene selection in cancer classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head><p>In this study, we used SVM classification method to analyze the gene expression data. A lot of research has been shown that SVM is the most effective classifier in performing accurate cancer diagnosis from gene expression data (<ref type="bibr" target="#b36">Statnikov et al., 2005;</ref><ref type="bibr" target="#b37">Statnikov et al., 2008</ref>). SVM is interesting (<ref type="bibr" target="#b0">Abeel et al., 2010</ref>) because the number of parameters to be estimated essentially depends on the number of samples rather than on the number of features, which is particularly relevant with very small sample-to-feature ratios. Moreover, SVM has many mathematical features that make them attractive for gene expression analysis (<ref type="bibr" target="#b12">George and Raj, 2011</ref>), including their flexibility in choosing a similarity function, sparseness of solution when dealing with large datasets, the ability to handle large feature spaces and the ability to identify outliers. We have first examined the performance of our method using SVM on six microarray datasets in terms of precision, TP rate, FP rate and AUC. In ECBGS, there is a parameter, the relevance threshold. Different settings of will directly affect the number of selected genes. The closer is set to 1, the smaller the number of selected genes is. From the experiments, we found that the larger number of genes does not always lead to better performance. Therefore, it is very important to choose the appropriateThe last row is the total average accuracy of four methods on six datasets. The bold values indicate the highest classification accuracy. threshold for improving classification accuracy. However, choosing a proper threshold is difficult because the distinct values for each dataset are different. Through the experiments on six datasets, we found that the threshold was closely related with the 'degree' of the relevance of the genes. For example, in the Lymphoma dataset, there are 4300 genes with SU value that is 40.5. In contrast, there are only 37 genes that satisfy the threshold 0.5 in Breast_B dataset. Thus, the appropriate threshold for the dataset that has higher 'degree' of the relevance has to be larger than the one for the dataset that has lower 'degree'.(h) indicate the result on Lymphoma dataset, (i) and (j) indicate the results on MLL_Leukemia dataset, (k) and (l) indicate the results on Prostate dataset. We performed two experiments for each dataset: one applied the SBS strategy, and the other one applied the SFS strategy for three feature selection methods. As in the figure, when the training dataset is small, the proposed method shows high prediction accuracy, and other methods primarily make poor predictions. Moreover, the proposed method outperforms other methods in most cases</p><p>Based on this observation, we can make some general recommendations based on the experiments. (i) The threshold could be selected as the mean of all the SU value of the genes respect to the class or (ii) decide the threshold as the following equation:</p><p>¼ ðSU max À SU min Þ Ã 0:7 ð5Þ where SU max indicates the SU value of the most relevant gene respect to the class, and SU min refers to the lowest one.</p><p>We have also compared ECBGS to the FCBF algorithm. The result shows that the proposed ECBGS is able to generate the most meaningful and discriminative genes in most cases. However, if the number of features is 550, FCBF tends to be more effective; it is because ECBGS produces the feature subsets by removing top informative features. If the datasets have a small number of features with a lack of useful information, removing some informative features will result in less discriminative or meaningless subsets to train the classifier. Furthermore, we found that relevant and non-redundant features are selected before repeating our method more than 10 times. We have also made a comparison between the proposed method and other feature selection methods in terms of classification accuracy and speed. One interesting observation is that our method is still more powerful than other methods even when small data are given. It indicates that ECBGS is more appropriate than others for analyzing small datasets, such as gene expression data. Moreover, our method is significantly faster than other feature selection methods. Additionally, when the number of selected features is 42000, the computational costs of the other three feature selection methods are very expensive. The selection of discriminant genes is a common task for cancer classification. Research in Biology and Medicine may benefit from the examination of the top ranking genes to confirm recent discoveries in cancer research, or suggest new avenues to be explored (<ref type="bibr" target="#b15">Guyon et al., 2002</ref>). Recently, several gene selection approaches (Jirapech<ref type="bibr" target="#b21">Umpai and Aitken, 2005;</ref><ref type="bibr" target="#b27">Li et al., 2004</ref>) have been proposed to solve the cancer classification problem. In contrast to these methods, the prediction accuracy of our method is competitive with a small subset of genes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this work, we proposed an ensemble gene selection algorithm based on SU and SVM for cancer classification. To select multiple gene subsets, we used different starting points during the redundancy analysis step. In this way, we selected more informative genes than FCBF. We found that between two redundant genes, the less relevant gene makes a poor prediction; however, a combination of genes of this type can sometimes produce a competitive result. During the experiments, we used six freely accessible benchmark datasets from the Internet to meet our objective, which was to evaluate and investigate the performance of our method using the classifiers trained from both 10-cross validation and different sizes of datasets. The results show that the classification model with our proposed gene selection algorithm has higher prediction accuracy, and that our method can still achieve high accuracy when the number of training instances is small. Compared with other methods published in the literature, our method yields good results. However, for different datasets, the relevance threshold is different under the context of classification performance. Therefore, how to determine the relevance threshold in a self-adaptive matter will be focused on our future work. Moreover, we believe that our mechanism is also applicable to other feature selection problems and can be expanded to other classifications of disease states.<ref type="figure" target="#tab_1">Table 13</ref>. The best accuracy (%) and the number of selected genes obtained with our method and other approaches in the literature. The bold letters indicate the results of our proposed methodThe bold values indicate the results of our proposed method.The running time for GR þ SVM, IG þ SVM and ReliefF þ SVM is much 4100 000 s on the No. 4, 5 and 6 datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*</head><figDesc>To whom correspondence should be addressed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.2.</head><figDesc>Fig. 2. ECBGS algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><figDesc>Classification accuracy ¼ (TP þ TN) / (TP þ FP þ FN þ TN) Precision ¼ TP / (TP þ FP) TP rate ¼ TP / (TP þ FN) FP rate ¼ FP / (FP þ TN) Area Under Receiver Operating Characteristic Curve (AUC) is a single-value measurement that ranges from 0 to 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.3.</head><figDesc>Fig. 3. Classification accuracy of different sizes of training and test datasets. (a) and (b) indicate the results on the Breast_B dataset, (c) and (d) indicate the results on the CNS dataset, (e) and (f) indicate the result on Leukemia dataset, (g) and (h) indicate the result on Lymphoma dataset, (i) and (j) indicate the results on MLL_Leukemia dataset, (k) and (l) indicate the results on Prostate dataset. We performed two experiments for each dataset: one applied the SBS strategy, and the other one applied the SFS strategy for three feature selection methods. As in the figure, when the training dataset is small, the proposed method shows high prediction accuracy, and other methods primarily make poor predictions. Moreover, the proposed method outperforms other methods in most cases</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>, F 3 , F 10 and F 8 are redundant with F 9 such that the subset {F 9 , F 2 } will be selected, and subset {F 3 , F 10 , F 8 } will be moved into the removed subset. We repeat this procedure until there are no features in the removed subset. Thus, the subsets generated from our method are {F 11</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>Table 2.</figDesc><table>The SU value among the features in the first iteration 

F 11 
F 9 
F 3 
F 10 
F 8 
F 2 

F 11 
— 
0 . 4 2 7 

a 

0.689 
0.085 

a 

F 3 
— 
— 
— 
— 
— 

a 

a 

The SU value between two features is smaller than the SU value respect to the 
class. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><figDesc>Table 1. SU value between each feature and the class</figDesc><table>F 11 
F 9 
F 3 
F 10 
F 8 
F 2 

SU value 
0.225 
0.211 
0.184 
0.184 
0.07 
0.05 

The features that have 0 SU value (F 4 , F 1 , F 12 , F 13 , F 5 , F 7 and F 6 ) are not presented 
in this table. All of the features are sorted in descending order by SU value. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>Table 3. The SU value among features in the second iteration The SU value between two features is smaller than the SU value respect to the class.</figDesc><table>F 9 
F 3 
F 10 
F 8 
F 2 

F 9 
— 
0.218 
0.368 
0.10 

a 

F 2 
— 
— 
— 
— 
— 

a 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><figDesc>Table 4. The classification accuracy (%) of FCBF and our method</figDesc><table>Dataset 
FCBF 
Set 2 
Set 3 
Set 4 
Set 5 
Set 6 
Set 7 

1 
9 0 
95 
85 
70 
85 
85 
75 
2 
75 
75 
70.83 
83.33 
70.83 
83.33 
75 
3 
86.21 
93.10 
93.10 
93.10 
93.10 
93.10 
93.10 
4 
92.11 
94.74 
92.11 
92.11 
94.74 
92.11 
92.11 
5 
100 
100 
100 
100 
100 
100 
100 
6 
95.12 
92.68 
95.12 
90.24 
92.68 
92.68 
97.65 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><figDesc>Table 5. The precision, TP rate, FP rate and AUC of SVM trained using ECBGS varying the relevance threshold on the Breast_B dataset</figDesc><table>Breast_B 

Threshold 
0 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
0.55 
0.6 
0.65 
0.7 
0.75 
0.8 
#Genes 
39 
39 
39 
39 
38 
27 
17 
10 
5 
4 
1 
1 
0 
0 
0 
0 
0 
Precision 
0.94 
0.94 
0.94 
0.94 
0.94 
0.96 
0.86 
0.86 
0.86 
0.86 
0.15 
0.15 
NA 
NA 
NA 
NA 
NA 
TP rate 
0.94 
0.94 
0.94 
0.94 
0.94 
0.96 
0.86 
0.86 
0.86 
0.86 
0.39 
0.39 
NA 
NA 
NA 
NA 
NA 
FP rate 
0.03 
0.03 
0.03 
0.03 
0.03 
0.02 
0.07 
0.07 
0.07 
0.05 
0.04 
0.04 
NA 
NA 
NA 
NA 
NA 
AUC 
0.98 
0.98 
0.98 
0.98 
0.98 
0.98 
0.94 
0.93 
0.93 
0.93 
0.58 
0.58 
NA 
NA 
NA 
NA 
NA 

The third row shows the number of selected genes corresponding to the relevance threshold. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><figDesc>Table 6.</figDesc><table>The precision, TP rate, FP rate and AUC of SVM trained using ECBGS varying the relevance threshold on the CNS dataset 

CNS 

Threshold 
0 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
0.55 
0.6 
0.65 
0.7 
0.75 
0.8 
# G e n e s 
3 6 
3 6 
3 6 
3 6 
2 3 
7 
4 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
Precision 
0.90 
0.90 
0.90 
0.90 
0.88 
0.74 
0.44 
0.44 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
TP rate 
0.90 
0.90 
0.90 
0.90 
0.88 
0.73 
0.67 
0.67 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
FP rate 
0.13 
0.13 
0.13 
0.13 
0.16 
0.48 
0.44 
0.67 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
AUC 
0.88 
0.88 
0.88 
0.88 
0.86 
0.63 
0.50 
0.50 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
NA 

The third row shows the number of selected genes corresponding to the relevance threshold. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="true"><figDesc>Table 7. The precision, TP rate, FP rate and AUC of SVM trained using ECBGS varying the relevance threshold on the Leukemia dataset</figDesc><table>Leukemia 

Threshold 
0 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
0.55 
0.6 
0.65 
0.7 
0.75 
0.8 
# G e n e s 
5 9 
5 9 
5 9 
5 9 
2 7 
6 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
Precision 
0.90 
0.90 
0.90 
0.90 
0.90 
0.85 
0.76 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
TP rate 
0.90 
0.90 
0.90 
0.90 
0.90 
0.85 
0.76 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
FP rate 
0.11 
0.11 
0.11 
0.11 
0.15 
0.23 
0.29 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
AUC 
0.90 
0.90 
0.90 
0.90 
0.88 
0.81 
0.74 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
NA 
NA 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="true"><figDesc>Table 9. The precision, TP rate, FP rate and AUC of SVM trained using ECBGS varying the relevance threshold on the MLL_Leukemia dataset</figDesc><table>MLL_Leukemia 

Threshold 
0 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
0.55 
0.6 
0.65 
0.7 
0.75 
0.8 
#Genes 
117 
117 
117 
117 
113 
105 
89 
55 
53 
31 
30 
14 
11 
5 
3 
1 
0 
Precision 
1 
1 
1 
1 
1 
1 
1 
0.99 
1 
0.99 
0.99 
0.96 
0.95 
0.55 
0.55 
0.52 
NA 
TP rate 
1 
1 
1 
1 
1 
1 
1 
0.99 
1 
0.99 
0.99 
0.96 
0.94 
0.69 
0.68 
0.56 
NA 
FP rate 
0 
0 
0 
0 
0 
0 
0 
0.01 
0 
0.01 
0.01 
0.02 
0.03 
0.19 
0.20 
0.28 
NA 
AUC 
1 
1 
1 
1 
1 
1 
1 
0.99 
1 
0.99 
0.99 
0.99 
0.98 
0.78 
0.78 
0.66 
NA 

The third row shows the number of selected genes corresponding to the relevance threshold. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="true"><figDesc>Table 10. The precision, TP rate, FP rate and AUC of SVM trained using ECBGS varying the relevance threshold on the Prostate dataset</figDesc><table>Prostate 

Threshold 
0 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
0.55 
0.6 
0.65 
0.7 
0.75 
0.8 
#Genes 
76 
76 
76 
64 
46 
30 
18 
9 
9 
5 
2 
1 
0 
0 
0 
0 
0 
Precision 
0.97 
0.97 
0.97 
0.98 
0.98 
0.96 
0.96 
0.96 
0.94 
0.94 
0.91 
0.79 
NA 
NA 
NA 
NA 
NA 
TP rate 
0.97 
0.97 
0.97 
0.98 
0.98 
0.96 
0.96 
0.96 
0.94 
0.94 
0.91 
0.70 
NA 
NA 
NA 
NA 
NA 
FP rate 
0.03 
0.03 
0.03 
0.02 
0.02 
0.04 
0.04 
0.04 
0.06 
0.06 
0.09 
0.32 
NA 
NA 
NA 
NA 
NA 
AUC 
0.97 
0.97 
0.97 
0.98 
0.98 
0.96 
0.96 
0.96 
0.94 
0.94 
0.91 
0.69 
NA 
NA 
NA 
NA 
NA 

The third row shows the number of selected genes corresponding to the relevance threshold. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="true"><figDesc>Table 8. The precision, TP rate, FP rate and AUC of SVM trained using ECBGS varying the relevance threshold on the lymphoma dataset</figDesc><table>Lymphoma 

Threshold 
0 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 
0.55 
0.6 
0.65 
0.7 
0.75 
0.8 
#Genes 
89 
89 
89 
89 
89 
86 
86 
79 
75 
69 
61 
54 
37 
31 
23 
9 
40 . 9 8 
1 
0 . 9 8 
0 . 9 8 
1 
1 
1 
0 . 9 5 
F Pr a t e 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 . 0 1 
0 
0 . 0 1 
0 . 0 1 
0 
0 
0 
0 . 0 1 
A U C 
1 
1 
1 
1 
1 
1 
1 
1 
1 
0 . 9 9 
1 
0 . 9 9 
0 . 9 9 
1 
1 
1 
0 . 9 8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="true"><figDesc>Table 11. 10-fold cross validation classification accuracy (%) of four feature selection methods</figDesc><table>Dataset 
GR þ SVM 
IG þ SVM 
ReliefFþSVM 
ECBGS 

SFS 
SBS 
SFS 
SBS 
SFS 
SBS 

1 
87.76 
93.88 
87.76 
97.96 
93.88 
91.84 
95.92 
2 
75 
73.33 
73.33 
90.00 
65 
83.33 
90.00 
3 
86.11 
87.5 
90.28 
90.28 
90.28 
87.50 
90.28 
4 
100 
100 
98.31 
95.16 
100 
100 
100 
5 
95.83 
98.61 
94.44 
95.83 
94.44 
97.22 
100 
6 
95.10 
98.04 
93.14 
94.12 
93.14 
96.08 
98.04 
Average 
89.97 
91.89 
89.54 
93.89 
89.46 
92.66 
95.71 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" validated="true"><figDesc>Table 12. Running time (s) for each feature selection algorithm</figDesc><table>Dataset ECBGS GR þ SVM 
IG þ SVM 
ReliefF þ SVM 

SFS SBS 
SFS 
SBS 
SFS 
SBS 

1 
7 
2959 20 090 14 885 18 735 12 471 
410 5 
2 
231 
286 
4129 
234 
1994 10 837 
410 5 
3 
243 
1041 
8823 
608 12 083 10 807 
410 5 
4 
9 0 
410 5 
410 5 
410 5 
410 5 
410 5 410 5 
5 
8 0 9 
410 5 
410 5 
410 5 
410 5 
410 5 410 5 
6 
8 6 3 
410 5 
410 5 
410 5 
410 5 
410 5 410 5 

</table></figure>

			<note place="foot">ß The Author 2012. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">Y.Piao et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">An ECBGS algorithm at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">P r e c i s i o n 1 1 1 1 1 1 1 1 1 0. 9 9 1 0. 9 9 0. 9 9 1 1 1 0. 9 6 T Pr a t e 1 1 1 1 1 1 1 1 1</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors are grateful to anonymous referees for their valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust biomarker identification for cancer diagnosis with ensemble feature selection methods</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Abeel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="392" to="398" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Model selection for the LS-SVM. Application to handwriting recognition</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Adankon</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Cheriet</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="3264" to="3270" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Support vector machines combined with feature selection for breast cancer diagnosis</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">F</forename>
				<surname>Akay</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3240" to="3247" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Distinct types of diffuse large B-cell lymphoma identified by gene expression profiling</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Alizadeh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">403</biblScope>
			<biblScope unit="page" from="503" to="511" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">MLL translocations specify a distinct gene expression profile that distinguishes a unique leukemia</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">A</forename>
				<surname>Armstrong</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="41" to="47" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Classifying gene expression data of cancer using classifier ensemble with mutually exclusive features</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Cho</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Ryu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>. IEEE</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1744" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Cancer classification using ensemble of neural networks with multiple significant gene subsets</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Cho</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Won</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="243" to="250" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient dimensionality reduction approaches for feature selection</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Deisy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Intelligence and Multimedia Applications</title>
		<meeting><address><addrLine>Sivakasi, Tamil Nadu</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="121" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Gene selection and classification of microarray data using random forest</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Dıáz-Uriarteb</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Boosting for tumor classification with gene expression data</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Dettling</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Bu¨hlmannbu¨hlmann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1061" to="1069" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<monogr>
		<title level="m" type="main">An evaluation of feature selection methods and their application to computer security</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Doak</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Classification of heterogeneous microarray data by maximum entropy kernel</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Fujibuchi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Kato</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="267" to="277" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Review on feature selection techniques and the impact of SVM for cancer classification using gene expression profile</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>George</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Raj</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Sci. Eng. Surv</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature subset selection in large dimensionality domains</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Gheyas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Smith</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="5" to="13" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Molecular classification of cancer: class discovery and class prediction by gene expression monitoring</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">R</forename>
				<surname>Golub</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="page" from="531" to="537" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Gene selection for cancer classification using support vector machines</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Guyon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine. Learn</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="389" to="422" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<monogr>
		<title level="m" type="main">Attribute-oriented induction in data mining Advances in Knowledge Discovery sand Data Mining</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Han</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Fu</surname>
			</persName>
		</author>
		<editor>Fayyad,U.M. et al.</editor>
		<imprint>
			<date type="published" when="1996" />
			<publisher>AAAI Press/The MIT Press</publisher>
			<biblScope unit="page" from="339" to="421" />
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Brain reading using full brain support vector machines for object recognition: there is no &apos;face&apos; identification area</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Hanson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Halchenko</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="486" to="503" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Classification by pairwise coupling</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hastie</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="451" to="471" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<monogr>
		<title level="m" type="main">A Practical Guide to Support Vector Classification</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">W</forename>
				<surname>Hsu</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2010-09-25" />
		</imprint>
	</monogr>
	<note>date. last accessed</note>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Hybrid feature selection by combining filters and wrappers</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">H</forename>
				<surname>Hsu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="8144" to="8150" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature selection and classification for microarray data analysis: evolutionary methods for identifying predictive genes</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Jirapech-Umpai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Aitken</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">148</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">A novel hybrid feature selection via symmetrical uncertainty ranking based local memetric search algorithm</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Kannan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="580" to="585" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature selection for unsupervised learning via evolutionary search</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Kim</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="365" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Wrappers for feature subset selection</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Kohavi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">H</forename>
				<surname>John</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="273" to="324" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">An extensive comparison of recent classification tools applied to microarray data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Lee</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Stat. Data Anal</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">QSE: a new 3-D solvent exposure measure for the analysis of protein structure</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Li</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteomics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3793" to="3801" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">A comparative study of feature selection and multiclass classification methods for tissue classification based on gene expression</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Li</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2429" to="2437" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Ensemble gene selection for cancer classification</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Liu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="2763" to="2772" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Feature selection with selective sampling</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Liu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth International Conference on Machine Learning</title>
		<meeting>the Nineteenth International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="395" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">A probabilistic approach to feature selection—a filter solution</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Liu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Setiono</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Machine Learning</title>
		<meeting>the Thirteenth International Conference on Machine Learning<address><addrLine>Bari, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="319" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Toward integrating feature selection algorithms for classification and clustering</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Liu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Yu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="491" to="502" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Regularized ROC method for disease classification and biomarker selection with microarray data</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Ma</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="4356" to="4362" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Prediction of central nervous system embryonal tumour outcome based on gene expression</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">L</forename>
				<surname>Pomeroy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">415</biblScope>
			<biblScope unit="page" from="436" to="442" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Effective feature selection framework for cluster analysis of microarray data</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Pok</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="385" to="389" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">Gene expression correlates of clinical prostate cancer behavior</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Singh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Cell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="203" to="209" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<analytic>
		<title level="a" type="main">A comprehensive evaluation of multicategory classification methods for microarray gene expression cancer diagnosis</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Statnikov</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="631" to="643" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b37">
	<analytic>
		<title level="a" type="main">A comprehensive comparison of random forests and support vector machines for microarray-based cancer classification</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Statnikov</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">319</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b38">
	<analytic>
		<title level="a" type="main">Ensemble machine learning on gene expression data for cancer classification</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">C</forename>
				<surname>Tan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Gilbert</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="3583" to="3593" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b39">
	<analytic>
		<title level="a" type="main">An evaluation of filter and wrapper methods for feature selection in categorical clustering</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Talavera</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 6th International Symposium on Intelligent Data Analysis</title>
		<meeting>6th International Symposium on Intelligent Data Analysis<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="440" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b40">
	<monogr>
		<title level="m" type="main">The Nature of Statistical Learning Theory</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<forename type="middle">N</forename>
				<surname>Vapnik</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b41">
	<analytic>
		<title level="a" type="main">Predicting the clinical status of human breast cancer by using gene expression profiles</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>West</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA, 98</title>
		<meeting>. Natl Acad. Sci. USA, 98</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="11462" to="11467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b42">
	<analytic>
		<title level="a" type="main">Using support vector machines with a novel hybrid feature selection method for diagnosis of erythemato-squamous diseases</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Xie</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Wang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="5809" to="5815" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b43">
	<analytic>
		<title level="a" type="main">Feature (Gene) selection in gene expression-based tumor classification</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Xiong</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Genet. Metab</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="239" to="247" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b44">
	<analytic>
		<title level="a" type="main">A stable gene selection in microarray data analysis</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Yang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">228</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b45">
	<analytic>
		<title level="a" type="main">IG-GA: a hybrid filter/wrapper method for feature selection of microarray data</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">H</forename>
				<surname>Yang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Biol. Eng</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="23" to="28" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b46">
	<analytic>
		<title level="a" type="main">Applying data mining techniques for cancer classification on gene expression data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">Y</forename>
				<surname>Yeh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cybern. Syst. Int. J</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="583" to="602" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b47">
	<analytic>
		<title level="a" type="main">Feature selection for high-dimensional data: a fast correlation-based filter solution</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Yu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Liu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Machine Learning (ICML-2003</title>
		<meeting>the Twentieth International Conference on Machine Learning (ICML-2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="856" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficient feature selection via analysis of relevance and redundancy</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Yu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Liu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1205" to="1224" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b49">
	<analytic>
		<title level="a" type="main">A review of feature selection techniques in bioinformatics</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Saeys</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2507" to="2517" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b50">
	<analytic>
		<title level="a" type="main">Text classification: neural networks vs support vector machines</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Zaghloul</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ind. Manag. Data Syst</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="708" to="717" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b51">
	<analytic>
		<title level="a" type="main">A novel representation for apoptosis protein subcellular localization prediction using support vector machine</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Theor. Biol</title>
		<imprint>
			<biblScope unit="volume">259</biblScope>
			<biblScope unit="page" from="361" to="365" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>