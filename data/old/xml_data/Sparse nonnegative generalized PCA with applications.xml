
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Systems biology Sparse non-negative generalized PCA with applications to metabolomics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011">. 21 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Genevera</forename>
								<forename type="middle">I</forename>
								<surname>Allen</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Pediatrics-Neurology</orgName>
								<orgName type="institution">Baylor College of Medicine</orgName>
								<address>
									<addrLine>Jan and Dan Duncan</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Neurological Research Institute at Texas Children&apos;s Hospital</orgName>
								<address>
									<addrLine>1250 Moursund St. Suite 1365</addrLine>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Rice University</orgName>
								<address>
									<addrLine>6100 Main St. MS-138</addrLine>
									<postCode>77005</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Mirjana</forename>
								<surname>Maletí C-Savatí</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Pediatrics-Neurology</orgName>
								<orgName type="institution">Baylor College of Medicine</orgName>
								<address>
									<addrLine>Jan and Dan Duncan</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Neurological Research Institute at Texas Children&apos;s Hospital</orgName>
								<address>
									<addrLine>1250 Moursund St. Suite 1365</addrLine>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Systems biology Sparse non-negative generalized PCA with applications to metabolomics</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">27</biblScope>
							<biblScope unit="page" from="3029" to="3035"/>
							<date type="published" when="2011">. 21 2011</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btr522</idno>
					<note type="submission">Received on June 27, 2011; revised on August 23, 2011; accepted on September 10, 2011</note>
					<note>[10:08 4/10/2011 Bioinformatics-btr522.tex] Page: 3029 3029–3035 Associate Editor: Jonathan Wren Contact: gallen@rice.edu Supplementary Information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Nuclear magnetic resonance (NMR) spectroscopy has been used to study mixtures of metabolites in biological samples. This technology produces a spectrum for each sample depicting the chemical shifts at which an unknown number of latent metabolites resonate. The interpretation of this data with common multivariate exploratory methods such as principal components analysis (PCA) is limited due to high-dimensionality, non-negativity of the underlying spectra and dependencies at adjacent chemical shifts. Results: We develop a novel modification of PCA that is appropriate for analysis of NMR data, entitled Sparse Non-Negative Generalized PCA. This method yields interpretable principal components and loading vectors that select important features and directly account for both the non-negativity of the underlying spectra and dependencies at adjacent chemical shifts. Through the reanalysis of experimental NMR data on five purified neural cell types, we demonstrate the utility of our methods for dimension reduction, pattern recognition, sample exploration and feature selection. Our methods lead to the identification of novel metabolites that reflect the differences between these cell types.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Metabolomics, one of the newest fields within systems biology approaches to biomarker discovery in medicine, investigates an abundant pool of small molecules present in cells and tissues (<ref type="bibr" target="#b5">Bollard et al., 2005;</ref><ref type="bibr" target="#b13">Hollywood et al., 2006;</ref><ref type="bibr" target="#b14">Holmes et al., 2008</ref>). One of the commonly used technologies for acquisition of this data is nuclear magnetic resonance (NMR) spectroscopy. It is a high-throughput technology for acquiring reproducible and resolved spectra that can be used to study the complete metabolic profile of a biological sample (<ref type="bibr" target="#b24">Nicholson and Lindon, 2008</ref>). The spectra contain thousands of chemical resonances, which may belong to hundreds of metabolites (<ref type="bibr" target="#b8">De Graaf, 2007</ref>). However, many metabolites resonate at multiple resonances and thus, unlike the typical DNA microarray data, different metabolite spectra overlap * To whom correspondence should be addressed. and introduce complexities that need to be addressed by signal processing and careful statistical analysis (<ref type="bibr" target="#b10">Ebbels and Cavill, 2009;</ref><ref type="bibr" target="#b31">Weljie et al., 2006</ref>). As understanding relationships between the set of biological samples and the underlying spectra is a challenge, principal components analysis (PCA) is commonly used for both dimension reduction and pattern recognition with NMR data (<ref type="bibr" target="#b6">Coen et al., 2008;</ref><ref type="bibr" target="#b9">Dunn et al., 2005;</ref><ref type="bibr" target="#b12">Goodacre et al., 2004;</ref><ref type="bibr" target="#b22">Maleti´cMaleti´c-Savati´cSavati´c et al., 2008;</ref><ref type="bibr" target="#b30">Weckwerth and Morgenthal, 2005</ref>). In high-dimensional settings, however, it is well known that PCA can perform poorly due to the large number of irrelevant variables (<ref type="bibr" target="#b16">Johnstone and Lu, 2009</ref>). Hence, many have proposed to incorporate sparsity into the principal component directions, thus selecting important features (<ref type="bibr" target="#b16">Johnstone and Lu, 2009;</ref><ref type="bibr" target="#b17">Jolliffe et al., 2003;</ref><ref type="bibr" target="#b27">Shen and Huang, 2008;</ref><ref type="bibr" target="#b35">Zou et al., 2006</ref>). Non-negativity of the matrix factors, or principal component directions, has also been proposed in a number of settings to improve interpretability of the factors (<ref type="bibr" target="#b20">Lee and Seung, 1999;</ref><ref type="bibr" target="#b26">Sajda et al., 2004</ref>). Several recent papers have combined these concepts to encourage both sparsity and non-negativity into the model (<ref type="bibr" target="#b15">Hoyer, 2004;</ref><ref type="bibr" target="#b19">Kim and Park, 2007;</ref><ref type="bibr" target="#b33">Zass and Shashua, 2007</ref>). In this article, we make the following statistical contributions:</p><p>(i) propose a framework for incorporating sparsity, known structural dependencies and non-negativity into the principal component (PC) loadings and (ii) develop a fast, computationally efficient algorithm to compute these in high-dimensional settings. This work is presented in Section 2. Then, in Section 3, we evaluate the performance of our methods on real NMR data. We also demonstrate how to interpret the PC loadings to understand important biological patterns and identify candidate metabolites. In Section 4, we conclude with a summary of the implications of our work and future areas of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head><p>We introduce a framework for PCA that incorporates structural dependencies, sparsity and non-negativity to better understand relationships between the samples and recognize patterns among the variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Review: generalized PCA</head><p>Recently,<ref type="bibr" target="#b4">Allen et al. (2011)</ref>introduced a new matrix decomposition, the Generalized Least Squares Matrix Decomposition (GMD), and showed how this decomposition can be used to generalized PCA by directly incorporating known structural information or dependencies. Here, we</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.I.Allen and M.Maletí c-Savatí c</head><p>review the Generalized PCA (GPCA) problem and specifically discuss its utility in the context of spectroscopy data. We observe data, X ∈∈ n×p , for n samples and p variables that has previously been normalized. (With NMR data, this includes baseline correction, normalizing by the integral of the spectrum and standardizing the variables at each ppm.) Let R ∈∈ p×p be a positive semi-definite matrix called the quadratic operator that captures the noise structure in the data. Then, GPCA seeks the linear combination of variables maximizing the sample variance in the inner product space induced by R:</p><formula>maximize v k v T k RX T XRv k subject to v T k Rv k = 1 &amp; v T k Rv k = 0 ∀ k &lt; k.</formula><formula>(1)</formula><p>The k-th GPC is z k = XRv k. If R = I, then we have the standard PCA optimization problem. Additionally,<ref type="bibr" target="#b4">Allen et al. (2011)</ref>have shown that an extension of the power method for computing eigenvectors can be used to calculate these GPCs. GPCA can be used to directly account for dependencies between adjacent variables in the spectra. The quadratic operator, R, behaves like an inverse covariance matrix of multivariate normal data (<ref type="bibr" target="#b4">Allen et al., 2011</ref>). We can let R encode the inverse covariance of dependencies or structure in the data that do not contribute, and are independent of the signal of interest. The resulting GPCA solution can be interpreted as a decomposition of the covariance given by: Cov(X) = VD 2 V T +R −1 , where D 2 is diagonal with entries, d 2 k = v T k RX T XRv k. With NMR spectroscopy, variables at adjacent chemical shifts are strongly positively correlated. These dependencies, however, do not contribute to the biological signal, or the peaks and groups of peaks that vary across the samples. Thus, letting R encode these dependencies between adjacent chemical shifts, allows GPCA to ignore the biologically irrelevant structure and estimate more of the biologically relevant variability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Kernel smoothers as quadratic operators</head><p>To this end, we employ kernel smoothers that are a function of the distance between the variables. Take the p×p distance matrix, D, where D ij is the pair-wise distance between variables i and j. Then, the quadratic operator R can be taken as R ij = k(D ij ,γ) where k() is a kernel and γ is the smoothing parameter. Standard kernels used in local linear regression, such as the Gaussian</p><formula>kernel, k(D ij ,γ) = 1 γ √ 2π exp(− D 2 ij 2γ 2 )</formula><p>, can be employed. If γ = 10, for example, then elements in the kernel smoother are weighted according to a normal distribution with a SD of 10 distance units apart. For NMR data, the GPCA loading vectors multiply the data through a range of adjacent chemical shifts weighted by the kernel smoother. Thus, we directly account for dependencies between neighboring variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sparse non-negative GPCA</head><p>While GPCA directly accounts for biologically irrelevant structure in NMR data, the problems of high dimensionality and the non-negativity of the spectra are left unsolved. To this end, we introduce Sparse Non-Negative GPCA, which gives interpretable PCA direction vectors by incorporating feature selection through sparsity and by constraining the loadings to be non-negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Problem and solution</head><p>We introduce the single-factor sparse nonnegative GPCA optimization problem. Let u ∈∈ n , λ ≥ 0, and R and v as defined previously, and consider the following:</p><formula>maximize v,u u T XRv−λ||v|| 1 subject to u T u ≤ 1, v T Rv ≤ 1, &amp; v ≥ 0.</formula><formula>(2)</formula><p>The PCA loading vectors, v k are constrained to be non-negative, and sparsity is encouraged via the 1-norm or lasso penalty on the loadings (<ref type="bibr" target="#b28">Tibshirani, 1996</ref>). Here, λ is a penalty parameter controlling the amount of sparsity.</p><p>This simple criterion for the single-factor sparse non-negative GPCA is related to many existing approaches to sparse PCA and non-negative PCA. First, if λ = 0, the non-negativity constraint is removed, and the remaining inequalities hold with equality, Equation (2) is equivalent to the GPCA or GMD optimization problem (<ref type="bibr" target="#b4">Allen et al., 2011</ref>). This is related to the Lagrangian form of the sparse PCA approach in<ref type="bibr" target="#b32">Witten et al. (2009)</ref>, and is also a constrained version of the regression-based sparse PCA approach of Shen and Huang (2008). This single factor problem, however, differs from the multicomponent problem for sparse non-negative PCA of Zass and Shashua (2007). Also, notice that we do not require subsequent direction vectors to be orthogonal. Many have noted that orthogonality of sparse PCA factors is unwarranted and hence is often not imposed (<ref type="bibr" target="#b18">Journée et al., 2010;</ref><ref type="bibr" target="#b27">Shen and Huang, 2008;</ref><ref type="bibr" target="#b35">Zou et al., 2006</ref>). Our single-factor approach has many advantages. Notice that the problem is biconcave, meaning that it is concave in v with u fixed and in u with v fixed. This leads to a simple maximization strategy that is guaranteed to increase the objective and converge to a local maximum: alternate maximizing with respect to u and v. These coordinate-wise maximization problems turn out to have a simple solution:</p><formula>v * = ˆ v/||ˆv||||ˆv|| R if ||ˆv||||ˆv|| R &gt; 0 0 otherwise, &amp; u * = XRv ||XRv|| 2 .</formula><p>(All proofs are given in the Supplementary Materials).</p><p>The solution to the single-factor sparse non-negative GPCA problem, (2), can be obtained by solving a simple lasso penalized non-negative regression problem. This non-negative regression problem in turn can be solved via a fast coordinate descent algorithm:This coordinate descent approach is related to the fast shooting algorithms of<ref type="bibr" target="#b11">Friedman et al. (2010)</ref>, and the speed can be further improved by employing active set learning and warm starts. We note that this algorithmic approach is a major improvement in terms of computational efficiency over the least angle-based approach to the non-negative lasso of<ref type="bibr" target="#b25">Renard et al. (2008</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Algorithm</head><p>We have presented an optimization problem and solution to the single-factor sparse non-negative GPCA problem, and we are also interested in extracting multiple components. Then, we employ a greedy approach to estimating multiple components that is closely related to the power method algorithm for computing eigenvectors. This algorithm is summarized in Algorithm 1. The sparse non-negative GPCA algorithm begins with the standardized data and computes the first component by solving the single-factor problem via coordinate descent. Subsequent components are calculated by solving the single-factor problem for the residual where the previously computed outer product has been removed. Each component is calculated in a greedy manner and is hence conditional on the previously estimated components. Thus, the components are not necessarily ordered in terms of the amount of variance they explain. This approach is common among existing methods for sparse PCA (<ref type="bibr" target="#b4">Allen et al., 2011;</ref><ref type="bibr" target="#b21">Lee et al., 2010;</ref><ref type="bibr" target="#b27">Shen and Huang, 2008;</ref><ref type="bibr" target="#b32">Witten et al., 2009;</ref><ref type="bibr" target="#b35">Zou et al., 2006</ref>). As the dominant operation in our algorithm is solving a non-negative lasso problem, the computational complexity is O(n 3 ). While traditional PCA methods may be faster to compute, our algorithm requires comparable computational time to existing sparse and/or non-negative PCA methods (<ref type="bibr" target="#b27">Shen and Huang, 2008;</ref><ref type="bibr" target="#b33">Zass and Shashua, 2007</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse non-negative GPCA</head><p>Algorithm 1 Sparse Non-Negative GPCA Algorithm 1. Standardize the columns of X and setˆXsetˆ setˆX</p><formula>(1) = X 2. For k = 1...K:</formula><p>(a) Initialize u k and v k to the first left and right GMD factor ofˆXofˆ ofˆX (k) , respectively.</p><p>(b) Repeat until convergence:</p><formula>@BULLET Set u k = ˆ X (k) Rv k ||ˆX||ˆ ||ˆX (k) Rv k || 2 .</formula><p>@BULLET For j = 1,...,p,1,...,p,1,...</p><formula>– SetˆvSetˆ Setˆv j = 1 R jj R rj X T u −R j, =jˆv=jˆ =jˆv =j −λ +. @BULLET Set v k = ˆ v/||ˆv||||ˆv|| R if ||ˆv||||ˆv|| R &gt; 0 0 otherwise. (c) Set d k = u T k ˆ X (k) Rv k .</formula><p>(d) SetˆXSetˆ SetˆX</p><formula>(k+1) = ˆ X (k) −u k d k v T k. 3. Return principal components, Z =[XRv 1 ,...XRv K ], loading vectors V =[v 1 ,...v K ], sample principal components U = [u 1 ,...,u K ]</formula><p>and scaling factors D = diag(d 1 ,...d K ).np df (λ). Here, df (λ) denotes the degrees of freedom associated with the value of λ. For the non-negative lasso, df (λ) =|{v(λ)}|, that is the number of non-zero elements of v. This follows from a result of Tibshirani and Taylor (2011). The criterion can be derived from considering each update in the power method algorithm as a generalized least squares problem with unknown variance (<ref type="bibr" target="#b4">Allen et al., 2011;</ref><ref type="bibr" target="#b21">Lee et al., 2010</ref>). While other methods such as cross-validation may be employed to find the optimal regularization parameter, minimizing the BIC is computationally more efficient and leads to greater flexibility to select differing penalty parameters for each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Selecting regularization parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Amount of variance explained</head><p>When using PCA methods for dimension reduction and exploratory analysis, the amount of variance explained by each principal component is an important measure to consider. As our GPCA and sparse non-negative GPCA methods incorporate structural information through the quadratic operator, R, the formulas for calculating the variance explained by each component are altered.</p><formula>Proposition 3.</formula><p>(Note that the proportion of variance explained by individual sparse nonnegative GPCs can be found by taking the differences of the cumulative proportion explained. Thus, the proportion of variance explained by our methods can be interpreted as the ratio of the R-norm projected sample variance of the k-th linear projection relative to the total variance of the data in the R-norm. Notice that as the sparse non-negative GPCA factors are not constrained to be orthogonal, the sample variance explained must be adjusted for possible correlations among the factors as discussed in<ref type="bibr" target="#b27">Shen and Huang (2008)</ref>. Given these results, we can compare our methods to traditional PCA and sparse PCA methods in terms of the variance explained and dimension reduction.A major motivation of our work is to incorporate feature selection into the traditional PCA framework and assess its utility for NMR data. We compare the degree of sparsity seen in the PCs for the sparse non-negative PCA and GPCA methods in<ref type="figure" target="#fig_3">Figure 3</ref>. By directly accounting for the dependencies at adjacent chemical shifts, sparsethe first seven PCs, which explain over 90% of the sample variance. Scaled PC loadings are superimposed on the average scaled spectra of neural stem cells, neurons, microglia and 'Glia', which includes oligodendrocytes and astrocytes. Sparse non-negative GPCA loadings reveal important patterns across the samples and spikes in the loadings denote the location of peaks that vary greatly across the samples. For example, PC3 exhibits peaks that have higher intensities in neural stem cells, while the peaks selected by PC5 have higher concentrations in microglia. PC loading vector indicates that more irrelevant variables have been discarded from the model. As sparse non-negative PCA does not incorporate structural information, many more variables are selected as the method tries to explain both the dependencies between neighboring chemical shifts and the biological variation. By directly accounting for these spatial dependencies, however, sparse nonnegative GPCA is free to select features that explain the biological variation in the samples. Overall, these results indicate that sparse non-negative GPCA outperforms PCA, GPCA and sparse nonnegative PCA in terms of sample exploration, dimension reduction and feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 3032 3029–3035</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.I.Allen and M.Maletí c-Savatí c</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse non-negative GPCA</head><p>Sparse non-negative GPCA can be used to understand important biological patterns in the NMR data.<ref type="figure" target="#fig_4">Figure 4</ref>gives the sparse nonnegative GPCA loadings for the first seven sparse non-negative GPCs which explain over 90% of the variance in the data. Along with these loadings, we give heatmaps of the sample PCs to show how each of the samples contribute to the patterns seen in the loading vectors. The loading vectors are scaled and superimposed on the mean spectra from neurons, neural stem cells, microglia and 'glia', which includes astrocytes and oligodendrocytes. (Plots of the loading vectors for PCA, GPCA and sparse non-negative PCA are given in the Supplementary Materials.)</p><p>Page: 3034 3029–3035Boldfaced locations denote peaks with especially strong signals as indicated by the loading vectors. Information on which cell types exhibited the highest intensity as well as metabolites that have previously been identified at the locations is also given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.I.Allen and M.Maletí c-Savatí c</head><p>By constraining the PC loading vectors to be non-negative, interpretation of the relationships between the features selected and the samples is made simpler. Spikes selected in the loading vectors indicate peaks that vary greatly across the samples. The positive sample PCs or scores (shown in the heatmaps of<ref type="figure" target="#fig_4">Fig. 4</ref>) have higher intensities at the peaks selected by the associated loading vector. The groups of spikes selected by each loading vector then indicate an important metabolic pattern that is up-or downregulated in each sample as revealed by the sample PCs. These metabolic patterns will consist of both metabolites that resonate at multiple peaks and also metabolites belonging to the same pathway. Thus, further testing of the peaks selected by our methods should be done to resolve the specific metabolites responsible for the metabolic pattern identified. Considering the first loading vector, the features selected are at chemical shifts where there are few peaks. This occurs as the first direction vector accounts for the baseline height difference between the samples due to normalization to the integral. This behavior is observed also in the first loading vector for the three competing methods (shown in the Supplementary Materials). Loading vectors two and three denote peaks that have higher concentrations in neural stem cells. Loading vector four exhibits a pattern of peaks that are upregulated in neurons and microglia, while the peaks selected in loading vectors five have higher intensities in microglia. Peaks in loading vectors six and seven denote metabolites that are upregulated in astrocytes and both oligodendrocytes and astrocytes, respectively. In Table 1, we give the locations of important selected peaks in parts per million, the cell types in which these peaks exhibited the highest intensities, as well as metabolites that have previously been identified at these peak locations. A previous analysis of this data using traditional PCA methods identified the peaks at 1.28, 2.02 and 3.23 ppm as higher in neural stem cells, neurons, and astrocytes, respectively (<ref type="bibr" target="#b23">Manganas et al., 2007</ref>). Our methods however, identify several other novel biomarkers, especially for microglia. In future work, we will identify candidate metabolites for the novel biomarkers in<ref type="figure" target="#tab_1">Table 1</ref>via public databases such as BioMagResBank (BMRB) (<ref type="bibr" target="#b30">Ulrich et al., 2008</ref>), metabolite identification models (<ref type="bibr" target="#b7">Crockford et al., 2005;</ref><ref type="bibr" target="#b34">Zheng et al., 2011</ref>) and spike-in experiments. Thus, our results are consistent with the existing literature, andt also identify novel biomarkers for consideration. These results demonstrate the many advantages of using sparse non-negative GPCA for NMR spectroscopy. Not only does our method exhibit greater dimension reduction, better clustering of samples according to biological relationships and provide more feature selection than competing methods, but also yields easily interpretable results that lead to understanding of important biological patterns in the spectra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head><p>We have presented a framework for incorporating structural dependencies, sparsity and non-negativity into PCA. By comparing our techniques to traditional PCA methods on real NMR data, we have demonstrated the many advantages of our methods. Future areas of research are to extend our framework to supervised multivariate analysis techniques such as partial least squares and linear discriminant to better classify NMR samples. While we have demonstrated our methods on 1D H-NMR spectroscopy, our approach can be applied to many other high-throughput metabolomics technologies. Mass spectrometry and other spectroscopy techniques also produce a spectrum of non-negative variables. Additionally, many researchers employ multidimensional spectroscopy to further identify metabolites in a sample (<ref type="bibr" target="#b8">De Graaf, 2007</ref>). In this data, each sample consists of a matrix of spectroscopy variables. Sparse non-negative GPCA can be applied to this multidimensional data in a straightforward manner by vectorizing the matrix of variables and employing a 2D kernel smoother over the lattice of variables. As a future area of research, one can also extend our methods to tensors or higher order PCA to find patterns and achieve dimension reduction for this multidimensional metabolomics data. In addition to metabolomics data, our methods are general and hence applicable to a variety of other structured biomedical data. As the dependencies of the noise must be known to construct the quadratic operator, our methods can be used to find patterns in data where these noise dependencies are well established. Possible further applications of our methods then include copy number variation and methylation data in which variables strongly depend on known chromosomal location, and microscopy, neuroimaging and other bio-medical imaging data in which pixels are spatially correlated with adjacent pixels. In conclusion, we have developed a novel modification of PCA particularly suited to the challenges associated with analyzing NMR data. While our methods show numerous advantages in the analysis of metabolomics data, there are still many open research problems and potential extensions related to our work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>[10:08 4/10/2011 Bioinformatics-btr522.tex] Page: 3031 3029–3035</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.1.</head><figDesc>Fig. 1. Scatter plots of normalized sample PCs for the neural cell types data. Results from PCA, GPCA, Sparse Non-Negative PCA (SPCA) and Sparse Non-Negative GPCA (SGPCA) are compared for the five neural cell types. Sparse methods (bottom rows) demonstrate clearer separation of samples from different cell types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.2.</head><figDesc>Fig. 2. Amount of variance explained by the PCs for the five neural cell type data. Comparison of the percentage of variance explained by individual PCs (top panel) and cumulative percentage of variance explained (bottom) between PCA and GPCA (left), and sparse non-negative PCA and sparse non-negative GPCA (right). GPCA methods explain larger proportions of the sample variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.3.</head><figDesc>Fig. 3. Proportion of features selected on the five neural cell types data by sparse non-negative PCA and GPCA for individual PCs (top) and by the cumulative PCs (bottom). Sparse non-negative GPCA explains more of the sample variance with fewer features selected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.4.</head><figDesc>Fig. 4. Sparse non-negative GPCA loadings and sample PC heatmaps for the first seven PCs, which explain over 90% of the sample variance. Scaled PC loadings are superimposed on the average scaled spectra of neural stem cells, neurons, microglia and 'Glia', which includes oligodendrocytes and astrocytes. Sparse non-negative GPCA loadings reveal important patterns across the samples and spikes in the loadings denote the location of peaks that vary greatly across the samples. For example, PC3 exhibits peaks that have higher intensities in neural stem cells, while the peaks selected by PC5 have higher concentrations in microglia.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>Proposition 1. LetˆvLetˆ Letˆv be the minimizer of the following: minimize</figDesc><table>v 

1 
2 
||X T u−v|| 2 
R −λ||v|| 1 subject to v ≥ 0. 
(3) 

Then, the coordinate updates, u  *  and v  *  , maximizing the single-factor sparse 
non-negative GPCA problem, (2), are given by: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>Proposition 2. The solution to (3) can be obtained via coordinate descent with updates: ˆ v j = 1 R jj R rj X T u−R j, =jˆv=jˆ =jˆv =j −λ + , where R rj denotes the row elements of column j of R and () + denotes the positive part.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>The amount of sparsity in the GPCA loading vectors, v, is controlled by the regularization parameter, λ. We seek a data-driven mechanism for selecting the amount of sparsity in each of the components. To this end, we employ the λ value that minimizes the following Bayesian Information Criterion (BIC) for each factor, v k : BIC(λ) = log ||X−d k u k v T k || 2 R /np + log(np)</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>i) The proportion of variance explained by the k-th GPC is v T k RX T XRv k /tr(XRX T ). (ii) Define V k =[v 1 ,...v k ] and X k = XRV k V T k RV k −1 V T k. Then, the cumulative proportion of variance explained by the k-th sparse non-negative GPC is tr(X k RX T k )/tr(XRX T ).</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>Table 1.</figDesc><table>Locations in parts per million (ppm) of the most important peaks 
identified by the first seven sparse non-negative GPCA loadings 

Peak location (ppm) Cell types 
Metabolites 

0.96 
Neuron, microglia 
1.19 
Microglia, neuron 
1.28 
Neural stem cell, ogliodendrocyte 
Lipid moiety 
1.48 
Ogliodendrocyte 
2.02 
Neuron 
NAA 
2.65 
Astrocyte 
3.01 
Neuron 
3.04 
Ogliodendrocyte 
Creatine 
3.23 
Ogliodendrocyte, neuron, astrocyte Choline 
3.43 
Ogliodendrocyte 
3.66 
Microglia 

</table></figure>

			<note place="foot">© The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="3"> RESULTS We evaluate the utility of GPCA and Sparse Non-Negative GPCA for metabolomics through comparisons on real NMR data. (Simulation studies are given in the Supplementary Materials.) We use a dataset with 27 samples acquired by in vitro 1D H-NMR on five neural cell types: neurons, neural stem cells, microglia, astrocytes and oligodendrocytes (Manganas et al., 2007). [For methodology used on cell culturing, see Manganas et al. (2007)] The data are preprocessed in the traditional manner (Dunn et al., 2005): after acquisition, functional spectra is discretized by binning variables into bins of size 0.04 ppms yielding a total of 2394 variables. For each sample, the spectra are baseline corrected and normalized to their integral. Before applying multivariate techniques, the variables are standardized to have mean zero and variance one. While typically PCA is applied to unsupervised or unlabeled data, we apply our methods to this labeled data so that we may test their performance in terms of sample exploration, dimension reduction, pattern recognition and feature selection when the biological relationships between samples clear. We compare our GPCA method to traditional PCA and our sparse non-negative GPCA method to sparse non-negative PCA. The later is implemented via Algorithm 1 by setting R = I. The BIC method is used to select penalty parameters for both sparse PCA methods and the first 15 PCs are calculated for all methods. For the GPCA methods, the quadratic operator, R, was taken to be a Gaussian kernel smoother with smoothing parameter, γ = 20. Five possible values of γ were considered, γ = 5,10,15,20,25, with γ chosen to explain the most sample variance. In Figure 1, we compare scatter plots of the normalized sample PCs for the four methods. Notice that the scatterplots of all methods exhibit clustering of the neuron and neural stem cell samples, while the other cell types are more scattered. Sparse methods and especially sparse non-negative GPCA, however, cluster the remaining cell types better, illustrating the utility of incorporating sparsity in high-dimensional data analysis. Next, we compare the methods in terms of dimension reduction in Figure 2. As sparse PCA methods naturally explain less sample variance than PCA methods, we compare the two sets of methods separately. Also note that as sparse PCA methods calculate components in a greedy manner, they are not necessarily ordered in terms of how much variance they explain. Overall, by incorporating the known structure of spectroscopy data into the PCA problem, the GPCA methods explain a larger portion of the sample variance. Thus, the reduction of dimensions for GPCA methods is greater. This behavior is especially pronounced for the sparse non-negative methods where seven PCs explain over 90% of the variance for sparse non-negative GPCA, while 15 PCs are needed to explain the same amount of variance for sparse non-negative PCA. Thus, sparse non-negative GPCA provides over 50% more dimension reduction than sparse non-negative PCA. GPCA methods demonstrate a clear advantage over traditional PCA methods in terms of variance explained and dimension reduction.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">1010</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>btr522. .tex]</note>
</biblStruct>

<biblStruct   xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName>
				<surname>Page</surname>
			</persName>
		</author>
		<imprint>
			<biblScope unit="page" from="3029" to="3035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="m">Sparse non-negative GPCA the NIH Intellectual and Developmental Disabilities Research Grant (P30HD024064) (to M.M.-S.)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<monogr>
		<title level="m" type="main">Conflict of Interest: none declared REFERENCES</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<monogr>
		<title level="m" type="main">A generalized least squares matrix decomposition</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">I</forename>
				<surname>Allen</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">NMR-based metabonomic approaches for evaluating physiological influences on biofluid composition</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Bollard</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NMR Biomed</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="143" to="162" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">NMR-based metabolic profiling and metabonomic approaches to problems in molecular toxicology</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Coen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chem. Res. Toxicol</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="9" to="27" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Curve-fitting method for direct quantitation of compounds in complex biological mixtures using 1h NMR: application in metabonomic toxicology studies</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Crockford</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Anal. Chem</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="4556" to="4562" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<monogr>
		<title level="m" type="main">In Vivo NMR Spectroscopy: Principles and Techniques</title>
		<author>
			<persName>
				<forename type="first">De</forename>
				<surname>Graaf</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">A</forename>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>West Sussex, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Measuring the metabolome: current analytical technologies</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Dunn</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analyst</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="606" to="625" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Bioinformatic methods in NMR-based metabolic profiling</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Ebbels</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Cavill</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in Nuclear Magnetic Resonance Spectroscopy</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="361" to="374" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Regularization paths for generalized linear models via coordinate descent</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Friedman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Softw</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Metabolomics by numbers: acquiring and understanding global metabolite data</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Goodacre</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Biotechnol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="245" to="252" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Metabolomics: current technologies and future trends</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Hollywood</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteomics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="4716" to="4723" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Metabolic phenotyping in health and disease</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Holmes</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="714" to="717" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization with sparseness constraints</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Hoyer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1457" to="1469" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">On consistency and sparsity for principal components analysis in high dimensions</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Johnstone</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Lu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="682" to="693" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">A modified principal component technique based on the LASSO</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Jolliffe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graph. Stat</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="531" to="547" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalized power method for sparse principal component analysis</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Journée</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="517" to="553" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparse non-negative matrix factorizations via alternating non-negativity-constrained least squares for microarray data analysis</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Kim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Park</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">1495</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by non-negative matrix factorization</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Lee</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Seung</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Biclustering via sparse singular value decomposition</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Lee</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="1087" to="1095" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Metabolomics of neural progenitor cells: a novel approach to biomarker discovery</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Maleti´cmaleti´c-Savati´csavati´c</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cold Spring Harb. Symp. Quant. Biol</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="389" to="401" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Magnetic resonance spectroscopy identifies neural progenitor cells in the live human brain</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Manganas</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="page" from="318" to="980" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Systems biology: metabonomics</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Nicholson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Lindon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">455</biblScope>
			<biblScope unit="page" from="1054" to="1056" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">NITPICK: peak identification for mass spectrometry data</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Renard</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">355</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization for rapid recovery of constituent spectra in magnetic resonance chemical shift imaging of the brain</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Sajda</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Imag. IEEE Trans</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1453" to="1465" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Sparse principal component analysis via regularized low rank matrix approximation</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Shen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Multivar. Anal</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1015" to="1034" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">The solution path of the generalized lasso</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Taylor</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1335" to="1371" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Metabolomics: from pattern recognition to biological interpretation</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Ulrich</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomagresbank. Nucleic Acids Res. Drug Discov. Today</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1551" to="1558" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Suppl. . 1),</note>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Targeted profiling: quantitative analysis of 1h NMR metabolomics data</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Weljie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Anal. Chem</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="4430" to="4442" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">M</forename>
				<surname>Witten</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="515" to="534" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Nonnegative sparse PCA</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Zass</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Shashua</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Informat. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">1561</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Identification and quantification of metabolites in 1H NMR spectra by Bayesian model selection</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Zheng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="27" to="1637" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">Sparse principal component analysis</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Zou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graph. Stat</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="265" to="286" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>