
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fragment-free approach to protein folding using conditional neural fields</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010">2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Feng</forename>
								<surname>Zhao</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Jian</forename>
								<surname>Peng</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName>
								<forename type="first">Jinbo</forename>
								<surname>Xu</surname>
							</persName>
							<email>jinboxu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fragment-free approach to protein folding using conditional neural fields</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">26</biblScope>
							<biblScope unit="page" from="310" to="317"/>
							<date type="published" when="2010">2010</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btq193</idno>
					<note>[12:00 12/5/2010 Bioinformatics-btq193.tex] Page: i310 i310–i317 BIOINFORMATICS</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: One of the major bottlenecks with ab initio protein folding is an effective conformation sampling algorithm that can generate native-like conformations quickly. The popular fragment assembly method generates conformations by restricting the local conformations of a protein to short structural fragments in the PDB. This method may limit conformations to a subspace to which the native fold does not belong because (i) a protein with really new fold may contain some structural fragments not in the PDB and (ii) the discrete nature of fragments may prevent them from building a native-like fold. Previously we have developed a conditional random fields (CRF) method for fragment-free protein folding that can sample conformations in a continuous space and demonstrated that this CRF method compares favorably to the popular fragment assembly method. However, the CRF method is still limited by its capability of generating conformations compatible with a sequence. Results: We present a new fragment-free approach to protein folding using a recently invented probabilistic graphical model conditional neural fields (CNF). This new CNF method is much more powerful than CRF in modeling the sophisticated protein sequence-structure relationship and thus, enables us to generate native-like conformations more easily. We show that when coupled with a simple energy function and replica exchange Monte Carlo simulation, our CNF method can generate decoys much better than CRF on a variety of test proteins including the CASP8 free-modeling targets. In particular, our CNF method can predict a correct fold for T0496_D1, one of the two CASP8 targets with truly new fold. Our predicted model for T0496 is significantly better than all the CASP8 models. Contact:</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Despite significant progress in recent years, ab initio protein folding is still one of the most challenging problems in computational structural biology. Fragment-based ab initio protein folding (<ref type="bibr" target="#b2">Bowie and Eisenberg, 1994;</ref><ref type="bibr" target="#b5">Claessens et al., 1989;</ref><ref type="bibr" target="#b18">Jones and Thirup, 1986;</ref><ref type="bibr" target="#b23">Levitt, 1992;</ref><ref type="bibr" target="#b34">Simon et al., 1991;</ref><ref type="bibr" target="#b36">Sippl, 1993;</ref><ref type="bibr" target="#b38">Unger et al., 1989;</ref><ref type="bibr" target="#b39">Wendoloski and Salemme, 1992</ref>) and lattice-models (<ref type="bibr" target="#b21">Kihara et al., 2001;</ref><ref type="bibr" target="#b41">Xia et al., 2000;</ref><ref type="bibr" target="#b45">Zhang et al., 2003</ref>) has been extensively studied. These two popular methods and their combination for protein modeling have achieved great success in critical assessment of structure prediction (CASP) competitions (<ref type="bibr" target="#b28">Moult et al., 2003</ref><ref type="bibr" target="#b30">Moult et al., , 2007</ref>). For example, the widely-used fragment assembly program Rosetta (<ref type="bibr" target="#b25">Misura et al., 2006;</ref><ref type="bibr" target="#b35">Simons et al., 1997</ref>) is one of the most successful ab initio protein folding programs. The TASSER program (<ref type="bibr" target="#b43">Zhang and Skolnick, 2005</ref>) and its derivative Zhang-Server (<ref type="bibr" target="#b40">Wu et al., 2007</ref>) have achieved outstanding * To whom correspondence should be addressed. performance in both CASP7 and CASP8 by combining lattice model and threading-generated fragments and distance restraints. Although fragment-based ab initio protein folding demonstrates encouraging performance, several important issues remain with this method. First, there is no guarantee that the local conformations of a protein can be accurately covered by short structural fragments in the PDB since a protein with new fold is likely to be composed of some structural motifs that rarely occur in the PDB (Andras Fiser, CASP8 talk). Second, the conformation space defined by a fragment library is discrete in nature. This discrete nature may exclude the native fold from the conformational search space since even a slight change in backbone angles, especially in the middle region of a protein, can result in a totally different fold. To resolve these two limitations, this article will propose a fragment-free folding method that can efficiently explore protein conformations in a continuous space. In literature there are quite a few fragment-free methods for ab initio. protein folding. For example, Joe et al. described an iterative folding method (<ref type="bibr" target="#b7">DeBartolo et al., 2009</ref>), which folds a protein by mimicking folding pathway and explores the conformation space by directly sampling the backbone angles using a trimer library. Shakhnovich group also described a method that can directly sample backbone angles using a trimer library (<ref type="bibr" target="#b4">Chen et al., 2007;</ref><ref type="bibr" target="#b42">Yang et al., 2007</ref>).<ref type="bibr" target="#b11">Faraggi et al. (2009)</ref>first predict the backbone angles of a protein using a machine learning method and then explore protein conformation search space using a genetic algorithm, based upon the predicted backbone angles. Recently,<ref type="bibr">Hamelryck et al.</ref>have developed two hidden Markov models (HMMs) (i.e. FB5-HMM and Torus-HMM) (<ref type="bibr" target="#b1">Boomsma et al., 2008;</ref><ref type="bibr" target="#b15">Hamelryck et al., 2006</ref>) for fragment-free conformation sampling. Using a Torus-HMM model, they can generate local conformations as accurately as the fragment assembly method (<ref type="bibr" target="#b1">Boomsma et al., 2008</ref>). However, these HMM models have not been applied to realworld ab initio folding yet. Recently, we have proposed a protein conformation sampling algorithm based on conditional random fields (CRF) (<ref type="bibr" target="#b46">Zhao et al., 2008</ref><ref type="bibr" target="#b47">Zhao et al., , 2009</ref>) and directional statistics. The CRF model is a generalization of the HMM models and much more powerful than HMM. Our CRF model can accurately describe the complex sequence-angle relationship and estimate the probability distribution of (virtual) backbone angles directly from sequence information and predicted secondary structure. We have shown that by using the CRF models, we can sample protein conformations with much better quality than FB5-HMM (<ref type="bibr" target="#b46">Zhao et al., 2008</ref>). We have also shown that by coupling our CRF model with a simple energy function, our method compares favorably with fragment assembly in the CASP8 blind prediction (<ref type="bibr" target="#b47">Zhao et al., 2009</ref>). This article presents a new probabilistic graphical model conditional neural fields (CNFs) for ab initio protein folding. CNF is recently invented by our group for the modeling of sequential data. See<ref type="bibr" target="#b31">Peng et al. (2009)</ref>for a detailed exposition. CNF isPage: i311 i310–i317</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Protein folding using conditional neural fields</head><p>similar to but much more powerful than CRF in that CNF can naturally model the non-linear relationship between input and output while CRF cannot do so. Thus, CNF can model better the sophisticated relationship between backbone angles, sequence profile and predicted secondary structure, estimate the probability distribution of backbone angles more accurately and sample protein conformations more efficiently. In addition, this work also differs from our previous CRF method (<ref type="bibr" target="#b46">Zhao et al., 2008</ref><ref type="bibr" target="#b47">Zhao et al., , 2009</ref>) in that (i) instead of using a simulated annealing (SA) method for folding simulation, we developed a replica exchange Monte Carlo (REMC) method for folding simulation. The REMC method enables us to minimize energy function to a lower level and thus possibly produce better decoys. (ii) Our previous CRF method uses the positionspecific frequency matrix (PSFM) generated by PSI-BLAST as the input. This work will use the position-specific scoring matrix (PSSM) generated by PSI-BLAST as the input of our CNF model. It has been proved that PSSM contains more information than PSFM for structure prediction such as secondary-structure prediction. We did not use PSSM with CRF because CRF cannot easily take PSSM as input. In contrast, we can easily feed PSSM into our CNF model. We will show that our new method is much more effective than our previous method and can dramatically improve sampling efficiency and we can generate much better decoys than before on a variety of test proteins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Continuous representation of conformations</head><p>In our previous work (<ref type="bibr" target="#b46">Zhao et al., 2008</ref><ref type="bibr" target="#b47">Zhao et al., , 2009</ref>), we used a simplified representation of a protein model and demonstrated that even with such a representation, we can achieve good folding performance. In this simplified representation only the main-chain and C β atoms are considered. This work will continue to use such a simplified representation. That is, we assume that the distance between two adjacent C α atoms is constant and represent the C α-trace of a protein using a set of pseudo backbone angles (θ, τ). Given a residue at position i, its θ is defined as the pseudo bond angle formed by the C α atoms at positions i−1, i and i+1; τ is a pseudo dihedral angle around virtual bond between i−1 and i and can be calculated from the C α atoms at positions i−2, i−1, i and i+1. Therefore, given the first three C α positions and sub-sequential (θ, τ) angles, we can build the C α trace of a protein. Using the C α trace, we then can build the coordinates for the main chain and C β atoms using a method similar to BBQ (<ref type="bibr" target="#b14">Gront et al., 2007</ref>). To employ the KMB hydrogen-bonding energy (<ref type="bibr" target="#b26">Morozov et al., 2004</ref>) for β-containing proteins, we also build the backbone hydrogen atoms using a quick and dirty method (<ref type="bibr" target="#b3">Branden and Tooze, 1999</ref>). The preferred conformations of a residue in the protein backbone can be described as a probabilistic distribution of (θ, τ). Each (θ, τ) corresponds to a unit vector in the three-dimensional space (i.e. a point on a unit sphere surface). We can use the five-parameter Fisher–Bingham (FB5) distribution to model the probability distributions over unit vectors (<ref type="bibr" target="#b20">Kent, 1982</ref>). FB5 is the analog on the unit sphere of the bivariate normal distribution with an unconstrained covariance matrix. The probability density function of the FB5 distribution is given by</p><formula>f (u) = 1 c κ,β exp κγ 1 ·u+β γ 2 ·u 2 − γ 3 ·u 2 ,</formula><p>where u is a unit vector variable and c(κ,β) is a normalizing constant. The parameters κ and β determine the concentration of the distribution and the ellipticity of the contours of equal probability, respectively. The higher κ and β are, the more concentrated and elliptical the distribution is, respectively.</p><p>The three vectors γ 1 , γ 2 and γ 3 are the mean direction, the major and minor axes, respectively. The latter two vectors determine the orientation of the equal probability contours on the sphere, while the first vector determines the common center of the contours. We cluster all the (θ, τ) angles in a set of ∼3000 non-redundant proteins with high-resolution X-ray structures into 100 groups. Then we calculate the FB5 distribution of each group using KentEstimator (<ref type="bibr" target="#b15">Hamelryck et al., 2006</ref>). See<ref type="bibr" target="#b46">Zhao et al. (2008)</ref>for a detailed description of how we calculate the FB5 distributions. Once we have the distribution of (θ, τ) at one residue, we can sample the real-valued (θ, τ) angles by probability and thus, explore protein conformations in a continuous space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A second-order CNF model of conformation space</head><p>Previously we developed a CRF method for protein conformation sampling (<ref type="bibr" target="#b46">Zhao et al., 2008</ref><ref type="bibr" target="#b47">Zhao et al., , 2009</ref>). This CRF method uses a linear combination of input features (i.e. PSI-BLAST sequence profile and predicted secondary structure) to estimate the probability distribution of backbone angles. This kind of linear parameterization implicitly assumes that all the features are linearly independent, which contradicts with the fact that some input features are highly correlated. For example, the predicted secondary structure is correlated with sequence profiles since the former is usually predicted from the latter using tools such as PSIPRED (<ref type="bibr" target="#b17">Jones, 1999</ref>). To model the correlation between predicted secondary structure and sequence profiles, an easy way is to explicitly enumerate all the possible combinations of secondary-structure type and amino acid identity in the linear CRF model. In fact, we can always combine some basic features to form a complex feature. However, explicitly defining complex features may introduce a number of serious issues. First, it will result in a combinatorial explosion in the number of complex features, and hence, in the model complexity. It is challenging to train a model with a huge number of parameters without overfitting. Second, explicit enumeration may miss some important complex features. For example, the CRF model presented in<ref type="bibr" target="#b46">Zhao et al. (2008</ref><ref type="bibr" target="#b47">Zhao et al. ( , 2009</ref>) does not accurately model the correlation among sequence information at several adjacent positions. Finally, explicit enumeration of complex features may also introduce a large number of unnecessary features, which will increase the running time of probability estimation. Instead of explicitly enumerating all the possible non-linear combinations of the basic sequence and structure features, we can use a better graphical model to implicitly account for the non-linear relationship between sequence and structure. Very recently, we have developed a new probabilistic graphical model CNF (<ref type="bibr" target="#b31">Peng et al., 2009</ref>), which can implicitly model non-linear relationship between input and output. As shown in<ref type="figure">Figure 1</ref>, CNF consists of at least three layers: one or more hidden layers, input (i.e. sequence profile and secondary structure) and output (i.e. backbone angles) while CRF consists of only two layers: input and output. The relationship between the backbone angles and the hidden layer is still linear. However, the hidden layer uses some gate functions to non-linearly transform the input features into complex features. Here we use</p><formula>G θ (x) = 1 1+exp −θ T x</formula><p>as the gate function where θ is the parameter vector and x a feature vector. CNF can also be viewed as the seamless integration of CRF and neural networks (NN). The neurons in the hidden layer will automatically extract non-linear relationship among input features. Therefore, without explicit enumeration, CNF can directly model non-linear relationship between input and output. The training of a CNF model is similar to that of a CRF, but more complicated. We have tested this CNF model for protein secondary-structure (SS) prediction from sequence profiles.<ref type="figure" target="#tab_1">Table 1</ref>compares the performance of various machine learning methods for SS prediction. The results are averaged on a 7-fold cross-validation on the CB513 data set, except that SPINE uses 10-fold cross-validation. As shown in<ref type="figure" target="#tab_1">Table 1</ref>, by using only one hidden layer to model non-linear relationship between output and input, CNF achieves almost 10% relative improvement over CRF. CNF also outperforms other methods including SVMpro (<ref type="bibr" target="#b16">Hua and Sun, 2001</ref>), SVMpsi (<ref type="bibr" target="#b22">Kim and Park, 2003</ref>), YASSPP (<ref type="bibr" target="#b19">Karypis, 2006</ref>), PSIPRED (<ref type="bibr" target="#b17">Jones, 1999</ref>), SPINE (<ref type="bibr" target="#b9">Dor and Zhou, 2007</ref>) and TreeCRFpsi (<ref type="bibr" target="#b8">Dietterich et al., 2004</ref>). The linear CRF is the worst since it does not model non-linear relationship between secondary i311Page: i312 i310–i317<ref type="figure">Fig. 1</ref>. A first-order CNF model consists of three layers: input, output and hidden layer. A second-order model is similar but not shown for the purpose of simplicity. In contrast, a CRF model consists of only input and output.structure and sequence profile. This result indicates that we can indeed benefit from modeling non-linear sequence-structure relationship. We expect that using CNF, we are able to more accurately model sequence–angle relationship and thus, to sample conformations more efficiently. In the context of CNF, the PSI-BLAST sequence profile (i.e. PSSM) and predicted secondary structure are viewed as observations; the backbone angles and their FB5 distributions are treated as hidden states or labels. Let H denote the 100 groups (i.e. states or labels) generated from clustering of the backbone angles. Each group is described by an FB5 distribution. Given a protein with solved structure, we calculate its backbone angles at each position and determine one of the 100 groups (i.e. states or labels) to which the angles at each position belong.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.Zhao et al.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><formula>Let S = {s 1 ,s 2 ,...,s N } s i ∈ H</formula><p>denote such a sequence of states/labels (i.e. FB5 distributions) for this protein. We also denote the sequence profile of this protein as M and its secondary structure as X. As shown in<ref type="figure">Figure 1</ref>, our CNF model defines the conditional probability of S given M and X as follows:</p><formula>P S | M,X = exp N i=1 F S,M,X,i Z M,X where = λ 1 ,λ 2 ,...,λ p is the model parameter and Z M,X = S exp N i=1 F S,M,X,i</formula><p>is a normalization factor summing over all the possible labels for the given M and X. F S,M,X,i consists of two edge feature functions and one label feature function at position i. It is given by</p><formula>F(S,M,X,i) = e 1 s i−1 ,s i +e 2 s i−1 ,s i ,s i+1 + i+w j=i−w v s i−1 ,s i ,M j ,X j where e 1 s i−1 ,s i and e 2 s i−1 ,s i ,s i+1</formula><p>are the first-and second-order edge feature functions, respectively, and v s i−1 ,s i ,M j ,X j is the label feature function. The edge functions describe the interdependency between two or three neighboring labels. CNF is different from CRF in the label feature function. In CRF, the label feature function is defined as a linear combination of features. In CNF, there is an extra hidden layer between the input and output, which consists of K gate functions (see<ref type="figure">Fig. 1</ref>). The K gate functions extract a K-dimensional implicit non-linear representation of input features. Therefore, CNF can be viewed as a CRF with its inputs being K homogeneous hidden feature-extractors at each position. The label feature function of CNF is defined as follows:</p><formula>v s i−1 ,s i ,X,M = K g=1 w s i−1 ,s i ,g G θg f X,M,i .</formula><p>That is, the label feature function is a linear combination of K gate functions G. In the above definition, w is the parameter vector and f is a vector of basic features at position i. In our current implementation, f contains 23×Page: i313 i310–i317</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Protein folding using conditional neural fields</head><p>models. Later we will show that CNF can do conformation sampling much better than CRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Conformation sampling and resampling</head><p>Using the trained CNF model, we can sample the whole conformation of a protein or propose a new conformation from an existing one by resampling the local conformation of a segment. This procedure is very similar to the conformation sampling algorithm in our CRF method (<ref type="bibr" target="#b46">Zhao et al., 2008</ref><ref type="bibr" target="#b47">Zhao et al., , 2009</ref>). That is, we can use the forward–backward algorithm to first sample labels (i.e. angle distribution) by probability estimated from our CNF model and then sample real-valued angles from the labels. See<ref type="bibr" target="#b46">Zhao et al. (2008)</ref>for a detailed description of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">REMC simulation</head><p>The energy function we used for folding simulation consists of three items: DOPE (a pairwise statistical potential) (<ref type="bibr" target="#b13">Fitzgerald et al., 2007</ref>; Shen and<ref type="bibr" target="#b32">Sali, 2006</ref>), KMBhbond (hydrogen bonding energy) (<ref type="bibr" target="#b26">Morozov et al., 2004</ref>) and ESP (a simplified solvent accessibility potential) (<ref type="bibr" target="#b12">Fernandez et al., 2002</ref>). We use the weight factors previously trained for the CRF model for these three energy items. Therefore, the energy function is not biased towards our CNF method. The weight factor for DOPE is always fixed to 1, so only two weight factors shall be determined. See<ref type="bibr" target="#b47">Zhao et al. (2009)</ref>for a detailed description of weight determination. Previously we employ a SA algorithm to minimize energy function, based upon the algorithm proposed by<ref type="bibr" target="#b0">Aarts and Korst (1991)</ref>. In this work, we employ a REMC method (<ref type="bibr" target="#b10">Earl and Deem, 2005</ref>; Swendsen and<ref type="bibr" target="#b37">Wang, 1986</ref>) to minimize energy function. By using REMC, we can minimize energy function to lower values and thus produce better decoys for most of our test proteins. Our REMC method employs 20 replicas and the highest temperature is set to 100. The temperature for replica i (i = 1,2,...,20) is set to 5i. We have also tested other temperature assignment, but have not seen much difference in terms of folding performance. Each replica consists of 24 000 time steps. At each time step a new conformation is proposed and then accepted with probability min 1,exp −E/T i where E is the energy difference between the new and old conformations and T i is the temperature for this replica. The conformations between two neighboring replicas are exchanged every 30 time steps. Therefore, in total 800 conformation exchange events will happen between two neighboring replicas during the whole folding simulation. It will make our simulation process very inefficient if we yield only the decoy with the lowest energy at the end of the folding simulation. To generate more decoys from a single folding simulation, we output the final decoy of each replica as long as it has an energy value within 15% of the lowest energy we can achieve. Experimental results indicate that on average, each folding simulation can generate ∼10 decoys.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>Since in our previous work (<ref type="bibr" target="#b47">Zhao et al., 2009</ref>), we have demonstrated that our CRF method compares favorably with the popular fragment-based Robetta server in the CASP8 blind prediction, in this article we will focus on the comparison between our CNF and CRF methods, and show that our nre method is indeed superior over our previous method. We test our new method using two datasets and compare it with our previous method. These two datasets were used to evaluate our previous method before. The first dataset consists of 22 proteins: 1aa2, 1beo, 1ctfA, 1dktA, 1enhA, 1fc2C, 1fca, 1fgp, 1jer, 1nkl, 1pgb, 1sro, 1trlA, 2croA, 2gb1A, 4icbA, T052, T056, T059, T061, T064 and T074. These proteins have very different secondary-structure type and their sizes range from 40 to 120 residues. Some proteins (e.g. T052, T056, T059, T061, T064 and T074) in this dataset are very old CASP targets. Therefore, we denote this dataset as 'old testset'. The second dataset contains 12 CASP8 free-modeling targets: T0397_D1, T0405_D1, T0405_D2, T0416, T0443_D1, T0443_D2, T0465, T0476, T0482, T0496_D1, T0510_D3 and T0513_D2. These proteins are called free-modeling targets because a structurally similar template cannot be identified for them using a template-based method. We denote this dataset as 'CASP8 testset'. To avoid bias, we removed all the proteins similar to the first dataset from our training set (see Section 2.3). Since the training set was constructed before CASP8 started, there is no overlap between our training data and the CASP8 testset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Performance on the old testset</head><p>As shown in<ref type="figure" target="#tab_2">Table 2</ref>, we evaluate our CNF and CRF methods in terms of their capability of generating good decoys. We run both methods on each test protein and generate similar number of decoys (5000–10 000). Each decoy is compared to its native structure and RMSD to the native is calculated for this decoy. Then we rank all the decoys of one test protein in an ascending order by RMSD. Finally we calculate the average RMSD of the top 1, 2, 5 and 10% decoys, respectively. We do not compare these two methods using the best decoys because they may be generated by chance and usually the more decoys are generated, the better the best decoys will be. In terms of the average RMSD of the top 5 or 10% decoys, our CNF method outperforms the CRF method on all test proteins except 1ctfA, 1dktA, 1fc2C and 1fgp. The CNF method reduces the average RMSD of top 10% decoys by at least 1 Å for many proteins such as 1aa2, 1beo, 1fca, 1pgb, 1sro, 2gb1A, 4icbA, T052, T056, T059, T061 and T064. Furthermore, our CNF method dramatically reduces the average RMSD of top 10% decoys for some proteins. For example, our CNF method reduces the average RMSD of top 10% decoys for 4icbA from 8.0 to 5.2 Å, for T056 from 11.1 to 7.2 Å and for T061 from 7.6 to 5.6 Å. Even for some test proteins (e.g. 1enhA, 1pgb and 2gb1A) on which the CRF method has already performed well, our CNF method still improves a lot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance on the CASP8 testset</head><p>To further compare our CRF and CNF methods, we also evaluate them on the 12 CASP8 free-modeling (FM) targets, as shown in<ref type="figure" target="#tab_3">Table 3</ref>. During the CASP8 competition, structurally similar templates cannot be identified for these targets. Similarly, we evaluate both methods in terms of the average RMSD of the top 1, 2, 5 and 10% decoys, respectively. Compared to CRF, our CNF method does not significantly worsen the decoy quality of any of the 12 CASP8 targets. Instead, our CNF method outperforms the CRF method on 10 of the 12 targets and yields slightly worse performance on another two targets: T0397_D1 and T0482. In particular, our CNF method reduces the average RMSD of the top 10% decoys by at least 1 Å for the following seven targets: T0405_D1, T0405_D2, T0416_D2, T0443_D2, T0476, T0496_D1 and T0510_D3. Our CNF method reduces the average RMSD of top 10% decoys for T0510_D3 from 9.1 to 6.3 Å and for T0496_D1 from 10.1 to 8.1 Å. Even for T0416_D2, a target on which our CRF method performed well, our CNF method improves the average RMSD of the top 10% decoys by 1 Å. We have also examined the average TMscore/GDT-TS of the top 10% decoys, on average our CNF method is better than the CRF method by ∼10% (data not shown due to space limitation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>i313</head><formula>9 7 .3 7 .7 8 .1 7β R 6.9 8 .4 8 .7 9 .2 9 .6 T061 76 N 2.8 3 .4 3 .7 4 .6 5 .6 4α R 5.9 6 .6 6 .8 7 .2 7 .6 T064 103 N 6.5 7 .0 7 .2 7 .5 7 .9 8α R 5.9 7 .1 7 .5 8 .2 8 .9 T074 98 N 3.7 5 .0 5 .4 5 .9 6 .3 4α R 5.0 6 .0 6 .4 6 .7 6 .9</formula><p>Column 's/t' lists the size and secondary-structure content of the test proteins. Column 'M' indicates methods. 'N' and 'R' represent the CNF and CRF methods, respectively. Column 'x%' lists the average RMSD (Å) of the decoys among the top x% of the generated decoys. Column 'best' lists the RMSD of the best decoys.</p><p>We have also examined the relationship between RMSD and energy. Due to space limitation, here we only visualize the RMSD-energy relationship for several typical targets: T0397_D1, T0416_D2, T0476, T0482, T0496_D1 and T0510_D3, as shown in<ref type="figure" target="#fig_3">Figure 2</ref>. Note that in the figure, we normalize the energy of a decoy by the mean and SD calculated from the energies of all the decoys of one target. By energy normalization, we can clearly see the energy difference between the decoys generated by the CNF/CRF methods.<ref type="figure" target="#fig_3">Figure 2</ref>clearly demonstrates that our CNF method can generate</p><formula>(a) T0397_D1 (b) T0416_D2 (c) T0476 (d) T0482</formula><p>(e) T0496_D1 (f) T0510_D3</p><formula>) for (a) T0397_D1, (b) T0416_D2, (c) T0476, (d) T0482, (e) T0496_D1 and (f)</formula><p>T0510_D3. The red and blue colors represent the CRF and CNF methods, respectively. See text for the energy normalization methods. decoys with much lower energy than the CRF method. However, decoys with lower energy might not have better quality if the correlation between RMSD and energy is very weak. For example, our CNF method can generate decoys for T0397_D1 and T0482 with much lower energy, but cannot improve decoy quality for them. To improve the decoy quality for T0397_D1 and T0482, we have to improve the energy function. In contrast, the correlation between RMSD and energy is positive for T0416_D2, T0476, T0496_D1 and T0510_D3. Therefore, we can improve decoys quality for these four targets by generating decoys with lower energy. Our CNF method dramatically improves the decoy quality on T0416_D2 over the CRF method, as shown in<ref type="figure" target="#fig_3">Figure 2b</ref>. The underlying reason is that our CNF method can estimate the backbone angle probability more accurately. Around half of the decoys generated by the CRF method for T0416_D2 are the mirror images of the other half. These mirror images are introduced by the non-nativelike backbone angles around residue #31, as shown in<ref type="figure" target="#fig_5">Figure 3</ref>. We calculated the marginal probability of the 100 angle states at these residues and found out the native-like angle states have much higher marginal probability in the CNF model than in the CRF model. Thus, our CNF method can sample native-like angles at these residues more frequently than the CRF method and avoid generating a large number of mirror images. In addition to the CNF sampling method, i314Page: i315 i310–i317the rank of the #1 cluster centroid or the best cluster centroid among the first CASP8 server models or all the CASP8 server models, respectively. Column 'Internal rank' lists the percentile ranking (%) of a cluster centroid among all the decoys we generated for the target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Protein folding using conditional neural fields</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with CASP8 models</head><p>In order to compare our method with the CASP8 results, we use MaxCluster 1 to cluster the decoys of the 12 CASP8 FM targets. We ran MaxCluster so that for a given target, the first cluster contains ∼30% of all the decoys and the top five clusters in total cover ∼70% of the decoys. We examine only the top five clusters because CASP8 evaluated at most five models for a FM target. As shown in<ref type="figure" target="#tab_4">Table 4</ref>, we list the GDT-TS of a cluster centroid, its rank among the CASP8 models and its percentile ranking among all the decoys we generated. As shown in this table, our method did pretty well on T0405_D1, T0416_D2, T0443_D1, T0476, T0496_D1, T0510_D3 and T0513_D2; reasonably well on T0397_D1, T0405_D2 and T0465; and badly on T0443_D2 and T0482. Roughly speaking, our method can do well on mainly-alpha or small beta proteins, but not well on large beta proteins. This is expected since our CNF method can model well local sequence-structure relationship, but cannot model long-range hydrogen bonding. Note that we generated decoys using domain definition we decided during the CASP8 season. Therefore, our domain definition may not be consistent with the CASP8 official definition. In this case, we calculate the GDT-TS of a model using the native structure common to our domain definition and CASP8 definition. The GDTTS of a model is calculated using the TM-score program and may be slightly different from the CASP8 official GDT-TS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Specific examples</head><p>In CASP8, we did prediction using the CRF method for T0476, T0496_D1 and T0510_D3, but not for T0416_D2 because our CRF method was not ready at the beginning of CASP8. The server model generated by our CRF method for T0510_D3 is among the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.Zhao et al.</head><p>(a) T0416_D2 (b) T0476</p><p>(c) T0496_D1 (d) T0510_D3(d) T0510_D3 (x-axis is percentile ranking and y-axis GDTTS). Our first and best cluster centroids are plotted in black and magenta lines, respectively. The #1 models submitted by the CASP8 server are ordered by their GDT-TS and their percentile ranking is displayed as a cyan curve, so are the best models from each server but as a green curve. best CASP8 server models. 2 Our CNF method further improves predictions for these four targets over the CRF method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">1 T0416_D2</head><p>The first and best cluster centroids have GDTTS 69.3 and 76.8, respectively. As shown in<ref type="figure" target="#fig_6">Figure 4a</ref>, the best cluster centroid is better than all the CASP8 server models. In fact the best cluster centroid is also better than all the CASP8 human models (data not shown). The best cluster centroid also has a small RMSD 2.7 Å.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">T0476</head><p>The first and best cluster centroids have GDT-TS 34.2 and 35.6, respectively. Our first and best cluster centroids for T0476 are ranked No. 4 out of 66 and No. 15 out of 287 CASP8 server models, respectively. The best human model for T0476 has GDT-TS 48.3 and RMSD 7.8 Å. Our best cluster centroid also has RMSD 7.8 Å.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">T0496_D1 According to</head><p>Grishin group, T0496_D1 is one of the only two CASP8 targets representing new folds (<ref type="bibr" target="#b33">Shi et al., 2009</ref>). Our first and best cluster centroids have GDT-TS 30.5 and 49.1, respectively. As shown in<ref type="figure" target="#fig_6">Figure 4c</ref>, the best cluster centroid is significantly better than all the CASP8 server models. In fact the best cluster centroid is also significantly better than all the CASP8 human models. The best CASP8 model has GDT-TS only 33.96. The smallest RMSD among the CASP8 models with 100% coverage is 11.34 Å. Our best cluster centroid has a pretty good RMSD 6.2 Å considering that this target has more than 100 residues. In summary, our CNF method can predict an almost correct fold for this target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">T0510_D3</head><p>The first and best cluster centroids have GDTTS 47.7 and 51.7, respectively. The best cluster centroid has RMSD 6.9 Å. As shown in<ref type="figure" target="#fig_6">Figure 4d</ref>, our first cluster centroid is better than all the #1 models submitted by the CASP8 servers. If all the 321 CASP8 models are considered, our first cluster centroid is worse than only three of them 3 and our best centroid is ranked No. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>This article has presented a new fragment-free approach to protein ab initio folding by using a recently-invented probabilistic graphical model CNF. Our fragment-free approach can overcome some limitations of the popular fragment assembly method. That is, this new method can sample protein conformations in a continuous space while the fragment-based methods cannot do so. This CNF method is also better than our previous CRF method in that (i) this method can easily model non-linear relationship between protein sequence and structure; and (ii) we can also minimize energy function to lower values. Experimental results indicate that our CNF method clearly outperforms the CRF method on most of the test proteins. Previously, we have compared our CRF method with the popular fragment-based Robetta server in the CASP8 blind prediction and shown that our CRF method is on average better than Robetta on mainly-alpha or small beta proteins (<ref type="bibr" target="#b47">Zhao et al., 2009</ref>). This article further confirms our advantage on mainly-alpha or small beta proteins. Since CNF is better than CRF in modeling non-linear sequence-structure relationship, we are going to incorporate more information (such as amino acid physical–chemical property profile) to our model so that we can improve sampling efficiency further. We will also extend our CNF method so that long-range hydrogen bonding can also be modeled.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>[12:00 12/5/2010 Bioinformatics-btq193.tex]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>[12:00 12/5/2010 Bioinformatics-btq193.tex]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>[12:</head><figDesc>00 12/5/2010 Bioinformatics-btq193.tex]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.2.</head><figDesc>Fig. 2. The relationship between RMSD (y-axis) and energy (x-axis) for (a) T0397_D1, (b) T0416_D2, (c) T0476, (d) T0482, (e) T0496_D1 and (f) T0510_D3. The red and blue colors represent the CRF and CNF methods, respectively. See text for the energy normalization methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><figDesc>[12:00 12/5/2010 Bioinformatics-btq193.tex]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.3.</head><figDesc>Fig. 3. Two typical mirror images generated by the CRF method for T0416_D2. The decoys in blue and gold represent the lower and upper regions in Figure 2b, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig.4.</head><figDesc>Fig. 4. Ranking of our CNF predictions for (a) T0416_D2, (b) T0476, (c) T0496_D1 and (d) T0510_D3 (x-axis is percentile ranking and y-axis GDTTS). Our first and best cluster centroids are plotted in black and magenta lines, respectively. The #1 models submitted by the CASP8 server are ordered by their GDT-TS and their percentile ranking is displayed as a cyan curve, so are the best models from each server but as a green curve.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>Table 1.</figDesc><table>Secondary-structure prediction accuracy 

Methods 
Q3 (%) 
Methods 
Q3 (%) 

CRF 
72.3 
CNF 
80.1 
TreeCRFpsi 
77.6 
YASSPP 
77.8 
SVMpro 
73.5 
PSIPRED 
76.0 
SVMpsi 
76.6 
SPINE 
76.8 

Bold in this table indicates the best performance. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>Table 3. Performance of our CNF and CRF methods on the CASP8 testset</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><figDesc>Table 4. Clustering result of the 12 CASP8 free-modeling targets</figDesc><table>Target 
First cluster 
Best cluster 

GDT CASP8 Internal 
GDT CASP8 
Internal 
rank 
rank (%) 
rank 
rank (%) 

T0397_D1 25.7 
12/60 
50.6 
28.6 
28/262 
18.8 
T0405_D1 39.2 
6/63 
41.6 
48.4 
14/285 
6.5 
T0405_D2 27.0 
10/62 
72.3 
34.6 
19/280 
5.1 
T0416_D2 69.3 
1/53 
5.4 
76.8 
1/242 
3.5 
T0443_D1 46.9 
3/64 
38.2 
49.2 
6/253 
19.7 
T0443_D2 24.8 
26/59 
35.3 
27.9 
73/252 
12.1 
T0465 
31.3 
12/65 
12.6 
31.3 
34/286 
12.6 
T0476 
34.2 
4/66 
17.5 
35.6 
15/287 
10.0 
T0482 
34.2 
34/65 
4.3 
34.2 
132/279 4.3 
T0496_D1 30.5 
1/59 
30.3 
49.1 
1/266 
0.4 
T0510_D3 47.7 
1/54 
15.7 
51.7 
2/244 
3.3 
T0513_D2 57.7 
5/50 
3.8 
57.7 
17/225 
3.8 

Column 'GDT' lists the GDT-TS of the first and best cluster centroids. Column 'CASP8 
rank' lists </table></figure>

			<note place="foot">at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="9"> (=207) elements, corresponding to the sequence profile and secondarystructure information in a window of size nine centered at position i. We use PSIPRED to predict the secondary structure of a protein from its sequence profile. PSIPRED generates likelihood score of three secondary structure types for each residue, which is used as the input of our CNF model. Similar to CRF, we use the maximum likelihood method to train the model parameters such that P S|M,X is maximized. That is, we maximize the occurring probability of a set of ∼3000 non-redundant high-resolution protein structures. Although both the output and hidden layers contain model parameters, all the parameters can be learned together by gradient-based optimization. We use LBFGS (Liu and Nocedal, 1989) as the optimization routine to search for the optimal model parameters. Since CNF contains a hidden layer of gate functions G, the log-likelihood function is not convex any more. Therefore, it is very likely that we can only obtain a local optimal solution of the model parameters. To achieve a good solution, we run the training algorithm several times and use the solution with the best objective function as the final solution of the model. See Peng et al. (2009) for a detailed description of training CNF. 2.3 Model parameter training To do a fair comparison between our previous CRF model and this CNF model, we used exactly same data to train both CRF and CNF models. That is, we use a set of ∼3000 non-redundant proteins to train the parameters in our CNF and CRF models. Any two proteins in the training set share no more than 30% sequence identity and the resolution of a training protein is at least 2.0 Å. To avoid overlap between the training data and the test proteins, we removed the following proteins from our training set: (i) the proteins sharing at least 25% sequence identity with our test proteins; (ii) the proteins in the same fold class as our test proteins according to the SCOP classification; and (iii) the proteins having a TM-score (Zhang and Skolnick, 2007) at least 0.5 with our test proteins. Finally, the training data was prepared before CASP8 started. Therefore, we can use our CRF/CNF models to test the CASP8 free-modeling targets without worrying about bias. The training set is randomly divided into five sets of same size and then used for 5-fold cross validation. To train a CNF model, we shall determine the number of gate functions at the hidden layer. In addition, since the CNF model contains a very large number of model parameters, to avoid overfitting, we shall also control the model complexity. We achieve this by regularizing the L 2-norm of the model parameters using a regularization factor. We trained our CNF model by enumerating the number of gate functions (50, 100, 200 and 300) and different regularization factors: 25, 50, 100 and 200 to see which one yields the best F 1-value. F 1-value is widely-used to measure the prediction capability of a machine learning model. F 1-value is an even combination of precision p and recall r and defined as 2pr/(p+r). The higher the F 1-value is, the better the CNF model. Our CNF model achieves the best F 1-value (23.44%) when 200 gate functions are used with regularization factor 50. In contrast, the best F 1-value achieved by our previous CRF method is 22.0%. The F 1-value improvement achieved by CNF over CRF seems not to be very big, partially because in total 100 labels are used in our i312 at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="1"> http://www.sbg.bio.ic.ac.uk/∼maxcluster/index.html. i315 at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="2"> CASP8 results are available at http://predictioncenter.org/casp8/results.cgi.</note>

			<note place="foot" n="3"> There are very few human predictions for T0510_D3. i316 at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">i317 at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was made possible by the facilities of SHARCNET (http:// www.sharcnet.ca) and the Open Science Grid Engagement VO. The authors are also grateful to Dr John McGee and Mats Rynge for their help with computational resources.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<monogr>
		<title level="m" type="main">Simulated Annealing and Boltzmann Machines: A Stochastic Approach to Combinatorial Optimization and Neural Computing</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Aarts</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Korst</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">A generative, probabilistic model of local protein structure</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Boomsma</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8932" to="8937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">An evolutionary approach to folding small $\alpha$-helical proteins that uses sequence information and an empirical guiding fitness function</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">U</forename>
				<surname>Bowie</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Eisenberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA, 91</title>
		<meeting>. Natl Acad. Sci. USA, 91</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="4436" to="4440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<monogr>
		<title level="m" type="main">Introduction to Protein Structure</title>
		<author>
			<persName>
				<forename type="first">C.-I</forename>
				<surname>Branden</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Tooze</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Garland Publishing</publisher>
			<pubPlace>New York, London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">A knowledge-based move set for protein folding</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">W</forename>
				<surname>Chen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ProteinsStruct. Funct. Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="682" to="688" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Modelling the polypeptide backbone with &apos;spare parts&apos; from known protein structures</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Claessens</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Eng</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="335" to="345" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="0" to="12" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>btq193. .tex]</note>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Mimicking the folding pathway to improve homology-free protein structure prediction</title>
		<author>
			<persName>
				<forename type="first">Page</forename>
				<surname>Fields Debartolo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Protein folding using conditional neural Proc. Natl Acad. Sci. USA</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="317" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Training conditional random fields via gradient tree boosting</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Dietterich</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th International Conference on Machine Learning (ICML)</title>
		<meeting>the 21th International Conference on Machine Learning (ICML)<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Achieving 80% ten-fold cross-validated accuracy for secondary structure prediction by large-scale training</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Dor</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<forename type="middle">Q</forename>
				<surname>Zhou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins-Struct. Funct. Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="838" to="845" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Parallel tempering: theory, applications, and new perspectives</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">J</forename>
				<surname>Earl</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">W</forename>
				<surname>Deem</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Chem. Chem. Phys</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3910" to="3916" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting continuous local structure and the effect of its substitution for secondary structure in fragment-free protein structure prediction</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Faraggi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structure</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1515" to="1527" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamics of hydrogen bond desolvation in protein folding</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Fernandez</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">321</biblScope>
			<biblScope unit="page" from="659" to="675" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Reduced Cbeta statistical potentials can outperform all-atom potentials in decoy identification</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">E</forename>
				<surname>Fitzgerald</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Sci</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2123" to="2139" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Backbone building from quadrilaterals: a fast and accurate algorithm for protein backbone reconstruction from alpha carbon coordinates</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Gront</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Chem</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1593" to="1597" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Sampling realistic protein conformations using local structural bias</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hamelryck</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">A novel method of protein secondary structure prediction with high segment overlap measure: support vector machine approach</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">J</forename>
				<surname>Hua</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Sun</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Z.R. J. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="page" from="397" to="407" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Protein secondary structure prediction based on position-specific scoring matrices</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">T</forename>
				<surname>Jones</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mol Biol</title>
		<imprint>
			<biblScope unit="volume">292</biblScope>
			<biblScope unit="page" from="195" to="202" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Using known substructures in protein model building and crystallography</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">A</forename>
				<surname>Jones</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Thirup</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMBO J</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="819" to="823" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">YASSPP: better kernels and coding schemes lead to improvements in protein secondary structure prediction</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Karypis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins-Struct. Funct. Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="575" to="586" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">The Fisher-Bingham distribution on the sphere</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">T</forename>
				<surname>Kent</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Royal Statist. Soc</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="71" to="80" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">TOUCHSTONE: an ab initio protein structure prediction method that uses threading-based tertiary restraints</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Kihara</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA, 98</title>
		<meeting>. Natl Acad. Sci. USA, 98</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="10125" to="10130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Protein secondary structure prediction based on an improved support vector machines approach</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Kim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Park</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Eng</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="553" to="560" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Accurate modeling of protein conformation by automatic segment matching</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Levitt</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">226</biblScope>
			<biblScope unit="page" from="507" to="533" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">On the limited memory method for large scale optimization</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">C</forename>
				<surname>Liu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Nocedal</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program. B</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Physically realistic homology models built with ROSETTA can be more accurate than their templates</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">M</forename>
				<surname>Misura</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5361" to="5366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Close agreement between the orientation dependence of hydrogen bonds observed in protein structures and quantum mechanical calculations</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">V</forename>
				<surname>Morozov</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci</title>
		<meeting>. Natl Acad. Sci</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="6946" to="6951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">A decade of CASP: progress, bottlenecks and prognosis in protein structure prediction</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Moult</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Opin. Struct. Biol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="285" to="289" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Critical assessment of methods of protein structure prediction (CASP)-round V</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Moult</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Struct. Funct. Genet</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="334" to="339" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Critical assessment of methods of protein structure prediction (CASP)-round 6</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Moult</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Struct. Funct. Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="61" to="64" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Suppl. 7</note>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Critical assessment of methods of protein structure predictionRound VII</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Moult</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Struc. Funct. Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="3" to="9" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Conditional neural fields</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Peng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS). NIPS foundation</title>
		<editor>Bengio,Y. et al.</editor>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1419" to="1427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Statistical potential for assessment and prediction of protein structures</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">Y</forename>
				<surname>Shen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Sali</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Sci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2507" to="2524" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<monogr>
		<title level="m" type="main">Analysis of casp8 targets, predictions and assessment methods. Database [E-pub ahead of print</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Shi</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Calculation of protein conformation as an assembly of stable overlapping segments: application to bovine pancreatic trypsin inhibitor</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Simon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="3661" to="3665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">Assembly of protein tertiary structures from fragments with similar local sequences using simulated annealing and Bayesian scoring functions</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">T</forename>
				<surname>Simons</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="page" from="209" to="225" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<analytic>
		<title level="a" type="main">Recognition of errors in three-dimensional structures of proteins</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Sippl</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Struct. Funct. Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="355" to="362" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b37">
	<analytic>
		<title level="a" type="main">Replica Monte-Carlo simulation of spin-glasses</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">H</forename>
				<surname>Swendsen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">S</forename>
				<surname>Wang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="2607" to="2609" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b38">
	<analytic>
		<title level="a" type="main">A 3D building blocks approach to analyzing and predicting structure of proteins</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Unger</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Struct. Funct. Genet</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="355" to="373" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b39">
	<analytic>
		<title level="a" type="main">PROBIT: a statistical approach to modeling proteins from partial coordinate data using substructure libraries</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">J</forename>
				<surname>Wendoloski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">R</forename>
				<surname>Salemme</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mol. Graphics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="124" to="126" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b40">
	<analytic>
		<title level="a" type="main">Ab initio modeling of small proteins by iterative TASSER simulations</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Wu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Biol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b41">
	<analytic>
		<title level="a" type="main">Ab initio construction of protein tertiary structures using a hierarchical approach</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Xia</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">300</biblScope>
			<biblScope unit="page" from="171" to="185" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b42">
	<analytic>
		<title level="a" type="main">All-atom ab initio folding of a diverse set of proteins</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">S</forename>
				<surname>Yang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structure</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="53" to="63" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b43">
	<analytic>
		<title level="a" type="main">The protein structure prediction problem could be solved using the current PDB library</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Skolnick</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1029" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b44">
	<analytic>
		<title level="a" type="main">Scoring function for automated assessment of protein structure template quality</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Skolnick</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins-Struct. Funct. Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="702" to="710" />
			<date type="published" when="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b45">
	<analytic>
		<title level="a" type="main">TOUCHSTONE II: a new approach to ab initio protein structure prediction</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biophys. J</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="1145" to="1164" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b46">
	<analytic>
		<title level="a" type="main">Discriminative learning for protein conformation sampling</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Zhao</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Struct. Funct. Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="228" to="240" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b47">
	<analytic>
		<title level="a" type="main">A probabilistic graphical model for ab initio folding</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Zhao</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Research in Computational Molecular Biology</title>
		<editor>Batzoglou,S.</editor>
		<meeting><address><addrLine>Berlin Heidelberg ; Tucson, Arizona</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="59" to="73" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>