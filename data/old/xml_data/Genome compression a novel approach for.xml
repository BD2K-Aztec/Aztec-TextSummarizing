
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequence analysis Genome compression: a novel approach for large collections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013">20 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName>
								<forename type="first">Sebastian</forename>
								<surname>Deorowicz</surname>
							</persName>
							<email>sebastian.deorowicz@polsl.pl</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Informatics</orgName>
								<orgName type="institution">Silesian University of Technology</orgName>
								<address>
									<addrLine>Akademicka 16</addrLine>
									<postCode>44-100</postCode>
									<settlement>Gliwice</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Agnieszka</forename>
								<surname>Danek</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Informatics</orgName>
								<orgName type="institution">Silesian University of Technology</orgName>
								<address>
									<addrLine>Akademicka 16</addrLine>
									<postCode>44-100</postCode>
									<settlement>Gliwice</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Szymon</forename>
								<surname>Grabowski</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Engineering Department</orgName>
								<orgName type="institution">Technical University of Ló</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sequence analysis Genome compression: a novel approach for large collections</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">29</biblScope>
							<biblScope unit="page" from="2572" to="2578"/>
							<date type="published" when="2013">20 2013</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btt460</idno>
					<note type="submission">Received on February 16, 2013; revised on July 5, 2013; accepted on August 5, 2013</note>
					<note>BIOINFORMATICS ORIGINAL PAPER Associate Editor: Michael Brudno Availability: http://sun.aei.polsl.pl/tgc (also as Supplementary Material) under a free license. Supplementary data: Supplementary data are available at Bioinformatics online. Contact:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Genomic repositories are rapidly growing, as witnessed by the 1000 Genomes or the UK10K projects. Hence, compression of multiple genomes of the same species has become an active research area in the past years. The well-known large redundancy in human sequences is not easy to exploit because of huge memory requirements from traditional compression algorithms. Results: We show how to obtain several times higher compression ratio than of the best reported results, on two large genome collections (1092 human and 775 plant genomes). Our inputs are variant call format files restricted to their essential fields. More precisely, our novel Ziv-Lempel-style compression algorithm squeezes a single human genome to $400 KB. The key to high compression is to look for similarities across the whole collection, not just against one reference sequence, what is typical for existing solutions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The DNA sequencing technology has become so affordable that there are several large-scale projects in which at least hundreds of individuals of some species are sequenced. From many perspectives, including the advent of personalized medicine, the Homo sapiens data belong to the most interesting, and this is the reason why large projects like the 1000 Genomes Project (1000GP) (<ref type="bibr" target="#b0">The 1000</ref><ref type="bibr">Genome Project Consortium, 2012</ref>) and the UK10K Project (http://www.uk10k.org), with thousands of human genomes sequenced so far, were initiated. Among such projects, the most ambitious perhaps is the Personal Genome Project (PGP) (<ref type="bibr" target="#b1">Ball et al., 2012</ref>), with genomes of 100 000 individuals as the anticipated outcome. Large repositories are built not only for human genomes, to mention 1001 Genomes Project (1001GP), with Arabidopsis thaliana genetic variation (http://www. 1001genomes.org/about.html). It is a well-known fact that two organisms of the same species are highly similar; it was estimated that the genomes of two persons are identical in 99.5% (<ref type="bibr" target="#b16">Levy et al., 2007</ref>). The huge amount of data obtained in the large-scale projects demands efficient ways of storing them. Taking into account the high similarity of organisms, it becomes obvious that some compression method may be effectively applied. Compression of single genomic sequences is hardly efficient, as the best obtained compression ratios achieve a factor of 4 or 5 (<ref type="bibr" target="#b4">Cao et al., 2007;</ref><ref type="bibr" target="#b18">Manzini and Rastero, 2004;</ref><ref type="bibr" target="#b20">Pinho et al., 2011</ref>) only. When realized that instead of the complete genomic sequence, storing only differences between it and some referential sequence is enough, the task became easier. In their seminal paper,<ref type="bibr" target="#b5">Christley et al. (2009)</ref>showed how to store the description of variations between James Watson's (JW) genome and a referential genome in only 4.1 MB. However, the authors' prior knowledge was not only the reference sequence but also the single nucleotide polymorphism (SNP) map. As the input, they took the information about the SNPs and insertion or deletion (indels) variations between JW genome and referential genome, and compressed these data using some clever, but simple, techniques. Comparing with $3.1 Gbases of human genome, this means $750-fold compression. In the following years, a number of articles on relative compression of genomes were published (<ref type="bibr" target="#b8">Deorowicz and Grabowski, 2011;</ref><ref type="bibr" target="#b15">Kuruppu et al., 2011;</ref><ref type="bibr" target="#b21">Pinho et al., 2012;</ref><ref type="bibr" target="#b26">Wandelt and Leser, 2012</ref>). In all the articles, the input sequences were complete genomes, not differences between genomes and a reference genome. This complicates the compression problem, as it is necessary to find the differences between genomes without any prior knowledge and without a database of variants (i.e. SNP and indel database). The most successful of the algorithms seems to be GDC (<ref type="bibr" target="#b8">Deorowicz and Grabowski, 2011</ref>), which differentially compressed a collection of 69 human genomes from Complete Genomics Inc. to 215 MB (3.1 MB per individual). It is based on the Ziv–Lempel (<ref type="bibr" target="#b24">Salomon and Motta, 2010;</ref><ref type="bibr" target="#b25">Storer and Szymanski, 1982</ref>) paradigm and finds approximate matches between the genome sequences. Recently,<ref type="bibr" target="#b19">Pavlichin et al. (2013)</ref>showed how to improve the technique from<ref type="bibr" target="#b5">Christley et al. (2009)</ref>, compressing the JW genome to 2.5 MB, with very similar average results on multiple 1000GP genomes. The introduced novelties are partly biologically inspired, e.g. making use of tag SNPs characterizing haplotypes. Another line of research concerns indexing genomic collections (or more generally, repetitive sequences), i.e. building data structures enabling fast pattern search in the genomes (<ref type="bibr" target="#b6">Claude et al., 2010;</ref><ref type="bibr" target="#b9">Do et al., 2012;</ref><ref type="bibr" target="#b11">Gagie et al., 2011</ref><ref type="bibr" target="#b10">Gagie et al., , 2012</ref><ref type="bibr" target="#b14">Kreft and Navarro, 2013;</ref><ref type="bibr" target="#b17">Ma¨kinenMa¨kinen et al., 2010</ref>). Such indexes are efficient when the index resides in the main computer memory, which is challenging considering the sheer volume of indexed data. Some of the listed works are rather theoretical and their *To whom correspondence should be addressed. implementations are not yet available, whereas the works that have been implemented (<ref type="bibr" target="#b6">Claude et al., 2010;</ref><ref type="bibr" target="#b10">Gagie et al., 2012;</ref><ref type="bibr" target="#b14">Kreft and Navarro, 2013;</ref><ref type="bibr" target="#b17">Ma¨kinenMa¨kinen et al., 2010</ref>) are tested on relatively small collections, not exceeding $1 GB. In this article, we try to answer the question how well a collection of genomes of the same species can be compressed, when knowledge of the possible variants is given. The cited works of<ref type="bibr" target="#b5">Christley et al. (2009) and</ref><ref type="bibr" target="#b19">Pavlichin et al. (2013)</ref>are so far the only attempts to compress a (single) genome sequence with a variant database. In this work, we take two large collections of genomes (H.sapiens and A.thaliana) and try to exploit cross-sequence correlations in the variant loci. Our solution is a specialized Ziv-Lempel-style compressor, where the input sequences are basically formed by binary flags denoting if successive variants from the database were found in the individuals. This approach appears highly successful, allowing to store the human collection in 432 MB (395 KB per individual) and the plant collection in 110 MB (142 KB per individual). We point out that the general idea of exploiting common features for improved compression is known for some other NGS tasks, including compression of (both mapped and unmapped) reads (<ref type="bibr" target="#b2">Bonfield and Mahoney, 2013;</ref><ref type="bibr" target="#b3">Cox et al., 2012;</ref><ref type="bibr" target="#b12">Hach et al., 2012;</ref><ref type="bibr" target="#b13">Jones et al., 2012;</ref><ref type="bibr" target="#b22">Popitsch and Haeseler, 2013</ref>). In the next section, we present the input data and the general idea of our approach. Then, we show some details of the proposed compression algorithm. Finally, we evaluate the compressor. The last section concludes the article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MATERIALS AND METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Datasets</head><p>Large collections of genomes of the same species in public repositories are nowadays often represented as one reference genome and multiple variant files. There are several formats for storing the variants, e.g. variant call format (VCF) (<ref type="bibr" target="#b7">Danecek et al., 2011</ref>) used in the 1000GP, general feature format (GFF) used in the PGP. These formats are much more compact than, e.g. FASTA (with raw genomic sequences), yet large collections may still require hundreds of gigabytes of storage. We use two large datasets in the experiments. The publicly available database of Phase 1 of the 1000GP contains data for 1092 human individuals. The genomes are in VCF files, one file for each chromosome, and so there are 24 files in total. These files contain the information about each variant [SNP, insertion (INS), deletion (DEL) and structural variant (SV)] that was found in at least one genome in the dataset. The genomes are phased, i.e. there is information on which of the two chromosomes of each pair (or on none/both) each variant is found. Similar information about variants is present in the 1001GP for A.thaliana. For this collection, we have 775 haploid sequences, each consisting of seven chromosomes. These data were scattered, and we gather them from four 'subprojects'. Our research goal concerns only genome collection compression [similarly as in the works of<ref type="bibr" target="#b5">Christley et al. (2009) and</ref><ref type="bibr" target="#b19">Pavlichin et al. (2013)]</ref>, and VCF files usually contain much more information than needed to recover the DNA sequences (e.g. in FASTA format). We ignore the non-essential VCF fields, i.e. keep only the information on which positions the changes in each genome may be found. For the 1000GP dataset, it meant removing extra fields from the original VCF files. For the 1001GP dataset, we directly converted the available data to a stripped VCF (sub)format, which we call VCF minimal (VCFmin). We point out that these are valid VCF files. As a side note, let us remark that the GFF data (http://evidence.personalgenomes.org/guide_upload_and_annotated_file_ formats) used in the large-scale PGP (<ref type="bibr" target="#b1">Ball et al., 2012</ref>) are compatible with that of ours stored in VCFmin. The basic dataset characteristics are presented in<ref type="figure" target="#tab_1">Table 1</ref>(URLs and other technical descriptions of the data, including preprocessing details for the 1001 dataset, are given in the Supplementary Material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The compression algorithm</head><p>Our tool, Thousands Genomes Compressor (TGC), assumes that the input data are in VCFmin form. Such textual file consists of rows, one per each variant. A single row contains the following data: Description of a variant (position, type and information about the changes to the reference genome), Evidence of occurrence of the variant in each single genome. The VCFmin files can be compressed quite efficiently by general tools, but much better results are possible. The biggest hurdle for a generic compressor is the 'non-locality' of the VCFmin format, i.e. the genomes are stored in columns, so the occurrences of the successive variants of the same genome are at long distances. This means that if two genomes have similarities, the compressor must find and encode their similar (identical) areas, which are far away and are relatively short (the description of the occurrence of a variant in a single genome takes a few bytes). The main idea of our algorithm is to transform the input data in a way to increase the locality and lengths of similar (identical) genome areas. To this end,A single database of variants containing only the basic information about each variant (see<ref type="figure" target="#fig_0">Fig. 1</ref>); multiple variant alleles are also supported, 2N (for diploid) or N (for haploid genomes) bit-vectors. Value 1 at some jth position in this vector means that jth variant in the database is found in the genome. To reduce the space, these bit-vectors are packed into byte-vectors (8 bits in a byte). A byte is then the smallest processing unit in the compression scheme.</p><p>For example, if the 1000th variant in the database is 776646 SNP A, and the 1000th bit for some genome is 1, then we know that an SNP occurs in this genome at position 776646 and the resulting nucleotide there is A. The collection of the database of variants and the byte vectors is later called as variant database þ byte vectors (VDBV) format. Using the dense byte vectors has a few advantages:</p><p>Processing compact input is usually faster and less memory demanding than with more 'bloated' input;</p><p>Identical patterns of successive variants in different genomes are represented with repeating byte sequences (which can be easily handled with standard dictionary compression techniques); Same variants in different genomes have the same positions, and thus encoding the repetitions of the common patterns in successive genomes is cheaper.</p><p>The resulting VDBV representation is already well compressible with generic tools, especially 7z, but universal solutions neglect some existing redundancy; in particular, they are not 'aware' about the aligned repetitions between genomes in our byte vectors. To exploit this, we devised a specialized compressor loosely based on the LZSS algorithm (<ref type="bibr" target="#b25">Storer and Szymanski, 1982</ref>). Each vector is processed from left to right. At every analyzed position k, we look for the longest match (identical byte substring) starting at kth position in any previously processed byte vector. The match position restriction is obvious, as we look for common haplotypes, and matches elsewhere in the vectors are accidental and thus unlikely to be long. (Moreover, with the restriction, the matches need fewer bits to encode.) The already processed data from the vectors are indexed using two hash tables (HTs) to make the search faster; one HT is for searching for long matches and if none such is found then the other HT is used (some more details on the hash scheme are given in the Supplementary<ref type="figure">Fig. S2</ref>). Similarly as for the classical LZSS algorithm, at each position we can find a match of length at least mml (minimal match length, set experimentally to five in our implementation) or a literal (if no sufficiently long match can be found). A match is described as a triple h1, vid, leni, where vid is the id of the vector in which we found the longest match, and len is the match length. A literal is represented with a pair h0, bvi, where bv is the value of the byte at position k in the byte vector. The first fields (flags), 0 or 1, distinguish between literals and matches. After processing a match, we shift to ðk þ lenÞth position, and in a case of literal to ðk þ 1Þth position. The sequence of pairs and triples is then compressed using an arithmetic coder (<ref type="bibr" target="#b24">Salomon and Motta, 2010</ref>) (We use a popular and fast arithmetic coding variant by Schindler (http://www.compressconsult. com/rangecoder/), also known as a range coder.). Broadly speaking, arithmetic coder encodes each symbol occurrence on (in general, fractional) number of bits related to the probability of the symbol occurrence. These probabilities are estimated on the basis of already encoded symbols, i.e. if a symbol has occurred frequently, the corresponding probability is high and the number of bits spent for encoding it is small. We note that using Huffman coding instead for our data would result in a few percent compression loss. There are several contextual models for compressing various fields, which means that different by-products of our scheme are handled based on different collected statistics (more details are given in the Supplementary data). The flags are compressed in one model. For the literals, another model is used, but before passing their byte values to the entropy coder, we do some trick. The byte value of the first literal after a match h1, vid, leni is 'xored' (if its value is not 0) with the byte following the repetition in vidth byte vector, i.e. the byte of index k þ len. We know these two byte values cannot be equal (otherwise the match could be extended by at least 1 byte). It was found experimentally that it is more likely to have a decreased number of resulting set bits in bv after the 'xor' operation than to have it increased (even if in most cases, the number of set bits is unchanged). In this way, the distribution of the bv values gets more skewed, which helps the compression. Contextual compression models are usually more practical if they are not too large, and this is the reason why numbers from a broad interval are often split before being processed by a statistical model (this approach is used, e.g. in gzip, bzip2). Following this rule, in our scheme, the length of the match is stored in two parts, both compressed with an entropy coder. First, we compress the binary logarithm of the match length (rounded to an integer) and then the remaining bits needed to recover its length. More precisely, the first part is log 2 ðlen À mml þ 1Þ AE Ç , and if len À mml ! 2, then the value of len À mml À 2 log 2 ðlenÀmmlþ1Þ d e À1 is encoded. Similarly, the vid field is split into two (byte) parts: vid=256 Ä Å and vid À 256 vid=256 Ä Å , both encoded with an entropy coder. The compression of variant database (excerpt of which is presented in<ref type="figure" target="#fig_0">Fig. 1</ref>) is rather straightforward. The main idea is to compress each variant type separately: SNP, insertion, deletion and SV. Thus, for each variant, we first store its type. The variants positions are then differentially encoded, i.e distances between consecutive SNPs, consecutive DELs, etc, are stored. Then, for SNP, we store the substituting symbol. For INS, we store its length and the symbols to insert. For DEL, only its length is stored. For SV, we encode the deletion length, the insertion length and finally (if necessary) the inserted symbols. All the values are encoded using a variant of arithmetic coding with appropriate contextual models (details can be found in the Supplementary data).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>As mentioned earlier, for the evaluation of the proposed compression algorithm, we use two datasets of 1092 H.sapiens and 775 A.thaliana genomes. In all experiments, the data are processed chromosome by chromosome. This approach, typical in the genomic compression literature (see, e.g. Deorowicz and<ref type="bibr" target="#b8">Grabowski, 2011;</ref><ref type="bibr" target="#b19">Pavlichin et al., 2013;</ref><ref type="bibr" target="#b26">Wandelt and Leser, 2012</ref>), reduces the memory footprint, speeds up computations and improves the compression ratio for the generic algorithms. There are several tools for compressing genomic sequences in FASTA format. Unfortunately, the amount of our test data, $6.8 TB of raw sequences if converted to FASTA, is so huge that the running times of some of those compressors would be counted in months. Thus, we started from a preliminary test in which we evaluated the most powerful as well as the most recent tools for only two human chromosomes (14 and 21) and also two plant chromosomes (1 and 4). The results are presented in<ref type="figure" target="#tab_2">Table 2</ref>. This and all further experiments were performed on a computer equipped with four 4-core 2.4 GHz AMD Opteron CPUs with 128-GB RAM running Red Hat 4.1.2-46 linux. Two of the presented compressors may be considered fast with regard to the compression speed: ABRC (<ref type="bibr" target="#b26">Wandelt and Leser, 2012</ref>) with $100 MB/s (run with 8 threads) and GDC-normal (<ref type="bibr" target="#b8">Deorowicz and Grabowski, 2011</ref>) with $40 MB/s speed (only a serial implementation exists), while the others are by about one [GDC-ultra (<ref type="bibr" target="#b8">Deorowicz and Grabowski, 2011</ref>) and RLZ (<ref type="bibr" target="#b15">Kuruppu et al., 2011)]</ref>or about two (7z) orders of magnitude slower. Interestingly, the only generic compressor in the tests, 7z, is the second best in the compression ratio (after GDC-ultra), but its compression speed is low (0.4 MB/s). The memory available for the compression has a major impact on the compression ratio. For example, 7z was run with its maximum setting, 1 GB for its LZ-buffer (translating to 410 GB of total memory use), yet it fit only a small part of the input for H.sapiens data: 510 individuals (each of size $108 MB) for chromosome 14 dataset and 21 individuals (each of size $48 MB) for chromosome 21 dataset. This, supposedly, was the main reason for which its compression ratio in the latter case is significantly higher. The hypothesis is indirectly confirmed by the results for much shorter A.thaliana chromosomes, for which more individuals fit the 1-GB LZ-buffer and the compression ratios are close to GDC-ultra. In the next experiment, we compared a few well-known generic compressors (gzip, bzip2, 7z) on VCFmin input files (<ref type="figure" target="#tab_3">Table 3</ref>). Compressed sizes (in megabytes) and compression times are reported for selected chromosomes and the collections in total. Surprisingly perhaps, the best compression was obtained by bzip2 compressor.<ref type="figure" target="#tab_4">Table 4</ref>is similar, but now the inputs are in our temporary 'dense' representation, VDBV. Here, the generic compressors are compared against our proposal, TGC. Two simple observations can be made: (i) the more compact of these two input representations, VDBV, is clearly more appropriate for the best generic compressor, 7z, both from the point of compression ratio and from speed; (ii) TGC is significantly better than VDBVþ7z in both measured aspects, which demonstrates that designing a specialized compression algorithm was a worthy goal in this case. The results are summarized in<ref type="figure" target="#tab_5">Table 5</ref>. For comparison purposes, we also show the sizes of the raw sequences as well as the compression results of the best compressor working on such representation, GDC-ultra. We resigned, however, from presenting the compression and decompression times of GDC-ultra, as it works on a completely different representation than VCFmin,which we use in the article. The compression ratios of GDC can be treated as a reference point. The most important 'numbers' from this summary are the average sizes of genomes in the most compact, TGC, representation. The obtained 395 KB (for the human data) is more than six times smaller than offered by the best so far genome sequence, GDC-ultra, compressor. Also, the very recent paper by<ref type="bibr" target="#b19">Pavlichin et al. (2013)</ref>, working on a representation similar to our VCFmin, reports more than six times larger files. When expressing the compression ratios in relation to the raw genome sequence sizes, it means that our algorithm squeezes H.sapiens genomes $15 500 times and A.thaliana $850 times.</p><p>In the last experiment, we compared TGC against SpeedGene (<ref type="bibr" target="#b23">Qiao et al., 2012</ref>), an algorithm for efficient storage of SNP datasets. For an honest comparison, we restricted the 1000GP set of variants to SNPs only. SpeedGene requires the input data to be in LINKAGE format, in which there is no distinction between chromosomes in each pair, i.e. for each SNP it describes only whether no SNP is found in a genome, one SNP (on any chromosome) is found, two SNPs are found (on both chromosomes) or the status of SNP is unknown. Thus, we changed our algorithm slightly and instead of processing each single chromosome, we joined chromosomes of each pair, and obtained vectors of 'dibits', i.e. bit pairs. These vectors are then transformed intoNote: There are two columns containing ratios: 'Ratio to raw' tells how many times the compressed file is smaller than the genome sequences in FASTA format. 'Ratio to VCFmin' is the compression ratio according to the size of VCFmin files. For GDC-ultra, compression and decompression times are not given, as this compressor uses a different input form than others and such a comparison would be irrelevant. The values marked in bold indicate best compression.Note: All sizes are in megabytes and times are in seconds. The 'VDBV c-time' column contains the conversion times from VCFmin format to VDBV format. In the remaining columns titled 'c-time', total compression time is given (i.e. 'VDBV þ 7z c-time' denotes the sum VCFmin-to-VDBV conversion time and 7z compression time; 'TGC c-time' is the total TGC processing time, comprising the conversion to VDBV and the actual compression). Note that the variant database (part of the VDBV representation) is of size 933 MB for H.sapiens and 320 MB for A.thaliana. After compressing by TGC, their sizes (included in 'TGC size' column) are $51.0 MB and 12.5 MB, respectively. The values marked in bold indicate best compression. The extended version of this table (with results for all chromosomes) can be found in Supplementary Table S2. byte vectors (each byte contains four consecutive dibits). In this way, our tool can compress these byte vectors without any change.<ref type="figure">Table 6</ref>presents the compressed sizes obtained by SpeedGene and TGC. We show the results for three chromosomes, as well as for the complete genome. As one can see, TGC reduces the dataset size more than four times better than SpeedGene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head><p>We examined the possibility of obtaining much better compression ratios of genomic collections than from existing tools, when additional knowledge is given. The knowledge was the information about the possible variants in genomes and the occurrence of these variants in specific genomes. This helps a lot in compression of genomic sequences, as all input sequences are perfectly aligned and the task of finding repetitions in data (usually the most important and time-consuming task handled by data compression algorithms) becomes rather simple. We should mention that in theory such perfect alignments can be found by compression algorithms, but the computational burden would be enormous. Thus, compression tools usually make some heuristic decisions when comparing the sequences in the hope that they do not lose too much. The success of our algorithm was possible not only because of the variant database, but also because we searched for crosscorrelations between individuals. In other words, for each individual, similarities to any other previously processed individual (i.e. runs of repeating variants) can be found. In principle, the available memory may be a limiting factor but processing the collection on the chromosome level resulted in 52.5 GB memory use for the larger (human) of the tested datasets. In the future, when much more genomes are available, we may need to re-address the memory issue though, possibly via working on blocks smaller than whole chromosomes, or trying to re-order the sequences in a way to maximize local similarities. In the compression method design, we sometimes traded compression ratio for reduced memory requirements, e.g. some (rather minor) improvements in compression would be possible owing to higher-order contextual modeling. Probably, a more practical approach is to make use of more biological knowledge; the very recent work of<ref type="bibr" target="#b19">Pavlichin et al. (2013)</ref>gives new insight, which might be possible to use in our scheme, but we leave it for future work. Why such experiments can be interesting? Although accurate and efficient analyses of such huge (several terabytes in raw format) genomic collections remain a major challenge, we believe that the mere compressibility of human genomes (e.g. as a 'lower bound' for memory requirements of future algorithms and tools) is a question worth investigating. For example, our compressed collection takes $430 MB, so including also a compressed reference genome (at most 700 MB) requires $1.1 GB of space, which seems quite modest. Naturally, running efficient queries over such data is another matter (clearly with some overhead in space use), but our results suggest this is not impossible. The information kept in VCF or genome variation format (GVF) files is often more detailed (e.g. may include quality scores) than what our tool preserves. Although clearly efficient compression methods for such data are also needed, we do not anticipate a possibility to obtain similar compression ratios to TGC, unless a (strongly) lossy mode is used. Unfortunately, we cannot see a way to easily adapt our compression techniques for such data. TGC allows extracting an arbitrary chromosome (or a whole genome) from the compressed collection, yet this solution is simple and rather slow. Making this extraction faster, or (even better) allowing for quick access to position-restricted arbitrary snippets of the genomes in the collection, is an important task left for future work. Clearly, there must be some space-time tradeoffs for such functionalities. A somewhat related functionality will be to add or remove an individual genome to/from the collection. Currently, changing the archive content requires recompressing the collection from scratch. The performed experiments showed that even the best genomic sequence compressor, GDC-ultra, is significantly (up to seven times) poorer in compression ratio than what can be obtained with extra knowledge. The main conclusions from our work are:Note: VCFmin means a simplified VCF with SNP calls only that spends only 4 bytes for each genotype. All sizes are in megabytes. The values marked in bold indicate best compression.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. Excerpt of the variant database. Successive columns contain variant id, chromosomal location, type and change: (i) substituting symbol for SNPs, (ii) inserted symbols for INSs, (iii) number of removed symbols for DELs and (iv) number of removed symbols and (optionally) symbols to insert for SVs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>Funding: The work was partially supported by the Polish Ministry of Science and Higher Education under the project [DEC-2011/ 01/B/ST6/06868] (S.D.) and by the European Union from the European Social Fund [UDA-POKL.04.01.01-00-106/09] (A.D.).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><figDesc>Conflict of Interest: none declared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><figDesc>Table 1. Characteristics of the datasets used in the experiments</figDesc><table>H.sapiens data 

Variant types 
Total 
Average found on 

23 
chromosome 
pairs 

23 
chromosomes 

No. of SNPs 
38 267 471 
3 701 254 
2 553 479 
No. of 1-symbol insertions 
392 515 
97 023 
67 586 
No. of 2-symbol insertions 
81 462 
27 325 
19 223 
No. of longer insertions 
103 918 
35 502 
25 205 
No. of 1-symbol deletions 
393 748 
94 031 
65 503 
No. of 2-symbol deletions 
166 554 
38 316 
25 795 
No. of longer deletions 
305 317 
64 907 
43 186 
No. of SVs 
14 422 
746 
462 
Total no. of variants 
39 725 407 
4 059 104 
2 800 439 

A.thaliana data 

Variant types 
Total 
Average found on 
5 chromosomes, 
chloroplast, 
mitochondria 

No. of SNPs 
13 123 220 
552 825 
No. of 1-symbol insertions 
261 428 
2271 
No. of 2-symbol insertions 
35 005 
249 
No. of longer insertions 
5652 
39 
No. of 1-symbol deletions 
1 046 670 
29 194 
Total no. of variants 
14 471 975 
584 579 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><figDesc>Table 3. Evaluation of universal compressors for variant data of 1092 (H.sapiens) and 775 (A.thaliana) samples</figDesc><table>Data 
VCF 
VCFmin 
VCFmin þ gzip 
VCFmin þ bzip2 
VCFmin þ 7z 

size 
size 
size 
c-time 
size 
c-time 
size 
c-time 

H.sapiens 
Chromosome 1 
93 087 
13 249 
306.6 
321 
138.9 
6259 
144.2 
2003 
Chromosome 11 
58 671 
8351 
196.5 
205 
89.0 
3960 
91.5 
1400 
Chromosome 21 
16 065 
2286 
55.9 
58 
25.7 
1090 
26.9 
467 
Complete 
1 223 470 
173 505 
4066.0 
4114 
1849.2 
82 128 
1936.8 
27 352 
A.thaliana 
Chromosome 1 
— 
4945 
111.3 
99 
63.3 
2596 
86.4 
1002 
Chromosome 4 
— 
3386 
76.3 
67 
43.7 
1608 
60.0 
700 
Complete 
— 
20 755 
466.8 
414 
265.0 
10 107 
362.8 
4100 

Note: The input data are in VCF format without any non-essential fields (VCFmin). For comparison, the sizes of the original VCF files from the 1000GP are given. All sizes 
are in megabytes. Columns 'c-time' contain compression time in seconds. The values marked in bold indicate best compression. The extended version of this table (with results 
for all chromosomes) can be found in Supplementary Table S1. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>Table 2.</figDesc><table>Evaluation of genome compressors for four sets: 2184 sequences 
of H.sapiens chromosome 14 (237.6 GB) and chromosome 21 (106.5 GB), 
775 sequences of A.thaliana chromosome 1 (23.9 GB) and chromosome 4 
(14.6 GB) 

Data 
7z 
RLZ GReEn ABRC GDC 
GDC 
Normal Ultra 

H.sapiens 
Chromosome 14 1068 270 
218 
472 
674 
2455 
Chromosome 21 1561 269 
211 
460 
685 
2397 
A.thaliana 
Chromosome 1 
242 
86 
64 
67 
154 
254 
Chromosome 4 
234 
80 
59 
61 
141 
230 

Note: The values are compression ratios (rounded to the nearest integer) of the 
collection, understood as the original data size divided by the compressed size. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><figDesc>Table 5. Summary of the results of the universal, specialized and proposed compressors</figDesc><table>Data 
Total size 
Size per individual 
Ratio to 
Ratio to 
Compression 
Decompression 
[MB] 
[MB] 
VCFmin 
raw 
time [s] 
time [s] 

H.sapiens 
Raw 
6 669 797 
6107.873 
— 
— 
— 
— 
GDC-ultra 
2952 
2.703 
58.8 
2259 
— 
— 
VCFmin 
173 505 
158.887 
— 
— 
— 
— 
VDBV 
11 680 
10.696 
14.9 
571 
1819 
1730 
VCFmin þ 7z 
1937 
1.774 
89.6 
3444 
27 352 
1951 
VDBV þ 7z 
734 
0.672 
236.4 
9088 
11 176 
1936 
TGC 
432 
0.395 
402.0 
15 453 
8364 
2067 
A.thaliana 
Raw 
94 047 
121.351 
— 
— 
— 
— 
GDC-ultra 
384 
0.495 
54.1 
245 
— 
— 
VCFmin 
20 756 
26.782 
— 
— 
— 
— 
VDBV 
1723 
2.223 
12.1 
55 
270 
286 
VCFmin þ 7z 
363 
0.468 
57.2 
259 
4100 
239 
VDBV þ 7z 
144 
0.185 
144.6 
655 
1673 
316 
TGC 
110 
0.142 
189.1 
857 
815 
363 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><figDesc>Table 4. Evaluation of universal compressors and the proposed algorithm (TGC) for variant data stored in the intermediate VDBV format</figDesc><table>Data 
VCFmin 
VDBV 
VDBV þ gzip 
VDBV þ bzip2 
VDBV þ 7z 
TGC 

size 
size 
c-time 
size 
c-time 
size 
c-time 
size 
c-time 
size 
c-time 

H.sapiens 
Chromosome 1 
13 249 
890.6 
136 
368.9 
311 
326.3 
251 
55.9 
1070 
32.3 
690 
Chromosome 11 
8351 
560.1 
86 
238.7 
200 
194.8 
160 
33.1 
598 
20.2 
392 
Chromosome 21 
2286 
153.0 
25 
66.8 
54 
39.3 
44 
10.4 
107 
6.3 
96 
Complete 
173 505 
11 679.9 
1819 
4813.6 
4075 
3907.0 
3396 
733.8 
11 176 
431.6 
8364 
A.thaliana 
Chromosome 1 
4945 
405.8 
64 
117.3 
121 
102.9 
107 
34.7 
395 
26.1 
231 
Chromosome 4 
3386 
281.4 
42 
79.7 
92 
67.9 
72 
23.7 
271 
18.2 
120 
Complete 
20 756 
1722.5 
270 
489.0 
526 
424.1 
453 
143.5 
1,673 
109.7 
815 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><figDesc>Table 6. Comparison of the ways of storing the dataset of SNPs from the 1000GP</figDesc><table>Data 
No. of 
SNPs 

VCFmin 
(SNP only) 

LINKAGE/ 
PLINK 

SpeedGene TGC 

Chromosome 1 
2 896 960 
12 687 
791.8 
180.5 
42.4 
Chromosome 11 
1 827 284 
8003 
498.8 
115.3 
26.9 
Chromosome 21 
497 824 
2180 
135.9 
33.0 
8.0 
Complete 
38 267 471 167 569 
10 444.4 
2426.4 
564.8 

</table></figure>

			<note place="foot">ß The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">Genome compression at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">S.Deorowicz et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">Modern genomic sequence compressors cannot come close in compression ratio to the proposed algorithm basically because of two, not fully independent, reasons: (i) (almost) all of them ignore external knowledge (variant information), and (ii) working on consensus sequences is extremely resource-consuming and keeping full statistics needed for efficient compression is practically impossible for a large collection even on a 128-GB machine. Even huge human genome databases can be stored in relatively small space, as the data size of a single individual is only 395 KB on average. When extrapolated, this would mean that modern 2-TB hard drive is sufficient to store the genomes of $5 million humans, size of a large city.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank the authors of SpeedGene for providing us with the source codes of their tool, used in the experiments, and the anonymous reviewers for helpful suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">An integrated map of genetic variation from 1092 human genomes</title>
	</analytic>
	<monogr>
		<title level="j">Genome Project Consortium. Nature</title>
		<imprint>
			<biblScope unit="volume">491</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">A public resource facilitating clinical use of genomes</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">P</forename>
				<surname>Ball</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="11920" to="11927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Compression of FASTQ and SAM format sequencing data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">K</forename>
				<surname>Bonfield</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">V</forename>
				<surname>Mahoney</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">59190</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale compression of genomic sequence databases with the Burrows-Wheeler transform</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Cox</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1415" to="1419" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple statistical algorithm for biological sequence compression</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">D</forename>
				<surname>Cao</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Data Compression Conference</title>
		<meeting>the Data Compression Conference<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press. IEEE Computer Society</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page">4352</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Human genomes as email attachments</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Christley</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="274" to="275" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Compressed q-gram indexing for highly repetitive biological sequences</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Claude</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE Conference on Bioinformatics and Bioengineering</title>
		<meeting>the 10th IEEE Conference on Bioinformatics and Bioengineering<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="86" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">The variant call format and VCFtools</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Danecek</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2156" to="2158" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust relative compression of genomes with random access</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Deorowicz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Grabowski</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2979" to="2986" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast relative Lempel-Ziv self-index for similar sequences</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">H</forename>
				<surname>Do</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint International Conference on Frontiers in Algorithmics and Algorithmic Aspects in Information and Management (FAW-AAIM</title>
		<editor>Snoeyink,J. et al.</editor>
		<meeting>the Joint International Conference on Frontiers in Algorithmics and Algorithmic Aspects in Information and Management (FAW-AAIM<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="291" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">A faster grammar-based self-index</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Gagie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Language and Automata Theory and Applications</title>
		<editor>Dediu,A.H. and Martı´nMartı´nVide,C.</editor>
		<meeting>the 6th International Conference on Language and Automata Theory and Applications<address><addrLine>, A Corunã, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="240" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Faster approximate pattern matching in compressed repetitive texts</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Gagie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Symposium on Algorithms and Computation</title>
		<meeting>the 22nd International Symposium on Algorithms and Computation<address><addrLine>Yokohama, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="653" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">SCALCE: boosting sequence compression algorithms using locally consistent encoding</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Hach</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3051" to="3057" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Compression of next-generation sequencing reads aided by highly efficient de novo assembly</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">C</forename>
				<surname>Jones</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">171</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">On compressing and indexing repetitive sequences</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Kreft</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Navarro</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">483</biblScope>
			<biblScope unit="page" from="115" to="133" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimized relative Lempel-Ziv compression of genomes</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Kuruppu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACSC Australasian Computer Science Conference</title>
		<editor>Reynolds,M.</editor>
		<meeting>the ACSC Australasian Computer Science Conference<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Australian Computer Society, Inc</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">The diploid genome sequence of an individual human</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Levy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Biol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">254</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Storage and retrieval of highly repetitive sequence collections</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Ma¨kinenma¨kinen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="281" to="308" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple and fast DNA compressor</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Manzini</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Rastero</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Software Pract. Ex</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1397" to="1411" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">The human genome contracts again</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Pavlichin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2199" to="2202" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">On the representability of complete genomes by multiple competing finite-context (Markov) models</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Pinho</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">21588</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">GReEn: a tool for efficient compression of genome resequencing data</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Pinho</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">NGC: lossless and lossy compression of aligned high-throughput sequencing data</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Popitsch</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Von Haeseler</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Handling the data management needs of high-throughput sequencing data: SpeedGene, a compression algorithm for the efficient storage of genetic data</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Qiao</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">100</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<monogr>
		<title level="m" type="main">Handbook of data compression</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Salomon</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Motta</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Data compression via textual substitution</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">A</forename>
				<surname>Storer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">G</forename>
				<surname>Szymanski</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="928" to="951" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Adaptive efficient compression of genomes</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Wandelt</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Leser</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>