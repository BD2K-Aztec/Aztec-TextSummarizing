
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining Optimized data fusion for K-means Laplacian clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011">2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Shi</forename>
								<surname>Yu</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Signals, Identification</orgName>
								<orgName type="department" key="dep2">Department of Electrical Engineering</orgName>
								<orgName type="laboratory">System Theory and Automation</orgName>
								<orgName type="institution" key="instit1">Katholieke Universiteit Leuven</orgName>
								<orgName type="institution" key="instit2">Leuven-Heverlee</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Xinhai</forename>
								<surname>Liu</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Signals, Identification</orgName>
								<orgName type="department" key="dep2">Department of Electrical Engineering</orgName>
								<orgName type="laboratory">System Theory and Automation</orgName>
								<orgName type="institution" key="instit1">Katholieke Universiteit Leuven</orgName>
								<orgName type="institution" key="instit2">Leuven-Heverlee</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Science and Engineering &amp; ERCMAMT</orgName>
								<orgName type="institution">Wuhan University of Science and Technology</orgName>
								<address>
									<addrLine>Wuhan</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Léon-Charles</forename>
								<surname>Tranchevent</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Signals, Identification</orgName>
								<orgName type="department" key="dep2">Department of Electrical Engineering</orgName>
								<orgName type="laboratory">System Theory and Automation</orgName>
								<orgName type="institution" key="instit1">Katholieke Universiteit Leuven</orgName>
								<orgName type="institution" key="instit2">Leuven-Heverlee</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Wolfgang</forename>
								<surname>Glänzel</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Managerial Economics, Strategy and Innovation</orgName>
								<orgName type="department" key="dep2">Centre for R &amp; D Monitoring</orgName>
								<orgName type="institution">Katholieke Universiteit Leuven</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Johan</forename>
								<forename type="middle">A K</forename>
								<surname>Suykens</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Signals, Identification</orgName>
								<orgName type="department" key="dep2">Department of Electrical Engineering</orgName>
								<orgName type="laboratory">System Theory and Automation</orgName>
								<orgName type="institution" key="instit1">Katholieke Universiteit Leuven</orgName>
								<orgName type="institution" key="instit2">Leuven-Heverlee</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Bart</forename>
								<surname>De Moor</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Signals, Identification</orgName>
								<orgName type="department" key="dep2">Department of Electrical Engineering</orgName>
								<orgName type="laboratory">System Theory and Automation</orgName>
								<orgName type="institution" key="instit1">Katholieke Universiteit Leuven</orgName>
								<orgName type="institution" key="instit2">Leuven-Heverlee</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Yves</forename>
								<surname>Moreau</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Signals, Identification</orgName>
								<orgName type="department" key="dep2">Department of Electrical Engineering</orgName>
								<orgName type="laboratory">System Theory and Automation</orgName>
								<orgName type="institution" key="instit1">Katholieke Universiteit Leuven</orgName>
								<orgName type="institution" key="instit2">Leuven-Heverlee</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining Optimized data fusion for K-means Laplacian clustering</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">27</biblScope>
							<biblScope unit="issue">1</biblScope>
							<biblScope unit="page" from="118" to="126"/>
							<date type="published" when="2011">2011</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btq569</idno>
					<note type="submission">Received on June 16, 2010; revised on September 6, 2010; accepted on October 1, 2010</note>
					<note>[10:56 10/12/2010 Bioinformatics-btq569.tex] Page: 118 118–126 Associate Editor: John Quackenbush Contact: shiyu@uchicago.edu Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: We propose a novel algorithm to combine multiple kernels and Laplacians for clustering analysis. The new algorithm is formulated on a Rayleigh quotient objective function and is solved as a bi-level alternating minimization procedure. Using the proposed algorithm, the coefficients of kernels and Laplacians can be optimized automatically. Results: Three variants of the algorithm are proposed. The performance is systematically validated on two real-life data fusion applications. The proposed Optimized Kernel Laplacian Clustering (OKLC) algorithms perform significantly better than other methods. Moreover, the coefficients of kernels and Laplacians optimized by OKLC show some correlation with the rank of performance of individual data source. Though in our evaluation the K values are predefined, in practical studies, the optimal cluster number can be consistently estimated from the eigenspectrum of the combined kernel Laplacian matrix. Availability: The MATLAB code of algorithms implemented in this paper is downloadable from</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Clustering is a fundamental problem in unsupervised learning and a number of different algorithms and methods have emerged over the years. K-means (KM) and spectral clustering are two popular methods for clustering analysis. K-means is proposed to cluster attribute-based data into K numbers of clusters with the minimal distortion (<ref type="bibr">Bishop, 2006;</ref><ref type="bibr" target="#b11">Duda et al., 2001</ref>). Another well-known method, spectral clustering (SC) (<ref type="bibr" target="#b23">Ng et al., 2001;</ref><ref type="bibr" target="#b25">Shi and Malik, 2000</ref>), is also widely adopted in many applications. Unlike KM, SC is specifically developed for graphs, where the data samples are represented as vertices connected by non-negatively weighted undirected edges. The problem of clustering on graphs belongs * To whom correspondence should be addressed. † Present address: Department of Medicine, Institute for Genomics and Systems Biology, The University of Chicago.</p><p>to another paradigm than the algorithms based on the distortion measure. The goal of graph clustering is to find partitions on the graph such that the edges between different groups have a very low weight (von<ref type="bibr" target="#b30">Luxburg, 2007</ref>). To model this, different objective functions are adopted and the typical criteria include the RatioCut (<ref type="bibr" target="#b14">Hagen and Kahng, 1992</ref>), the normalized cut (<ref type="bibr" target="#b25">Shi and Malik, 2000</ref>) and many others. To solve these objectives, the discrete constraint of the clustering indicators is usually relaxed to real values; thus, the approximated solution of spectral clustering can be obtained from the eigenspectrum of the graph Laplacian matrix. Many investigations (e.g.<ref type="bibr" target="#b9">Dhillon et al., 2004</ref>) have shown the connection between KM and SC. Moreover, in practical applications, the weighted similarity matrix is often used interchangeably as the kernel matrix in KM or the adjacency matrix in SC. Recently, a new algorithm, Kernel Laplacian (KL) clustering , is proposed to combine a kernel and a Laplacian simultaneously in clustering analysis (<ref type="bibr" target="#b31">Wang et al., 2009</ref>). This method combines the objectives of KM and SC in a quotient trace maximization form and solves the problem by eigen-decomposition. KL is shown to empirically outperform KM and SC on real datasets. This straightforward idea is useful to solve many practical problems, especially those pertaining to combine attribute-based data with interaction-based networks. For example, in web analysis and scientometrics, the combination of text mining and bibliometrics has become a standard approach in clustering science or technology fields toward the detection of emerging fields or hot topics (<ref type="bibr" target="#b21">Liu et al., 2010</ref>). In bioinformatics, protein–protein interaction network and expression data are two of the most important sources used to reveal the relevance of genes and proteins with complex diseases. Conventionally, the data are often transformed into similarity matrices or interaction graphs, then consequently clustered by KM or SC. In KL, the similarity-based kernel matrix and the interactionbased Laplacian matrix are combined, which provides a novel approach to combine heterogeneous data structures in clustering analysis. Our preliminary experiments show that when using KL to combine a single kernel and a single Laplacian, its performance strongly depends on the quality of the kernel and the Laplacian, which results in a model selection problem to determine the optimal settings of the kernel and the Laplacian. To perform model selection on unlabeled data is non-trivial because it is difficult to evaluate the models. To tackle the new problem, we propose a novel algorithm to incorporate multiple kernels and Laplacians in KL clustering. Our recent work proposes a method to integrate multiple kernel matrices<ref type="bibr">[</ref>clustering, submitted for publication). The main contribution of the present work lies in the additive combination of multiple kernels and Laplacians; moreover, the coefficients assigned to the kernels and the Laplacians are optimized automatically. This article presents the mathematical derivations of the additive integration form of kernels and Laplacians. The optimization of coefficients and clustering are achieved via a solution based on bi-level alternating minimization (<ref type="bibr" target="#b8">Csiszar and Tusnady, 1984</ref>). We validate the proposed algorithm on heterogeneous datasets taken from two real applications, where the advantage and reliability of the proposed method are systematically compared and demonstrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Combine kernel and Laplacian as generalized Ralyeigh quotient for clustering</head><p>We first briefly review the KL algorithm proposed by<ref type="bibr" target="#b31">Wang et al. (2009)</ref>. All the mathematical symbols used in the article are consistent and their representations are listed in Supplementary Material 1. Let us denote X as an attribute dataset and W as a graph affinity matrix, both of them are representations of the same sets of samples. The objective of the KL integration to combine X and W for clustering can be defined as</p><formula>J KL = κJ SC +(1−κ)J KM ,</formula><formula>(1)</formula><p>where J SC and J KM are, respectively, the objectives of SC and KM clustering, κ ∈[0,1] is a coefficient adjusting the effect of the two objectives. Let us denote A ∈ R N×K as the weighted scalar cluster membership matrix, given by</p><formula>A ab = 1 √ n b if x a ∈ C b 0 if x a / ∈ C b ,</formula><formula>(2)</formula><p>where n b is the number of data points belonging to cluster C b and A T A = I K , where I K denotes a K ×K identity matrix. Let us denote D as the diagonal matrix whose (a,a) entry is the sum of the entries of row a in the affinity matrix W. The normalized Laplacian matrix (von<ref type="bibr" target="#b30">Luxburg, 2007</ref>) is given by˜L</p><formula>by˜ by˜L = I −D − 1 2 WD − 1 2 .</formula><formula>(3)</formula><p>The objective of normalized cut-based SC is formulated as</p><formula>minimize A trace A T ˜ LA .</formula><formula>(4)</formula><p>As discussed in the literature (<ref type="bibr">Bishop, 2006;</ref><ref type="bibr" target="#b11">Duda et al., 2001;</ref><ref type="bibr" target="#b15">Hastie et al., 2009</ref>), if the data X has zero sample means, the objective of the KM is given by</p><formula>maximize A trace(A T X T XA).</formula><formula>(5)</formula><p>We further generalize (5) by applying the feature map φ(·) : R → F on X, then the centered data in F is denoted as X , given by</p><formula>X =[φ( x 1 )− − µ ,φ( x 2 )− − µ ,...,φ( x N )− − µ ],</formula><formula>(6)</formula><p>where φ( x i ) is the feature map applied on the column vector of the i-th data point in F , µ is the global mean in F (<ref type="bibr" target="#b13">Girolami, 2002</ref>). The inner product X T X in (5) can be combined using the kernel trick G( x u , x v ) = φ( x u ) T φ( x v ), where G(·,·) is a Mercer kernel. We denote G c as the centered kernel matrix as G c = PGP, where P is the centering matrix P = I N −(1/N) 1 T N , G is the kernel matrix, I N is the N ×N identity matrix, 1 N is a column vector of N ones. Without loss of generality, the KM objective in (5) can be equivalently written as maximize</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A</head><p>trace(A T G c A).</p><formula>(7)</formula><p>Then the objective of</p><formula>0 ≤ κ ≤ 1.</formula><p>To solve the optimization problem without tuning the ad hoc parameter κ, Wang et al. formulate it as a trace quotient of the two components (<ref type="bibr" target="#b31">Wang et al., 2009</ref>). The trace quotient is then further relaxed as a maximization of quotient trace, given by</p><formula>maximize A trace (A T ˜ LA) −1 (A T G c A)</formula><formula>(9) subject to A T A = I K .</formula><p>The problem in (9) is a generalized Rayleigh quotient and the optimal solution A * is obtained in the generalized eigenvalue problem. To maximize this objective, A * is approximated as the largest K eigenvectors of˜Lof˜ of˜L + G c , where˜L where˜ where˜L + is the pseudo inverse of˜Lof˜ of˜L (<ref type="bibr" target="#b31">Wang et al., 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Combine kernel and Laplacian as additive models for clustering</head><p>As discussed, the original KL algorithm is proposed to optimize the generalized Rayleigh quotient objective. In this article, we propose an alternative integration method using a different notation of Laplacian (von<ref type="bibr" target="#b30">Luxburg, 2007</ref>), ˆ L, given byˆL byˆ byˆL = D −1/2 WD −1/2 ,</p><formula>(10)</formula><p>where D and W are defined the same as in (3). The objective of spectral clustering is equivalent to maximizing the term as maximize</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A</head><p>trace(A T ˆ LA).</p><formula>(11)</formula><p>Therefore, the objective of the KL integration can be rewritten in an additive form, given by maximize</p><formula>0 ≤ κ ≤ 1,</formula><p>where A, G c are defined the same as in (8), κ is the free parameter to adjust the effect of kernel and Laplacian in KL integration. If κ is pre-defined, (12) is a Rayleigh quotient problem and the optimal A * can be obtained from eigenvalue decomposition, known as the spectral relaxation (<ref type="bibr" target="#b10">Ding and He, 2004</ref>). Therefore, to maximize this objective, we denote = κ ˆ L +(1−κ)G c thus A * is solved as the dominant K eigenvectors of. In Sections 2.1 and 2.2, two different methods are investigated to integrate a single Laplacian matrix with a single kernel matrix for clustering, where the main difference is to either optimize the cluster assignment affinity matrix A as a generalized Rayleigh quotient (ratio model) or as a Rayleigh quotient (additive model). The main advantage of the ratio-based solution is to avoid tuning the parameter κ. However, since the main contribution of this article is to optimize the combination of multiple kernels and Laplacians, the coefficients assigned on each kernel and Laplacian still need to be optimized. Moreover, the optimization of the additive integration model is computationally simpler than optimizing the ratio-based model. Therefore, in the following sections we will focus on extending the additive KL integration to multiple sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Clustering by multiple kernels and Laplacians: an additive model solved with bi-level optimization</head><p>Let us denote a set of graphs as H i , i ∈{1,...,r}, all having N vertices, and a set of LaplaciansˆLLaplaciansˆ LaplaciansˆL i constructed from H i as (10). Let us also denote a set of</p><p>Page: 120 118–126</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.Yu et al.</head><p>centered kernel matrices as G cj , j ∈{1,...,s} with N samples. To extend (12) by incorporating multiple kernels and Laplacians for clustering, we propose a strategy to learn their optimal-weighted convex linear combinations. The extended objective function is then given by Q1: maximize</p><formula>A, θ J Q1 = trace A T ( ˆ Ł+G)A (13) subject tõ Ł = r i=1 θ i ˜ L i , G = s j=1 θ j+r G cj , r i=1 θ δ i = 1, s j=1 θ δ j+r = 1, θ l ≥ 0, l = 1,...,(r +s), A T A = I K ,</formula><p>where θ 1 ,...,θ r and θ r+1 ,...,θ r+s are, respectively, the optimal coefficients assigned to the Laplacians and the kernels. G and˜Łand˜ and˜Ł are, respectively, the combined kernel matrix and the combined Laplacian matrix. The κ parameter in (12) is replaced by the coefficients assigned on each individual data sources. To solve Q1, in the first phase we maximize J Q1 with respect to A, keeping θ fixed (initialized by random guess). In the second phase, we maximize J Q1 with respect to θ, keeping A fixed. The two phases optimize the same objective and repeat until convergence locally. When θ is fixed, denoting = ˜ Ł+˜GŁ+˜ Ł+˜G, Q 1 is exactly a Rayleigh quotient problem and the optimal A * can be solved as a eigenvalue problem of. When A is fixed, the problem reduces to the optimization of the coefficients θ l with given cluster memberships. In Supplementary Material 2, we show that when the A is given, Q1 can be formulated as Kernel Fisher Discriminant (KFD) in the high-dimensional feature space F. We introduce W =[<ref type="bibr">[ w 1 ,..., w K ]</ref>, a projection matrix determining the pairwise discriminating hyperplane. Since the discriminant analysis is invariant to the magnitude of w, we assume that W T W = I K , thus Q1 can be equivalently formulated as Q2: maximize A,W , θ</p><formula>J Q2 = trace W T A T AW −1 W T A T (G+ˆŁG+ˆ G+ˆŁ)AW , (14) s.t. A T A = I k ,</formula><formula>W T W = I k , ˆ Ł = r i=1 θ i ˆ L i , G = s j=1 θ j+r G cj , θ l ≥ 0, l = 1,...,(r +s), r i=1 θ δ i = 1, s j=1 θ δ j+r = 1.</formula><p>The bi-level optimization to solve Q1 corresponds to two steps to solve Q2. In the first step (clustering), we set W = I k and optimize A, which is exactly the additive kernel Laplacian integration as (12); in the second step (KFD), we fix A and optimize W and θ. Therefore, the two components optimize toward the same objective as a Rayleigh quotient in F so the iterative optimization converges to a local optimum. Moreover, in the second step, we are not interested in the separating hyperplane defined in W , instead, we only need the optimal coefficients θ l assigned on the Laplacians and the kernels. It is known that Fisher discriminant analysis is related to the least squares approach (<ref type="bibr" target="#b11">Duda et al., 2001</ref>), and the KFD (<ref type="bibr" target="#b22">Mika et al., 1999</ref>) is related to and can be solved as a least squares support vector machine (LS-SVM), proposed by (<ref type="bibr" target="#b28">Suykens et al., 2002</ref>). The problem of optimizing multiple kernels for supervised learning (MKL) has been studied by<ref type="bibr">Lanckriet</ref>, we derive the MKL extension for LSSVM and propose some efficient solutions to solve the problem. In this article, the KFD problems are formulated as LSSVM MKL and solved by semi-infinite programming (SIP;<ref type="bibr" target="#b26">Sonnenburg et al., 2006</ref>). The concrete solutions and algorithms are presented in Yu et al.</p><formula>(2010b).</formula><formula>Algorithm 2.1: OKLC(G c1 ,...,G cs , ˆ L 1 ,..., ˆ L r ,K)</formula><p>comment: Obtain the (0) using the initial guess of θ</p><formula>(0) 1 ,...,θ</formula><p>(0) r+s</p><formula>A (0) ← Eigenvalue decomposition( (0) ,K) γ = 0 while (A &gt;&gt;) do ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ step1 : F (γ) ← A (γ) step2 : θ (γ) 1 ,...,θ (γ) r ← SIP-LSSVM-MKL( ˆ L 1 ,..., ˆ L r ,F (γ) ) step3 : θ (γ) r+1 ,...,θ (γ) r+s ← SIP-LSSVM-MKL(G c1 ,...,G cs ,F (γ) ) step4 : (r+1) ← θ (γ) 1 ˆ L (γ) 1 +...+θ (γ) r ˆ L (γ) r + θ (γ) r+1 G (γ) c1 +...+θ (γ) r+s G (γ) cs step5 : A (γ+1) ← Eigenvalue decomposition( (γ +1) ,K) step6 : A =||A (γ+1) −A (γ) || 2 /||A (γ+1) || 2 step7 : γ := γ +1 return (A (γ) ,θ (γ) 1 ,...,θ (γ) r ,θ (γ) r+1 ,...,θ (γ) r+s )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Optimize A with given θ</head><p>When θ are given, the kernel-Laplacian combined matrix is also fixed; therefore, the optimal A can be found as the dominant K number of eigenvectors of .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Optimize θ with given A When</head><p>A is given, the optimal θ assigned on Laplacians can be solved via the following KFD problem Q3: maximize</p><formula>W , θ J Q3 = trace W T A T AW −1 W T A T ˆ ŁAW (15) s.t. W T W = I k , ˆ Ł = r i=1 θ i ˆ L i , θ i ≥ 0, i = 1,...,r, r i=1 θ δ i = 1.</formula><p>In our recent work, we have found that the δ parameter controls the sparseness of source coefficients θ 1 ,...,θ r (<ref type="bibr" target="#b35">Yu et al., 2010b</ref>). The issue of sparseness in MKL is also addressed by<ref type="bibr" target="#b17">Kloft et al. (2009)</ref>. When δ is set to 1, the optimized solution is sparse, which assigns dominant values to only one or two Laplacians (kernels) and zero values to the others. The sparseness is useful to distinguish relevant sources from a large number of irrelevant data sources. However, in many applications, there are usually a small number of sources and most of these data sources are carefully selected and preprocessed. Thus, they often are directly relevant to the problem. In these cases, a sparse solution may be too selective to thoroughly combine the complementary information in the data sources. We may thus expect a non-sparse integration method which smoothly distributes the coefficients on multiple kernels and Laplacians and, at the same time, leverages their effects in the objective optimization. We have proved that when δ is set to 2, the KFD step in (15) optimizes the L 2-norm of multiple kernels, which yields a non-sparse solution. If we set δ to 0, the cluster objective is simplified as to averagely combine multiple kernels and Laplacians. In this article, we set δ Page: 121 118–126</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OKLC</head><p>to three different vales (0, 1, 2) to, respectively, optimize the sparse, average and non-sparse coefficients on kernels and Laplacians. When δ is set to 1, the KFD problem in Q3 is solved as LSSVM MKL (<ref type="bibr" target="#b35">Yu et al., 2010b</ref>), given by Q4: minimize β,t</p><formula>1 2 t + 1 2λ K b=1 β T b β b − K b=1 β T b Y −1 b 1 (16) s.t. N a=1 β ab = 0, b = 1,...,K, t ≥ K b=1 β T b ˆ L i β b , i = 1,...,r, b = 1,...,K,</formula><p>where β is the vector of dual variables, t is a dummy variable in optimization, a is the index of data samples, b is the cluster label index of the discriminating problem in KFD, Y b is the diagonal matrix representing the binary cluster assignment, the vector on the diagonal of Y b is equivalent to the b-th column of an affinity matrix F ab using {+1,−1} to discriminate the cluster assignments, given by</p><formula>F ab = +1 if A ab &gt; 0, a = 1,...,N, b = 1,...,K −1 if A ab = 0, a = 1,...,N, b = 1,...,K .</formula><formula>(17)</formula><p>The problem presented in Q4 has an efficient solution based on SIP, which is presented in Equation forty-one of (<ref type="bibr" target="#b35">Yu et al., 2010b</ref>). The optimal coefficients θ i correspond to the dual variables bounded by the quadratic constraint</p><formula>t ≥ K b=1 β T b ˆ L i β b in (16)</formula><p>. When δ is set to 2, the solution to Q3 is given by</p><formula>Q5: minimize β,t 1 2 t + 1 2λ K j=1 β T b β b − K b=1 β T b Y −1 b 1 (18) s.t. N a=1 β ab = 0, b = 1,...,K, t ≥|| s|| 2 ,</formula><formula>where s ={ K b=1 β T b ˆ L 1 β b ,..., K b=1 β T b ˆ L r β b } T ,</formula><p>other variables are defined the same as (16). The problem Q5 also has an efficient solution presented in Equation forty-two in our recent work (<ref type="bibr" target="#b35">Yu et al., 2010b</ref>). The main difference between Q4 and Q5 is that Q4 optimizes the L ∞ norm of multiple kernels, whereas Q5 optimizes the L 2 norm. The optimal coefficients solved by Q4 are more likely to be sparse; in contrast, the ones obtained by Q5 are non-sparse. The algorithm to solve Q4 and Q5 is concretely explained in Algorithm 0.2 in<ref type="bibr" target="#b35">Yu et al. (2010b)</ref>. Analogously, the coefficients assigned on kernels can also be obtained in the similar formulation, given by Q6: max</p><formula>W , θ J Q6 = trace W T A T AW −1 W T A T GAW (19) s.t. W T W = I K , G = s j=1 θ j+r G cj , θ j+r ≥ 0, j = 1,...,s, s j=1 θ δ j+r = 1,</formula><p>where most of the variables are defined in the similar way as Q3 in (15). The main difference is that the Laplacian matricesˆLmatricesˆ matricesˆL andˆLandˆ andˆL i are replaced by the centered kernel matrices G and G cj. The solution of Q6 is exactly the same as Q3, depending on the δ value, it can be solved either as Q4 or Q5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Algorithm: optimized kernel Laplacian clustering</head><p>As discussed, the proposed algorithm optimizes A and θ iteratively to convergence.</p><p>The coefficients assigned to the Laplacians and the kernels are optimized in parallel. Putting all the steps together, the pseudocode of the proposed optimized kernel Laplacian clustering (OKLC) is presented in Algorithm 2.1. The iterations in Algorithm 2.1 terminate when the cluster membership matrix A stops changing. The tolerance value is a constant value as the stopping rule of OKLC, and in our implementation it is set to 0.05. In our implementation, the final cluster assignment is obtained using the KM algorithm on A (γ). In Algorithm 2.1, we consider the δ as predefined values. When δ is set to 1 or 2, the SIP-LSSVM-MKL function optimizes the coefficients as the formulation in (16) or (18), respectively. It is also possible to combine Laplacians and kernels in an average manner. In this article, we compare all these approaches and implement three different OKLC models. These three models are denoted as OKLC model 1, OKLC model 2 and OKLC model 3 which respectively correspond to the objective Q2 in (14) when δ = 1, average combination, δ = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Datasets and experimental setup</head><p>The proposed OKLC models are validated in two real applications to combine heterogeneous datasets in clustering analysis. The datasets in the first experiment is taken from the work of multi-view text mining for disease gene identification (<ref type="bibr" target="#b34">Yu et al., 2010a</ref>). The datasets contain nine different gene-by-term text profiles indexed by nine controlled vocabularies. The original disease relevant gene dataset contains 620 genes which are known to be relevant to 29 diseases. To avoid the effect of imbalanced clusters which may affect the evaluation, we only keep the diseases that have 11–40 relevant genes. This results in 14 genetic diseases and 278 genes. Because the present article is focused on non-overlapping ('hard') clustering, we further remove 16 genes which are relevant to multiple diseases. The remaining 262 diseaserelevant genes are clustered into 14 clusters and evaluated biologically by their disease labels. For each vocabulary-based gene-by-term data source, we create a kernel matrix using the linear kernel function and the kernel normalization method proposed by (Shawe<ref type="bibr" target="#b24">Taylor and Cristianini, 2004</ref>), (<ref type="bibr">Chapter 5</ref>). An element in the kernel matrix is then equivalent to the value of cosine similarity of two vectors (Baeza<ref type="bibr" target="#b7">Yates and Ribeiro-Neto, 1999</ref>). This kernel is then regarded as the weighted adjacency matrix to create the Laplacian matrix. In total, nine kernels and nine Laplacian matrices are combined in clustering. The datasets in the second experiment are taken from Web of Science (WOS) database provided by Thomson Scientific (<ref type="bibr" target="#b21">Liu et al., 2010</ref>). After preprocessing, the dataset contains 8305 journals categorized in 22 scientific fields. To create a balanced benchmark data for evaluation, we select seven fields consisting 1421 journals. The titles, abstracts and keywords of the journal publications are indexed by a text mining program using no controlled vocabulary. The weights of terms are calculated using four weighting schemes: TF-IDF, IDF, TF and binary. The citations among journals are also investigated from four different aspects: cross-citation, co-citation, bibliographic coupling and binary cross-citation. The lexical similarities are represented as normalized linear kernel matrices (using the same methods applied on the disease data) and the citation metrics are regarded as weighted adjacency matrices to create the Laplacians. Totally, four kernels and four Laplacians are combined on journal data. The details about the two datasets are presented in Supplementary Material 3. The datasets used in our experiments are provided with labels; therefore, the clustering performance is evaluated as comparing the automatic partitions with the labels using adjusted rand index (ARI;<ref type="bibr" target="#b16">Hubert and Arabie, 1985</ref>) and normalized mutual information (NMI;<ref type="bibr" target="#b27">Strehl and Ghosh, 2002</ref>). To evaluate the ARI and NMI performance, we set K = 14 on disease data and K = 7 on journal data. We also tune the OKLC model using different K values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>We implement the proposed OKLC models to integrate multiple kernels and Laplacians on disease data and journal set data.To compare the performance, we also apply six popular ensemble clustering methods mentioned in relevant work (<ref type="bibr" target="#b34">Yu et al., 2010a</ref>) to combine the partitions of individual kernels and Laplacians as a consolidated partition. These six methods are CSPA (<ref type="bibr" target="#b27">Strehl and Ghosh, 2002</ref>), HGPA (<ref type="bibr" target="#b27">Strehl and Ghosh, 2002</ref>), MCLA (<ref type="bibr" target="#b27">Strehl and Ghosh, 2002</ref>), QMI (<ref type="bibr" target="#b29">Topchy et al., 2005</ref>), EACAL (<ref type="bibr" target="#b12">Fred and Jain, 2005</ref>) and AdacVote (<ref type="bibr" target="#b5">Ayad and Kamel, 2008</ref>). As shown in Tables 1 and 2, the performance of OKLC algorithms is better than all the compared methods and the improvement is significant. On disease data, the best performance is obtained by OKLC model 1, which uses sparse coefficients to combine nine text mining kernels and nine Laplacians to identify disease-relevant clusters (ARI: 0.5859, NMI: 0.7451). On journal data, all three OKLC models perform comparably well. The best one seems coming from OKLC model 3 (ARI: 0.7336, NMI: 0.7758), which optimizes the non-sparse coefficients on the four kernels and four Laplacians. To evaluate whether the combination of kernel and Laplacian indeed improve the clustering performance, we first systematically compared the performance of all the individual data sources using KM and SC. As shown in Supplementary Material 4, on disease data, the best KM performance (ARI 0.5441, NMI 0.7099) and SC (ARI 0.5199, NMI 0.6858) performance are obtained on LDDB text mining profile. Next, we enumerate all the paired combinations of a single kernel and a single Laplacian for clustering. The integration is based on Equation (12) and the κ value is set to 0.5 so the objectives of KM and SC are combined averagely. The performance of all 45 paired combinations is presented in Supplementary Material 5. As shown, the best KL clustering performance is obtained by integrating the LDDB kernel with KO Laplacian (ARI 0.5298, NMI 0.6949). Moreover, we also found that the integration performance varies significantly by the choice of kernel and Laplacian, which proves our previous point that the KL performance is highly dependent on the quality of kernel and Laplacian. Using the proposed OKLC algorithm, there is no need to enumerate all the possible paired combinations. OKLC combines all the kernels and Laplacians and optimizes their coefficients in parallel, yielding a comparable performance with the best paired combination of a single kernel and a single Laplacian. In<ref type="figure">Figure 1</ref>, two confusion matrices of disease data for a single run are depicted. The values on the matrices are normalized according to R ij = C j /T i , where T i is the total number of genes belonging in disease i and C j is the number of these T i genes that were clustered to belong to class j. First, it is worth noting that OKLC reduces the number of misclustered genes on breast cancer (Nr.1), cardiomyopathy (Nr.2) and muscular dystrophy (Nr.11). Among the misclustered genes in LDDB, five genes (TSG101, DBC1, CTTN, SLC22A18, AR) in breast cancer, two genes in cardiomyopathy (COX15, CSRP3) and two genes in muscular dystrophy (SEPN1, COL6A3) are correctly clustered in OKLC model 1. Second, there are several diseases where consistent misclustering occurs in both methods, such as diabetes (Nr.6) and neuropathy (Nr.12). The intuitive confusion matrices correspond to the numerical evaluation results; as shown, the quality of clustering obtained by OKLC model 1 (ARI = 0.5898, NMI = 0.7429) is higher than LDDB. The performance of individual data sources of journal data is shown in Supplementary Material 6. The best KM (ARI 0.6482, NMI 0.7104) is obtained on the IDF kernel and the best SC (ARI 0.5667, NMI 0.6807) is obtained on the cross-citation Laplacian. To combine the four kernels with four Laplacians, we evaluate all the 10 paired combinations and show the performance in Supplementary Material 7. The best performance is obtained by integrating the IDF kernel with the cross-citation Laplacian (ARI 0.7566, NMI 0.7702). As shown, the integration of lexical similarity information and citation-based Laplacian indeed improves the performance. In<ref type="figure">Figure 2</ref>, the confusion matrices (also normalized) of journal data for a single run are illustrated. We compare the best individual data source (IDF with kernel KM, figure on the left) with the OKLC model 1. In the confusion matrix of IDF KM, 79 journals belonging to agriculture science (Nr.1) are misclustered to environment ecology (Nr.3), 9 journals are misclustered to pharmacology and toxicology (Nr.7). In OKLC, the number of agriculture journals misclustered to environment ecology is reduced to 45, and the number to pharmacology and toxicology is reduced to 5. On other journal clusters, the performance of the two models is almost equivalent. We also investigated the performance of combining only multiple kernels or multiple Laplacians. On the disease dataset, we combined the nine kernels and the nine Laplacians for clustering, respectively, using all the compared methods in<ref type="figure" target="#tab_1">Tables 1</ref>between the ranks of weights and the ranks of performance on both datasets. The correlations of disease kernels, disease Laplacians, journal kernels and journal Laplacians are, respectively, 0.5657, 0.6, 0.8 and 0.4. In some relevant work, the average Spearman correlations are mostly around 0.4 (<ref type="bibr" target="#b18">Lanckriet et al., 2004;</ref><ref type="bibr" target="#b33">Ye et al., 2008</ref>). Therefore, the optimal weights obtained in our experiments are generally consistent with the rank of performance. As a spectral clustering algorithm, the optimal cluster number of OKLC can be estimated by checking the plot of eigenvalues (von<ref type="bibr" target="#b30">Luxburg, 2007</ref>). To demonstrate this, we investigated the dominant eigenvalues of the optimized combination of kernels and Laplacians. In<ref type="figure" target="#fig_4">Figure 3</ref>, we compare the difference of three OKLC models with the pre-defined K (set as equal to the number of class labels). In practical research, one can predict the optimal cluster number by checking the 'elbow' of the eigenvalue plot. As shown in<ref type="figure" target="#fig_4">Figure 3</ref>, the 'elbow' in disease data is quite obvious at the number of 14. In journal data, the 'elbow' is more likely to range from 6 to 12. All the three OKLC models show a similar trend on the eigenvalue plot. Moreover, in Supplementary Material 9 we also compare the eigenvalue curves using different K values as input. As shown, the eigenvalue plot is quite stable with respect to the different inputs of K, which means the optimized kernel and Laplacian coefficients are quite independent with the K value. This advantage enables a reliable prediction about the optimal cluster number by integrating multiple data sources. To investigate the computational time, we benchmark OKLC algorithms with other clustering methods on the two datasets. As shown in<ref type="figure" target="#tab_7">Table 7</ref>, when optimizing the coefficients, OKLC algorithm (models 1 and 3) spends longer time than the other methods to optimize the coefficients on the Laplacians andthe kernels. However, the proposed algorithm is still efficient. Considering the fact that the proposed algorithm yields much better performance and more enriched information (the ranking of the individual sources) than other methods, it is worth spending extra computational complexity on a promising algorithm.The reported values are averaged from 20 repetitions. The CPU time is evaluated on Matlab v7.6.0 + Windows XP2 installed on a Laptop computer with Intel Core 2 Duo 2.26 GHz and 2 G memory. and different experimental settings. The proposed OKLC algorithms perform significantly better than other methods. Moreover, the coefficients of kernels and Laplacians optimized by OKLC show strong correlation with the rank of performance of individual data source. Though in our evaluation the K values are predefined, in practical studies, the optimal cluster number can be consistently estimated from the eigenspectrum of the combined kernel Laplacian matrix. The proposed OKLC algorithm demonstrates the advantage of combining and leveraging information from heterogeneous data structures and sources. It is potentially useful in bioinformatics and many other application areas, where there is a surge of interest to integrate similarity-based information and interaction-based relationships in statistical analysis and machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 122 118–126</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.Yu et al.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.Yu et al.</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>± 0.0649 1.47E-16 0.4093 ± 0.0740 6.98E-14 All the comparing methods combine nine kernels and nine Laplacians. The mean values and the SDs are observed from 20 random repetitions. The best performance is shown in bold. The P-values are statistically evaluated with the best performance using paired t-test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>± 0.0485 8.84E-05 0.7173 ± 0.0291 1.25E-05 HGPA 0.6673 ± 0.0419 4.74E-06 0.7141 ± 0.0269 5.19E-06 MCLA 0.6571 ± 0.0746 6.55E-05 0.7128 ± 0.0463 2.31E-05 QMI 0.6592 ± 0.0593 5.32E-06 0.7250 ± 0.0326 1.30E-05 EACAL 0.5808 ± 0.0178 3.85E-11 0.7003 ± 0.0153 6.88E-09 AdacVote 0.5899 ± 0.0556 1.02E-07 0.6785 ± 0.0325 6.51E-09 All the comparing methods combine four kernels and four Laplacians. The mean values and the SDs are observed from 20 random repetitions. The best performance is shown in bold. The P-values are statistically evaluated with the best performance using paired t-test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>and2.Fig.1.Fig.2.</head><figDesc>Fig. 1. Confusion matrices of disease data obtained by kernel KM on LDDB (A) and OKLC model 1 integration (B). The numbers of cluster labels are consistent with the numbers of diseases presented in Supplementary Material 3. In each row of the confusion matrix, the diagonal element represents the fraction of correctly clustered genes and the off-diagonal non-zero element represents the fraction of misclustered genes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.3.</head><figDesc>Fig. 3. The plot of eigenvalues (A and B) of the optimal kernel-Laplacian combination obtained by all OKLC models. The parameter K is set as equivalent as the reference label numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><figDesc>Funding: The work was supported by (i) Research Council KUL: ProMeta, GOA Ambiorics, GOA MaNet, CoE EF/05/006, PFV/10/016 SymBioSys, START 1, Optimization in Engineering(OPTEC), IOF-SCORES4CHEM, several PhD/postdoc &amp; fellow grants; (ii) FWO: G.0302.07(SVM/Kernel), G.0318.05 (subfunctionalization), G.0553.06 (VitamineD), research communities (ICCoS, ANMMM, MLDM); G.0733.09 (3UTR),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>10:56 10/12/2010 Bioinformatics-btq569.tex] Page: 119 118–126 OKLC in kernel k-means clustering (Yu,S. et al. Optimized data fusion for kernel K-means</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>KL integration becomes minimize A trace A T ˜ LA −(1−κ) trace A T G c A (8) subject to A T A = I K ,</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>A trace κA T ˆ LA+(1−κ)A T G c A (12) subject to A T A = I k ,</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>et al. (2004) and Bach et al. (2004). In our recent work Yu et al. (2010b)</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>Table 1. Performance on disease dataset</figDesc><table>Algorithm ARI 
P-value 
NMI 
P-value 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><figDesc>Table 2. Performance on journal dataset</figDesc><table>Algorithm ARI 
P-value 
NMI 
P-value 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><figDesc>Table 3.</figDesc><table>The average values of coefficients of kernels and Laplacians in 
disease dataset optimized by OKLC model 1 

Rank of θ 
Source 
θ value 
Performance rank 

1 
LDDB kernel 
0.6113 
1 
2 
MESH kernel 
0.3742 
6 
3 
Uniprot kernel 
0.0095 
5 
4 
Omim kernel 
0.0050 
2 

1 
LDDB Laplacian 
1 
1 

The sources assigned with 0 coefficient are not presented. The performance is 
ranked by the average values of ARI and NMI evaluated on each individual sources 
(Supplementary Material 3). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><figDesc>Table 4. The average values of coefficients of kernels and Laplacians in journal data set optimized by OKLC model 1 Rank of θ Source θ value Performance rank</figDesc><table>1 
IDF kernel 
0.7574 
1 
2 
TF kernel 
0.2011 
3 
3 
Binary kernel 
0.0255 
2 
4 
TF-IDF kernel 
0.0025 
4 

1 
Bibliographic Laplacian 
1 
1 

The sources assigned with 0 coefficient are not presented. The performance is 
ranked by the average values of ARI and NMI evaluated on each individual sources 
(Supplementary Material 5). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="true"><figDesc>Table 5. The average values of coefficients of kernels and Laplacians in disease data set optimized by OKLC model 3 Rank of θ Source θ value Performance rank</figDesc><table>1 
LDDB kernel 
0.4578 
1 
2 
MESH kernel 
0.3495 
6 
3 
OMIM kernel 
0.3376 
2 
4 
SNOMED kernel 
0.3309 
7 
5 
MPO kernel 
0.3178 
3 
6 
GO kernel 
0.3175 
8 
7 
eVOC kernel 
0.3180 
4 
8 
Uniprot kernel 
0.3089 
5 
9 
KO kernel 
0.2143 
9 

1 
LDDB Laplacian 
0.6861 
1 
2 
MESH Laplacian 
0.2799 
4 
3 
OMIM Laplacian 
0.2680 
2 
4 
GO Laplacian 
0.2645 
7 
5 
eVOC Laplacian 
0.2615 
6 
6 
Uniprot Laplacian 
0.2572 
8 
7 
SNOMED Laplacian 
0.2559 
5 
8 
MPO Laplacian 
0.2476 
3 
9 
KO Laplacian 
0.2163 
9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><figDesc>Table 6.</figDesc><table>The average values of coefficients of kernels and Laplacians in 
journal dataset optimized by OKLC model 3 

Rank of θ 
Source 
θ value Performance rank 

1 
IDF kernel 
0.5389 
1 
2 
Binary kernel 
0.4520 
2 
3 
TF kernel 
0.2876 
4 
4 
TF-IDF kernel 
0.2376 
3 

1 
Bibliographic Laplacian 
0.7106 
1 
2 
Cocitation Laplacian 
0.5134 
4 
3 
Crosscitation Laplacian 
0.4450 
2 
4 
Binarycitation Laplacian 
0.1819 
3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="true"><figDesc>Table 7. Comparison of CPU time of all algorithms</figDesc><table>Algorithm 
Disease data (s) 
Journal data (s) 

OKLC model 1 
42.39 
1011.4 
OKLC model 2 
0.19 
13.27 
OKLC model 3 
37.74 
577.51 
CSPA 
9.49 
177.22 
HGPA 
10.13 
182.51 
MCLA 
9.95 
320.93 
QMI 
9.36 
186.25 
EACAL 
9.74 
205.59 
AdacVote 
9.22 
172.12 

</table></figure>

			<note place="foot">at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="120"> at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="121"> at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">or kernels (step 3). As shown in Supplementary Material 8, the performance of OKLC is also comparable to the best performance obtained either by kernel combination or Laplacian combination. In particular, of all the methods we compared, the best performance is all obtained on OKLC models or its simplified forms. It is interesting to observe that the average combination model (OKLC model 2) performs quite well on the journal dataset but not on the disease dataset. This is probably because most of the sources in journal dataset are relevant to the problem, whereas in disease dataset some data sources are noisy, and thus the integration of disease data sources is a non-trivial task. We expect that the other two OKLC models (models 1 and 3) optimize the coefficients assigned on the kernels and the Laplacians to leverage multiple sources in integration and, at the same time, to increase the robustness of the combined model on combining relevant and irrelevant data sources. To evaluate whether the optimized weights assigned on individual sources have correlation with the performance, we compare the rank of coefficients with the rank of performance from Tables 3–6. As shown, the largest coefficients correctly indicate the best individual data sources. It is worth noting that in multiple kernel learning, the rank of coefficients are only moderately correlated with the rank of individual performance. In our experiments, the MeSH kernel gets the second largest weights though its performance in evaluation is low. In MKL, it is usual that the best individual kernel found by cross-validation may not lead to a large weight when used in combination (Ye et al., 2008). Kernel fusion combines multiple sources at a refined granularity, where the &apos;moderate&apos; kernels containing weak and insignificant information could complement to other kernels to compose a &apos;good&apos; kernel containing strong and significant information. Though such complementary information cannot be incorporated when cross-validation is used to choose a single best kernel, these &apos;moderate&apos; kernels are still useful when combined with other kernels (Ye et al., 2008). Based on the ranks presented in Tables 5 and 6, we calculate the Spearman correlations 123 at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="4"> CONCLUSION In this article, we propose a new clustering approach, OKLC, to optimize the combination of multiple kernels and Laplacians in clustering analysis. The objective of OKLC is formulated as a Rayleigh quotient function and is solved iteratively as a bi-level optimization procedure. In the simplest interface, the proposed algorithm only requires one input parameter, the cluster number K, from the user. Moreover, depending on user&apos;s expectation to select the most relevant sources or to evenly combine all sources, the sparseness of coefficient vector θ can be controlled via the parameter δ. In our article, we propose three variants of the OKLC algorithm and validate them on two real applications. The performance of clustering is systematically compared with a variety of algorithms 124 at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="126"> at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<monogr>
		<title level="m" type="main">082409 (EGFR); (iii) IWT: PhD Grants</title>
		<author>
			<persName>
				<forename type="first">G</forename>
			</persName>
		</author>
		<imprint>
			<pubPlace>Eureka-Flite+, Silicos</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">O&amp;O-Dsquare; (iv) Belgian Federal Science Policy Office: IUAP P6/25 (BioMaGNet, Bioinformatics and Modeling: from Genomes to Networks</title>
		<author>
			<persName>
				<surname>Sbo-Bioframe</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Sbo-Moka</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Sbo Lecopro</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Sbo</forename>
				<surname>Sbo Climaqs</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Pom</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Tbm-Iota3</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IUAP P6/04 (DYSCO, Dynamical systems, control and optimization</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<monogr>
		<title level="m" type="main">Cancer plans; (vi) Centre for R&amp;D Monitoring of the Flemish Government; (vii) EU-RTD: ERNSI: European Research Network on System Identification; FP7-HEALTH CHeartED; FP7-HD-MPC (INFSO-ICT-223854), COST intelliCIS</title>
		<imprint>
			<publisher>ICT-248940</publisher>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<monogr>
		<title level="m" type="main">Conflict of Interest: none declared</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">REFERENCES</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Cumulative voting consensus method for partitions with a variable number of clusters</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">G</forename>
				<surname>Ayad</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">S</forename>
				<surname>Kamel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="160" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiple kernel learning, conic duality, and the SMO algorithm</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">R</forename>
				<surname>Bach</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st International Conference on Machine Learning</title>
		<meeting><address><addrLine>Banff, Alberta</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="6" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Pattern Recognition and Machine Learning</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Baeza-Yates</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Ribeiro-Neto</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Modern Information Retrieval. ACM press. Bishop,C.M</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Information geometry and alternating minimization procedures</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Csiszar</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Tusnady</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Decis</title>
		<imprint>
			<biblScope unit="page" from="205" to="237" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
	<note>Suppl. . 1</note>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Kernel k-means, spectral clustering and normalized cuts</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">S</forename>
				<surname>Dhillon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM KDD</title>
		<meeting>the 10th ACM KDD<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="551" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">K-means clustering via principal component analysis</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Ding</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>He</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st International Conference on Machine Learning</title>
		<meeting><address><addrLine>Banff, Alberta</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<monogr>
		<title level="m" type="main">Pattern Classification</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">O</forename>
				<surname>Duda</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>John Wiley &amp; Sons Inc</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
	<note>2nd. edn</note>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining multiple clusterings using evidence accumulation</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">L N</forename>
				<surname>Fred</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">K</forename>
				<surname>Jain</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="835" to="850" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Mercer kernel-based clustering in feature space</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Girolami</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="780" to="784" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">New spectral methods for ratio cut partitioning and clustering</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Hagen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kahng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. Aided Des</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1074" to="1085" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<monogr>
		<title level="m" type="main">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hastie</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>2nd. edn</note>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Comparing partition</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Hubert</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Arabie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Classific</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient and accurate Lp-norm multiple Kernel learning</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Kloft</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing System 22</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning the kernel matrix with semidefinite programming</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Lanckriet</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="27" to="72" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="m">Bioinformatics-btq569.tex] Page</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="56" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Yu</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Weighted hybrid clustering by combining text mining and bibliometrics on large-scale journal database</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Liu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Soc. Inform. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="1105" to="1119" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Fisher discriminant analysis with kernels</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Mika</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEE N.N. Singal. Process</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="41" to="48" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">On spectral clustering: analysis and an algorithm</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">Y</forename>
				<surname>Ng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing 14</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<monogr>
		<title level="m" type="main">Kernel Methods for Pattern Analysis</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Shawe-Taylor</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Cristianini</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Shi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Malik</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Large scale multiple Kernel learning</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Sonnenburg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1531" to="1565" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Cluster ensembles: a knowledge Reuse framework for combining multiple partitions</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Strehl</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Ghosh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="583" to="617" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<monogr>
		<title level="m" type="main">Least Squares Support Vector Machines</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">A K</forename>
				<surname>Suykens</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>World Scientific Publishing</publisher>
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Clustering ensembles: models of consensus and weak partitions</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Topchy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1866" to="1881" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Von Luxburg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Integrated KL(K-means-Laplacian) clustering: a new clustering approach by combining attribute data and pairwise relations</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Wang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proccedings of SDM 09</title>
		<meeting>cedings of SDM 09</meeting>
		<imprint>
			<publisher>SIAM Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="38" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Nonlinear adaptive distance metric learning for clustering</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Ye</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proccedings of the 13th ACM KDD</title>
		<meeting>cedings of the 13th ACM KDD<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="123" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-class discriminant kernel learning via convex programming</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Ye</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="719" to="758" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Gene prioritization and clustering by multi-view text mining</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Yu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">L2-norm multiple kernel learning and its application to biomedical data fusion</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Yu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="53" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>