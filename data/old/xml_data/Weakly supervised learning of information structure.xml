
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining Weakly supervised learning of information structure of scientific abstracts—is it accurate enough to benefit real-world tasks in biomedicine?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011">2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Yufan</forename>
								<surname>Guo</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Laboratory</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<postCode>CB3 0FD</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Anna</forename>
								<surname>Korhonen</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Laboratory</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<postCode>CB3 0FD</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Ilona</forename>
								<surname>Silins</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Environmental Medicine</orgName>
								<orgName type="institution">Karolinska Institutet</orgName>
								<address>
									<postCode>SE-171 77</postCode>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Ulla</forename>
								<surname>Stenius</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Environmental Medicine</orgName>
								<orgName type="institution">Karolinska Institutet</orgName>
								<address>
									<postCode>SE-171 77</postCode>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining Weakly supervised learning of information structure of scientific abstracts—is it accurate enough to benefit real-world tasks in biomedicine?</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">27</biblScope>
							<biblScope unit="issue">22</biblScope>
							<biblScope unit="page" from="3179" to="3185"/>
							<date type="published" when="2011">2011</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btr536</idno>
					<note type="submission">Received on April 21, 2011; revised on August 29, 2011; accepted on September 19, 2011</note>
					<note>[12:39 3/11/2011 Bioinformatics-btr536.tex] Page: 3179 3179–3185 Associate Editor: Jonathan Wren</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Many practical tasks in biomedicine require accessing specific types of information in scientific literature; e.g. information about the methods, results or conclusions of the study in question. Several approaches have been developed to identify such information in scientific journal articles. The best of these have yielded promising results and proved useful for biomedical text mining tasks. However, relying on fully supervised machine learning (ML) and a large body of annotated data, existing approaches are expensive to develop and port to different tasks. A potential solution to this problem is to employ weakly supervised learning instead. In this article, we investigate a weakly supervised approach to identifying information structure according to a scheme called Argumentative Zoning (AZ). We apply four weakly supervised classifiers to biomedical abstracts and evaluate their performance both directly and in a real-life scenario in the context of cancer risk assessment. Results: Our best weakly supervised classifier (based on the combination of active learning and self-training) performs well on the task, outperforming our best supervised classifier: it yields a high accuracy of 81% when just 10% of the labeled data is used for training. When cancer risk assessors are presented with the resulting annotated abstracts, they find relevant information in them significantly faster than when presented with unannotated abstracts. These results suggest that weakly supervised learning could be used to improve the practical usefulness of information structure for real-life tasks in biomedicine. Availability: The annotated dataset, classifiers and the user test for cancer risk assessment are available online at</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many practical tasks in biomedicine require accessing specific types of information in scientific literature. For example, a biomedical scientist may be looking for information about the objective of the study in question, the methods used, the results obtained or the * To whom correspondence should be addressed. conclusions drawn by the authors. Similarly, many biomedical text mining tasks (e.g. information extraction, summarization) focus on the extraction of specific types of information in documents only. To date, a number of approaches have been proposed for the classification of sentences in scientific literature according to categories of information structure (or discourse, rhetorical, argumentative or conceptual structure, depending on the framework in question). Some of the approaches classify sentences according to typical section names seen in scientific documents (<ref type="bibr" target="#b11">Hirohata et al., 2008;</ref><ref type="bibr" target="#b17">Lin et al., 2006</ref>), while others are based e.g. on argumentative zones (<ref type="bibr" target="#b21">Mizuta et al., 2006;</ref><ref type="bibr" target="#b33">Teufel and Moens, 2002;</ref><ref type="bibr" target="#b34">Teufel et al., 2009</ref>), qualitative dimensions (<ref type="bibr" target="#b30">Shatkay et al., 2008</ref>) or conceptual structure (<ref type="bibr" target="#b16">Liakata et al., 2010</ref>) of documents. The best current approaches have yielded promising results and proved useful for information retrieval, information extraction and summarization tasks (<ref type="bibr" target="#b21">Mizuta et al., 2006;</ref><ref type="bibr" target="#b27">Ruch et al., 2007;</ref><ref type="bibr" target="#b32">Tbahriti et al., 2006;</ref><ref type="bibr" target="#b33">Teufel and Moens, 2002</ref>). However, relying on fully supervised machine learning (ml) and a large body of annotated data, existing approaches are expensive to develop and port to different domains and tasks, and thus intractable for use in real-life applications. A potential solution to this bottleneck is to develop techniques based on weakly supervised ml instead. Making use of a small amount of labeled data and a large pool of unlabeled data, weakly supervised learning (e.g. semi-supervision, active learning, co/tri-training, self-training) aims to keep the advantages of fully supervised approaches. It has been applied to a wide range of natural language processing (nlp) and text mining tasks, including namedentity recognition, question answering, information extraction, text classification and many others (<ref type="bibr" target="#b0">Abney, 2008</ref>), yielding performance levels similar or equivalent to those of fully supervised techniques. In this article, we investigate the potential of weakly supervised learning for Argumentative Zoning (az) of biomedical abstracts. az provides an analysis of the argumentative structure (i.e. the rhetorical progression of the argument) of a scientific document (<ref type="bibr" target="#b33">Teufel and Moens, 2002</ref>). It has been used to analyze scientific texts in various disciplines—including computational linguistics (<ref type="bibr" target="#b33">Teufel and Moens, 2002</ref>), law, (<ref type="bibr" target="#b8">Hachey and Grover, 2006</ref>), biology (<ref type="bibr" target="#b21">Mizuta et al., 2006</ref>) and chemistry (<ref type="bibr" target="#b34">Teufel et al., 2009</ref>)—and has proved useful for nlp tasks such as summarization (<ref type="bibr" target="#b33">Teufel and Moens, 2002</ref>). However, the application of az to different domains has resulted in laborious annotation exercises that suggests that a weakly supervised approach would be more practical for the real-world application of az.Taking two supervised classifiers as a comparison point—Support Vector Machines (svm) and Conditional Random Fields (crf)—we investigate the performance of four weakly supervised classifiers for az: two based on semi-supervised learning (transductive svm and semi-supervised crf) and two on active learning (Active svm alone and in combination with self-training). We apply these classifiers to az-annotated biomedical abstracts in the recent dataset of<ref type="bibr" target="#b6">Guo et al. (2010)</ref>. The results are promising. Our best weakly supervised classifier (Active svm with self-training) outperforms the best supervised classifier (svm), yielding high accuracy of 81% when using just 10% of the labeled data for training. When using just onethird of the labeled data, it performs as well as a fully supervised svm, which uses 100% of the labeled data. The abstracts in the dataset of<ref type="bibr" target="#b6">Guo et al. (2010)</ref>were selected on the basis of their suitability for cancer risk assessment (cra). This enables us to conduct user-based evaluation of the practical usefulness of our approach for the real-world task of cra. We investigate whether cancer risk assessors find relevant information in abstracts faster when the abstracts are annotated for az using our best weakly supervised approach. The results are promising: although manual annotations yield the biggest time savings: 10–13% (compared with the time it takes to examine unannotated abstracts), considerable savings are also obtained with weakly supervised ml annotations: 7–8% (using active svm with self-training). In sum, our investigation shows that weakly supervised az can be employed to improve the practical applicability and portability of az to different information access tasks and that its accuracy is high enough to benefit a real-life task in biomedicine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>We used in our experiments the recent dataset of<ref type="bibr" target="#b6">Guo et al. (2010)</ref>, consisting of 1000 cra abstracts (7985 sentences and 225 785 words) annotated according to az. Originally introduced by Teufel and Moens (2002), az is a scheme that provides an analysis of the argumentative structure of a document, following the knowledge claims made by the authors. The dataset of<ref type="bibr" target="#b6">Guo et al. (2010)</ref>has been annotated according to the version of az developed for biology papers (<ref type="bibr" target="#b21">Mizuta et al., 2006</ref>) (with only minor modifications concerning zone names). Seven categories of this scheme (out of the 10 possible) actually appear in abstracts and in the resulting dataset. These are shown and explained in<ref type="bibr" target="#b6">Guo et al. (2010)</ref>reported the inter-annotator agreement between their three annotators: one linguist, one computational linguist and one domain expert. The agreement (κ = 0.85) is relatively high according to Cohen (1960).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weakly supervised learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Automatic classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Features and feature extraction</head><p>The first step in automatic classification is to select a set of features that may indicate az categories in abstracts. Following<ref type="bibr" target="#b6">Guo et al. (2010)</ref>, we implemented a set of features that have proved successful in related works, e.g. (<ref type="bibr" target="#b11">Hirohata et al., 2008;</ref><ref type="bibr" target="#b17">Lin et al., 2006;</ref><ref type="bibr" target="#b22">Mullen et al., 2005</ref>@BULLET Voice. The voice of verbs (active or passive) in the corpus.</p><p>These features were extracted from the corpus using a number of tools. A tokenizer was used to detect sentence boundaries and to perform basic tokenization (in extreme cases, processing complex biomedical terms e.g. 2amino-3,8-diethylimidazo<ref type="bibr">[4,5-f ]</ref>quinoxaline). The C&amp;C tools (<ref type="bibr" target="#b3">Curran et al., 2007</ref>) were used for pos tagging, lemmatization and parsing. The lemma output was used for Word, Bi-gram and Verb features, and the gr output for gr, Subj, Obj and Voice features. The 'obj' marker in a subject relation indicates passive voice [e.g. (ncsubj observed_14 difference_5 obj)]. Verb classes were obtained automatically using unsupervised spectral clustering (<ref type="bibr" target="#b31">Sun and Korhonen, 2009</ref>). To reduce data sparsity, we lemmatized the lexical items for all the features, and removed words and grs with &lt;2 occurrences and bi-grams with &lt;5 occurrences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Machine learning methods</head><p>The next step is to assign sentences in abstracts to zone categories using machine learning. Support vector machines (svm) and conditional random fields (crf) have proved the best performing fully supervised methods in recent related works e.g. (<ref type="bibr" target="#b6">Guo et al., 2010;</ref><ref type="bibr" target="#b11">Hirohata et al., 2008;</ref><ref type="bibr" target="#b22">Mullen et al., 2005;</ref><ref type="bibr" target="#b33">Teufel and Moens, 2002</ref>). We therefore implemented these methods as well as weakly supervised variations of them: active svm with and without self-training, transductive svm and semi-supervised crf. Supervised methods: svm aims to find the maximum-margin hyperplane, which has the largest distance to the nearest data points of any class. The problem is defined as:</p><formula>Maximize 2 |w| (in w,b) subject to y(w·x −b) ≥ 1,</formula><p>where x is data, y is its label, w is a normal vector to the hyperplane and 2 |w| is the margin. We used Weka software (<ref type="bibr" target="#b9">Hall et al., 2009</ref>) employing the smo algorithm (Platt, 1999b) with linear kernel for svm experiments. crf is an undirected graphical model that defines a probability distribution over the hidden states (e.g. label sequences) given the observations. The probability of a label sequence y given an observation sequence x can be written as:</p><formula>p(y|x,θ) = 1 Z(x) exp ⎛ ⎝ j θ j F j (y,x) ⎞ ⎠ ,</formula><p>where F j (y,x) is a real-valued feature function of the states and the observations; θ j is the weight of F j , and Z(x) is a normalization factor. We used Mallet software (http://mallet.cs.umass.edu) employing the l-bfgs algorithm (<ref type="bibr" target="#b24">Nocedal, 1980</ref>) for crf experiments. Weakly supervised methods: Active svm (asvm) starts with a small amount of labeled data, and iteratively chooses a certain amount of unlabeled data, about which the classifier is least certain, to be manually labeled (the labels can be restored from the fully annotated corpus) for the next round of learnig. We used an uncertainty sampling query strategy (<ref type="bibr" target="#b15">Lewis and Gale, 1994</ref>). In particular, we compared the posterior probabilities of the best estimate given each unlabeled instance, and chose the instances with the lowest probabilities to be labeled for later use. The probabilities can be calculated by fitting a Sigmoid after the standard svm (<ref type="bibr" target="#b25">Platt, 1999a</ref>) and, in the multi-class case, combined using a pairwise coupling algorithm (<ref type="bibr" target="#b10">Hastie and Tibshirani, 1998</ref>). We used the-M flag in Weka for computing the posterior probabilities. Active svm with self-training (assvm) is an extension of asvm where each round of learning has two steps:</p><p>(i) Active learning (a) Train a new classifier on all the labeled examples.</p><p>(b) Apply the current classifier to each unlabeled example.</p><p>(c) Find n examples about which the classifier is least certain to be manually labeled.</p><p>(ii) Self-training</p><p>(a) Train a new classifier on both labeled and unlabeled/machinelabeled data using the estimates from step (i)(b).</p><p>(b) Test the current classifier on test data. Transductive svm (tsvm) is an extension of svm that aims to:</p><formula>Maximize 2 |w| (in w,b,y (u) ), Subject to y (l) (w·x (l) −b) ≥ 1, y (u) (w·x (u) −b) ≥ 1, y (u) ∈{−1,1}, where x (u)</formula><p>is unlabeled data and y (u) the estimate of its label. The idea is to find a prediction on unlabeled data such that the decision boundary has the maximum margin on both the labeled and the unlabeled (now labeled) data. The latter guides the decision boundary away from dense regions. We used UniverSVM software (http://3t.kyb.tuebingen.mpg.de/bs/people/fabee/ universvm.html) employing the cccp algorithm (<ref type="bibr" target="#b2">Collobert et al., 2006</ref>) for tsvm experiments. Semi-supervised crf (sscrf) can be implemented by entropy regularization (<ref type="bibr" target="#b13">Jiao et al., 2006</ref>), which extends the objective function on Labeled data</p><formula>L logp(y (l) |x (l) ,θ) with an additional term U Y p(y|x (u) ,θ)logp(y|x (u)</formula><p>,θ) to minimize the conditional entropy of the model's predictions on Unlabeled data. We used Mallet software for sscrf experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Evaluation methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Y.Guo et al.</head><p>as test data and the remaining nine folds as training data (with x% being manually labeled). The results were then averaged. As randomly selected labeled data were used for svm, crf, tsvm and sscrf, the results for these methods were averaged from five runs. Following Dietterich (1998), we used McNemar's test (<ref type="bibr" target="#b20">McNemar, 1947</ref>) to measure the statistical significance of the differences between the results of supervised and weakly supervised learning. The chosen significance level was 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">User test in the context of cra</head><p>A major time-consuming component of chemical cancer risk assessment (cra) is the review and analysis of existing scientific literature on the chemical in question. MEDLINE (http://www.nlm.nih.gov/databases/ databases_medline.html) abstracts are typically used as a starting point in this work. Risk assessors (e.g. toxicologists, biologists) read the abstracts of interest, looking for various information in them (e.g. about the methods, results and conclusions of the study in question) (<ref type="bibr" target="#b14">Korhonen et al., 2009</ref>). One way to speed up this work is to annotate the abstracts with categories of information structure so that the information of interest can be found faster.<ref type="bibr" target="#b7">Guo et al. (2011)</ref>investigated this idea first and showed that time savings can be obtained in literature review when abstracts are annotated either manually or automatically (using fully supervised ml) according to different information structure schemes. We evaluated our weakly supervised approach to az in a similar way but re-designed the evaluation of<ref type="bibr" target="#b7">Guo et al. (2011)</ref>so that it is better controlled and covers a wider range of information. Cancer risk assessors working in Karolinska Institutet (Stockholm, Sweden) provided us with a list of 10 questions considered when studying abstracts for cra purposes.<ref type="figure" target="#tab_3">Table 3</ref>). We then designed an online questionnaire where each question–answer pair is displayed to an expert on a separate page and the zone(s) most relevant for answering the question are highlighted with colors as to attract expert's attention (<ref type="figure" target="#fig_2">Fig. 1</ref>). Two experts participated in the test: one professor level expert (A) with a long experience in cra (over 25 years) and one more junior expert (B) with a PhD in toxicology and over 5 years of experience in cra. Each expert was presented with the same set of 200 abstracts focusing on four chemicals (butadiene, diethylnitrosamine, diethylstilbestrol and phenobarbital): (i) 50 unannotated, (ii) 50 manually annotated, (iii) 50 assvm-annotated and (iv) 50 randomly annotated abstracts (i.e. annotated so that sentences were assigned to zones on the basis of their observed distribution in the training data). We compared the time it took for experts to answer the questions when presented with abstracts in (i)–(iv), and examined whether the differences are statistically significant (significance level of 0.05, Mann–Whitney U Test (<ref type="bibr" target="#b18">Mann and Whitney, 1947;</ref><ref type="bibr" target="#b36">Wilcoxon, 1945)</ref>). In addition, we evaluated the impact of (i)–(iv) on the quality of experts'answers by examining inter-expert agreement.<ref type="figure" target="#tab_4">Table 4</ref>shows the results for the four weakly supervised and two supervised methods when using 10% of the labeled data (i.e. ∼700 sentences). assvm is the best performing method, with an accuracy of 81% and macro F-score of 0.76. asvm performs nearly aswell, with an accuracy of 80% and F-score of 0.75. Both methods outperform supervised svm with a statistically significant difference (P &lt; 0.001). tsvm is the lowest performing svm-based method: its performance is lower than that of the supervised svm. Yet, it outperforms both crf-based methods. sscrf performs slightly better than crf with 1% higher accuracy and 0.01 higher F-score. Only two methods (asvm, tsvm) find six out of the seven possible zone categories. Other methods find five categories. The 1–2 missing categories are low frequency categories, accounting for 1% of the corpus data each (<ref type="figure" target="#tab_2">Table 2</ref>). The results for other categories also seem to reflect the amount of corpus data available per category (<ref type="figure" target="#tab_2">Table 2</ref>), with res being the highest and obj the lowest performing category with most methods.<ref type="figure" target="#fig_3">Figure 2</ref>shows the learning curve of different methods (in terms of accuracy) when using 0–100% of the labeled data. assvm outperforms other methods, reaching its best performance of 88% accuracy when using ∼40% of the labeled data. It outperforms asvm (the second best method) in particular when 20–40% of the labeled data is used. When using 33% of the labeled data, it performs already as well as fully supervised svm (i.e. using 100% of the labeled data). svm and tsvm tend to perform quite similarly with each other when &gt;10% of the labeled data are used, but when less data are available, tsvm performs better. Looking at the crf-based methods, sscrf outperforms crf in particular when 10–25% of the labeled data are used. However, neither of them reaches the performance level of svm-based methods, which is in line with the results of fully supervised crf and svm in<ref type="bibr" target="#b7">Guo et al. (2011)</ref>. To investigate which features are the most useful for weakly supervised learning, we took our best performing method assvm and conducted leave-one-out analysis of the features with 10% of the labeled data. The results in<ref type="figure" target="#tab_5">Table 5</ref>show that Location is by far the most useful feature, in particular for bkg, meth and con. The overall performance drops 8% in accuracy and 0.09 in F-score when removing this feature. Removing POS has almost equally strong effect, in particular on bkg and meth. Also Voice, Verb class and GR contribute to general performance. Among the least helpful features are those which suffer from sparse data problems, e.g. Word, Bi-gram and Verb). They perform particularly badly when applied to low frequency zones.<ref type="figure" target="#tab_6">Table 6</ref>shows the time (measured in seconds) it took for experts A and B to answer questions (individual and total) when presented with (i) unannotated, (ii) manually annotated, (iii) assvm annotated and (iv) randomly annotated abstracts (see Section 2 for details of(i)). time stands for the sample mean, and save for the percentage of time savings.<ref type="figure" target="#tab_7">Table 7</ref>shows the statistical significance (P-values, Mann–Whitney U Test) of the differences between the results for different abstract groups [e.g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Automatic classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">User test</head><formula>(i) v.(ii)]</formula><p>. Looking at the overall figures (i.e. Total), both manual (ii) and assvm (iii) annotations help users find relevant information significantly faster than plain text abstracts (i): the percentage of time savings ranges between 7% and 13%, and the corresponding P-values ranges between &lt; 0.001 and 0.027. Although manual annotations save more time than assvm annotations (13% versus 7% for A, and 10% versus 8% for B), assvm annotations are surprisingly useful. Random annotations (iv) have a negative effect: both experts spend more time examining (iv) than (i) abstracts: 6% for A and 19% for B. Looking at the results for individual questions, (ii) and (iii) are more helpful for answering broader questions (e.g. Q9 Is the outcome of the study expected/unexpected/neutral?) than more specific questions (e.g. Q4 Is exposure length mentioned?). Although (ii) is more helpful than (iii) for most questions, the majority of differences are not statistically significant, showing that assvm annotations are almost equally useful as manual annotations. assvm annotations have a negative effect on Q4 and Q5. Q4 and Q5 focus on meth which is a higher frequency (accounting for 18% of the corpus) but less predictable (0.76 F-score for assvm) category.time save time save time save time save time save time save time save time save time save time save time save</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 3184 3179–3185</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Y.Guo et al.</head><formula>(%) (%) (%) (%) (%) (%) (%) (%) (%) (%) (%)</formula><p>AAs we mentioned in Section 2.3: 'we compared the time it took for experts to answer the questions when presented with abstracts in (i)–(iv), and examined whether the differences are statistically significant [significance level of 0.05, Mann-Whitney U Test (<ref type="bibr" target="#b18">Mann and Whitney, 1947;</ref><ref type="bibr" target="#b36">Wilcoxon, 1945)]</ref>. Values in bold are less than 0.05, indicating that the differences are statically significant. * After rounding, this value is 0.00Since Q3 is a multiple-choice question, we report the inter-expert agreement for each option: Q3a,b,c.<ref type="figure" target="#tab_8">Table 8</ref>shows the joint probability of users' agreement on the answers. Annotations (ii), (iii) and (iv) do not affect the users' agreement a lot: 0.82–0.86 and 0.81 with and without annotations. Interestingly, experts tend to agree the most on the answers when using assvm annotated abstracts. This demonstrates that automatic annotation does not affect the quality of the answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION AND CONCLUSIONS</head><p>Our results show that weakly supervised ml can be used for the identification of information structure in biomedical abstracts. In our experiments, the majority of weakly supervised methods: assvm, asvm and sscrf outperformed their corresponding supervised methods: svm and crf. assvm/asvm selects the most difficult instances (or the instances distinct from the existing labeled data) to be manually labeled and then used for the next round of learning, offering a wider coverage of the possible inputs than svm. sscrf extends crf by taking into account the conditional entropy of the model's predictions on unlabeled data (favoring peaked, confident predictions) so that the decision boundary is moved into the sparse regions of input space. The best performing weakly supervised methods were those based on active learning. When using 10% of the labeled data, active learning combined with self-training (assvm) outperformed the best supervised method svm with a statistically significant difference. assvm reached its top performance (88% accuracy) when using 40% of the labeled data, and performed equally well as fully supervised svm when using just one-third of the labeled data. This result is in line with the results of other text classification works where active learning has proved similarly useful, e.g. Esuli and<ref type="bibr" target="#b5">Sebastiani (2009)</ref>;<ref type="bibr" target="#b15">Lewis and Gale (1994)</ref>. In addition, we have demonstrated that the accuracy of our best weakly supervised method (assvm) is high enough to benefit a real-life task in biomedicine: cancer risk assessors find relevant information in abstracts significantly faster (7–8%) when the abstracts are annotated using assvm (as opposed to being unannotated). In sum, our research shows that application of az-style approaches to real-world biomedical tasks can be realistic as only a limited amount of labeled data is needed for it. To the best of our knowledge, no previous work has been done on weakly supervised learning of textual information structure according to the family of schemes we have focused on<ref type="bibr" target="#b7">Guo et al. (2011);</ref><ref type="bibr" target="#b11">Hirohata et al. (2008);</ref><ref type="bibr" target="#b16">Liakata et al. (2010)</ref>;<ref type="bibr" target="#b17">Lin et al. (2006);</ref><ref type="bibr" target="#b21">Mizuta et al. (2006)</ref>;<ref type="bibr" target="#b30">Shatkay et al. (2008)</ref>. Previous works on these schemes have been fully supervised in nature. In addition, although some works have been evaluated in the context of text mining tasks (e.g. information extraction, summarization), the only previous work which has reported user-centered evaluation in the context of a real-life biomedical task is that of<ref type="bibr" target="#b7">Guo et al. (2011)</ref>. In the future, we plan to improve and extend this work in several directions. Semi-supervised learning (tsvm and sscrf) did not perform equally well as active learning in our experiments, although it has proved successful in related works e.g. (<ref type="bibr" target="#b13">Jiao et al., 2006</ref>). We suspect that this is due to the high dimensionality and sparseness of our labeled dataset. Given the high cost of obtaining labeled data, methods not needing it are preferable. We plan to thus experiment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 3185 3179–3185</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weakly supervised learning</head><p>with more sophisticated active learning algorithms, e.g. margin sampling (<ref type="bibr" target="#b28">Scheffer et al., 2001</ref>), query-by-committee (QBC) (<ref type="bibr" target="#b29">Seung et al., 1992</ref>) and svm simple margin (<ref type="bibr" target="#b35">Tong and Koller, 2001</ref>). Combinations of other weakly supervised methods, e.g. EM+active learning (<ref type="bibr" target="#b19">McCallum and Nigam, 1998</ref>) and co-training+EM+active learning (<ref type="bibr" target="#b23">Muslea et al., 2002</ref>) would also be worth investigating. In addition, we plan to replace the svm-based model with other models e.g. Logistic Regression, which outperforms svm in active learning as reported in (<ref type="bibr" target="#b12">Hoi et al., 2006</ref>). crf-based active learning might be a good option too. The work presented in this article has focused on the az scheme. In the future, we plan to investigate the usefulness of weakly supervised learning for identifying information structure according to other popular schemes, e.g. (<ref type="bibr" target="#b11">Hirohata et al., 2008;</ref><ref type="bibr" target="#b16">Liakata et al., 2010;</ref><ref type="bibr" target="#b17">Lin et al., 2006;</ref><ref type="bibr" target="#b30">Shatkay et al., 2008</ref>) and not only in scientific abstracts but also in full journal papers, which typically exemplify a larger set of scheme categories. Focusing on full journal papers will also enable further user-based evaluation. For example, although abstracts are used as a typical starting point in cra, subsequent steps of cra focus on information in full articles. These more challenging steps may benefit from az (and other type of) annotations to a greater degree.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>We evaluated the ml results in terms of accuracy, precision ( true positive true positive+false positive ), recall ( true positive true positive+false negative ) and F-score ( 2 * precision * recall precision+recall ) against manual annotations. We used 10-fold cross-validation for all the methods to avoid the possible bias introduced by relying on any particular split of the data. The data were randomly assigned to 10-folds of roughly the same size. Each fold was used once [12:39 3/11/2011 Bioinformatics-btr536.tex] Page: 3182 3179–3185</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4.</head><figDesc>Results when using 10% of the labeled data80 0.75 0.88 0.56 0.68 0.87 0.78 0.33 assvm 0.81 0.76 0.86 0.56 0.76 0.88 0.76 – – tsvm 0.76 0.72 0.82 0.57 0.69 0.82 0.72 0.08 – sscrf 0.71 0.65 0.78 0.50 0.48 0.77 0.73 – – mf: Macro F-score calculated for the five high frequency categories: bkg, obj, meth, res, con which are found by all the methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.1.</head><figDesc>Fig. 1. An example of the questionnaire.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.2.</head><figDesc>Fig. 2. Learning curve of different methods when using 0–100% of the labeled data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Table5.:</head><figDesc>Leave-one feature-out results for assvm with 10% of labeled dataEmploying all the features. the experts and abstract groups), along with the percentage of time savings obtained when using annotations (ii)–(iv) (compared with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>7.</head><figDesc>Significance of the results in the previous table Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 Total A (i) v. (ii) 0.035 0.837 0.063 0.975 0.397 0.421 0.032 0.296 0.015 0.285 &lt;0.001 * (i) v. (iii) 0.083 0.924 0.405 0.162 0.221 0.075 0.154 0.550 0.139 0.413 0.005 (i) v. (iv) 0.200 0.135 0.005 0.018 0.864 0.248 0.872 0.232 0.315 0.781 0.159 (ii) v. (iii) 0.570 0.633 0.235 0.141 0.052 0.242 0.326 0.530 0.321 0.851 0.041 B (i) v. (ii) 0.122 0.923 0.180 0.666 0.986 0.149 0.901 0.006 0.002 0.781 0.005 (i) v. (iii) 0.266 0.321 0.565 0.381 0.338 0.070 0.532 0.018 0.005 0.786 0.027 (i) v. (iv) 0.024 0.008 0.003 0.106 0.385 0.027 0.008 0.193 0.009 0.077 &lt;0.001 * (ii) v. (iii) 0.682 0.188 0.050 0.729 0.535 0.693 0.477 0.341 0.795 0.667 0.619</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><figDesc>(i) 0.63 0.51 0.78 0.90 0.80 0.86 0.90 0.92 0.86 0.76 0.92 0.90 0.81 (ii) 0.70 0.66 0.92 0.96 0.92 0.82 0.92 0.90 0.72 0.76 0.88 0.88 0.84 (iii) 0.82 0.68 0.96 0.98 0.90 0.86 0.86 0.90 0.74 0.86 0.90 0.88 0.86 (iv) 0.66 0.54 0.90 0.92 0.92 0.74 0.88 0.90 0.82 0.82 0.84 0.90 0.82</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><figDesc>Funding: Royal Society (UK); Swedish Research Council; FAS (Sweden); Cambridge International Scholarship (to Y.G.) and EPSRC (EP/G051070/1 UK). Conflict of Interest: none declared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><figDesc>Table 1. Categories of AZ appearing in the corpus of Guo et al. (2010)</figDesc><table>Category 
Abbreviation 
Definition and example 

Background 
bkg 
The circumstances pertaining to the current work, situation, or its causes, history, etc. 
e.g. Concerns about the possible toxic effects of workplace exposures in the synthetic rubber industry have centered 
on 1,3-butadiene (BD), styrene and dimethyldithiocarbamate (DMDTC). 

Objective 
obj 
A thing aimed at or sought, a target or goal 
e.g. The objective of this research was to evaluate techniques for the rapid detection of chromosomal alterations 
occurring in humans exposed to butadiene. 

Method 
meth 
A way of doing research, esp. according to a defined and regular plan; a special form of procedure or characteristic 
set of procedures employed in a field of study as a mode of investigation and inquiry 
e.g. The hypoxanthine-guanine phosphoribosyltransferase (HPRT) and thymidine kinase (TK) mutant frequencies 
(MFs) were measured using a cell cloning assay. 

Result 
res 
The effect, consequence, issue or outcome of an experiment; the quantity, formula, etc. obtained by calculation 
e.g. Replication past the N3 2'-deoxyuridine adducts was found to be highly mutagenic with an overall mutation 
yield of approximately 97%. 

Conclusion 
con 
A judgment or statement arrived at by any reasoning process; an inference, deduction, induction; a proposition 
deduced by reasoning from other propositions; the result of a discussion, or examination of a question, final 
determination, decision, resolution, final arrangement or agreement 
e.g. Thus, in terms of mutagenic efficiency, stereochemical configurations of EB and DEB are not likely to play a 
significant role in the mutagenicity and carcinogenicity of BD. 

Related work 
rel 
A comparison between the current work and the related work 
e.g. These data are much lower compared to previously reported values measured by GC-MS/MS. 

Future work 
fut 
The work that needs to be done in the future 
e.g. Additional studies are needed to examine the importance of base excision repair (BER) in maintaining genomic 
integrity, the differential formation of DNA and protein adducts in deficient strains, and the potential for enhanced 
sensitivity to BD genotoxicity in mice either lacking or deficient in both biotransformation and DNA repair activity. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><figDesc>Table 2. Distribution of sentences in the AZ-annotated corpus</figDesc><table>bkg 
obj 
meth 
res 
con 
rel 
fut 

Word 
36 828 23 493 41 544 89 538 30 752 2456 1174 
Sentence 
1429 
674 
1473 
3185 
1082 
95 
47 
Sentence (%) 
18 
8 
18 
40 
14 
1 
1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>Table 1.</figDesc><table>The table also shows one example 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><figDesc>Table 3. Questions and highlighted zones</figDesc><table>Question 
Zone 

Q1 Do the authors discuss previous or related research on the 
topic? y/n 

bkg rel 

Q2 Do the authors describe the aim of the research? y/n 
obj 

Q3 What is the main type of study the abstract focuses on? 
animal study/human study/in vitro study 

meth 

Q4 Is exposure length mentioned? y/n 
meth 

Q5 Is dose mentioned? y/n 
meth 

Q6 Is group size mentioned? y/n 
meth 

Q7 How many endpoints are mentioned? 0/1/more 
res 

Q8 Are the results positive? y/n/unclear 
res 

Q9 Is the outcome of the study expected/unexpected/neutral? 
con 

Q10 Do the authors mention a need for future research on the 
topic? y/n 

fut 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><figDesc>Table</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><figDesc>Table 6.</figDesc><table>Time savings 

Q1 
Q2 
Q3 
Q4 
Q5 
Q6 
Q7 
Q8 
Q9 
Q10 
Total 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><figDesc>Table</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><figDesc>Table 8.</figDesc><table>Quality of answers (inter-expert agreement) 

Q1 Q2 Q3a Q3b Q3c Q4 Q5 Q6 Q7 Q8 Q9 Q10 Total 

</table></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning for Computational Linguistics</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Abney</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Chapman &amp; Hall / CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">A coefficient of agreement for nominal scales</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Cohen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educ. Psychol. Measur</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Trading convexity for scalability</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Collobert</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning</title>
		<meeting>the 23rd International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="201" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Linguistically motivated large-scale nlp with c&amp;c and boxer</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">R</forename>
				<surname>Curran</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2007 Demonstrations Session. ACL</title>
		<meeting>the ACL 2007 Demonstrations Session. ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Approximate statistical tests for comparing supervised classification learning algorithms</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">G</forename>
				<surname>Dietterich</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1895" to="1923" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Active learning strategies for multi-label text classification</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Esuli</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Sebastiani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval</title>
		<meeting>the 31th European Conference on IR Research on Advances in Information Retrieval<address><addrLine>Berlin/Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="102" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Identifying the information structure of scientific abstracts: an investigation of three different schemes</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Guo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BioNLP. ACL</title>
		<meeting>BioNLP. ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="99" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">A comparison and user-based evaluation of models of textual information structure in the context of cancer risk assessment</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Guo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Extractive summarisation of legal texts</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Hachey</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Grover</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Law</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="305" to="345" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">The weka data mining software: an update</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Hall</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Classification by pairwise coupling</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hastie</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="451" to="471" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Identifying sections in scientific abstracts using conditional random fields</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Hirohata</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 3rd International Joint Conference on Natural Language Processing. ACL</title>
		<meeting>3rd International Joint Conference on Natural Language Processing. ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="381" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale text categorization by batch mode active learning</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">C H</forename>
				<surname>Hoi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on World Wide Web</title>
		<meeting>the 15th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="633" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised conditional random fields for improved sequence segmentation and labeling</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Jiao</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING/ACL. ACL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">The first step in the development of text mining technology for cancer risk assessment: identifying and organizing scientific evidence in risk assessment literature</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Korhonen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">303</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">A sequential algorithm for training text classifiers</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">D</forename>
				<surname>Lewis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">A</forename>
				<surname>Gale</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, Inc.</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Corpora for the conceptualisation and zoning of scientific papers</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Liakata</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC&apos;10. European Language Resources Association (ELRA)</title>
		<meeting>LREC&apos;10. European Language Resources Association (ELRA)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative content models for structural analysis of medical abstracts</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Lin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BioNLP-06</title>
		<meeting>BioNLP-06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">On a test of whether one of two random variables is stochastically larger than the other</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">B</forename>
				<surname>Mann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">R</forename>
				<surname>Whitney</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Stat</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="50" to="60" />
			<date type="published" when="1947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Employing em and pool-based active learning for text classification</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Mccallum</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Nigam</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Conference on Machine Learning</title>
		<meeting>the Fifteenth International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="350" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Note on the sampling error of the difference between correlated proportions or percentages</title>
		<author>
			<persName>
				<forename type="first">Q</forename>
				<surname>Mcnemar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="153" to="157" />
			<date type="published" when="1947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Zone analysis in biology articles as a basis for information extraction</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Mizuta</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Med. Informat. Nat. Lang. Process. Biomed. Appl</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="468" to="487" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">A baseline feature set for learning rhetorical zones using full articles in the biomedical domain</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Mullen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Process. Text Min</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="52" to="58" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Active + semi-supervised learning = robust multi-view learning</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Muslea</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth International Conference on Machine Learning</title>
		<meeting>the Nineteenth International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="435" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Updating Quasi-Newton matrices with limited storage</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Nocedal</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Comput</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="773" to="782" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">C</forename>
				<surname>Platt</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Large Margin Classiers</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="61" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Using analytic qp and sparseness to speed training of support vector machines</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">C</forename>
				<surname>Platt</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1998 Conference on Advances in Neural Information Processing Systems II</title>
		<meeting>the 1998 Conference on Advances in Neural Information Processing Systems II</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="557" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Using argumentation to extract key sentences from biomedical abstracts</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ruch</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Med. Inform</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="195" to="200" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Active hidden Markov models for information extraction</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Scheffer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Advances in Intelligent Data Analysis</title>
		<meeting>the 4th International Conference on Advances in Intelligent Data Analysis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="309" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Query by committee</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">S</forename>
				<surname>Seung</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Annual Workshop on Computational Learning Theory</title>
		<meeting>the Fifth Annual Workshop on Computational Learning Theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="287" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-dimensional classification of biomedical text: toward automated, practical provision of high-utility text to diverse users</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Shatkay</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2086" to="2093" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving verb clustering with automatically acquired selectional preference</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Sun</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Korhonen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP. ACL</title>
		<meeting>EMNLP. ACL</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="638" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Using argumentation to retrieve articles with similar citations</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Tbahriti</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Med. Inform</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="488" to="495" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Summarizing scientific articles: Experiments with relevance and rhetorical status</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Teufel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Moens</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Ling</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="409" to="445" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards domain-independent argumentative zoning: Evidence from chemistry and computational linguistics</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Teufel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1493" to="1502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Tong</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Koller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<analytic>
		<title level="a" type="main">Individual comparisons by ranking methods</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Wilcoxon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomet. Bull</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="80" to="83" />
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>