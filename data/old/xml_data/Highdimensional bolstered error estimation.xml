
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining High-dimensional bolstered error estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011">. 21 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Chao</forename>
								<surname>Sima</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Computational Biology Division</orgName>
								<orgName type="institution">Translational Genomics Research Institute</orgName>
								<address>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Ulisses</forename>
								<forename type="middle">M</forename>
								<surname>Braga-Neto</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<settlement>College Station</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName>
								<forename type="first">Edward</forename>
								<forename type="middle">R</forename>
								<surname>Dougherty</surname>
							</persName>
							<email>edward@mail.ece.tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computational Biology Division</orgName>
								<orgName type="institution">Translational Genomics Research Institute</orgName>
								<address>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<settlement>College Station</settlement>
									<region>TX</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Pathology</orgName>
								<orgName type="institution">University of Texas</orgName>
								<address>
									<addrLine>M. D. Anderson Cancer Center</addrLine>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining High-dimensional bolstered error estimation</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">27</biblScope>
							<biblScope unit="page" from="3056" to="3064"/>
							<date type="published" when="2011">. 21 2011</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btr518</idno>
					<note type="submission">Received on February 10, 2011; revised on September 2, 2011; accepted on September 5, 2011</note>
					<note>[11:00 3/10/2011 Bioinformatics-btr518.tex] Page: 3056 3056–3064 Associate Editor: Jonathan Wren Contact:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: In small-sample settings, bolstered error estimation has been shown to perform better than cross-validation and competitively with bootstrap with regard to various criteria. The key issue for bolstering performance is the variance setting for the bolstering kernel. Heretofore, this variance has been determined in a non-parametric manner from the data. Although bolstering based on this variance setting works well for small feature sets, results can deteriorate for high-dimensional feature spaces. Results: This article computes an optimal kernel variance depending on the classification rule, sample size, model and feature space, both the original number and the number remaining after feature selection. A key point is that the optimal variance is robust relative to the model. This allows us to develop a method for selecting a suitable variance to use in real-world applications where the model is not known, but the other factors in determining the optimal kernel are known. Availability: Companion website at</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Throughout most of the history of pattern recognition, the number of features was much smaller than the numbers currently being generated in high-throughput biology. Less than 15 years ago, in two studies on feature selection most cases considered involved &lt;30 features and the maximum number considered was 65 (<ref type="bibr" target="#b14">Jain and Zongker, 1997;</ref><ref type="bibr" target="#b17">Kudo and Sklansky, 2000</ref>). The advent of high-throughput technologies has radically altered the landscape. In conjunction with large numbers of features, bioinformatics is confronted by small sample sizes, often &lt;100, which forces one to train and test on the same data, where bias, variance (Braga<ref type="bibr" target="#b4">Neto and Dougherty, 2004b</ref>) and lack of correlation with the true error (<ref type="bibr" target="#b9">Hanczar et al., 2007</ref><ref type="bibr" target="#b10">Hanczar et al., , 2010</ref>) can severely degrade error estimation. Performance can degrade even further in the presence of feature selection (<ref type="bibr" target="#b18">Molinaro et al., 2005</ref>). Recent articles have pointed out the difficulty in establishing performance advantages for proposed classification rules (<ref type="bibr" target="#b0">Boulesteix, 2010;</ref><ref type="bibr" target="#b15">Jelizarow et al., 2010;</ref><ref type="bibr" target="#b20">Rocke et al., 2009</ref>). Two statistically grounded sources of overoptimism have been highlighted: (i) applying a classification rule to numerous datasets and then reporting only the results on the dataset for which the designed classifier possesses the lowest estimated error * To whom correspondence should be addressed.</p><p>(<ref type="bibr" target="#b30">Yousefi et al., 2010</ref>); and (ii) applying multiple classification rules to a dataset and comparing the classification rules according to the estimated errors of the designed classifiers (<ref type="bibr" target="#b1">Boulesteix and Strobl, 2009</ref>). In both cases, optimism is a result of inaccurate error estimation. A good error estimator ideally would have small bias and small variance. This is a difficult trade-off in small-sample settings. In small-sample cases, resubstitution generally has small variance but tends to be quite optimistically biased. Cross-validation has small bias, but tends to display high variance. Bolstered error estimation (Braga<ref type="bibr" target="#b3">Neto and Dougherty, 2004a</ref>) attempts to achieve a compromise to this bias-variance dilemma in small-sample settings. It is based on the idea of modifying ('bolstering') the empirical distribution of the data by placing kernels at each data point and then estimating classifier error by the error on this bolstered empirical distribution in such a way that it reduces bias, while at the same time reducing variance. Bolstered error estimation has shown good performance when compared with popular error estimators in small-sample settings, in particular, for feature-set ranking and when used internally within a feature-selection algorithm (<ref type="bibr" target="#b23">Sima et al., 2005a</ref>) and for ranking feature sets (<ref type="bibr" target="#b24">Sima et al., 2005b</ref>). Its good performance, including the latter applications, has been demonstrated in the context a small number of features, including feature selection via sequential forward selection (SFS), where it is applied to small potential feature sets in the SFS algorithm. A critical aspect of the method is selecting the right amount of bolstering, which is given by the variance of the bolstering kernels. The original bolstering paper (Braga<ref type="bibr" target="#b3">Neto and Dougherty, 2004a</ref>) proposed a non-parametric estimator for the kernel variance, which was found empirically to perform well in low-dimensional spaces; however, estimation was found to degrade in high dimensions, so that a correction factor can be required (<ref type="bibr" target="#b29">Vu and Braga-Neto, 2008</ref>). In fact, it was demonstrated in a preliminary study that a correction factor can also be beneficial for low-dimensional bolstering (<ref type="bibr" target="#b13">Huynh et al., 2007</ref>). This leads us to consider optimal bolstering, specifically, finding an optimal variance for the bolstering kernels. Error estimators like resubstitution and cross-validation (assuming the number of folds is preset) are non-parametric. They contain no free parameters. This is not the case for bootstrap. In general, bootstrap has the form of a convex error estimator, namely,</p><formula>ˆ ε a boot = (1−a)ˆ ε resub +â ε zero ,</formula><formula>(1)</formula><p>wherê ε resub andˆεandˆ andˆε zero are the resubstitution and zero-bootstrap estimators and 0 ≤ a ≤ 1. The zero-bootstrap utilizes the empirical distribution F * , which puts mass 1 n on each of the n available<ref type="bibr">[</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-dimensional bolstered error estimation</head><p>data points. A bootstrap sample S * n from F * consists of n equallylikely draws with replacement from the original data S n. The basic bootstrap zero estimator (<ref type="bibr" target="#b6">Efron, 1983</ref>) is written in terms of the empirical distribution asˆ0</p><formula>asˆ asˆ0 = E F * |Y −g(S * n ,X)|: (X,Y ) ∈ S n \S * n .</formula><formula>(2)</formula><p>In practice, the expectation E F * has to be approximated by a Monte-Carlo estimate based on independent replicates S * b n , for b = 1,...,B, in which case the classifier is designed on the bootstrap sample and tested on the original data points left out. An optimal bootstrap estimator results from a value of a that minimizes the mean-square error betweenˆεbetweenˆ betweenˆε a boot and the true error for a given feature-label distribution (<ref type="bibr" target="#b22">Sima and Dougherty, 2006b</ref>). Setting a = 0.632, as is commonly done (<ref type="bibr" target="#b6">Efron, 1983</ref>), can lead to a far from optimal estimator (optimal weights). The present article considers optimal bolstering relative to its one free parameter, kernel variance and the manner in which optimal bolstering can be used to arrive at practical implementation of bolstering in high-dimensional feature space. The end product is an implementation protocol in which optimal kernel variances across different models are combined to produce a suitable kernel variance for the problem at hand. Throughout, we will assume feature selection because that would be the standard way to approach classification in the high-dimensional setting we are considering, although this is not a mandatory requirement of the approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SYSTEMS AND METHODS</head><p>This section will be broken into subsections, with the aim of arriving at the implementation protocol for real-world data. Section 2.1 briefly reviews the necessary essentials of error estimation, mainly bolstering. Section 2.2 defines the scaling factor by which to adjust the bolstering kernel to high dimensions. Section 2.3 discusses optimization of the scaling factor and illustrates the construction of a set of optimal scaling factors across a family of models varying in both structure and classification difficulty. Section 2.4 provides the implementation of high-dimensional bolstered resubstitution based on a family of optimal scaling factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Error estimation</head><p>In two-group statistical pattern recognition, there is a feature vector X ∈ R p and a label Y ∈{0,1}. The pair (X,Y ) has a joint probability distribution F, which is unknown in practice. Hence, a classifier is designed from training data, which is a set of n independent observations,</p><formula>S n = {(X 1 ,Y 1 ),...,(X n ,Y n )</formula><p>}, drawn from F. A classification rule is a mapping n :{R p ×{0,1}} n ×R p → F , where F is the set of mappings from R p into {0,1}. It maps S n into a classifier ψ n : R p →{0,1}. The classification error ε n is the probability of an erroneous classification:</p><formula>ε n = P(ψ n (X) = Y |S n ) = E F (|Y −ψ n (X)|), (3)</formula><p>where E F denotes expectation with respect to F. Were F known, then the error could be found via Equation (3). In practice, one must use an error estimatorˆεestimatorˆ estimatorˆε n. An error estimator can suffer from bias, Bias= E[ˆ ε n −ε n ], and deviation variance, Var dev =Var[ˆ ε n −ε n ]. These combine to contribute to the most common measure (used herein) for evaluating the accuracy of an error estimator, the root-mean-square (RMS):</p><formula>RMS = E[|ˆε|ˆε n −ε n | 2 ]=</formula><p>Var dev +Bias 2 .</p><formula>(4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Classical error estimation</head><p>The simplest way to estimate the error in the absence of independent test data is to compute its error directly on the sample data itself. This resubstitution estimator, ˆ ε resub , is usually optimistic (i.e. biased low), sometimes very much so. In k-fold cross-validation, the dataset S n is partitioned into k folds S</p><p>(i) n , for i = 1,...,k (for simplicity, we assume that k divides n). Each fold is left out of the design process and used as a test set, and the estimate, ˆ ε cv , is the overall proportion of error on all folds. A k-fold crossvalidation estimator is unbiased as an estimator of ε n−n/k. Cross-validation estimators are pessimistic, since they use smaller training sets to design the classifier; however, their bias tends to be small. Their main drawback is their large variance (Braga-Neto and Dougherty, 2004b;<ref type="bibr" target="#b5">Devroye et al., 1996</ref>). Sometimes cross-validation is repeated some number of times with different fold partitions and the results averaged. In this article, we use 10-fold cross-validation without repetition. A recently developed estimation method, called adjusted bootstrap (ˆ ε abs ), which carries out further bootstrap resampling in each fold, has been found to have good RMS performances (<ref type="bibr" target="#b16">Jiang and Simon, 2007</ref>). Specifically, S n is partitioned into n folds and, for each sample left out for testing, B bootstrap sample sets of size ln are drawn from the remaining n−1 points, l = 1,2,...,L. For each l, the error e l is the proportion of misclassified samples across n folds and B bootstrap sample sets. Finally, the adjusted bootstrap errorˆεerrorˆ errorˆε abs is computed in the formˆε</p><formula>formˆ formˆε abs = ˆ an −ˆc−ˆc + ˆ b,</formula><p>wherê a, ˆ b andˆcandˆ andˆc are least squares estimates for the function</p><formula>e l = a(n·u l ) −c +b,</formula><p>and u l is the proportion of the expected number of non-repeated samples in a size ln bootstrap sample set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Bolstered error estimation The empirical feature-label distribution</head><p>F * is a discrete distribution that puts mass 1 n on each of the n available data points. The resubstitution estimator can be written in terms of the empirical feature-label distribution asˆε</p><formula>asˆ asˆε resub = E F * [|Y −ψ n (X)|].</formula><formula>(5)</formula><p>Relative to F * , no distinction is made between points near or far from the decision boundary. If one spreads the probability mass of the empirical distribution at each point, then variation is reduced because points near the decision boundary will have more mass on the other side of the boundary than will points far from the decision boundary. Consider a probability density function f ♦ i , for i = 1,...,n, called a bolstering kernel, and define the bolstered empirical distribution F ♦ , with probability density function given by</p><formula>f ♦ (X) = 1 n n i=1 f ♦ i (X −X i ).</formula><formula>(6)</formula><p>The bolstered resubstitution estimator (Braga<ref type="bibr" target="#b3">Neto and Dougherty, 2004a</ref>) is obtained by replacing F * by F ♦ in Equation (5) to obtainˆε</p><formula>obtainˆ obtainˆε bolst = E F ♦ [|Y −ψ(X)|].</formula><formula>(7)</formula><p>Bolstering can be applied to other error estimators; however, we only use bolstered resubstitution, the bolstering method used the most to date. The bolstered resubstitution estimator is given byˆε</p><formula>byˆ byˆε bolst = 1 n n i=1 I y i =0 A 1 f ♦ i (x −x i )dx +I y i =1 A 0 f ♦ i (x −x i )dx ,</formula><formula>(8)</formula><p>where A j ={x | ψ(x) = j}. The integrals are the error contributions made by the data points, according to whether y i = 0 or y i = 1. If the classifier is linear, then the decision boundary is a hyperplane and it is usually possible to find</p><p>Page: 3058 3056–3064</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.Sima et al.</head><p>analytical expressions for the integrals; otherwise, Monte-Carlo integration can be employed. The amount of bolstering determines the variance and bias properties (hence, RMS also) of the bolstered estimator. As a general rule, wider bolstering kernels lead to lower variance estimators, but after a certain point this advantage becomes offset by increasing pessimistic bias. In the other direction, insufficiently wide kernels tend to result in optimistic bias. A zeromean, spherical Gaussian bolstering kernel f i with covariance matrix of the form κ 2 i I, where I is the identity matrix, has been proposed (Braga-Neto and<ref type="bibr" target="#b3">Dougherty, 2004a</ref>), and has been shown to work well in low-dimensional feature spaces. Since bolstered estimators spread the test points, the task is to find the amount of spreading that makes the test points to be as close as possible to the true mean distance to the training data points. The true mean distance can be estimated by its sample-based estimate:</p><formula>ˆ d y = n i=1 min j =i {||x i −x j ||} : I y i =y n i=1 I y i =y .</formula><formula>(9)</formula><p>The estimatê d y is the mean minimum distance between points belonging to class y. Next, let f ,1 i be a unit-variance bolstering kernel, R i be the random variable equal to the distance of a point randomly selected from f ,1 i to the origin and F R i (x) be the cumulative distribution function of R i. In the case of the bolstering kernel f i with covariance matrix κ 2 i I, all distances get multiplied by κ i. In Braga-Neto and Dougherty (2004a), a single variance κ 2 y is estimated for all points from class y, such that the median distance of a test point to the origin is equal to the estimated true mean distancê d y. This implies that half of the ' mass' (i.e. the ' test points' ) of the bolstering kernel will be farther from the center thanˆdthanˆ thanˆd y and the other half will be nearer. Hence, κ y is the solution of the equation</p><formula>κ y F −1 R i (1/2) = ˆ d y. Letting α p = F −1 R i</formula><p>(1/2), and recognizing that the R i are identically distributed, the estimated SDs for the bolstering kernels are given by</p><formula>κ i = ˆ d y i α p ,</formula><formula>(10)</formula><p>for i = 1,2,...,n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">High-dimensional bolstered resubstitution</head><p>In high-dimensional settings, it is commonplace to perform feature selection and, when performed, feature selection is part of the classification rule, with the entire set of potential features constituting the feature set relative to the classification rule. Feature selection constrains the space of functions from which a classifier might be chosen, but it does not reduce the number of features in the design process. This is why when using cross-validation error estimation, feature selection has to be carried out in each partitioned fold. If we perform feature selection on a D-dimensional dataset S D n and arrive at a d-dimensional set S d n (d &lt; D), then the bolstered error estimator can use the previously defined kernel size κ i , computed on S D n , not S d n. Specifically, the mean minimum distancê d y is estimated on S D n and α p = α D. For high dimensions, we replace κ i by</p><formula>κ * i = k D × ˆ d D y i α D ,</formula><formula>(11)</formula><p>where k D is an additional scaling factor determined by the dimension and where we have indicated the dimension in the mean minimum distance estimate. The idea is to adjust the kernel size by choosing k D so the bolstered error estimator will be optimal (minimum RMS). k D = 1 yields the previously proposed kernel variance. In essence, κ i is a parameter for the bolstered estimator and Equation (11) sets it free, thereby allowing for optimization. The situation is akin to 0.632 bootstrap as opposed to optimal bootstrap. Given the kernel sizes, the bolstered resubstitution error estimate is given by Equation (8) in D dimensions. For Gaussian kernels with independent variables, this integral reduces. Let f ♦,d i</p><formula>(x −x i ) and f ♦,D−d i (x −x i )</formula><p>denote the Gaussian kernels in d-and (D−d)-dimensional spaces, respectively, so that the D-dimensional Gaussian kernel decomposes as</p><formula>f ♦ i (x −x i ) = f ♦,d i (x −x i )f ♦,D−d i (x −x i ).</formula><formula>(12)</formula><p>Denoting x −x i as x i , then Equation (8) can be rewritten asˆε</p><formula>asˆ asˆε D bolst = 1 n n i=1 I y i =0 A 1 f ♦,d i (x i )f ♦,D−d i (x i )dx +I y i =1 A 0 f ♦,d i (x i )f ♦,D−d i (x i )dx = 1 n n i=1 I y i =0 A d 1 f ♦,d i (x i )dx ∞ −∞ f ♦,D−d i (x i )dx+ I y i =1 A d 0 f ♦,d i (x i )dx ∞ −∞ f ♦,D−d i (x i )dx = 1 n n i=1 I y i =0 A d 1 f ♦,d i (x i )dx +I y i =1 A d 0 f ♦,d i (x i )dx ,</formula><formula>(13)</formula><p>where A d j ,j = 0,1, is the projection of the classifier decision region A j into d-dimensional space, and we added a superscript 'D' to the bolstered error estimator to indicate it refers to the error in D-dimensional space. The previous result indicates that the integrals necessary to find the bolstered error estimate in D-dimensional space can be equivalently carried out in ddimensional space. This is akin to resubstitution, where the error count is the same whether it is done in D-or d-dimensional space. For performance comparison purposes, we will also estimate the kernel size using only the low-dimensional data S d n , resulting in a bolstered error estimatorˆεestimatorˆ estimatorˆε d bolst , which uses the originally proposed kernel variance (no correction, or k D = 1). For feature selection, we will use sequential forward floating search (SFFS) (<ref type="bibr" target="#b19">Pudil et al., 1994</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Optimization method</head><p>To find the optimal kernel scaling factor k D , we utilize the following procedure:</p><formula>Protocol 1</formula><p>(1) Generate a sample set S D n of size n and a total of D features from a specified synthetic model.</p><p>(2) Select a size-d feature set A using a feature-selection method F on S D n , resulting in a reduced dimension sample set S d n for the feature set A.</p><formula>(3) Design a classifier ψ n for S d n</formula><p>according to the given classification rule n .</p><p>(4) Compute the true error ε n using the underlying distribution of the model.</p><p>(5) Compute the 10-fold cross-validation errorˆεerrorˆ errorˆε cv (keeping in mind that feature selection must be repeated for each fold).</p><p>(6) Compute the bolstered errorˆεerrorˆ errorˆε d bolst .</p><p>(7) Compute the bolstered errorsˆεerrorsˆ errorsˆε D,i bolst for a list of kernel scaling factors</p><formula>k D,1 ,k D,2 ,....</formula><p>(8) Calculate RMS for each error estimator by repeating Steps 1 through 7 a number N of times.</p><p>(</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-dimensional bolstered error estimation</head><formula>= 2 = G = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ ρ 0 ··· 0 0 ρ ··· 0 . . . . . . .. . . . . 0 0 ··· ρ ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ G blocks ,</formula><p>where ρ has 1 on the diagonal and ρ off the diagonal. Here a i = 1 for i = 1,2,...D.<ref type="figure" target="#tab_1">Table 1</ref>gives a summary of the simulation experiments. Two limiting factors should be noted. First, the maximum total number of features, 500, is smaller than those often considered in microarray studies and, second, the number of selected features is kept to 5 or 10. There are three reasons for this, one pragmatic to our set of simulations and the others having to do with the nature of feature selection. The pragmatic reason is computational: we wish to do a large simulation study and therefore want to limit the computational burden. As for feature selection, given the sample sizes, it is prudent to keep the numbers of total and selected features small to have satisfactory feature selection (<ref type="bibr" target="#b21">Sima and Dougherty, 2006a</ref>) and the number of selected features small to avoid the peaking phenomena (<ref type="bibr" target="#b11">Hua et al., 2005</ref><ref type="bibr" target="#b12">Hua et al., , 2009</ref>). Regarding the total number of features, limiting the total number of features via prior biological knowledge or requirements on data quality raises the likelihood of finding good feature sets via feature selection (<ref type="bibr" target="#b32">Zhao et al., 2010</ref>). Regarding the efficacy of selecting small feature sets, studies have shown that good classification can be achieved with two or three genes when re-examining data from studies that had originally used much larger feature sets (<ref type="bibr" target="#b2">Braga-Neto, 2007;</ref><ref type="bibr" target="#b8">Grate, 2005</ref>). We plot the RMS versus kernel scaling factor k D forˆεforˆ forˆε D bolst , using all combinations of simulation parameters displayed in<ref type="figure" target="#tab_1">Table 1</ref>. Additionally, we compute the RMS for LDA with D = 200, d = 3 and n = 50 for E<ref type="bibr">[</ref>Here, we present some typical results, the complete set of plots appearing on the companion website. Note that due to the intensive computing inˆεinˆ inˆε abs we only compute it for LDA with D = 200, d = 3 and n = 50.<ref type="figure" target="#fig_3">Figure 1</ref>shows the result for LDA, n = 50, and selecting d = 3 out ofWhen n is large, the benefits of usingˆεusingˆ usingˆε D,opt bolst tend to diminish.<ref type="figure" target="#fig_7">Figure 3a</ref>and b show RMS curves for LDA for n = 100 and n = 150, respectively, E[ε n ]= 0.10 (more on the companion website), d = 3 and D = 200. If the model is known, an optimaî ε D bolst is achievable, but robustness diminishes. For n = 100, there is still some robustness, but for n = 150, even a small deviation from k min D</p><p>can result a worse performance thanˆεthanˆ thanˆε cv. Hence, for n = 150, choosing an appropriatê ε D,opt bolst is not feasible in practice; however, since our interest is using bolstered error estimation for very small samples, this is not a significant drawback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Implementation for real data</head><p>For practical application, based on the sample size, the total and selected numbers of features, and the classification rule, we will perform a modelbased analysis like the ones we have performed, thereby resulting in a look-up</p><formula>table of pairs (E[ε n ],k min D</formula><p>) as in<ref type="figure" target="#fig_3">Figure 1</ref>. To illustrate, by averaging across the four models, we obtain the following table<ref type="bibr">(0,35,1.6</ref>). Upon designing a classifier from the data, we will obtain the 10-fold crossvalidation error estimate, ε 0 , and then, in the fashion of the method of moments, set E<ref type="bibr">[</ref><ref type="figure" target="#fig_3">Figure 1</ref>, we have:selection of expected errors and interpolation can be obtained. One might also use a coarser interpolation for computational purposes, with some loss of performance. In fact, that is precisely what we do here because we will subsequently perform a computationally intensive robustness analysis. Here we use:</p><formula>for (E[ε n ],k min D ): (0.05,0.8), (0.10,0.8), (0.15,0.9), (0,20,1.0), (0.25,1.2), (0.30,1.3),</formula><formula>k min D = 0.8 for ε 0 &lt; 0.125, k min D = 0.9 for 0.125 ≤ ε 0 &lt; 0.175, k min D = 1.0 for 0.175 ≤ ε 0 &lt; 0.225, k min D = 1.2 for 0.225 ≤ ε 0 &lt; 0.275, k min D = 1.3 for 0.275 ≤ ε 0 &lt; 0.325 and k min D = 1</formula><formula>k min D = 0.8 for ε 0 &lt; 0.125; k min D = 1 for 0.125 ≤ ε 0 &lt; 0.225; k min D = 1.2 for 0.225 ≤ ε 0 ≤ 0.275; k min D = 1.4 for 0.275 ≤ ε 0 ≤ 0.325; and k min D = 1.6 for ε 0 &gt; 0.325.</formula><p>The final bolstered error estimate is computed from the data using this scaling factor. The success of the procedure depends on robustness in choosing a scaling factor because (i) the estimated model will be inaccurateowing to small sample size, (ii) cross-validation has significant variance for small samples, (iii) the estimated model will differ to some extent from the models involved in creating the look-up table and (iv) the method of moments is not optimal. The following protocol is used to obtain the bolstered resubstitution error estimate:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-dimensional bolstered error estimation</head><formula>(a) (b) (c)</formula><formula>Protocol 2</formula><p>(1) Given a sample set S D n with size n and dimension D, select a size-d feature set A using a feature-selection method F on S D n , resulting in a reduced dimension sample set S d n for the feature set A.</p><p>(2) Design a classifier ψ n for S d n according to the given classification rule n , and compute the 10-fold cross-validation error estimate ε 0 .</p><p>(3) From the look-up table</p><formula>(E[ε n ],k min D</formula><p>) choose the kernel scaling factor</p><formula>k min D by setting E[ε n ]=ε 0 .</formula><p>(4) Compute the bolstered error estimatê ε D,data bolst using the selected scaling factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS AND DISCUSSION</head><p>To illustrate application, we have applied the method to two gene expression datasets: @BULLET Myeloma dataset: data are downloaded from the NIH Gene Expression Omnibus (GEO) under accession numbers GSE5900 and GSE2658, which contain 54 613 probe sets and 559 multiple myeloma (MM) samples, as well as 3 other subtypes [monoclonal gammopathy of undetermined significance (MGUS)], 44 samples; smoldering MM (SMM), 12 samples; healthy donors with normal plasma cell (NPC), 22 samples (<ref type="bibr" target="#b31">Zhan et al., 2006</ref>). Samples are labeled into two classes, one for MGUS/SMM/NPC and the other for MM. Due to the significant unbalance of the samples between the two classes, only 156 samples are randomly selected from the 559 MM samples. The number 156 has been chosen as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.Sima et al.</head><p>a compromise to take as many samples as possible from MM without significant unbalance between the two classes. Furthermore, only D = 200 features with the largest variances across samples are selected from the total 54 613 probe sets. It is advantageous to limit ourselves to the 200 features with the largest variances, because these are more likely to reveal class discrimination and feature selection tends to perform poorly for very large numbers of features when samples are small (<ref type="bibr" target="#b21">Sima and Dougherty, 2006a</ref>). Here we must put in a word of caution concerning the methodology. We are using feature variance to produce a set of 200 features to be taken as the full feature set for our performance analysis and will apply feature selection, classifier design and error estimation based on this set, including cross-validation. In practice, this approach would be unacceptable, because the actual dataset to which we are applying data-dependent feature selection is the full 54 613 probe sets. For instance, cross-validation would have to use the variance-based feature reduction from the full 54 613 on each fold, else it would be optimistically biased. But that is not our goal here. We are a priori assuming that there are onlybelong to the 'good-prognosis' class and 180 belong to the 'poor-prognosis' class. Referring to our cautionary comment regarding the multiple myeloma data, we note here that feature selection was used originally to obtain the 70 genes, but, again, from our performance perspective, that is not important for our analysis.</p><p>We consider sample size n = 50 and d = 3 features selected from the D = 200 and D = 70 features in the myeloma and breast cancer datasets, respectively, and LDA for classification. We repeatedly draw (stratified) n = 50-point samples with replacement from the empirical distribution (full dataset) as training data with the remaining sample points held out for true error estimation in computing the RMS (ε 0 is still computed from the training data). The total number of repetitions is 200. The average true error and SD for the myeloma dataset are 0.2170 and 0.0309, respectively. For the breast cancer dataset, the average true error and SD are 0.2340 and 0.0362, respectively.<ref type="figure" target="#fig_10">Figure 4</ref>shows the RMS for the two patient datasets. In both cases, ˆ ε D,data bolst performs significantly better. Owing to robustness of the optimal scaling factor, a coarse selection of expected errors and interpolation has proven sufficient. To further demonstrate the effectiveness of Protocol 2, we have applied it to four models in<ref type="figure" target="#fig_3">Figure 1</ref><ref type="figure" target="#fig_3">Figure 1</ref>, we see that all these averages are centered within the range of scaling factors where optimal bolstering outperformsˆεoutperformsˆ outperformsˆε abs .</p><formula>(a) ( b) (c) ( d)</formula><formula>E[ε n ]=0.20, (b) M1, E[ε n ]=0.35, (c) M2, E[ε n ]=0.20, (d) M2, E[ε n ]=0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Robustness to non-Gaussian data</head><p>Although k min D is derived with Gaussian models, it is robust enough for models where this assumption is violated, as with the patient data, where the underlying distribution is almost certainly not Gaussian. To further investigate this issue, we take the model M2 in Section 2.3, but perturb the skewness and kurtosis of the class at the origin to obtain a Pearson system (<ref type="bibr" target="#b7">Elderton and Johnson, 1969</ref>).<ref type="figure" target="#fig_12">Figure 6</ref>shows the eight different distributions in the Pearson system with varying skewness and kurtosis. For the resulting model M p and each skewness and kurtosis combination, where valid, we do the following:</p><p>(1) Generate a sample set S D n of size n = 50 and a total of D = 200 features from the model M p .</p><p>(2) Feature select a size-d = 3 feature set A, resulting in a reduced dimension sample set S d n .</p><p>(3) Design a classifier ψ n for S d n using LDA.(4) Compute the true error ε n using the underlying distribution of the model M p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 3063 3056–3064</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-dimensional bolstered error estimation</head><p>(5) Compute the 10-fold cross-validation errorˆεerrorˆ errorˆε cv .</p><formula>(6) Computê ε D,data bolst using k min D from the previous section.</formula><p>(7) Calculate RMS forˆεforˆ forˆε cv andˆεandˆ andˆε D,data bolst by repeating Steps 1 through 6 a number N = 400 of times.<ref type="figure" target="#fig_13">Figure 7</ref>shows the values of RMS forˆεforˆ forˆε D,data bolst minus the RMS forˆεforˆ forˆε cv for different skewness and kurtosis in a heatmap. Due to symmetry, only positive skewness is shown. In all cases, ˆ ε D,data bolst is superior tô ε cv .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Concluding remarks</head><p>We have derived an optimal kernel scaling factor that can be used for bolstered error estimation in high feature dimensions. This bolstered error estimator achieves a significant RMS improvement over crossvalidation when samples are small, with continued, albeit smaller, performance improvement over the adjusted bootstrap. This superior performance is robust over a wide range of models. Hence, we have been able to incorporate optimality criteria from across a collection of families to arrive at suitable bolstering kernels for practical situations, thereby facilitating its use in applications like classification of genomic data when samples are small.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>ε n ] = 0.20, 0.25, 0.30, 0.35, 0.40 and 0.45. For comparison, RMS values forˆεforˆ forˆε d bolst , ˆ ε cv andˆεandˆ andˆε abs are also plotted, which appear as horizontal lines as they are not related to k D .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>D = 200 features for nine values of E[ε n ]. Letting k min D denote the value of k D achieving minimum RMS, we see that k min D increases for increasing expected error, the increase being slight for small expected errors but becoming significant for large expected errors (for E[ε n ] = 0.40 and 0.45, k min D is to the right of where we have stopped the plots at k D = 2.0; see extended plots to k D = 3.0 for these cases on the companion website). We observe thatˆεthatˆ thatˆε cv andˆεandˆ andˆε abs typically perform better thanˆεthanˆ thanˆε d bolst , sometimes by a large margin. However, for an appropriate kernel scaling factor, ˆ ε D bolst outperformsˆεoutperformsˆ outperformsˆε abs and often outperformsˆεoutperformsˆ outperformsˆε cv by a wide margin. This improvement is achieved by a range of scaling factors and is robust across different models and complexities. Regarding model robustness, for a fixed value of E[ε n ] the RMS curves are remarkably similar; in particular, the value, k min D , of k D achieving minimum RMS is consistent. In three cases, k min D remains fixed across the models and in the others it changes by not &gt; 0.2. Moreover, in the latter cases, the RMS at the different values of k min D is approximately the same. The overall robustness has important practical implications, because in real-world problems we do not know the data model or its level of difficulty, but we do know the sample size n, the total and selected numbers of features, D and d, and the classification rule. As we will subsequently see, the fact that k min D is robust relative to the data model means that, in practice, we can derive a value of k D , albeit not optimal, that can be used inˆεinˆ inˆε D bolst for a better error estimator. There is also robustness with respect to the classification rule and number of features. Figure 2a and b show robustness curves for 3NN and CART, respectively, for complexity E[ε n ]=0.10 (more on the companion website) for n = 50, d = 3 and D = 200. The curves bear a strong resemblance to the corresponding curves for LDA in Figure 1 and for all models k min D = 0.8, as with LDA. We again have LDA in Figure 2c for E[ε n ]=0.10 (more on the companion website), but now with D = 500. Again there is resemblance to the corresponding case in Figure 1 and again k min D = 0.8 for all models. The preceding observations are mostly constrained to small samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><figDesc>.6 for 0.325 ≤ ε 0. If one so desires, then a finer Page: 3060 3056–3064 C.Sima et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.1.</head><figDesc>Fig. 1. RMS versus scaling factor k D for LDA with sample size n = 50, total feature size D = 200 and selected feature size d = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.2.</head><figDesc>Fig. 2. RMS versus scaling factor k D for sample size n = 50 and selected feature size d = 3, for (a) 3NN with total feature size D = 200, (b) CART with D = 200 and (c) LDA with D = 500.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig.3.</head><figDesc>Fig. 3. RMS versus scaling factor k D for LDA with total feature size D = 200 and selected feature size d = 3, for (a) sample size n = 100 and (b) n = 150.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><figDesc>: models M1 and M2 with expected errors 0.20 and 0.35. The performance graphs corresponding to Figure 4 are provided in Figure 5. Of particular interest are the scaling factors produced by the protocol. The average scaling factors for the four models are given by: M1, E[ε n ]=0.20 – average scaling factor 1.10; M1, E[ε n ]=0.35 – average scaling factor 1.39; M2 E[ε n ]=0.20 – average scaling factor 1.09; and M2, (b) (a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig.4.</head><figDesc>Fig. 4. RMS using LDA for (a) myeloma dataset, total feature size D = 200 and (b) breast cancer dataset, total feature size D = 70. For both datasets: sample size n = 50 and selected feature size d = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig.5.</head><figDesc>Fig. 5. RMS using LDA and protocol 2 for (a) M1, E[ε n ]=0.20, (b) M1, E[ε n ]=0.35, (c) M2, E[ε n ]=0.20, (d) M2, E[ε n ]=0.35. All with sample size n = 50 and selected feature size d = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig.6.</head><figDesc>Fig. 6. The plane of (skewness, kurtosis) pairs and their corresponding probability distributions in a Pearson system. In particular, Type 0 (Gaussian distribution) has a skewness of 0 and kurtosis of 3, which is represented by a single point on the plane. There are eight different types of distributions in a Pearson system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig.7.</head><figDesc>Fig. 7. Heatmap for RMS ofˆεofˆ ofˆε D,data bolst minus RMS ofˆεofˆ ofˆε cv for different skewness and kurtosis. All values are negative.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>9) Repeat Steps 1 through 8 for different models M, different levels of model complexities and different classification rules n. We consider four data models, each a two-class Gaussian model with equally likely classes and class-conditional densities having covariance matrices 1 and 2. One class mean is located at − − µ and the other at µ, with the location of µ = δ * [a 1 a 2 ... a D ] depending on the model. The parameter δ is chosen to achieve prescribed values for the expected classification error E[ε n ]; different values of E[ε n ] represent different levels of difficulty at sample size n. Page: 3059 3056–3064</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>Table 1. Summary of simulation experiments</figDesc><table>Data models 
M 
M1,M2,M3,M4 
Model difficulty 
E[ε n ] 
0.05,0.10,0.15 
Classification rules 
n 
LDA,3NN, LSVM 
NNet,CART 
Feature-selection methods 
F 
SFFS 
No. of repetitions 
N 
500 
No. of sample size 
n 
50,100,150 
No. of selected features 
d 
5,10 
No. of total features 
D 
200,500 
Kernel scaling factors 
k D 
0.2 to 2.0 in 0.2 increment 

d 0 = 10,G = 20,c = 2.25,ρ = 0.25 

LDA, linear discriminant analysis; 3NN, 3-nearest-neighbor; LSVM, linear support 
vector machine; NNet, neural net; CART, classification and regression tree. 

M1: A simple linear model in which 1 = 2 = I, the identity matrix, so that 
all features are uncorrelated. a i = 1 for i = 1,2,...d 0 and uniformly 
distributed over [0,1] for i = d 0 +1,d 0 +2,...D before all a i 's are 
randomly permuted. 

M2: Similar to M1 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>M4: Similar to M3, but with 1 = G and 2 = c G .</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>ε n ]=ε 0 and choose the corresponding value of k min D to serve as the scaling factor for bolstering. Since the look-up table is discrete, E[ε n ]=ε 0 must be solved approximately by interpolation. Corresponding to the seven values of k min D in the look-up table for</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><figDesc>.35. All with sample size n = 50 and selected feature size d = 3.</figDesc><table>E[ε n ]=0.35 – average scaling factor 1.43. Referring to </table></figure>

			<note place="foot">© The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We would also like to thank the High-Performance Biocomputing Center of TGen for providing the clustered computing resources used in this study; this includes the Saguaro-2 cluster supercomputer, a collaborative effort between TGen and the ASU Fulton High Performance Computing Initiative.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Over-optimism in bioinformatics research</title>
		<author>
			<persName>
				<forename type="first">A.-L</forename>
				<surname>Boulesteix</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="437" to="439" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimal classifier selection and negative bias in error rate estimation: an empirical study on high-dimensional prediction</title>
		<author>
			<persName>
				<forename type="first">A.-L</forename>
				<surname>Boulesteix</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Strobl</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med. Rese. Methodol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Fads and fallacies in the name of small-sample microarray classification</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sig. Proc. Mag</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Bolstered error estimation</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1267" to="1281" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Is cross-validation valid for smallsample microarray classification?</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<forename type="middle">M</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="374" to="380" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<monogr>
		<title level="m" type="main">A Probabilistic Theory of Pattern Recognition</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Devroye</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Estimating the error rate of a prediction rule: Improvement on crossvalidation</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Efron</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="316" to="331" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<monogr>
		<title level="m" type="main">Systems of Frequency Curves</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">W</forename>
				<surname>Elderton</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Johnson</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1969" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Many accurate small-discriminatory feature subsets exist in microarray transcript data: biomarker discovery</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Grate</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">97</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Decorrelation of the true and estimated classifier errors in high-dimensional settings</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Hanczar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Bioinformatics Syst. Biol</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Small-sample precision of roc-related estimates</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Hanczar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="822" to="830" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimal number of features as a function of sample size for various classification rules</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Hua</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1509" to="1515" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Performance of feature-selection methods in the classification of high-dimension data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Hua</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="409" to="424" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved bolstering error estimation for gene ranking</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Huynh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE EMBS</title>
		<meeting>the IEEE EMBS<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature selection: evaluation, application, and small sample performance</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Jain</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Zongker</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="153" to="158" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Over-optimism in bioinformatics: an illustration</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Jelizarow</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">A comparison of bootstrap methods and an adjusted bootstrap approach for estimating the prediction error in microarray classification</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Jiang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Simon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Med</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="5320" to="5334" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Comparison of algorithms that select features for pattern classifiers</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Kudo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Sklansky</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="25" to="41" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Prediction error estimation: a comparison of resampling methods</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">M</forename>
				<surname>Molinaro</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="3301" to="3307" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Floating search methods in feature-selection</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Pudil</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1119" to="1125" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Papers on normalization, variable selection, classification or clustering of microarray data</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">M</forename>
				<surname>Rocke</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="701" to="702" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">What should be expected from feature selection in small-sample settings</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Sima</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2430" to="2436" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimal convex error estimators for classification</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Sima</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1763" to="1780" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Impact of error estimation on feature selection</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Sima</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2472" to="2482" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Superior feature-set ranking for small samples using bolstered error estimation</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Sima</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1046" to="1054" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">A gene-expression signature as a predictor of survival in breast cancer</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">J</forename>
				<surname>Van De Vijver</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">347</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">1110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName>
				<surname>Page</surname>
			</persName>
		</author>
		<imprint>
			<biblScope unit="page" from="3056" to="3064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Gene expression profiling predicts clinical outcome of breast cancer</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Sima</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">415</biblScope>
			<biblScope unit="page" from="530" to="536" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Preliminary study on bolstered error estimation in high-dimensional spaces</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Vu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE GENSIPS</title>
		<meeting>the IEEE GENSIPS<address><addrLine>Phoenix, AZ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Reporting bias when using real data sets to analyze classification performance</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">R</forename>
				<surname>Yousefi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="68" to="76" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">The molecular classification of multiple myeloma</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Zhan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Blood</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="2020" to="2028" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Characterization of the effectiveness of reporting lists of small feature sets relative to the accuracy of the prior biological knowledge</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Zhao</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Inform</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="49" to="60" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>