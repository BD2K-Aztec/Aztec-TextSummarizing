
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequence analysis smallWig: parallel compression of RNA-seq WIG files Downloaded from</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-09-30">Advance Access Publication Date: 30 September 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Zhiying</forename>
								<surname>Wang</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Tsachy</forename>
								<surname>Weissman</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Olgica</forename>
								<surname>Milenkovic</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sequence analysis smallWig: parallel compression of RNA-seq WIG files Downloaded from</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Bioinformatics</title>
						<imprint>
							<biblScope unit="volume">32</biblScope>
							<biblScope unit="issue">2016</biblScope>
							<biblScope unit="page" from="173" to="180"/>
							<date type="published" when="2015-09-30">Advance Access Publication Date: 30 September 2015</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btv561</idno>
					<note type="submission">Received on November 27, 2014; revised on September 4, 2015; accepted on September 23, 2015</note>
					<note>*To whom correspondence should be addressed. Associate Editor: Ivo Hofacker 173 Original Paper at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Contact: zhiyingw@stanford.edu Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Contributions: We developed a new lossless compression method for WIG data, named smallWig, offering the best known compression rates for RNA-seq data and featuring random access func-tionalities that enable visualization, summary statistics analysis and fast queries from the compressed files. Our approach results in order of magnitude improvements compared with bigWig and ensures compression rates only a fraction of those produced by cWig. The key features of the smallWig algorithm are statistical data analysis and a combination of source coding methods that ensure high flexibility and make the algorithm suitable for different applications. Furthermore, for general-purpose file compression, the compression rate of smallWig approaches the empirical en-tropy of the tested WIG data. For compression with random query features, smallWig uses a simple block-based compression scheme that introduces only a minor overhead in the compression rate. For archival or storage space-sensitive applications, the method relies on context mixing techniques that lead to further improvements of the compression rate. Implementations of smallWig can be executed in parallel on different sets of chromosomes using multiple processors, thereby enabling desirable scaling for future transcriptome Big Data platforms. Motivation: The development of next-generation sequencing technologies has led to a dramatic decrease in the cost of DNA/RNA sequencing and expression profiling. RNA-seq has emerged as an important and inexpensive technology that provides information about whole transcriptomes of various species and organisms, as well as different organs and cellular communities. The vast volume of data generated by RNA-seq experiments has significantly increased data storage costs and communication bandwidth requirements. Current compression tools for RNA-seq data such as bigWig and cWig either use general-purpose compressors (gzip) or suboptimal compression schemes that leave significant room for improvement. To substantiate this claim, we performed a statistical analysis of expression data in different transform domains and developed accompanying entropy coding methods that bridge the gap between theoretical and practical WIG file compression rates. Results: We tested different variants of the smallWig compression algorithm on a number of integer-and real-(floating point) valued RNA-seq WIG files generated by the ENCODE project. The results reveal that, on average, smallWig offers 18-fold compression rate improvements, up to 2.5-fold compression time improvements, and 1.5-fold decompression time improvements when compared with bigWig. On the tested files, the memory usage of the algorithm never exceeded 90 KB. When more elaborate context mixing compressors were used within smallWig, the obtained compression rates were as much as 23 times better than those of bigWig. For smallWig used in the random query mode, which also supports retrieval of the summary statistics, an overhead in the compression rate of roughly 3–17% was introduced depending on the chosen system parameters. An increase in encoding and decoding time of 30% and 55% represents an additional performance loss caused by enabling random data access. We also implemented smallWig using multi-processor programming. This parallelization feature decreases the encoding delay 2–3.4 times compared with that of a single-processor implementation, with the number of processors used ranging from 2 to 8; in the same parameter regime, the decoding delay decreased 2–5.2 times. Availability and implementation: The smallWig software can be downloaded from:</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Next-generation sequencing technologies have resulted in a dramatic decrease of genomic data sequencing time and cost. As an illustrative example, the HiSeq X machines introduced by Illumina in 2014 enable whole human genome sequencing in less than 15 h and at a cost of only $1000 (http://www.illumina.com/systems/hiseq-x-sequencingsystem.ilmn). A suite of other-seq techniques has closely followed this development (for a comprehensive overview, see http://res.illumina. com/documents/products/research_reviews/), including the by now well-documented RNA-seq method. RNA-seq is a shotgun sequencing technique for whole transcriptomes (<ref type="bibr" target="#b12">Marioni et al. 2008</ref>) used for quantitative and functional genomic studies. In addition to generating sequence-related information, RNA-seq methods also provide dynamic information about gene or functional RNA activities as measured by their expression (abundance) values. This makes RNA-seq techniques indispensable for applications such as mutation discovery, fusion transcript detection and genomic medicine (<ref type="bibr" target="#b24">Wang et al. 2009</ref>). As a result, the volume of data produced by RNA-seq methods can be foreseen to increase at a much faster rate than Moore's law. It is therefore imperative to develop highly efficient lossless compression methods for RNA-seq data. The problem of DNA and RNA sequence and expression compression has received much attention in the bioinformatics community. Compression methods for whole genomes include direct sequence compression (e.g.<ref type="bibr" target="#b0">Cao et al. 2007;</ref><ref type="bibr" target="#b16">Pinho et al. 2011;</ref><ref type="bibr" target="#b21">Tabus and Korodi 2008</ref>) and reference-based compression schemes (e.g.<ref type="bibr" target="#b9">Kuruppu et al. 2011;</ref><ref type="bibr" target="#b17">Pinho et al. 2012;</ref><ref type="bibr" target="#b23">Wang and Zhang 2011</ref>). The former class of methods explores properties of genomic sequences such as small alphabet size and large number of repeats. The latter techniques use previously sequenced genomes as references with which to compare the target genome or sequencing reads, leading to dramatic reductions in compressed file sizes. Related similarity-discovery-based schemes are usually applied to a large collection of genomes and they achieve very small per genome compression rates (e.g.<ref type="bibr" target="#b2">Deorowicz et al. 2013</ref>). Moreover, recent work also includes the compressive genomics paradigm, which allows for direct computation and alignment on compressed data (<ref type="bibr" target="#b10">Loh et al. 2012</ref>). The aforementioned methods and some information-theoretic techniques to biological data compression were reviewed in (<ref type="bibr" target="#b22">Vinga 2013</ref>). For every base pair in the genome, an RNA-seq WIG file contains an integer or floating-point expression value. Human transcriptome WIG files may contain hundreds of millions of expression values, which amounts to GB of storage space (e.g. one of the subsequently analyzed WIG files randomly chosen from the ENCODE (<ref type="bibr">Encode Project Consortium 2004</ref>) project has a size of 5 GB). WIG files are usually compressed by bigWig (<ref type="bibr" target="#b7">Kent et al. 2010</ref>), which basically performs gzip compression on straightforwardly preprocessed data. Unfortunately, the bigWig format does not appear to offer significant data volume reductions and about 10% of the tracks from the UCSC ENCODE hg19 browser in bigWig format take up 31% in storage space (Hoang and Sung 2014). Recently, another compression suite, termed cWig (Hoang and Sung 2014), was implemented as an alternative to bigWig. The cWig method outperforms bigWig in terms of compression rate, and random query time, although it still relies on suboptimal compression techniques such as Elias delta and gamma coding (<ref type="bibr" target="#b20">Salomon 2007</ref>). This work focuses on transform and arithmetic compression methods for expression data in the WIG format. Since WIG files capture expressions of correlated RNA sequence blocks, modeling these values as independent and identically distributed random variables is inadequate for the purpose of compression. Hence, we first perform a statistical analysis of expression values to explore their dependencies/correlations and then proceed to devise a new suite of compression algorithms for WIG files. Since the WIG format is not limited to RNA-seq data, our compression methods are also suitable for other types of dense data, or quantitative measurements, such as GC content values, probability scores, proteomic measurements and metabolomic information. The main analytic and algorithmic contributions of our work are as follows:</p><p>i. Devising a new combination of run length and delta encoding that allows for representing the expression data in highly compact form. As part of this procedure, we identified runs of locations with the same expression value and then computed the differences of adjacent run values. The resulting transformed sequences are referred to as run difference sequences and specialized statistical analysis of difference sequences constitutes an important step towards identifying near-optimal compression strategies. ii. Analyzing the probability distributions of the difference sequences and inferring mixture Markov models for the data. As part of this step, we estimated information-theoretic quantities, such as the (conditional) entropy, to guide us in our design and evaluation process. More precisely, we first fitted power-law distributions to the empirical probability distributions of the difference sequences. Second, we showed that strong correlations exist between adjacent run differences, while there exists only a relatively small correlation between the sequences of run length differences and that of the corresponding run expression differences. These findings provide a strong basis for performing separate compression of the run length and the expression information. iii. Developing arithmetic encoders for compression of the difference sequences, including options such as basic arithmetic coding and context-mixing coding based on the work in<ref type="bibr" target="#b11">Mahoney (2002)</ref>. In this step, we were guided by the results of the statistical analysis and performed alphabet size reduction in the difference sequences and subsequent run length and run expression compression. With this step, we were able to achieve 17fold improvements in the compression rate when compared with bigWig: as an illustration, a typical WIG file of size 5 GB was compressed to roughly 64–69 MB, depending on the userdefined operational mode; in comparison, traditional gzip and the bigWig (<ref type="bibr" target="#b7">Kent et al. 2010</ref>) compressors produced files of sizes 1.1 GB and 1.2 GB, respectively.</p><p>Our new compression algorithm follows the standard requirements for expression data representation/visualization by allowing random access features via data blocking and separate block compression. It also encodes data summary statistics, akin to bigWig data formats. Furthermore, smallWig has two implementation modes, one of which runs on a single processor and another which uses multiple processors in parallel. The parallelized version of the algorithm offers significant savings in computational time, with identical rate performance as the serial version. The remainder of the article is organized as follows. Section 2 provides the idea behind our sequence transformations and coding methods. Section 3 contains our statistical analysis. A detailed description of the smallWig algorithm is provided in Section 4. Compression results and a comparative study of compression methods is given in Section 5. A discussion of our findings and concluding remarks are given in Sections 6 and 7, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods: sequence transformations</head><p>We start our analysis by introducing WIG data transforms that allow for efficient run length encoding and by explaining how to use difference values in subsequent compression steps. To illustrate some of the concepts behind our analysis, we make use of a WIG file from the ENCODE project (<ref type="bibr">Encode Project Consortium 2004</ref>) pertaining to RNASeq cell line GM12878, for which the RNA fraction is Long PolyAþ and the compartment is Nucleus. Throughout, we use capital letters to represent sequences, and lower case letters to represent elements in sequences. We write ½a ¼ f1; 2;. .. ; ag, for any positive integer a 2 N þ , and ½a; b ¼ fa; a þ 1;. .. ; bg, for two positive integers a b. The WIG files of interest comprise two sequences: @BULLET The Location Sequence A ¼ ða 1 ; a 2 ;. .. ; a M Þ, where M denotes the length of the sequence. Sequence A contains chromosomal positions (or locations) satisfying a i 2 N þ for all i 2 ½M, and a i &lt; a iþ1 for all i 2 ½M À 1. This sequence typically contains consecutive indices of the base pairs, for which a iþ1 ¼ a i þ 1, except for skipped locations with expression value equal to zero, for which a iþ1 &gt; a i þ 1.A run is defined as a sequence of consecutive locations with identical expression value. The number of locations in the run is called the run length, and the corresponding expression value is called the run expression. Note that if for some integers i j, one has a tþ1 ¼ a t þ 1 for all t 2 ½i; j À 1, and b i ¼ b iþ1 ¼. .. ¼ b j , then the sequence corresponds to a run of length j À i þ 1 with run expression equal to b i. On the other hand, if for some integer i there exist skipped locations, i.e. locations for which a iþ1 &gt; a i þ 1, and b i ; b iþ1 6 ¼ 0, then the run is of length a iþ1 À a i À 1 with corresponding run expression value equal to 0. Thus, there is a 1-1 mapping from the sequences A and B to the run length and run expression sequences described below: @BULLET The Run length Sequence C ¼ ðc 1 ; c 2 ;. .. ; c N Þ; c i 2 N þ ; i 2 ½N, a sequence of run lengths that describes the runs of consecutive locations with identical expression values. Alternatively, one may define the sequence by stating that for locations confined to ½ P i t¼1 c t þ 1; P iþ1 t¼1 c t , the expression values are identical. Here, N denotes the number of runs. If N 0 denotes the number of skipped runs of value 0 in a WIG file, then P N i¼1 c i ¼ M þ N 0. @BULLET The Run Expression Sequence D ¼ ðd 1 ; d 2 ;. .. ; d N Þ; d i 2 R, i 2 ½N, a sequence of expression values that corresponds to the runs. More precisely, the sequence specifies that the ith run has expression value d i , for all i 2 ½N.</p><p>For our running example, the original WIG sequences were of length N ¼ 3:2e þ 8, while the run and difference sequences were of length</p><p>Here, we also point out that a related run length transformation has been investigated in BedGraph format, while the idea of run length and difference-value transformations has been studied in (<ref type="bibr" target="#b5">Hoang and Sung 2014</ref>). In the latter work, the authors also demonstrated the potential benefit of using these transformations on WIG data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods: statistical data analysis</head><p>In what follows, we describe how to fit empirical probability mass functions and compute empirical entropies for the run length and run expression difference sequences X, Y. Moreover, we study higher order dependencies of adjacent elements within the sequences X and Y, as well as dependencies between the sequences X and Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fitting empirical probability mass functions</head><p>We computed the empirical frequencies for the run expression and run length difference sequences. Both sequences roughly follow a power-law distribution, with probability density function</p><formula>pðxÞ ' a À 1 b x b Àa ;</formula><p>which can consequently be used to approximate the empirical probability mass function [similar to the method in Pavlichin et al.</p><formula>(2013)]</formula><p>. Therefore, we can parameterize the two empirical distributions with only two parameters, a and b. Goodness of fit may be estimated via the standard Kolmogorov-Smirnov statistics or some other means. For a more detailed analysis of the empirical probability distributions, the reader is referred to the Supplementary Material. smallWig: parallel compression of RNA-seq WIG files</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Empirical entropy computation</head><p>Next, we propose three correlation models for the difference sequences X, Y and estimate the entropy of their underlying distributions. A detailed description of these models may be found in the Supplementary Material. Let Z, W be discrete random variables with alphabet Z; W, respectively. The Shannon entropy of Z is defined as</p><formula>HðZÞ ¼ À X z2Z P Z ðzÞlog 2 P Z ðzÞ:</formula><p>Similarly, the conditional entropy of Z given W is defined as</p><formula>HðZjWÞ ¼ À X w2W P W ðwÞ X z2Z P ZjW ðzjwÞlog 2 P ZjW ðzjwÞ:</formula><p>In what follows, we assume that X, Y are two random sequences of length N of the formIn the Supplementary Material, we list the entropies of all the aforementioned models and for 14 Wig and 10 different BedGraph files, computed using the information theoretic tools described in (<ref type="bibr" target="#b6">Jiao et al. 2015</ref>). As one may see, the entropy is 19–56% smaller for the Markov model than the independent run difference model. In most cases, this reduction in entropy may be attributed to dependencies in the run length differences. In other words, run length values are more likely to be affected by adjacent run length values. On the other hand, considering dependencies between the run length differences and run expression differences only reduces the entropy by about 5–16%. As a result, the most effective compression strategy appears to be separate compression of the difference sequences X and Y. Because of the large variations in the run expression and run length difference values, computing and storing all conditional probabilities (about 10 10 such entries) under the Markov model requires very large memory. Hence, we first focus on a compression algorithm for the independent model and then discuss our generalized compression scheme based on context mixing, which requires specialized means for overcoming the memory overflow problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Compression algorithms</head><p>We start by describing our basic compression algorithms based on arithmetic coding and then show how to enable random queries within the given algorithmic coding framework. In addition, we describe how to reduce the compressed file sizes even further via context-mixing methods. We conclude this section by introducing parallelization techniques for the proposed algorithms. Diagrams of our compression and decompression architectures are given in the Supplementary Material. To compress the RNA-seq expressions, we used two individual arithmetic encoders and decoders for the difference sequences X, Y defined in Section 2. We observe that since expression values can be real valued, any errors in computing the expression differences may cause error propagation during decompression. As a result, the expression difference alphabet has to be stored as well, with a precision large enough to allow for correct decompression. Arithmetic compression (<ref type="bibr" target="#b19">Rissanen and Langdon 1979</ref>) is an entropy coding method that converts the entire input sequence into a range of values (interval) determined by its cumulative frequency. On length-n sequences in Z n , one defines a total order W 0 Z by requiring that W precedes Z lexicographically. For a sequence Z, its code word is the binary representation of a real number between P W:W0Z PðWÞ and P W:W0Z PðWÞ þ PðZÞ. For sequences with independent and identically distributed entries, arithmetic coding is an entropy-approaching compression scheme, given that the distribution of the sequence is known. To ensure small computational complexity, we used arithmetic compression algorithms with range encoding (<ref type="bibr" target="#b13">Martin 1979</ref>) and some techniques from the package rangemapper by Polar (http:// ezcodesample.com/reanatomy.html?Source¼Toþarticleþandþsour ceþcode). Our implementation is similar to the original version of arithmetic coding, except that the underlying probabilities are represented with binary sequences of fixed length, which allows for more efficient computations. Moreover, encoding/decoding may be performed in a streaming fashion. A buffer is used to store the " unresolved range " depending on the yet unobserved part of the sequence. In our implementation, the precision of the buffer was limited to 30 bits so as to control the number of operations performed. Unlike range encoding, in which the calculations are performed base 256, we used base 2 so as to achieve the best compression rate. To facilitate random queries during decompression, we divided the difference sequences into blocks of fixed length. The length— subsequently termed block size—can be chosen by the user, to allow for desired trade-offs between compression rate and query time. Since the compressed sequences have lengths that vary from block to block, we also store the address of each block. Moreover, to quickly obtain the original sequences from the difference sequences, we also store the (start location, expression, run length) triple ða i ; b i ; c j Þ for every starting element in a block. For this purpose, we implemented a simple binary search procedure originating from the start location to identify the blocks corresponding to a random query. For fast visualization of the WIG data and summary statistics analysis, we stored an additional summary vector for every block. The summary vector contains the following six values: (i) the minimum expression value in the block; (ii) the maximum expression value in the block; (iii) the mean value of the block; (iv) the standard deviation of the block; (v) the number of locations covered in the block and (vi) the total number of locations within the block. If a random region is queried, the aggregated summary vector for the queried region is computed as follows. First, all blocks that are completely included in the queried region are identified, and their summary information is computed from the summary vectors of the blocks; then, the starting and ending blocks partially contained within the query region are retrieved and their summary information is computed directly from the symbols in the blocks. The complexity of the statistics query is linear in the number of queried blocks and in the block size. To explore dependencies among elements in the run sequences, we used the context-mixing algorithm implemented as part of the lpaq1 package (<ref type="bibr" target="#b11">Mahoney 2002</ref>). To illustrate the idea behind context mixing, we focus on context-tree weighting (<ref type="bibr" target="#b25">Willems et al. 1995</ref>). In this model, we assume that every element x t 2 f0; 1g in a sequence is generated based on a suffix set S, which can be represented by a degree-two tree of depth not more than D. Here, D is a parameter that indicates the dependency between symbols at a certain distance in the sequence and consequently determines the memory requirements of the algorithm. The root of the tree is indexed by the empty string, while the left (or right) edge of every node represents a 1 (or 0), and every node corresponds to the string associated with its path to the root. Each suffix/leaf s is associated with a parameter h s , which equals the probability of the next source symbol x tþ1 being equal to 1 conditioned on the suffix of the semi-infinite sequence .. . x tÀ2 x tÀ1 x t beings. Let h s ðx t 1 Þ ¼ Pðx tþ1 ¼ 1jx t tÀjsjþ1 ¼ sÞ denotes the conditional probability of a symbol given the preceding jsj symbols being equal to s, where we denote by x j i the string x i ; x iþ1 ;. .. ; x j ; for i j. The probability h s (called the model parameter) is usually not known but can be estimated using the Krichevsky–Trofimov (KT) method (Krichevsky and Trofimov 1981). Moreover, the actual tree generating the sequence (called the model) is also unknown. The context-tree weighting algorithm takes a weighted sum over all tree models with depth not exceeding D. The redundancy introduced by the lack of knowledge of both the parameter and the model is bounded, and context-tree weighting is optimal since it achieves the lower bound of redundancy derived in Rissanen (1984). More details regarding this method are provided in the Supplementary Material. The context-mixing algorithm lpaq1 (<ref type="bibr" target="#b11">Mahoney 2002</ref>) that we used in our implementation predicts the next bit based on the previous six bytes, as well as the last matching context. Hash tables are built to store the history and the context. During compression, the algorithm adaptively updates the probability distribution of the next bit based on its current prediction and uses arithmetic coding with time varying probability values. Since adaptive schemes perform poorly for short sequence lengths, the context-mixing scheme is only recommended in the one-block compression mode which does not allow random query. As a result, context-mixing compression should be used for archival storage. To speed up compression/decompression, we also implemented a parallel scheme for arithmetic coders with random query. The scheme partitions the original sequences based on its chromosome index and compresses each substring on a separate processor. Details of this implementation and its performance are discussed in the Results Section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We tested our compression algorithm on 14 integer-valued WIG files with sizes ranging from 1.5 to 5.3 GB and on 10 integer and real-valued BedGraph files. All Wig files contain human transcriptome RNA-seq data from the ENCODE hg19 browser. Since smallWig is designed for WIG files, here we mainly focus on the 14file set. A more detailed report on the performance of smallWig on both file sets can be found in Supplementary Material. We measured the performance of smallWig and other existing algorithms through the: @BULLET Compression rate (compression ratio), the compressed file size divided by the original file size. @BULLET Running time of: (i) the encoding, (ii) the decoding and (iii) the random query process.<ref type="figure" target="#fig_2">Figure 1</ref>shows the compression rates achieved by various variants of smallWig, compared with the rates of gzip, bigWig and cWig through BedGraph. The depicted entropy is under the independent run difference model. With arithmetic coding, our algorithm offers 18-fold rate improvements compared with bigWig. In fact, the compressed file size of our running example is only 1/80 of the original WIG file. Furthermore, the compression rate is only 1.6% larger than the empirical entropy and may be attributed to storing the empirical probabilities. With context-mixing, one can further improve the compression rate to 23 times compared with bigWig. For compression with random queries, smallWig offers 17-fold rate improvements compared with bigWig. According to the report in Hoang and Sung (2014), the compression rate of the state-of-the art cWig method is about 3.1 times better than that of bigWig. However, we found that one can obtain an even better rate by first converting a WIG file into a BedGraph file and then converting the BedGraph file to cWig with some simple additional processing (BedGraph files are compact representations of WIG files that fundamentally rely on run length coding). Our sequential WIG-BedGraph-cWig pipeline performs about 8.5 times better than bigWig. The newly introduced smallWig method still performs twice as well as the proposed modification of cWig. For databases containing TB/PB of WIG files, a 2-fold reduction in file sizes may lead to exceptionally important storage cost savings. In<ref type="figure">Figure 2</ref>, we present the running time of smallWig encoding/ decoding schemes, as well as those of gzip, bigWig and cWig. With arithmetic coding, smallWig has a 2.5 times smaller encoding and 1.5 times smaller decoding time compared with that of bigWig. Arithmetic coding with random query has 1.9 times smaller encoding time than bigWig. Context-mixing algorithms are computationally intensive compared with arithmetic coding and require significantly longer running time. To compare the effect of different block sizes used for random query on compression rate and encoding/decoding time, we refer the reader to<ref type="figure" target="#fig_5">Figure 3</ref>. In the experiments, the block sizes ranged from 512 to 4096. To enable random query, we introduced a 3–17% overhead in compression rate and a 30% and 55% overhead in encoding and decoding time, respectively.<ref type="figure" target="#tab_1">Table 1</ref>lists the random query time. Note that the start positions (and for long queries the end positions) of the queries were generated uniformly at random among all allowed chromosomal locations for every chromosome. For short queries, the query length was fixed to 1000, so that one query falls within a single block; in this case, the query time corresponds to the time needed to retrieve the corresponding block. One can see that smallWig is comparable in performance to bigWig for short queries and runs about three times faster for long queries. It is also comparable to cWig for both types<ref type="bibr" target="#b7">Kent et al. 2010</ref>), cWig (Hoang and Sung 2014) through BedGraph and smallWig methods, which encompass arithmetic coding, arithmetic coding on blocks of size 1024 and context-mixing algorithms using lpaq1 (<ref type="bibr" target="#b11">Mahoney 2002</ref>). To test cWig, we constructed our own WIG-BedGraph-cWig pipeline. All presented results are averaged over 14 sample files taken from ENCODE hg19. A more detailed table is included in the Supplementary Material smallWig: parallel compression of RNA-seq WIG filesof queries. Moreover, to facilitate visualization, in the random query functions, smallWig outputs the exact summary information together with the queried location-expression pairs. On the other hand, the bigWig summary function only outputs information corresponding to the overlapped blocks but not to that of the exact queried region. We observe that for all the tested files, smallWig with arithmetic coding had a relatively small memory usage, as listed in<ref type="figure" target="#tab_2">Table 2</ref>. In particular, during most of the compression tests, the memory usage was less than 10 KB. With different user-defined parameters, smallWig with context mixing had higher and more variable memory usage, ranging from 90 KB to 1200 MB. We note here that since gzip does not offer random access and summary information, its memory usage is smaller than that of the other algorithms. In<ref type="figure">Figure 4</ref>, we show the running time of parallel multiprocessor compression methods. The encoding time is decreased by 2–3.4 times as the number of processors increases from 2 to 8. Furthermore, the decoding time is decreased by 2–5.2 times. The time does not decrease linearly since we used a uniform sequence partition procedure for individual chromosomes, and chromosomes have largely different lengths. Moreover, after every step in the algorithm (e.g. sequence transformation, empirical probability computation, arithmetic coding), some components of the pipeline have to pause until all processors have finished their computations and their information is aggregated. We also tested smallWig on 10 WIG files that were generated from BedGraph files including integer-valued as well as floatingpoint-valued expressions. The average compression rates are shown in<ref type="figure">Figure 5</ref>. Note that BedGraph already takes into account the run length transformations and hence the compression rate improvements for these files are not as large as those for WIG files. For integer-valued files, smallWig is 5 and 1.8 times more efficient than bigWig and cWig, respectively. For floating point-valued files, smallWig is 4.3 and 1.9 times more efficient than bigWig and cWig, respectively. More details about these tests can be found in Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In what follows, we describe the differences in compression strategies used by various methods and attempt to intuitively explain the improved performance of smallWig compared with cWig and bigWig. All three algorithms—bigWig, cWig and smallWig—use run length encoding. Both cWig and smallWig use delta encoding. Moreover, all three algorithms use blocks of a certain size for random query purposes: bigWig and cWig only operate with fixed</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Running Time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compression Decompression</head><p>Fig. 2. Encoding and decoding time of gzip, bigWig and smallWig algorithms using arithmetic coding, arithmetic coding on blocks of size 1024 and our context-mixing algorithm. The encoding/decoding time is expressed in seconds per MB of the original WIG file. All the results were averaged over 14 sample files from ENCODE hg19. The error bars indicate the standard deviation. A more detailed table is included in the Supplementary MaterialThe label " no block " indicates that the whole sequence is compressed as a single block. The encoding/decoding time is expressed in seconds per MB in the original file. The y-label is for both the rate and the speed (s/MB). All the results are averaged over 14 sample files from ENCODE hg19We list the average query time in seconds per queried location for long queries, and the average query time in seconds for short queries, together with the corresponding standard deviations, over all 14 WIG files and 240 queries on each file.<ref type="bibr">1977)</ref>] on each block. As already pointed out, gzip is a universal source coding scheme that does not rely on prior knowledge about the probability distributions. It approaches the entropy rate if the source is stationary and ergodic and as the sequence length goes to infinity. Since the alphabet sizes of the sequences are fairly large (a few thousand to several tens of thousand) but the block sizes are only 512, gzip offers somewhat poor performance. On the other hand, cWig uses Huffman codes for frequent values, and Elias delta codes for less frequent values. Both codes perform symbol-by-symbol encoding. Assume that the data source is producing independent and identically distributed outputs with probability mass function pðÁÞ. Since the code word of a symbol x must be represented by a binary sequence, say of length 'ðxÞ, the individual symbol redundancy rðxÞ ¼ 'ðxÞ À log 2 1 pðxÞ is a real number in ½0; 1Þ. Even for Huffman encoding (i.e. the optimal prefix encoding), the expected per-symbol redundancy may be large enough to create " visible " rate losses. There exist a number of results on the upper and lower bounds of the expected redundancy r ¼ E X ðrðXÞÞ for a random variable X. For example,<ref type="bibr" target="#b4">Gallager (1978)</ref>showed an upper bound based on the largest symbol probability;<ref type="bibr" target="#b1">Capocelli and De Santis (1991)</ref>bounded the redundancy both from above and below based on the largest and the smallest symbol probability and<ref type="bibr" target="#b14">Mohajer et al. (2012)</ref>showed a tight upper and lower bound based on one known symbol probability p. In particular, in the latter case, the redundancy is lower bounded by r ! mp À HðpÞ À ð1 À pÞlog 2 ð1 À 2 Àm Þ; where m &gt; 0 is either dÀlog 2 peorbÀlog 2 pc, depending on which value minimizes the overall expression. Here, H denotes the binary Shannon entropy function: HðpÞ ¼ Àplog 2 p À ð1 À pÞlog 2 ð1 À pÞ. For our running example, the run expression difference x ¼ –1 has the largest probability, pðÀ1Þ ¼ 0:3374, which leads to the corresponding redundancy of Huffman coding r!0:0275: For a given distribution and a given symbol-by-symbol codebook which may not be optimal, there exists a non-negative and non-negligible coding redundancy r; on blocks of length 512, the overall redundancy equals 512r, which is at least 14 bits per block for Huffman codes. If a different suboptimal code or unmatched Huffman code is used, this redundancy may be even larger. At the same time, arithmetic coding only causes a redundancy up to 2 bits per block if the probability distribution is known. As a result, smallWig files are significantly smaller than cWig files. Furthermore, smallWig is flexible in terms of the block size and enables context-mixing as well as parallel processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We studied compression methods for RNA-seq expression data. We proposed a new algorithm, termed smallWig, which achieves a compression ratio that is at least one order of magnitude better than currently used algorithms. At the same time, the algorithm also improves the running time and flexibility of random access. The presented results included detailed performance evaluations of smallWig in the standard, random access, context mixing and parallel operation mode.<ref type="figure">Fig. 5</ref>. Compression rates for BedGraph files. The bars on the left correspond to compression rates for integer-valued files, and the bars on the right correspond to compression rates for floating-point-valued file formats. Since the tested files have a large variance on their sizes, we take the sum of the compressed file sizes divided by the sum of the original BedGraph file sizes as our average rate smallWig: parallel compression of RNA-seq WIG files</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>@BULLET The Expression Sequence B ¼ ðb 1 ; b 2 ;. .. ; b M Þ. The sequence B contains expression values b i 2 R. The b i 's indicate the number of RNA transcripts that include location a i. Note that the sequences A and B have the same length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>Independent run difference model. Assume that the sequences X, Y are independent and that the elements of X (Y) are independent and identically distributed. The entropy in this case reads as H I ¼ HðX 1 Þ þ HðY 1 Þ: Markov run difference model. Again we assume that the sequences X, Y are independent, but let X (Y) be a first order Markov sequence. We write the entropy under this model as H M ¼ HðX 2 jX 1 Þ þ HðY 2 jY 1 Þ: Paired sequence model. We assume the sequence of pairs ððX 1 ; Y 1 Þ; ðX 2 ; Y 2 Þ;. .. ; ðX N ; Y N ÞÞ is a first order Markov sequence. Thus, the entropy formula reads as H P ¼ HðX 2 ; Y 2 jX 1 ; Y 1 Þ:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.1.</head><figDesc>Fig. 1. Compression rates achieved by gzip, bigWig (Kent et al. 2010), cWig (Hoang and Sung 2014) through BedGraph and smallWig methods, which encompass arithmetic coding, arithmetic coding on blocks of size 1024 and context-mixing algorithms using lpaq1 (Mahoney 2002). To test cWig, we constructed our own WIG-BedGraph-cWig pipeline. All presented results are averaged over 14 sample files taken from ENCODE hg19. A more detailed table is included in the Supplementary Material</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.3.</head><figDesc>Fig. 3. Compression rate, encoding time and decoding time given different block sizes fed to arithmetic encoders. The label " no block " indicates that the whole sequence is compressed as a single block. The encoding/decoding time is expressed in seconds per MB in the original file. The y-label is for both the rate and the speed (s/MB). All the results are averaged over 14 sample files from ENCODE hg19</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>Table 1.</figDesc><table>Comparison of smallWig, bigWig and cWig with respect to 
random query time 

Query Type 
Measure 
bigWig 
cWig 
smallWig 

Long query 
Average (s/bp) 
1.99E-7 
5.20E-8 
5.87E-8 
std 
4.74E-6 
6.48E-8 
6.08E-7 
Short query 
Average (s) 
0.0565 
0.0574 
0.0711 
std 
0.0515 
0.1360 
0.0176 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>Table 2. Maximum memory usage during encoding and decoding in bytes for gzip, bigWig, cWig, smallWig and smallWig with context mixing on the tested files</figDesc><table>gzip 
bigWig 
cWig 
smallWig 
smallWig cntx 

enc. 
664 
2315M 
1935M 
89K 
90K–1200M 
dec. 
932 
24K 
45K 
19K 
90K–1200M 

</table></figure>

			<note place="foot">at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">Z.Wang et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">M ¼ 3:9e þ 7. Note that a similar transform is used in the bigBed format, which provides a more succinct representation of sparse WIG data (we will revisit the Bed format in the results section). Since adjacent runs tend to have similar lengths and expressions, the differences between consecutive runs may lead to further compaction of WIG information. To describe the difference sequences, let c 0 ¼ 0; d 0 ¼ 0. • The Run length Difference Sequence, X ¼ ðx 1 ; x 2 ;. .. ; x N Þ, is defined by x i ¼ c i À c iÀ1 , so that x i 2 Z for all i 2 ½N. • Run Expression Difference Sequence, Y ¼ ðy 1 ; y 2 ;. .. ; y N Þ, is defined by y i ¼ d i À d iÀ1 , so that y i 2 R for all i 2 ½N.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank the editor and the anonymous reviewers for their comments and suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>The work is partially supported by the Center for Science of Information (CSoI), funded under grant agreement CCF-0939370, and by NIH BD2K 1U01CA198943-01. Conflict of Interest: none declared.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">A simple statistical algorithm for biological sequence compression</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">D</forename>
				<surname>Cao</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Compression Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">New bounds on the redundancy of Huffman codes</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Capocelli</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">De</forename>
				<surname>Santis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1095" to="1104" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Genome compression: a novel approach for large collections</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Deorowicz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2572" to="2578" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">The ENCODE (ENCyclopedia of DNA elements) project</title>
	</analytic>
	<monogr>
		<title level="j">Encode Project Consortium. Science</title>
		<imprint>
			<biblScope unit="volume">306</biblScope>
			<biblScope unit="page" from="636" to="640" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Variations on a theme by Huffman</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">G</forename>
				<surname>Gallager</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="668" to="674" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Cwig: compressed representation of wiggle/bedgraph format</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">H</forename>
				<surname>Hoang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W.-K</forename>
				<surname>Sung</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2543" to="2550" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Minimax estimation of functionals of discrete distributions</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Jiao</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="page" from="61" to="2835" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Bigwig and bigbed: enabling browsing of large distributed datasets</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">J</forename>
				<surname>Kent</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2204" to="2207" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">The performance of universal encoding</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Krichevsky</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Trofimov</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="199" to="207" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimized relative Lempel-Ziv compression of genomes</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Kuruppu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth Australasian Computer Science Conference</title>
		<meeting>the Thirty-Fourth Australasian Computer Science Conference</meeting>
		<imprint>
			<publisher>Australian Computer Society, Inc</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Compressive genomics</title>
		<author>
			<persName>
				<forename type="first">P.-R</forename>
				<surname>Loh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="627" to="630" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">The paq1 data compression program</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">V</forename>
				<surname>Mahoney</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Draft</title>
		<imprint>
			<date type="published" when="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">RNA-seq: an assessment of technical reproducibility and comparison with gene expression arrays</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">C</forename>
				<surname>Marioni</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1509" to="1517" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Range encoding: an algorithm for removing redundancy from a digitised message</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">N N</forename>
				<surname>Martin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Institution of Electronic and Radio Engineers International Conference on Video and Data Recording. Institution of Electronic and Radio Engineers</title>
		<meeting>Institution of Electronic and Radio Engineers International Conference on Video and Data Recording. Institution of Electronic and Radio Engineers</meeting>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Tight bounds on the redundancy of Huffman codes</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Mohajer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="6737" to="6746" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">The human genome contracts again</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">S</forename>
				<surname>Pavlichin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2199" to="2202" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">On the representability of complete genomes by multiple competing finite-context (Markov) models</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Pinho</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">21588</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">GReEn: a tool for efficient compression of genome resequencing data</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Pinho</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Universal coding, information, prediction, and estimation</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Rissanen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="629" to="636" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Arithmetic coding</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Rissanen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">G</forename>
				<surname>Langdon</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Jr</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM J. Res. Dev</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="149" to="162" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<monogr>
		<title level="m" type="main">Variable-Length Codes for Data Compression</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Salomon</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<monogr>
		<title level="m" type="main">Genome compression using normalized maximum likelihood models for constrained Markov sources</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Tabus</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Korodi</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="261" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Information theory applications for biological sequence analysis</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Vinga</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brief. Bioinform</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="376" to="389" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">A novel compression tool for efficient storage of genome resequencing data</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">45</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">RNA-seq: a revolutionary tool for transcriptomics</title>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Wang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Genet</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="57" to="63" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">The context-tree weighting method: basic properties</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">M</forename>
				<surname>Willems</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="653" to="664" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">A universal algorithm for sequential data compression</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Ziv</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Lempel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="337" to="343" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>