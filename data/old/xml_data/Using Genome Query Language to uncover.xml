
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Genome analysis Using Genome Query Language to uncover genetic variation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014">2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Christos</forename>
								<surname>Kozanitis</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">University of California San Diego</orgName>
								<address>
									<addrLine>9500 Gilman Drive</addrLine>
									<postCode>92123</postCode>
									<settlement>San Diego</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Andrew</forename>
								<surname>Heiberg</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">University of California San Diego</orgName>
								<address>
									<addrLine>9500 Gilman Drive</addrLine>
									<postCode>92123</postCode>
									<settlement>San Diego</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">George</forename>
								<surname>Varghese</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<addrLine>1065 La Avenida</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Vineet</forename>
								<surname>Bafna</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">University of California San Diego</orgName>
								<address>
									<addrLine>9500 Gilman Drive</addrLine>
									<postCode>92123</postCode>
									<settlement>San Diego</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Genome analysis Using Genome Query Language to uncover genetic variation</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">30</biblScope>
							<biblScope unit="issue">1</biblScope>
							<biblScope unit="page" from="1" to="8"/>
							<date type="published" when="2014">2014</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btt250</idno>
					<note type="submission">Received on March 15, 2013; revised on April 25, 2013; accepted on April 26, 2013</note>
					<note>BIOINFORMATICS ORIGINAL PAPER Associate Editor: Michael Brudno Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: With high-throughput DNA sequencing costs dropping 5$1000 for human genomes, data storage, retrieval and analysis are the major bottlenecks in biological studies. To address the large-data challenges, we advocate a clean separation between the evidence collection and the inference in variant calling. We define and implement a Genome Query Language (GQL) that allows for the rapid collection of evidence needed for calling variants. Results: We provide a number of cases to showcase the use of GQL for complex evidence collection, such as the evidence for large structural variations. Specifically, typical GQL queries can be written in 5–10 lines of high-level code and search large datasets (100 GB) in minutes. We also demonstrate its complementarity with other variant calling tools. Popular variant calling tools can achieve one order of magnitude speed-up by using GQL to retrieve evidence. Finally, we show how GQL can be used to query and compare multiple datasets. By separating the evidence and inference for variant calling, it frees all variant detection tools from the data intensive evidence collection and focuses on statistical inference. Availability: GQL can be downloaded from</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As sequencing costs drop, we envision a scenario where every individual is sequenced, perhaps multiple times in their lifetime. There is already a vast array of genomic information across various large-scale sequencing projects including the 1000 genome project (1000 Genomes<ref type="bibr" target="#b0">Project Consortium et al., 2010</ref>) and the cancer genome atlas (TCGA) (<ref type="bibr" target="#b22">Koboldt et al., 2012</ref>). In many of these projects, a re-sequencing strategy is applied in which whole genomes are sequenced redundantly with coverage between 4 and 40Â. The clone inserts ($500 bp) and sequenced reads ( 150 bp) are typically short and are not de novo assembled. Instead, they are mapped back to a standard reference to decipher the genomic variation in the individual relative to the reference. Even with advances in single-molecule sequencing and genomic assembly (<ref type="bibr" target="#b7">Clarke et al., 2009</ref>), we are many years away from having finished and error-free assembled sequences from human donors. At least in the near to mid-term, we expect that the bulk of sequencing will follow the resequencing/mapping/variant calling approach (e.g.<ref type="bibr" target="#b28">McKenna et al., 2010</ref>). Mapped reads (represented by BAM files) from a single individual sequenced with 40Â coverage are relatively inexpensive to generate, but they are storage intensive ($100GB). As sequencing becomes more accessible, and larger numbers of individuals are sequenced, the amount of information will increase rapidly; this will pose a serious challenge to available community resources. Although raw archiving of large datasets is possible (<ref type="bibr" target="#b39">Wheeler et al., 2008</ref>), the analysis of this huge amount of data remains a challenge. To facilitate access, some of the large datasets have been moved to commercially available cloud platforms. For example, the 1000 genome data are available on Amazon's EC2 (1000genomescloud, 2012). The genomes on Amazon can be analyzed remotely using appropriate software frameworks like Galaxy [that allow for the pipelining/integration of multiple analysis tools (<ref type="bibr" target="#b16">Goecks et al., 2010)]</ref>, as well as tools like Genome Analysis Toolkit (GATK) (<ref type="bibr" target="#b28">McKenna et al., 2010</ref>) and samtools (<ref type="bibr" target="#b26">Li et al., 2009</ref>). The promise of this approach is that much of the analysis can be done remotely, without the need for extensive infrastructure on the user's part. Even with these developments, a significant challenge remains. Each individual genome is unique, and the inference of variation, relative to a standard reference remains challenging. In addition to small indels and substitutions (the so-called single-nucleotide variations or SNVs), an individual might have large structural changes, including, but not limited to, insertions, deletions, inversions (<ref type="bibr" target="#b32">Sharp et al., 2006</ref>), translocations of large segments (10 2 –10 6 bp in size) (<ref type="bibr" target="#b15">Giglio et al., 2001</ref>), incorporation of novel viral and microbial elements and recombination-mediated rearrangements (<ref type="bibr" target="#b29">Perry et al., 2006</ref>). Further, many of these rearrangements may overlap leading to more complex structural variations. The detection of these variations remains challenging even for the simplest SNVs, and there is little consensus on the best practices for the discovery of more complex rearrangements. For large, remotely located datasets, it is often difficult to create a fully customized analysis. It is often desirable to download the evidence (reads) required for the detection of variation to a local machine, and experiment with a collection of analysis tools for the actual inference. In that case, we are back again to the problem of building a large local infrastructure, including clusters and large disks, at each analysis site in addition to the resources in the cloud. As an example, we consider the use of paired-end sequencing and mapping (PEM) for identifying structural variation. In *To whom correspondence should be addressed. PEM, fixed length inserts are selected for sequencing at both ends, and the sequenced sub-reads are mapped to a reference genome. Without variation, the distance and orientation of the mapped reads match the a priori expectation. However, if a region is deleted in the donor relative to the reference, ends of the insert spanning the deleted region will map much further apart than expected. Similarly, the expected orientation of the read alignments for Illumina sequencing is (þ,À). A (þ,þ) orientation is suggestive of an inversion event. Using PEM evidence, different callers still use different inference mechanisms. GASV (<ref type="bibr" target="#b33">Sindi et al., 2009</ref>) arranges overlapping discordantly mapping pair-end reads on the Cartesian plane and draws the grid of possible break point locations under the assumption that the discordancy is a result of a single SV. Breakdancer (<ref type="bibr" target="#b6">Chen et al., 2009</ref>) finds all areas that contain at least two discordant pair-end reads, and it uses a Poisson model to evaluate the probability that those areas contain a SV as a function of the total number of discordant reads of each of those areas. VariationHunter (<ref type="bibr" target="#b18">Hormozdiari et al., 2009</ref>) reports that regions of SV are the ones that minimize the total number of clusters that the pair ends can form. Given the complexity of the data, and the different inference methodologies, all of these methods have significant type 1 (false-positive), and type 2 (falsenegative) errors. Further, as the authors of VariationHunter (<ref type="bibr" target="#b18">Hormozdiari et al., 2009</ref>) point out, there are a number of confounding factors for discovery. For example, repetitive regions, sequencing errors, could all lead to incorrect mappings. At the same time, incorrect calls cannot be easily detected because tools need to be modified to re-examine the source data. In addition, the run time of the entire pipeline of those tools is not negligible given that they have to parse the raw data. A starting point of our work is the observation all tools follow a two-step procedure, implicitly or explicitly, for discovery of variation. The first step—the evidence-step—involves the processing of raw data to fetch (say) the discordant pair-end reads; the second step—the inference-step—involves statistical inference on the evidence to make a variant call. Moreover, the evidence gathering step is similar and is typically the data-intensive part of the procedure. For example, in SNV discovery, the evidence step is the alignment ('pile-up') of nucleotides to a specific location, whereas the inference step involves SNV estimation based on alignment quality and other factors. By contrast, for SVs such as large deletions, the evidence might be in the form of (a) lengthdiscordant reads and (b) concordant reads mapping to a region; the inference might involve an analysis of the clustering of the length-discordant reads, and looking for copy-number decline and loss of heterozygosity in concordant reads. In this article, we propose a Genome Query Language (GQL) that allows for the efficient querying of genomic fragment data to uncover evidence for variation in the sampled genomes. Note that our tool does not replace other variant calling tools, but it is complementary to existing efforts. It focuses on the collection of evidence that all inference tools can use to make custom inference. First, by providing a simple interface to extract the required evidence from the raw data stored in the cloud, GQL can free callers from the need to handle large data efficiently. Second, we show how existing tools can be sped up and simplified using GQL, with larger speed-ups possible through a cloud based parallel GQL implementation. Third, analysts can examine and visualize the evidence for each variant, independent of the tool used to identify the variant. Software layers and interfaces for genomics: We also place GQL in the context of other genomics software. It is helpful to think of a layered, hourglass, model for genomic processing. At the bottom is the wide, instrument layer (customized for each instrument) for calling reads. This is followed by mapping/ compression layers (the 'narrow waist' of the hourglass), and subsequently, multiple application layers. Some of these layers have been standardized. Many instruments now produce sequence data as 'fastq' format, which in turn is mapped against a reference genome using alignment tools, such as BWA (<ref type="bibr" target="#b24">Li and Durbin, 2010</ref>) and MAQ (<ref type="bibr" target="#b25">Li et al., 2008</ref>); further, aligned reads are often represented in the compressed, BAM format (<ref type="bibr" target="#b26">Li et al., 2009</ref>) that also allows random access. Recently, more compressed alignment formats have come into vogue including SlimGene (<ref type="bibr" target="#b23">Kozanitis et al., 2011</ref>) CRAM (<ref type="bibr" target="#b19">Hsi-Yang Fritz et al., 2011</ref>) and others (<ref type="bibr" target="#b1">Asnani et al., 2012;</ref><ref type="bibr" target="#b10">Cox et al., 2012;</ref><ref type="bibr" target="#b30">Popitsch and von Haeseler, 2013;</ref><ref type="bibr" target="#b38">Wan et al., 2012;</ref><ref type="bibr" target="#b40">Yanovsky, 2011</ref>) as well as compression tools for unmapped reads (<ref type="bibr" target="#b20">Jones et al., 2012</ref>). At the highest level, standards such as VCF (VCF<ref type="bibr" target="#b36">Tools, 2011</ref>) describe variants (the output of the inference stage of<ref type="figure" target="#fig_0">Fig. 1b</ref>). In this context, we propose additional layering. Specifically, we advocate the splitting of the processing below the application layer to support a query into an evidence layer(deterministic, large data movement, standardized) and an inference layer (probabilistic, comparatively smaller data movement, little agreement on techniques). For evidence gathering, the closest tools are samtools (<ref type="bibr" target="#b26">Li et al., 2009</ref>), BAMtools<ref type="bibr" target="#b2">Barnett et al. (2011</ref><ref type="bibr">), BEDtools (Dale et al., 2011</ref><ref type="bibr" target="#b31">Quinlan and Hall, 2010</ref>), BioHDF (<ref type="bibr" target="#b27">Mason et al., 2010</ref>) and GATK (<ref type="bibr" target="#b28">McKenna et al., 2010</ref>). Samtools consists of a toolkit and an API for handling mapped reads; together, they comprise the first attempt to hide the implications of raw data handling by treating datasets uniformly regardless of the instrument source.Samtools also provide quick random access to large files and provide a clean API to programmatically handle alignments. The tool combines index sorted BAM files with a lightweight and extremely efficient binning that clusters reads that map in neighboring locations. Thus, samtools can quickly return a set of reads that overlap with a particular location or create a pileup (i.e. all bases seen in reads that map to any reference locus). BAMtools is a Cþþ API built to support queries in a JSON format. BEDtools, closely aligned with samtools, allows intervalrelated queries through a clean unix and a python interface. Although powerful, these tools still require programmer-level expertise to open binary files, assign buffers, read alignments and manipulate various fields. The GATK (<ref type="bibr" target="#b28">McKenna et al., 2010</ref>) is built on top of samtools and reduces the programming complexity of data collection. GATK's API provides two main iterator categories to an application programmer. The first iterator traverses individual reads; the second iterator walks through all genome loci, either individually or in adjacent groups. The toolkit, which is written based on the Map Reduce framework and thus easily parallelizable, is an excellent resource for developers of applications that need to determine local realignment, quality score re-calibration and genotyping (<ref type="bibr" target="#b12">DePristo et al., 2011</ref>). The support of many of these tools for looking at paired-ends, and consequently for structural variation, is limited, depending (in GATK's case) on the existence of the optional fields RNEXT and PNEXT of the SAM/BAM alignments (gatk-pairend, 2012). The single biggest difference between our proposed tool, GQL and others is that GQL has a (SQL-like) declarative syntax in its own language, as opposed to a procedural syntax, designed to help programmers rather than the end user. Declarative languages, such as GQL and SQL, not only raise the level of abstraction of data access but also allow automatic data optimization without programmer intervention. By asking users to specify what data they want as opposed to how they want to retrieve it, we will show that GQL can facilitate automatic optimizations, such as the use of indices and caching; these seem harder to support in other tools without explicit programmer directives. Further, it is feasible to compile GQL queries to a distributed, cloud based, back-end. Finally, GQL queries allow genomes to be browsed for variations of interest, allowing an interactive exploration of the data as we show in our results. Although the UCSC browser also allows genome browsing, it does only by position or string, which we refer to as syntactic genome browsing. By contrast, GQL allows browsing for all regions containing reads that satisfy a specified property (e.g. discrepant reads) and view the results on the UCSC browser. We refer to this as semantic genome browsing and give many examples in Section 2. Our performance results indicate that such browsing can be done interactively in minutes using a single cheap CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">An overview of GQL: language features and implementation</head><p>We chose to use an SQL-like syntax to express GQL because SQL is an accepted and popular standard for querying databases. Although our syntax is SQL-like, we need some special operators and relations for genomic queries that do not appear to fit well with existing off-the-shelf Relational Database Management Systems (RDBMS). Thus, we developed our own compiler to process GQL queries and translate them into Cþþ API calls (Section 4). Our compiler also allowed us the freedom to heavily optimize our GQL implementation (by several orders of magnitude), which would be harder to do with existing RDBMS. We conceptualize the genomic fragment data as a database with some key relations. The first is a relation called READS that describes the collection of mapped fragment reads and all their properties but does not contain the actual bases or quality scores. For example, for paired-end sequencing, each entry in the READS table will contain information about a read, its mapping location and the location of its paired-end. The reads table is constructed by pre-processing a BAM file through a set of scripts that accompanies the source code that split, index and move the contents of the file to the appropriate directory that GQL can access; it contains fields such as the mapping location, the strand, the read length, the location of the pair-ends. In addition, GQL accepts a freely formatted Text table that can be any table that a user can define. Text table can, for example, be used to describe gene annotations. GQL also accepts interval tables, which have three special fields (chromosome, and begin and end location within the chromosome) demarcating the interval. The user has the option of creating an interval table from any table by marking two of the attributes as begin and end; the chromosome field is updated automatically during the iteration through all chromosomes. The most interesting aspects of GQL semantics lie in allowing interval manipulation. In programming languages terminology, intervals are first-class entities in GQL. The Supplementary Information summarizes all GQL tables and the respective attributes of the language. Language constructs. All GQL statements (like SQL) have the form SELECT hattributesi FROM htablesi WHERE hconditioni. The FROM statement specifies the input tables to the statement. The SELECT statement corresponds to the projection operator in the relational calculus (<ref type="bibr" target="#b8">Codd, 1970</ref>). It returns a subset of the attributes of the input table.</p><p>The WHERE statement selects the subset of records of the input tables that satisfy the filter expression that follows the WHERE statement. The using intervals() expression optionally follows a table specifier in the FROM statement. It produces an interval for each entry of the corresponding table according to the specified expression. If the input table is of type READS the user has the ability to add the keyword both_mates as a third argument to the expression specified by using intervals to denote that a pair end is treated as a single interval. This expression does not return any table and can only be used with the create_intervals or MAPJOIN operations.</p><p>The create_intervals function constructs a table of intervals from the input table. When the function is called, the table in the FROM statement is followed by the statement using intervals(a,b) so that the function knows which fields to use as intervals.</p><p>The MAPJOIN statement takes two tables as input and concatenates any two entries of these tables whose corresponding intervals intersect. The user specifies intervals with the expression using intervals next to each input table.</p><p>The merge_intervals(interval_count op const) statement is a function whose input table needs to be of type Intervals. It creates an output table of intervals from the intervals in the input table that overlap with at least or at most the number of intervals specified inside the parenthesis. This statement uses op to specify at least or at most.</p><p>The implementation of GQL consists of a front-end that parses user input and a back-end that implements the remaining functionality of the system. The front-end is implemented using the flex (<ref type="bibr" target="#b13">Flex, 1990</ref>) and Bison (<ref type="bibr">Bison, 1988</ref>) tools and is based on the GQL grammar (Supplementary Information). It performs syntactic and semantic analysis and converts the GQL statements into a series of back-end procedure calls with the proper arguments. It also converts any user expressions, such as the ones found in the WHERE and using intervals statements into customizable Cþþ files. These are compiled and run as executables on the back-end. The back-end implements the basic table types and all remaining GQL functionality (see Section 4 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RESULTS AND DISCUSSION</head><p>We demonstrate the flexibility and efficiency of GQL by walking through some use cases that involve identifying donor regions with variations, and supplying the read evidence supporting the variation. We use the Yoruban trio (both parents and a child) from the Hapmap project (1000 Genomes Project<ref type="bibr" target="#b0">Consortium et al., 2010</ref>) that was sequenced as part of the 1000 Genome Project. The genomes are labeled NA18507, NA18508 and NA18506 for the father, mother and the child, respectively. Each genome was sequenced to $40Â coverage ($1B mapped reads) using 2 Â 100 bp paired-end reads from 300 bp inserts. We use the large deletion detection problem for a case study. The corresponding input BAM file sizes are all in the range 73–100 GB. Large deletions on NA18506. Paired-end mapping provides an important source of evidence for identifying large structural variations (<ref type="bibr" target="#b3">Bashir et al., 2007;</ref><ref type="bibr" target="#b21">Kidd et al., 2008</ref>). Length discordant clones (pairs of reads with uncharacteristically long alignment distance between the two mapped ends) are indicative of a deletion event. We start by using GQL to identify all genomic regions (in reference coordinates) that have an 'abundance' of lengthdiscordant clones.</p><p>(1) Select inserts where both ends are mapped, and the insert size is at least 350 bp (i.e. the insert size exceeds the mean insert size by more than 5Â the standard deviation) and at most 1 Mb (Supplementary Information).Note that entire query is a few lines of GQL. Further, the query took 10 min to execute on the entire genome. All the results we report used a single i7 Intel CPU with 18 GB of random access memory and 2 TB of disk. Much of the execution time was spent on printing the large output (12 K regions, with 44 MB of supporting evidence). These observations suggest that complex GQL queries could efficiently be performed in the cloud. Given that organizations can easily obtain 100 Mbps speeds today (Amazon's Direct Connect even allows 1 Gbps speeds), the output of 44 MB can be retrieved interactively to a user desktop in 54 s. By contrast, downloading the original BAM files would take 42 h at 100 Mbps. Second, although we measured the query to take 10 min on a single processor, simple parallelism by chromosome (easily and cheaply feasible in the cloud) should provide a factor of 20 speed-up, leading to a query time of 30 s. In addition, given the decreasing cost of random access memory, an implementation of GQL on a memory only database system, such as SAP HANA, can provide additional speedup by eliminating disk accesses. Currently, the output products of each SELECT statement are stored to disk and are loaded again by subsequent SELECT statements. A memory only database will be able to remove this overhead. Further, writing output BAM files comprises a clear performance bottleneck caused by a large number of alternating read and write disk accesses, which can also be eliminated by a memory only database. Despite these advantages of memory only databases, cloud implementations are also useful because many current genomic archives are stored in the cloud, such as the 1000 genomes archive on EC2. Further, we wrote a program to convert the intervals output by a GQL query to the BED format, which can then be uploaded to the UCSC genome browser for further investigation, including comparisons with other reference annotations. See<ref type="figure" target="#fig_2">Figure 2a</ref>and b for examples showing overlap between output regions, and known CNVs, and a previously reported Indel in NA18507 (<ref type="bibr" target="#b4">Bentley et al., 2008</ref>), the father of NA18506.<ref type="bibr" target="#b9">Conrad et al. (2006)</ref>used array hybridization to identify deletions in a number of genomes, including NA18506. We wrote a GQL query to compare the two sets of predictions as follows: we created a table with all of Conrad's predictions and performed a MAPJOIN with our predicted intervals. We then output the common regions that overlap and the corresponding discordant read evidence. This query ran in 7 min, and the total size of the produced BAM files was 164 KB. Our results overlap with 15 of the 23 findings of<ref type="bibr" target="#b9">Conrad et al. (2006)</ref>. To look at support for regions identified by<ref type="bibr" target="#b9">Conrad et al. (2006)</ref>, but not by us, we used MAPJOIN to identify all discordant and concordant reads that overlap with Conrad-only predictions using the following GQL query.out ¼ select * from mapjoin Refseq_genes using intervals(txStart, txEnd), mapped_reads using intervals (location, location þ length, both_mates) Efficacy of Mapjoin implementation. In most databases, Joins are typically the most time-expensive operation. Typical GQL queries use intermediate MapJoin operations extensively. We implement a special Lazy Join procedure to greatly improve the runtime, explained here with an example as 'output all reads that overlap with genes whose transcriptional start is in a CpG island'. Tables describing CpG islands are available (e.g. Gardiner<ref type="bibr" target="#b14">Garden and Frommer, 1987</ref>) and can be downloaded from the UCSC genome browser. A non-expert user might write the following sub-optimal GQL code that applies the position restriction on the (large) output of the MAPJOIN between all reads and genes. mapped_reads ¼ select * from reads where location40 reads_genes ¼ select * frommapjoin Refseq_genes using intervals(txStart, txEnd), mapped_reads using intervals (location, location þ length) out ¼ select * from mapjoin cpgIsland_hg18 using intervals(chromStart, chromEnd), reads_genes using intervals(location, location þ length) In the absence of lazy evaluation, the execution time of this snippet would be bounded by the extremely large execution time (288 min) of the data intensive query of the previous paragraph. Lazy evaluation, which allows us to join using virtual joins and bypasses intermediate data generation, provides the same result within 42 min for the entire genome. Common deletions in the YRI population. Here, we extend queries from single donor to multiple donors.<ref type="bibr" target="#b21">Kidd et al. (2008)</ref>validated deletions in eight geographically diverse individuals using a fosmid sub-cloning strategy, including NA18507. Of these deletions, the ones that overlap with genes suggest a phenotypically interesting deletion. Therefore, we ask how many of such Chr 1 deletions are prevalent in the entire HapMap YRI sub-population (77 individuals).The query takes 5 min to find the common regions of chr1 across the entire population and 30 min to print the accompanying reads that support the query.<ref type="figure" target="#fig_4">Figure 3</ref>shows the rate according to which each validated deletion appears to other Yoruban individuals and the affected genes. Eight of the deletions are common in at least 30% of the individuals, two are common in at least 50% and one deletion is common in 80% of the YRI population. The information provides a quick first look at deletion polymorphisms in the Yoruban population. For example, 81% of the individuals have a deletion in 1q21.1 containing the Neuroblastoma Breakpoint gene family (NBPF), so called because of prevalent translocation event in neuroblastoma (<ref type="bibr" target="#b35">Vandepoele et al., 2005</ref>). Also, 42% of the individuals have a deletion in 1p36.11, where the deletion removes a large part of the RHD gene, responsible for Rh group D antigen. Such deletions have previously been reported in other populations (<ref type="bibr" target="#b37">Wagner and Flegel, 2000</ref>). We also find a common deletion (22%) involving complement factor H-related protein, which has been associated with age-related macular degeneration (AMD) (<ref type="bibr" target="#b34">Sivakumaran et al., 2011</ref>). Other common deletions involving multiple genes are shown in<ref type="figure" target="#fig_4">Figure 3</ref>. Integration with Variant Callers. In this experiment, we demonstrate the speed-up that the output of GQL can potentially provide to existing SV detection tools. Here, we use Breakdancer (<ref type="bibr" target="#b6">Chen et al., 2009</ref>), which runs in two steps. The first, quick, step collects statistical information from the input BAM file, including read-coverage and distribution of insert sizes. The second (so-called Breakdancer_max) step involves the major processing of the input.A normal run of Breakdancer_max with input being the NA18506 chr1 alignments (a BAM file of size 6 GB) takes 15 min and produces a collection of SVs, including deletion, inversion, insertion and intra-chromosomal translocation events. We used GQL to filter the original BAM file to retain only reads (76 MB) that are needed by Breakdancer (<ref type="bibr" target="#b6">Chen et al., 2009</ref>). Next, we ran Breakdancer_max again using the filtered input. This time the tool needed only 2:30 min, 7Â improvement in speed that can be attributed to the reduced data in the input file. Note that the results are not identical because of the stochastic nature of the caller, but overlap strongly. We call an identified variant from the initial experiment consistent with the second run if it overlaps by at least 50% of its length with the latter. With this definition, we found that 947 of 948 of the deletions, all 204 intra-chromosomal translocations, 337 of 340 inversions and 397 of 461 of insertions are consistent. These results were based on our estimation of the evidence used by Breakdancer found by reading the article. Even more accurate results could be obtained if the writers of the caller wrote the GQL query to gather the evidence they need. Although this is only one experiment, it supports the vision that writers of callers can concentrate on the hard problems of inference and leave the management and filtering of large genomic datasets to GQL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CONCLUSIONS</head><p>The central message of this article is that a declarative query language such as GQL can allow interactive browsing of genome data that is hard to do with existing tools. We agree that in terms of functionality, especially with respect to interval calculus, there exist other tools with similar functionality, such as samtools, bedtools and others. However, the choice of a declarative syntax allows for richer syntax, including multiple join operations and operations on population data. Moreover, it separates the implementation from the query and allows for optimizations in the implementation that are transparent to the naı¨venaı¨ve user. The results suggest that a cloud implementation of GQL can be efficient and fast. In particular, for most selective queries, the resulting output is small (MB) and can be retrieved in a few seconds across the network. Further, the query times we report are in the order of minutes using a cheap single processor for genome-wide scans. Simple map-reduce style parallel implementation should reduce this to seconds. However, one of the optimization relates to separating files by chromosomes, which effectively disallow querying for discordant paired-ends that map to different chromosomes. These queries will be added in a future iteration. We note that we had to implement at least five non-trivial optimizations to reduce query processing times by at least three orders of magnitude. These include the use of a materialized view of the metadata inherent in reads, lazy joins, precompiled parsing of expressions, stack-based interval merging and interval trees. Although interval trees are commonly used in genomic processing, the other optimizations may be novel in a genomic context. These low-level optimizations will be described elsewhere. Finally, GQL is compatible with existing genomic software. Existing callers can be re-factored to retrieve evidence from cloud repositories using GQL, thereby relegating large data handling to GQL with consequent speed-ups as we demonstrated for Breakdancer. GQL is also compatible with SNP calling because GQL produces smaller BAM files that can be input to SNP callers. We have chosen to focus on the use of GQL for structural variation analysis because SNP calling is well studied in the literature, and there are a number of tools already to provide the evidence needed for SNP calling. Further, the results of GQL queries can be viewed using the UCSC browser. In principle, we can also support 'semantic browsing' by properties of genomic regions in addition to browsing by position (syntactic browsing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MATERIALS AND METHODS</head><p>The GQL pipeline starts with the parsing of a user's GQL code (Supplementary Information) by the front-end. We developed a syntax checker and a parser to interpret GQL queries. We used the open source tool flex (<ref type="bibr" target="#b13">Flex, 1990</ref>) to identify the keywords of GQL (Supplementary Information). Syntax checking code was created using the open source tool Bison (<ref type="bibr">Bison, 1988</ref>) and checks the current syntax of GQL described by a context free grammar. Next, we perform a semantic analysis of the code to understand the basic primitives. Appropriate keywords (Select, From and so forth) are recognized, checked in a specific order and used to make calls to back-end routines. The front-end also passes the algebraic part of the user statements (such as expressions) to the back-end by creating customized Cþþ files. This 'precompiled parsing' speeds up the back-end almost 100Â, which would otherwise have to interpret each expression when applied to each read in the input BAM file. The end of processing leads to a custom Cþþ queries that is automatically compiled and used to run GQL queries. READS table. The Reads table is the abstraction by which GQL provides to a user with access to BAM files and its implementation considers that these files are so large that they do not necessarily fit into main memory. Without care, the disk traffic can slow down query processing that involves genome-wide scans. GQL chooses to speed-up most common structural variation queries by extracting as metadata a subset of fields (namely, the read length, a pointer to the pair-end and the mapping location and strand) from each read which is small enough to fit into main memory at least for a per-chromosome execution. Thus, a query that only uses a combination of the metadata fields does not have to access the raw BAM file at all. This extraction needs to occur only once per genome during pre-processing, and it is highly efficient given that a BAM file follows our recommended formatting. We require that the input BAM files are sorted according to their alignment location, and all alignment locations are chromosome isolated: in other words, for every genome there should be a single file containing reads for each chromosome. Under these assumptions, a dataset of $90 M reads of size 6.5 GB from NA18507 that map to chr1 takes $6 min to extract the metadata. Text tables. This type of table includes all tab-separated text files that a user uploads. As no assumptions can be made about the nature or the size of the contents of those tables, the main functions of the tables are simple. The evaluation of an expression on an entry of a text table fetches the appropriate fields and converts them from ASCII strings to the proper type according to the specification that the user supplies to the compiler. The selection on a file of a text table prints to the output the entries for which the evaluation of the provided boolean expression is True. Interval Tables: creation and projection (merging). Recall the query for CNVs: H1 ¼ select create_intervals(.. .) from READS where location ! 0 out ¼ select merge_intervals(interval_coverage4200) from H1 We allow the user to create interval tables from any table, simply by identifying specific fields as begin and end. The function iterates over the entries of interest of the source table. It evaluates the user-provided interval expressions for begin and end on each entry. We discard entries of intervals whose end field is no less than begin. The 'merge-interval' command operates by virtually projecting all intervals (using an efficient data-structure) to a reference and maintaining a count-vector. Computing MAPJOIN. The MAPJOIN operation takes two interval tables as input, and outputs pairs of records where the intervals intersect. We allow for the joins to be applied to multiple tables, including user-defined ones. This significantly increases functionality, but it requires the use of 'lazy-joins' and interval trees for efficient implementation. In evaluating a SELECT operation on a MAPJOIN table, we simply evaluate the provided boolean expression on all tuples of the table and outputs those tuples that satisfy the expression.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. Abstraction and layering for genomics. The bottom (physical) layer is the instrumentation software for parsing raw data into sequences. Mapping against a known reference is the first level of abstraction of the data. Compression is used to reduce the storage requirements. Detection of variation involves an evidence layer to collect relevant reads, and an inference layer for statistical analysis. The inference results in variant calls (typically as VCF files) that can be used by other applications to make genetic discoveries</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>Discordant ¼ select * from READS where location !0 and mate_loc ! 0 and abs(mate_loc þ length-location)4350 and abs(mate_loc þ length-location)51 000 000) out ¼ select * from mapjoin conr_only_intrvls, Discordant using intervals(location, mate_loc, both_mates) Interestingly, none of the eight Conrad-only predictions were supported by discordant-reads. Further, six of the eight Conradonly regions had concordant read coverage exceeding 30; the other two had coverage exceeding 10, suggesting heterozygous deletion, at best. The GQL query to retrieve the entire evidence took 12 min, and a few lines of code. To validate regions with discordant read evidence output by us, but not predicted by Conrad et al., we ran the same deletionquery in the parents of NA18506 to see whether they are reproduced in the parents. Three of the predicted deletions overlapped with discordant reads in both parents and nine in one parent. Only three of our predicted deletions do not appear in any of the parents. In each case, further statistical analysis can be used on the greatly reduced dataset to help validate the predicted deletions. Inversions in the donor genome. To detect putative inversions, we locate regions that are covered by at least five different pairs of orientation discordant reads. Discordant ¼ select * from READS using intervals (location,mate_loc, both_mates) where location ! 0 and mate_loc ! 0 and strand¼¼mate_strand and abs(mate_loc þ length-location)4270 and abs(mate_loc þ length-location)51 000 000) The query needs 8 min to run and identifies 366 regions of possible inversion events and returns 47 324 alignments, which are stored in BAM files of size 3 MB. High CNV. The average coverage of reads in the dataset is 40. To identify high copy number regions (possible duplications), we locate regions with ! 200 coverage. H1 ¼ select create_intervals() from READS where location ! 0 out ¼ select merge_intervals(interval_coverage4200) from H1 The query needs 30 min to run, and the evidence data of the output consist of 9.7 M reads stored in BAM files of size 664 MB. We contrast this with the original BAM file of size of 72 GB that a caller would have parsed had GQL not been used. Reads mapping to specific intervals. Here, we output all reads that map to known genic regions. The query uses an external table that consists of all known genes based on the RefSeq database. The execution time is 288 min, limited by the huge output size (495 M reads). mapped_reads ¼ select * from reads where location40</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.2.</head><figDesc>Fig. 2. UCSC browser snapshots from areas of NA18506 that are candidate deletions. (a) An area that overlaps with a known CNV site. (b) An area that overlaps with a known deletion of the father NA18507</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(1)</head><figDesc>Get intervals with at least four length-discordant reads. Disc ¼ select * from Reads where 350 abs(location þ mate_loc-length) and abs(location þ mate_loc-length) 1 000 000 Del ¼ select merge_intervals(count44) from Disc (2) MapJoin with validated intervals. Del_Overlapping ¼ select * from MAPJOIN Del, Kidd_results using intervals (begin, end) (3) Map Join with known genes. Gene_overlapping ¼ select * from MAPJOIN Del, RefSeq_genes using intervals (txStart, txEnd)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.3.</head><figDesc>Fig. 3. Frequencies of common deletions across the YRI population that match the results of Kidd et al. (2008) for the chromosome 1 of NA18507. For each genome, we find candidate deleted areas and we apply a mapjoin operation with the former deletions. The figure shows how many times each deletion of Kidd et al. (2008) overlaps with some deletion of other genomes, and it also shows the genes that are affected by said deletions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>(4) To output the intervals, and the supporting discordant reads, we do a final MAPJOIN and print the result. This changes the output data type back to reads stored in a BAM file that can be used downstream by other software.</figDesc><table>Discordant ¼ select * from READS 

where location ! 0 

and mate_loc ! 0 

and abs(mate_loc þ length-location)4350 

and abs(mate_loc þ length-location)51 000 000) 

(2) Create an interval table, with an interval for each discord-
ant read (by specifying the begin and end coordinates). 
This changes the output data type from reads to intervals 
on the reference. 

Disc2Intrvl ¼ select create_intervals() from Discordant 

using intervals(location, mate_loc, both_mates) 

(3) We then merge overlapping intervals and identify maximal 
intervals that are overlapped by at least five clones. This 
set of intervals points to all regions of the reference with 
evidence of a deletion. 

Predicted_deletions ¼ select merge_intervals (interval_ 
count44) 

from Disc2Intrvl 

out ¼ select * from MAPJOIN Predicted_deletions, 
Discordant 

using intervals(location, mate_loc, both_mates) 

print out 

</table></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">C.Kozanitis et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">Using GQL to uncover genetic variation at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>The authors thank Alin Deutsch, Lucila Ohno-Machado, Nitin Udpa, Anand Patel and Sangwoo Kim for useful discussions and support. GQL can be downloaded from http://cseweb. ucsd.edu/$ckozanit/gql.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">A map of human genome variation from population-scale sequencing Using 1000 genomes data in the amazon web service cloud</title>
		<author>
			<persName>
				<surname>Genomes Project Consortium</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">467</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1061" to="1073" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>date. last accessed</note>
</biblStruct>

<biblStruct   xml:id="b1">
	<monogr>
		<title level="m" type="main">Lossy compression of quality values via rate distortion theory</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Asnani</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>ArXiv. e-prints</note>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">BamTools: a Cþþ API and toolkit for analyzing and managing BAM files</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">W</forename>
				<surname>Barnett</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1691" to="1692" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimization of primer design for the detection of variable genomic lesions in cancer</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Bashir</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2807" to="2815" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Accurate whole human genome sequencing using reversible terminator chemistry</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">R</forename>
				<surname>Bentley</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">456</biblScope>
			<biblScope unit="page" from="53" to="59" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Bison-GNU parser generator</title>
	</analytic>
	<monogr>
		<title level="j">Bison</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">BreakDancer: an algorithm for high-resolution mapping of genomic structural variation</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Chen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="677" to="681" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Continuous base identification for single-molecule nanopore DNA sequencing</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Clarke</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Nanotechnol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="265" to="270" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">A relational model of data for large shared data banks</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">F</forename>
				<surname>Codd</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="377" to="387" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">A high-resolution survey of deletion polymorphism in the human genome</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Conrad</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="75" to="81" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale compression of genomic sequence databases with the Burrows-Wheeler transform</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Cox</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1415" to="1419" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Pybedtools: a flexible Python library for manipulating genomic datasets and annotations</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">K</forename>
				<surname>Dale</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3423" to="3424" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">A framework for variation discovery and genotyping using next-generation DNA sequencing data</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Depristo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="491" to="498" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<monogr>
		<title level="m" type="main">The Fast Lexical Analyzer</title>
		<author>
			<persName>
				<surname>Flex</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1990-06-04" />
		</imprint>
	</monogr>
	<note>date. last accessed</note>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">CpG islands in vertebrate genomes Where does gatk get the mate pair info from bam files? http:// gatkforumsbroadinstitute.org/discussion/1529/where-does-gatk-get-the-matepair-info-from-bam-file</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Gardiner-Garden</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Frommer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">196</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="261" to="282" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
	<note>date. last accessed</note>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Olfactory receptor-gene clusters, genomic-inversion polymorphisms, and common chromosome rearrangements</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Giglio</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Hum. Genet</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="874" to="883" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<monogr>
		<title level="m" type="main">Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Goecks</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">86</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Combinatorial algorithms for structural variation detection in high-throughput sequenced genomes</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Hormozdiari</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1270" to="1278" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient storage of high throughput DNA sequencing data using reference-based compression</title>
		<author>
			<persName>
				<forename type="first">Hsi-Yang</forename>
				<surname>Fritz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="734" to="740" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Compression of next-generation sequencing reads aided by highly efficient de novo assembly</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">C</forename>
				<surname>Jones</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">171</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Mapping and sequencing of structural variation from eight human genomes</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">M</forename>
				<surname>Kidd</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">453</biblScope>
			<biblScope unit="page" from="56" to="64" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Comprehensive molecular portraits of human breast tumours</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">C</forename>
				<surname>Koboldt</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">490</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Compressing genomic sequence fragments using SlimGene</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Kozanitis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="401" to="413" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast and accurate long-read alignment with burrowswheeler transform</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Durbin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="589" to="595" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Mapping short DNA sequencing reads and calling variants using mapping quality scores</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Li</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1851" to="1858" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">The sequence alignment/map format and SAMtools</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Li</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2078" to="2079" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Standardizing the next generation of bioinformatics software development with BioHDF (HDF5)</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">E</forename>
				<surname>Mason</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Exp. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">680</biblScope>
			<biblScope unit="page" from="693" to="700" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">The Genome Analysis Toolkit: a MapReduce framework for analyzing next-generation DNA sequencing data</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Mckenna</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1297" to="1303" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Hotspots for copy number variation in chimpanzees and humans</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">H</forename>
				<surname>Perry</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="8006" to="8011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">NGC: lossless and lossy compression of aligned high-throughput sequencing data</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Popitsch</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Von Haeseler</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">BEDTools: a flexible suite of utilities for comparing genomic features</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">R</forename>
				<surname>Quinlan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<forename type="middle">M</forename>
				<surname>Hall</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="841" to="842" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Structural variation of the human genome</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Sharp</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Genomics Hum. Genet</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="407" to="442" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">A geometric approach for classification and comparison of structural variants</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Sindi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="222" to="230" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">A 32 kb critical region excluding Y402H in CFH mediates risk for age-related macular degeneration</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">A</forename>
				<surname>Sivakumaran</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">25598</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">A novel gene family NBPF: intricate structure generated by gene duplications during primate evolution</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Vandepoele</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Biol. Evol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2265" to="2274" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<monogr>
		<title level="m" type="main">Variant call format</title>
		<author>
			<persName>
				<forename type="first">Vcf</forename>
				<surname>Tools</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2011-06" />
		</imprint>
	</monogr>
	<note>date. last accessed</note>
</biblStruct>

<biblStruct   xml:id="b37">
	<analytic>
		<title level="a" type="main">RHD gene deletion occurred in the Rhesus box</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">F</forename>
				<surname>Wagner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">A</forename>
				<surname>Flegel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Blood</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="3662" to="3668" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b38">
	<analytic>
		<title level="a" type="main">Transformations for the compression of FASTQ quality scores of next-generation sequencing data</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Wan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="628" to="635" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b39">
	<analytic>
		<title level="a" type="main">Database resources of the National Center for Biotechnology Information</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">L</forename>
				<surname>Wheeler</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="13" to="21" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b40">
	<analytic>
		<title level="a" type="main">ReCoil-an algorithm for compression of extremely large datasets of DNA data</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Yanovsky</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>