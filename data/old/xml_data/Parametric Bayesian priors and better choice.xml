
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining Parametric Bayesian priors and better choice of negative examples improve protein function prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013">2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Noah</forename>
								<surname>Youngs</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Duncan</forename>
								<surname>Penfold-Brown</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Biology</orgName>
								<orgName type="department" key="dep2">Center for Genomics and Systems Biology</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<postCode>10003</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Kevin</forename>
								<surname>Drew</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Biology</orgName>
								<orgName type="department" key="dep2">Center for Genomics and Systems Biology</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<postCode>10003</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Dennis</forename>
								<surname>Shasha</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Richard</forename>
								<surname>Bonneau</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Biology</orgName>
								<orgName type="department" key="dep2">Center for Genomics and Systems Biology</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<postCode>10003</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining Parametric Bayesian priors and better choice of negative examples improve protein function prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">29</biblScope>
							<biblScope unit="issue">9</biblScope>
							<biblScope unit="page" from="1190" to="1198"/>
							<date type="published" when="2013">2013</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btt110</idno>
					<note type="submission">Received on October 18, 2012; revised and accepted on February 28, 2013</note>
					<note>BIOINFORMATICS ORIGINAL PAPER Associate Editor: Jonathan Wren Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Computational biologists have demonstrated the utility of using machine learning methods to predict protein function from an integration of multiple genome-wide data types. Yet, even the best performing function prediction algorithms rely on heuristics for important components of the algorithm, such as choosing negative examples (proteins without a given function) or determining key parameters. The improper choice of negative examples, in particular, can hamper the accuracy of protein function prediction. Results: We present a novel approach for choosing negative examples , using a parameterizable Bayesian prior computed from all observed annotation data, which also generates priors used during function prediction. We incorporate this new method into the GeneMANIA function prediction algorithm and demonstrate improved accuracy of our algorithm over current top-performing function prediction methods on the yeast and mouse proteomes across all metrics tested. Availability: Code and Data are available at: http://bonneaulab.bio.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The rate of new protein discovery has, in recent years, outpaced our ability to annotate and characterize new proteins and proteomes. To combat this functional annotation deficit, many groups have successfully turned to computational techniques, attempting to predict the function of proteins to guide experimental verification. Specifically, there has been a surge of interest in applying machine learning methods to the problem of protein function prediction (FP), to take advantage of the wealth of biological data available for each protein beyond its sequence, such as computationally predicted tertiary structure, which has already been shown to aid FP (<ref type="bibr" target="#b0">Drew et al., 2011</ref>). While traditional approaches to FP mainly involved either homology (with limitations of accuracy) or manual curation (dependent on rare expertise), these new methods present new evaluation and comparative challenges. The MouseFunc competition (Penã<ref type="bibr" target="#b16">Castillo et al., 2008</ref>) was organized to test the ability of machine learning methods to take advantage of large integrated datasets and provide useful predictions of gene function. The validity of integrative approaches to FP was first demonstrated by the works of<ref type="bibr" target="#b10">Marcotte et al. (1999) and</ref><ref type="bibr" target="#b21">Troyanskaya et al. (2003)</ref>, which respectively used linkage graphs and a Bayesian network to predict function. By the time of the MouseFunc competition, FP methods had become diverse, including Support Vector Machines, Random Forests, Decision Trees and several composite methods (<ref type="bibr" target="#b3">Guan et al., 2008;</ref><ref type="bibr" target="#b8">Lee et al., 2006;</ref><ref type="bibr" target="#b14">Obozinski et al., 2008;</ref><ref type="bibr" target="#b20">Tasan et al., 2008</ref>), but a recurring theme was to use protein–protein networks of various types to determine function based on guilt by association (<ref type="bibr" target="#b5">Kim et al., 2008;</ref><ref type="bibr" target="#b9">Leone et al., 2005;</ref><ref type="bibr" target="#b24">Qi et al., 2008;</ref><ref type="bibr" target="#b25">Zhang et al., 2008</ref>). In such a method, genes are represented by nodes in a network, with weighted edges defined by a similarity metric obtained from raw data (often the Pearson correlation coefficient of feature vectors). Predictions are then formed by propagating information from genes known to have a function, through the network to unlabeled genes. While providing unprecedented accuracy, the methods of the MouseFunc competition exposed several general challenges still remaining for the FP problem: (i) choosing a set of high-confidence negative examples, (ii) using available data to form prior beliefs about the biological functions of a gene and (iii) effectively combining disparate data sources. As no comprehensive database of functional negative examples currently exists, and nearly all major machine learning methods require a negative class for the training of a classifier, the selection of high-confidence negative examples is especially important for the FP problem. In this work, we begin to address these challenges by presenting a parameterizable Bayesian technique for computing prior functional biases for each gene, and a novel method for selecting negative examples using these biases. To apply our method, we use the framework of the GeneMANIA algorithm (<ref type="bibr" target="#b11">Mostafavi et al., 2008</ref>), one of the highest-performing competitors in MouseFunc. In addition to our new priors and negative examples, we present a framework for tuning our Bayesian parameters and other parameters in the original GeneMANIA algorithm. To facilitate this parameter tuning, we incorporate new optimization techniques that take advantage of the structure *To whom correspondence should be addressed of the optimization problem. We also integrate our novel negative examples into the GeneMANIA network combination algorithm that synthesizes heterogeneous data into one affinity network. While well-established procedures exist for the comparison of machine learning methods, recent work (<ref type="bibr" target="#b2">Greene and Troyanskaya, 2012;</ref><ref type="bibr" target="#b15">Pavlidis and Gillis, 2012</ref>) has exposed and discussed problems that can be introduced into these comparisons by the nature of biological data. To mitigate these biases, we heed the suggestions of Greene and Troyanskaya (2012), and focus on evaluation with a temporal holdout (an evaluation set of annotations obtained later in time than the training data, referred to in this article and in the MouseFunc competition as the 'novel evaluation setting'). We also include one of the few available gold standard evaluation sets (an exhaustive experimental evaluation of the presence of a particular protein function across an entire genome), obtained from Huttenhower et al. (2009). Our goal is to demonstrate the performance improvements of our new algorithm over the existing state of the art in a fair (apples-to-apples) comparison across several datasets. We expect that these comparative results will generalize to other datasets as they become available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PREVIOUS WORK</head><p>We present our novel methods using the framework of the GeneMANIA FP algorithm of<ref type="bibr" target="#b11">Mostafavi et al. (2008)</ref>, which incorporates prior beliefs and an intelligent network combination algorithm into its guilt-by-association framework. GeneMANIA is a form of Gaussian Random Field (GRF) label propagation, a semi-supervised technique pioneered by<ref type="bibr" target="#b26">Zhou et al. (2004) and</ref><ref type="bibr" target="#b27">Zhu et al. (2003)</ref>, and provides predictions for genes one function at a time. Given a set of nodes (genes) in a network the edges of which define pairwise similarity, and a vector ~ y of prior label biases for the nodes, given the current function being examined, the GRF algorithm assigns a discriminant value f i to each node, which can be ranked to produce predictions. The label biases y i take values in (À1,1), with À1 representing known negative labels, 1 representing known positive labels and values in between reflecting prior belief about the likelihood of a gene having the function in question. The final discriminant vector ~ f is obtained by solving the optimization problem:</p><formula>min f X ðf i À y i Þ 2 þ X X W ij ðf i À f j Þ 2 h i ð1Þ</formula><p>This equation has an analytical solution in the form of a linear system: Ax ¼ b (see Supplementary Material for details), and also guarantees that the discriminant values f i will lie in the range (À1,1), with larger values indicating greater likelihood of an unlabeled node being a positive example of the function in question. Intuitively, this algorithm allows prior information to flow through the network until equilibrium is reached. The objective function propagates known labels through the similarity network via the second 'smoothness' term in Equation (1), weighted by the strength of similarity between nodes as specified by the network, and also enforces adherence to the prior bias through the first 'consistency' term in Equation (1). Thus, the label biases, both the positive and negative examples as well as biases used for unlabeled nodes, play an important role in the algorithm.<ref type="bibr" target="#b12">Mostafavi and Morris (2009)</ref>explore variations on techniques to choose the label bias vector, but we expand on this work to improve accuracy in our algorithm by using more of the information contained in current functional annotations to determine functional biases and negative examples (see Section 3.1). The other key component of the GRF algorithm is the composite network defining similarity between all pairs of genes.<ref type="bibr" target="#b11">Mostafavi et al. (2008)</ref>proposed a method to combine disparate data sources, each represented as an affinity matrix, into one composite matrix, based on the work of<ref type="bibr" target="#b23">Tsuda et al. (2005)</ref>. This algorithm, for each Gene Ontology (GO) category of interest, maximizes the similarity between pairs of positively labeled genes and minimizes the similarity between genes of opposite labels (see Supplementary Material for details). This network combination algorithm is prone to overfitting in cases with few positive examples. The original GeneMANIA algorithm addressed this problem by introducing a regularization term, but later work (<ref type="bibr" target="#b13">Mostafavi and Morris, 2010</ref>) instead attempts to fit the composite data network for multiple GO categories simultaneously. Our algorithm expands on this second approach by directly incorporating our negative examples (see Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ALGORITHM</head><p>We propose novel techniques focusing on several key aspects of protein FP: choosing negative examples, forming label biases for unlabeled genes with some known annotations and an issue specific to GRF-based methods, namely, combining heterogeneous data types into one affinity network. In addition, we suggest a new optimization algorithm tailored to our techniques, and provide a framework for tuning parameters using the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Label biases</head><p>Mostafavi and<ref type="bibr" target="#b12">Morris (2009)</ref>showed that significant performance gain could be achieved by allowing existing GO annotations to inform the priors applied to genes in GRF FP, using a technique called Hierarchical Label Bias (HLBias). This idea is supported by the work of<ref type="bibr" target="#b6">King et al. (2003)</ref>, which showed that patterns of GO annotations alone provided enough signal to predict future annotations. HLBias specified that genes which possessed annotations for functions ancestral to the function of interest received a prior bias equal to the proportion of genes with the ancestral function that also are known to have the function in question. However, owing to the difficulty of defining a functional hierarchy, the structure of the GO tree is often altered by its curators, with terms being moved to different parents, virtually guaranteeing that there exist functional relationships that are non-ancestral. When considering the complexity of functional interactions, it would seem likely that the presence of some functions might influence the likelihood of a gene possessing other functions, regardless of whether the relationship between the two is ancestral. This is especially true when considering annotations in all three branches of the GO hierarchy simultaneously. Accordingly, we extend HLBias to include the likelihood of a given function co-occurring with all other existing annotations [across all three branches of the GO tree: Biological Process (BP), Molecular Function (MF) and Cellular<ref type="bibr">Component (CC)</ref>], in the following manner: Let ^ pðcjmÞ denote the empirical conditional probability of seeing annotation c, given the presence of annotation m, then</p><formula>^ pðcjmÞ ¼ n þ mc n þ m</formula><p>, where n þ mc is the number of gene products where both m and c appear, and n þ m is the number of gene products that have annotation m. For a protein i, let D i be the set of all GO terms annotated to i. For a given function c, we approximate the conditional prior probability of gene i having function c by the following score:</p><formula>prior i ¼ 1 jD i j X m2Di ^ pðcjmÞ ð 2Þ</formula><p>The label biases are then scaled to the range ðÀ1, 1Þ : y i ¼ 2 Ã prior i À 1. Owing to the hierarchical nature of GO categories, some of the conditional probabilities in this calculation will contain redundant information, and so when considering a protein with annotations D i , we remove from D i all GO terms that have a child in D i , leaving a set of only the most specific annotations of protein i to use in calculating the bias.<ref type="figure" target="#fig_0">Figure 1c</ref>provides an example of the most predictive GO terms for the GO function UDP-glycotransferase activity (UDPGA), which include many terms that have no ancestral relationship to UDPGA. Examining a specific prediction example, we find that the annotations informing the prior bias for gene Ogt (pictured in<ref type="figure" target="#fig_0">Fig. 1a</ref>), are all non-ancestral terms, and contribute to the algorithm making a correct positive prediction (<ref type="figure" target="#fig_0">Fig. 1b</ref>). Lastly, we observe a large bias introduced by categories with small sample size, where one category appears to be a perfect predictor of another. To reduce the potential for overfitting stemming from this phenomenon, we introduce a weighted pseudocount into the calculation of the empirical conditional probability, whereby</p><formula>^ pðcjmÞ ¼ n þ mc n þ m</formula><p>is replaced by the following:</p><formula>b p 0 ðcjmÞ ¼ n þ mc n þ m þ e n þ m ð3Þ</formula><p>This idea is motivated by the hypotheses that no two GO categories 'c1' and 'c2' should both appear in every protein where one appears, unless 'c1' and 'c2' have an ancestral relationship, and also that the number of undiscovered occurrences of a function is related to the number of currently known occurrences. This equation (via the two parameters and ) allows us to smoothly transition between two extreme assumptions about how missing and currently known annotations are distributed: (i) the number of observations in the data is a proxy for how well a function has been studied, and so the number of missing counts in the data should be inversely proportional to the number already seen, and (ii) the number of currently known occurrences is a better representation of the specificity of a function, and so the undiscovered occurrences should be directly proportional to the number already seen. To allow the data itself to choose one of these hypotheses, we sample from a range of combinations of parameters, including the potential for no pseudocounting (see Supplementary Material). The final value of the parameters is chosen by tuning with cross-validation over the training set, as described in Section 3.5. For genes with no previous annotations in GO, we follow<ref type="bibr" target="#b11">Mostafavi et al. (2008)</ref>and set the label bias to the mean of all the label biases calculated for genes with GO annotations, including the positive and negative example genes with values of ð1, À1Þ respectively. We refer to our label bias algorithm hereafter as ALBias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Negative examples</head><p>The choice of negative training examples for use in supervised machine learning algorithms is a recurring problem for FP</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Successive block conjugate gradient optimization</head><p>The network-weighting scheme defined above creates a single combined matrix W for all functional categories within the same GO branch. Thus, the coefficient matrix is identical for the optimization problem that is solved for each function, and so we are faced only with the issue of a different right-hand side (RHS) per function. In such cases, computational costs can be decreased by methods that solve all of the problems simultaneously, rather than iteratively solving each problem without using any of the information obtained by other solutions. We propose a modified version of the Successive Block Conjugate Gradient algorithm (SBCG) proposed by Suarjana and Law (1994). In this algorithm, the search direction is obtained simultaneously for all of the distinct RHS vectors in the problem. If at any point, the search direction matrix becomes rank deficient, dependent RHS vectors are moved to a secondary system, but are still updated with steps obtained from the search direction in the primary system, and so still proceed toward convergence. The speed of this secondary convergence is dependent on the angle between the vectors in the primary system and secondary system. Our algorithm differs from the original one proposed by<ref type="bibr" target="#b19">Suarjana and Law (1994)</ref>in several ways. Firstly, not all solutions converge to the desired tolerance in the same number of iterations, and so we save computation by removing alreadyconverged RHS vectors from the block calculation rather than updating the entire system until all RHS vectors converge. Secondly, when the RHS vectors in the secondary system are nearly orthogonal to those in the primary system, waiting for secondary convergence can require a large number of iterations. Instead, once all primary system RHS vectors are converged, we restart the algorithm in a second phase, with the secondary system as the primary system, but using the latest residuals as our starting point. Lastly, empirical observation has shown some low condition numbers can occur in the secondary phase when the number of dependent RHS vectors is large. We find that splitting up the total number of RHS vectors into a few smaller blocks alleviates this problem without significantly increasing computational cost. For the problem at hand, we chose to divide the FP problems into subproblems with a maximum of 500 RHS vectors. Pseudocode for our algorithm is presented in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Parameter tuning</head><p>The multiple RHS framework described in Section 3.4 lends itself well to parameter tuning, as the different combinations of the parameters and described in Section 3.1 simply yield more RHS label bias vectors to solve for with the same coefficient matrix. The original formulation of the GRF objective function in<ref type="bibr" target="#b26">Zhou et al. (2004)</ref>included the parameter , which describes the relative weight to be placed on each component of the objective function, which we formulate as follows:</p><formula>min f X ðf i À y i Þ 2 þ ð1 À Þ X X w ij ðf i À f j Þ 2 h i ð4Þ</formula><p>This parameter was ignored by the GeneMANIA algorithm, but we reintroduce it here, and test its impact on FP by adding it to our tuning methodology. To choose performance-maximizing parameters, we create a synthetic learning problem from the training data, which is characteristically similar to the original learning problem, and choose parameters that yield the best performance on this subproblem. Details are presented in the Supplementary Details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation datasets</head><p>We evaluate our algorithm on three datasets: the MouseFunc benchmark, yeast data and a gold standard dataset of yeast genes. With regard to MouseFunc data, we focus on the Molecular Function branch of the GO hierarchy. For fair comparison with prior work, we use only data available to participants at the time of the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Functional association data</head><p>Association networks are created from feature-based data types using the Pearson correlation coefficient, after a frequency transform as described in<ref type="bibr" target="#b11">Mostafavi et al. (2008)</ref>. Only the top 100 interactions are used for each gene in the training set to keep the networks sparse, and a normalization scheme of W 0 h ¼ D 1=2 h W h D 1=2 h is applied to each network and to the final combined network, where D h is again the diagonal matrix containing the row sums of W h .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation frameworks</head><p>We categorize protein function through GO ontology annotations, observing the common convention of excluding annotations denoted as 'Inferred Electronic Annotations' (IEA). As in the MouseFunc competition, performance is evaluated in two different scenarios: (i) a test set where all GO annotations are removed from a subset of data (1718 genes in mouse) and then predictions are made from the remaining training data, and (ii) a novel set where predictions are made for proteins that have received new annotations at a later date. The member genes of this second set consist of the intersection of all proteins that have received at least one new annotation in any of the GO categories for which we are attempting predictions (1954 genes in mouse, 362 genes in yeast), and so include many proteins that already had some annotations in the training set, as well as proteins with no annotations in the training set. We treat the novel scenario as the more important evaluation scheme for this work, as we believe it better reflects the true task facing computational biologists, and is less prone to evaluation biases (<ref type="bibr" target="#b2">Greene and Troyanskaya, 2012</ref>). The test set approach suffers from biases stemming from the underlying use of sequence-similarity methods in both input data and GO labeling (discussed in greater detail in the Supplementary Material), which likely explains the better performance of all algorithms in the test scenario versus the novel scenario. Error results are presented for the test scenario as well, to facilitate comparison with MouseFunc algorithms. For both the novel and test MouseFunc evaluations, predictions are made for the same set of GO Molecular Function categories as the original competition: 488 and 442 categories, respectively. For yeast, we show results only in the novel scenario, with data from June 2007, 1 year after the training data, which includes 511 GO BP categories with at least one new annotation. Any comparison of computational methods using GO annotations as the ground truth suffers from the lack of delineation between negative and absent annotation. This drawback is discussed at length in<ref type="bibr" target="#b4">Huttenhower et al. (2009)</ref>, and can create significant difficulty in evaluating computational prediction methods, as observed false-positive predictions may simply be a function of a lack of study rather than incorrect prediction. It is for this reason that performance evaluation in the novel scenario ignores any false positives for genes outside the novel set, as it is likely that these genes were not studied at all in the time interval between the annotation date for training and for testing. To further alleviate some of the uncertainty caused by incomplete annotation, we present performance evaluation metrics on a 'gold standard' benchmark of yeast genes experimentally verified by<ref type="bibr" target="#b4">Huttenhower et al. (2009)</ref>for GO:0007005 MOB. These gold standard annotations include 148 additional positive annotations that match genes in our yeast gene set, and are also added to the novel set used for the general yeast benchmark. Lastly, when calculating performance statistics on the MOB gold standard, we add an additional 2473 genes to the 342 comprising the yeast novel set. These additional genes are the negative examples from<ref type="bibr" target="#b4">Huttenhower et al. (2009)</ref>that are present in our gene set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evalutation metrics: precision-recall versus receiver-operator characteristic curve, TopScore</head><p>The performance of discriminant-based classification algorithms is most often represented by two plots: The receiver-operator characteristic (ROC) curve, and the precision-recall (PR) curve, each of which can be summarized by their AUC, the area that the curve encompasses. While both performance measures attempt to describe how well the ordering of discriminant values captures the true-positive and-negative labels, each has different strengths and weaknesses. Precision tends to be more easily interpretable for an experimentalist, but averaging AUC PR numbers over many classifiers can be misleading owing to the non-linear nature of precision scores (see Supplementary<ref type="figure" target="#fig_0">Fig. S1</ref>). Conversely AUC ROC provides a better global view of the rankings, but lacks a meaningful interpretation for experimentalists, and its magnitude depends on the skew of the dataset. See the Supplementary Material for a more detailed description of the pitfalls of each metric. When presented with computational predictions, experimentalists must determine the number of predictions to assay, as well as which functions to focus on. .. a task made more difficult by complicated performance metrics. To create a metric more robust to averaging than PR, but which still enjoys easy interpretability for experimentalists, we proposeyeast, providing insight into the usefulness of computational predictions at three different scales of experimental testing (see Supplementary Material for more details on TopScore).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Algorithm component exploration</head><p>Uncovering which component of our algorithm is responsible for what performance changes is a challenging undertaking, as many of our algorithmic changes are interlinked. For example, our choice of negative examples affects both the bias value of selected genes and the network combination algorithm. Additionally, our bias calculation can also produce genes with a prior of À1, but which were not chosen as negative examples for the purposes of network combination (owing to our restriction that a gene must have annotations in the branch of interest to be declared an official negative). We have performed additional experiments to isolate the performance contributions of each of our algorithm subcomponents, presented in Sections 5.1, 5.2 and 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Network combination algorithm SWSN</head><p>To examine the effect of our network combination algorithm, SWSN, we performed a comparison with the SW network weight algorithm, using no label biases and our negative examples, yielding mixed results across evaluation scenarios and metrics. SWSN slightly outperforms SW on the mouse novel set; the two algorithms are virtually tied on the mouse test set; and SW outperforms SWSN on the yeast novel and gold standard evaluations. Yet, we believe that further refinement of negativeexample choice will show SWSN to be a more successful method. To demonstrate this, we add to the comparison of the two algorithms in<ref type="figure" target="#tab_2">Table 2</ref>a third algorithm (SWSNOracle) in which our negative examples are granted access to a negative oracle, namely, the validation annotations, to ensure we do not select any negative examples that are demonstrated positives (there are almost certainly others among our negative examples that are true positives, but not yet studied at the time of the collection of validation data). This results in stronger performance on the mouse novel set and yeast gold standard, but makes no difference on the mouse test or yeast novel sets, as there were no instances of negative examples that were demonstrated positives in the mouse test and only 11 in the yeast novel benchmark. We believe this result indicates the promise of our SWSN algorithm, although it was likely not a significant factor in the current performance increase of our algorithm as a whole. Therefore, we submit SWSN as a logical extension of SW, as it uses the more accurate and specific negative example information now available. We hypothesize the likelihood of future performance gain from using SWSN, once even better negative example methods are uncovered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Parameter-tuning results</head><p>From the performance measurements presented in Section 5.1, we see that while the tuned parameters performed significantly better than the null parameters in the mouse novel and yeast MOB benchmarks, their performance was on par with, or occasionally worse than, the null guess in the mouse test and yeast novel benchmarks. We attribute the decrease in performance, primarily in the yeast novel set, to the inherent difference between the state of annotation in yeast and mouse. Our parameter-tuning algorithm was designed to re-create a learning problem where annotations are only partially known, yet in yeast, a well-studied organism, this type of learning problem was most likely not as representative of the true learning problem as it was in mouse, a less-studied organism. In general, adapting the tuning process to be representative of the original learning problem is a more intricate problem than first anticipated, and requires further exploration. For detailed analysis of the parameter-tuning results, and future avenues of approach, please refer to the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Prediction evaluations</head><p>We present the performance, evaluated by AUC ROC , AUC PR and TopScore metrics, of five algorithms: the original MouseFunc GeneMANIA algorithm, the SW GeneMANIA algorithm presented in Mostafavi and Morris (2010) using sibling negative examples, the SW algorithm combined with the HLBias algorithm of Mostafavi and Morris (2009) and two versions of our algorithm: SWSN with ALBias and naive parameters ð ¼ 0, ¼ 0, ¼ 0:5Þ and SWSN with ALBias and tuned parameters. Results are averaged over all categories, with an analysis of results broken down by function specificity available in Supplementary<ref type="figure" target="#fig_2">Figure S2a</ref>in the Supplementary Material. In the novel scenario for MouseFunc, our algorithms show a strong increase in performance across all metrics, especially our version with tuned parameters, as seen in<ref type="figure" target="#fig_2">Figure 2a</ref>. We see here a large difference in performance between ALBias with tuned parameters and ALBias with naive parameters, indicating that some of our algorithmic performance increase in mouse is due to the ability of our parametric pseudocounting procedure to prevent undue bias influence from understudied GO categories in mouse. For the mouse test set, the difference in performance is much smaller, as the test set is stripped of all labels, thus negating a key advantage of ALBias (see<ref type="figure" target="#fig_2">Fig. 2b</ref>for mouse test results). Yet, we still see a performance increase from our algorithm across most metrics, owing to better biases for genes sharing edges with test genes. In the yeast novel set, we compare all algorithms except the original MouseFunc GeneMANIA algorithm, and observe a striking performance advantage of our algorithms across all evaluation metrics (see<ref type="figure" target="#fig_2">Fig. 2c</ref>). Further analysis indicates that much of this performance gain is due to our algorithm's incorporation of information from all branches of GO into the label bias calculation. Examining an example GO term, 'DNA packaging', where our algorithm boosted performance in AUC ROC from 0.722 to 0.989 and AUC PR from 0.467 to 0.803, we find the primary cause to be the improvement in rankings of two true-positive genes with useful Cellular Component annotations. YBR090C-A moved from rank 102 to rank 5, owing to the Cellular Component term 'nuclear chromatin', which has a high joint probability with 'DNA packaging', and YCL060C moved from rank 298 to 18, owing to the terms 'nuclear chromosome', and 'chromosomal part'. Further examples are provided in the Supplementary Material. Lastly, on the yeast MOB gold standard (results in<ref type="figure">Table 3</ref>), we see strong performance from our tuned SWSN ALBias algorithm, which achieved significantly higher AUC ROC andTopScore 200 scores, and also from the SW, HLBias algorithm, which achieved the highest TopScore 10 and TopScore 50 scores, as well as a marginally higher AUC PR score. Thus, our algorithm provided a better global ranking of true positives, while the current GeneMANIA algorithm ranked the top true predictions more highly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Computational cost</head><p>A theoretical complexity analysis of the SBCG algorithm is not possible (see Supplementary Material), but empirical testing shows a 30% reduction in the number of flops required for the prediction task on original MouseFunc data, with SBCG converging to a solution with smaller residuals as well. On the yeast benchmark, the reduction in flops was less, at 22%, as the ratio between the number of genes and the number of functions to predict is much smaller (see<ref type="figure">Table 3</ref>in the Supplementary Material). As expected, there was an observable increase in computation saved as the number of categories increased, but this is bounded by the fact that our algorithm splits the categories into subsets with a maximum size of 500. This suggests that further computation could be saved by devising a suitable strategy to deal with low condition numbers for larger sets of RHS vectors. Suarjana and Law (1994) suggest that a pre-conditioner applied to the data might help reduce the number of iterations required as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We have addressed several of the key problems encountered by protein function prediction efforts by proposing novel algorithms, including a method of choosing negative examples, and a parameterized Bayesian methodology for computing prior functional biases from existing annotation data. These methods, applied using the framework of the GeneMANIA algorithm, have resulted in a significant performance increase across three large benchmarks. We also introduced a new optimization methodology, which significantly decreased computational costs. We devised a framework for tuning parameters in a synthetic novel set, which added further performance gain in the novel scenario in mouse, but it requires additional work to be more broadly applicable to other evaluation scenarios. Our new SWSN network combination algorithm shows even more promise in settings with more extensive negative example information. Finally, we presented a new evaluation metric designed to be easily interpretable by experimentalists, even when averaged over many function categories. When comparing performance statistics of different algorithms, a difference of a few percentage points can mean hundreds of new true annotations when applied across all functions. For example, a 1% increase in TopScore 10 would result in 187 new true annotations were an experimentalist to use that metric to guide experiments over the 1874 GO MF categories in the mouse genome (at the time of MouseFunc publication). Thus, we believe the algorithms presented here have the potential to guide experimentalists to a large number of fruitful assays, andNote: For both algorithms using the SW network combination algorithm, negative examples were chosen according to the sibling technique discussed in Section 3.2. are in general aligned with current biological understanding of how genes are functionally related to each other through different data types. We have shown that our algorithm can perform function prediction through data integration and guilt by association with substantially more accuracy and efficiency than previously published algorithms, and provided insight into some of the inherent difficulties encountered by the development and evaluation of protein function prediction algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. (a) A subsection of the association network before the algorithm is run, showing prior beliefs for genes for the function: GO:0008194, UDPglycotransferase activity, focusing on gene Ogt. The shading of the nodes represents the degree of positivity compared with the mean of all prior biases, with blue indicating greater likelihood of possessing the function in question, red representing lesser likelihood and white representing genes that had no GO annotations to use for a prior. Square nodes represent validated true positives (including the training positive example Wdfy3). (b) The same subsection of the association network as (a), but after label propagation, showing the final discriminant values of the genes. (c) The GO categories that are most predictive of the function GO:0008194, with darker shades of blue representing stronger predictors. Network visualized with Cytoscape (Smoot et al., 2011)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>MouseFunc exercise: these data include 10 networks (Interpro data, PFAM data, three Gene Expression networks, Protein–Protein Interaction data, Phenotype, two Conservation Profile networks, Disease Association data), 1874 molecular function categories, and 21 603 mouse genes, with all data gathered in 2006 (see Penã-Castillo et al., 2008). Predictions are made, as in MouseFunc, only for functional categories with between 3 and 300 annotations in the genome, but all functional categories are used in the bias calculation and negative example choice described in Sections 3.1 and 3.2. For our performance evaluation in yeast, we focus on the Biological Process branch of the GO tree, using data obtained from Mostafavi and Morris (2010), which includes 44 networks of data obtained from BIOGRID (Stark et al., 2006), covering 3904 genes with 1188 biological process categories (categories with between 3 and 300 annotations). We augment this yeast data with experimentally confirmed gold standard annotations in the BP category of GO:0007005, mitochondrion organization and biogenesis (MOB), obtained from Huttenhower et al. (2009; see Section 4.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.2.</head><figDesc>Fig. 2. Performance metrics in (a) the novel scenario in mouse (488 functions, 1954 genes), (b) the test scenario in mouse (442 functions, 1718 genes) and (c) the novel scenario in yeast (511 functions, 342 genes). Metrics are averaged over all GO functions (each with between 3 and 300 counts per genome), and error bars are 1 SD of the error in the mean</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>TopScore c , defined as ð#true positives5rank cÞ minðc, positive label countÞ . This score represents the fraction of a fixed number of experiments expected to yield a positive result, normalized by the maximum number of positive results possible. In this article, we present results for TopScore 10 , TopScore 100 and TopScore 1000 for mouse, and TopScore 10 , TopScore 50 and TopScore 200 for</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>Table 1.</figDesc><table>Performance metrics for different negative example choices: sib-
ling negatives (SibNeg) as in Mostafavi et al. (2008), using all non-posi-
tive genes with GO annotations as negative (AllNeg), and negative 
examples based on our ALBias method (ALBNeg) 

Algorithm 
AUC ROC AUC PR TS 10 
TS 100 

a 

TS 1000 

a 

Mouse novel 
SibNeg 
0.7347 
0.3236 
0.4103 0.5342 0.7411 
AllNeg 
0.8155 
0.3420 
0.4318 0.5783 0.8354 
ALBNeg 
0.8366 
0.3447 
0.4314 0.5793 0.8705 
Mouse test 
SibNeg 
0.8573 
0.5019 
0.6136 0.7622 0.8725 
AllNeg 
0.9232 
0.5168 
0.6207 0.7994 0.9530 
ALBNeg 
0.9330 
0.5171 
0.6160 0.8014 0.9745 
Yeast novel 
SibNeg 
0.7566 
0.3090 
0.3674 0.6014 0.8094 
AllNeg 
0.7563 
0.2865 
0.3299 0.5284 0.8405 
ALBNeg 
0.8711 
0.3387 
0.4133 0.7127 0.9633 
Yeast gold standard 
SibNeg 
0.7936 
0.3729 
0.8 
0.54 
0.5068 
AllNeg 
0.8679 
0.4685 
1 
0.74 
0.4932 
ALBNeg 
0.8413 
0.3896 
0.7 
0.6 
0.4865 

Note: All algorithms were run using the SW network combination method, and the 
GRF label propagation algorithm of Mostafavi et al. (2008). a For the yeast scen-
arios, TopScore 100 and TopScore 1000 are replaced by TopScore 50 and TopScore 200 . </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 2. Performance metrics for network combination algorithms: simultaneous weights (SW) from Mostafavi and Morris (2010), our own SWSN algorithm and SWSN with a negative oracle (SWSNOracle)</figDesc><table>Algorithm 
AUC ROC AUC PR TS 10 
TS 100 

a 

TS 1000 

a 

Mouse novel 
SW 
0.8366 
0.3447 
0.4315 0.5793 0.8705 
SWSN 
0.8376 
0.3460 
0.4396 0.5878 0.8755 
SWSNOracle 
0.8775 
0.3491 
0.4433 0.6027 0.9366 
Mouse test 
SW 
0.9330 
0.5171 
0.6160 0.8014 0.9745 
SWSN 
0.9315 
0.5177 
0.6211 0.8041 0.9684 
SWSNOracle 
0.9315 
0.5177 
0.6211 0.8041 0.9684 
Yeast novel 
SW 
0.8711 
0.3387 
0.4133 0.7127 0.9633 
SWSN 
0.8632 
0.3139 
0.3796 0.6294 0.9649 
SWSNOracle 
0.8636 
0.3139 
0.3796 0.6294 0.9656 
Yeast gold standard 
SW 
0.8413 
0.3896 
0.7 
0.6 
0.4865 
SWSN 
0.8315 
0.3729 
0.7 
0.52 
0.5135 
SWSNOracle 
0.8569 
0.3871 
0.7 
0.54 
0.5270 

Note: All algorithms were run using the GRF label propagation method of 
Mostafavi et al. (2008). a For the yeast scenarios, TopScore 100 and TopScore 1000 
are replaced by TopScore 50 and TopScore 200 . </table></figure>

			<note place="foot">ß The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">Parametric Bayesian priors and better choice of negative examples at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">N.Youngs et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from methods. While the GO database does include negative annotations, the number of such annotations is currently small. Thus, it is necessary to infer negative examples for each function (typically using a heuristic). Past heuristics include (i) designating all genes that do not have a particular label as being negative for that label (Guan et al., 2008), (ii) randomly sampling genes and assuming the probability of getting a false negative is low (often done when predicting protein–protein interactions, as in Gomez et al., 2003) and (iii) using genes with annotations in sibling categories of the category of interest as negative examples (Mostafavi and Morris, 2009). Mostafavi and Morris (2009) note in discussion that this last technique may often break down, as some genes are annotated to more than one sibling category, and many genes have few siblings to use. We present a new technique for choosing negative examples based on the label biases calculated for each function. Namely, all genes with an annotation in the same branch of GO as the term being predicted, and which have a prior i score of 0 for the function in question (with the prior score computed across all three branches of GO), are treated as negative examples for that function. Intuitively, this amounts to treating a gene &apos;g&apos; as a negative for annotation &apos;c&apos; if no annotation &apos;s&apos; among the most specific annotations of &apos;g&apos; ever appeared alongside annotation &apos;c&apos; in any other gene (note that the choice of pseudocounting parameters does not impact the negative examples, as only the magnitude of the label bias will be affected and not whether a bias is non-zero). Restricting the negative examples to having an annotation in the same branch as the GO term being predicted, rather than simply having an annotation in any branch, decreases the number of negative examples, and also more significantly decreases the number of validated true positives that were misclassified as negatives. The number of negatives in mouse decreased by 14.9% owing to this restriction, while the number of verifiable misclassified negatives dropped by 22.5%; in yeast, the number of negative examples decreased by 23.2%, while the verifiable misclassified negatives dropped by 91.5%. 3.3 Network weighting As mentioned in our description of previous work, one essential component of the GRF algorithm is synthesizing heterogeneous data sources into one pairwise affinity matrix. Mostafavi and Morris (2010) found that fitting this matrix for multiple GO functions simultaneously significantly decreased overfitting, especially in low-annotation categories. The authors simplified the calculation of this simultaneous fit by considering negative–negative pairs of labels as well as the positive–positive and positive– negative pairs used by the original network-weighting algorithm of Mostafavi et al. (2008). This simplification also requires the treatment of all non-positive genes as negative genes for each GO category (see Supplementary Material). Mostafavi and Morris (2010) showed that these simplifications do not hamper performance, and also found that fitting the combined network to all GO categories in a particular branch (GO-BP, GO-CC or GO-MF) worked better than any other subset or grouping of functions. We concur that fitting to all categories performs better than any of the subsets we attempted, but propose that the apparent indifference of this algorithm to the assumption that all non-positive nodes are negative was most likely due to a lack of any satisfactory alternative for choosing negative examples. We return to the unsimplified version of the simultaneous fit proposed by Mostafavi and Morris (2010), and use our more specific negative examples that are unique to each GO category (see Supplementary Material for details of the calculation). We refer to our modified network combination algorithm as Simultaneous Weights with Specific Negatives (SWSN).</note>

			<note place="foot">N.Youngs et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="5"> RESULTS AND DISCUSSION We present results for our proposed techniques based on the evaluation metrics and datasets described in Section 4, along with analysis of the different components of our algorithm: negative example choice, network combination and parameter tuning. Our tuned ALBias algorithm shows clear advantages over the current GeneMANIA methods in the majority of evaluations, with these differences being especially striking in the novel evaluation scenarios, where prior biases play a more important role in the algorithm than in the test scenario. In the yeast proteome, our algorithm achieved a performance increase of 11–26% points in every metric in the novel scenario, whereas in the mouse proteome, we improved all evaluation scores by 2–6% points in the novel scenario. 5.1 Negative example choice To investigate the impact of our novel negative example choice, we evaluate the SW network combination algorithm with no label bias method, using three different negative example methods: the sibling negative examples, setting all non-positive genes with GO annotations to negative examples and our new negative example approach. As shown in Table 1, our negative example choice outperforms previous choices in all three fullorganism evaluations, sometimes even approaching the performance of our full SWSN with ALBias and tuned parameters algorithm, indicating that our choice of negative examples is responsible for a significant part of our algorithm&apos;s final performance. On the yeast gold standard, even though our algorithm decreases the number of validated true positives that are misclassified as negatives from 110 to 6, our negative example choice results in lower evaluation metrics than the AllNeg selection. We attribute the counterintuitive decrease in predictive performance when using ALBNeg in this setting, to the fact that the particular category MOB is specific enough to have small prevalence in the genome (only 5.3% of yeast genes possess this function), yet it is common enough that many genes have shared an annotation with it, resulting in our algorithm only selecting 691 negative examples. Thus, AllNeg yields high precision by virtue of having so many more negative examples, whereby it avoids false positives, while the rarity of the category means that there are not many true positives, and thus the mislabeling of true positives is outweighed by the decrease in predicted false positives. Despite the success of our method in increasing performance in most evaluations, and reducing the instances of mislabeling validated true positives as negatives, in some GO categories, this mislabeling still occurs. Accordingly, we believe there is more potential for refined methods that correctly define high-confidence negative examples, and that these methods will have significant impact in the performance of machine learning algorithms. Indeed, we hypothesize that part of the performance gain demonstrated by the earlier HLBias algorithm was because the authors adjusted the labels for all non-positive genes, effectively turning any gene without a label in an ancestral category of the function in question into a negative example.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The author would like to thank Sara Mostafavi for making her code and data for the GeneMANIA algorithm publicly available.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">The Proteome Folding Project: proteome-scale prediction of structure and function</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Drew</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1981" to="1994" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to predict protein-protein interactions</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">M</forename>
				<surname>Gomez</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1875" to="1881" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Accurate evaluation and analysis of functional genomics data and methods</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">S</forename>
				<surname>Greene</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">O</forename>
				<forename type="middle">G</forename>
				<surname>Troyanskaya</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. NY Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">1260</biblScope>
			<biblScope unit="page" from="95" to="100" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting gene function in a hierarchical context with an ensemble of classifiers</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Guan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="issue">9 S3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Suppl. . 1</note>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">The impact of incomplete knowledge on evaluation: an experimental benchmark for protein function prediction</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Huttenhower</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2404" to="2410" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Inferring mouse gene functions from genomic-scale data using a combined functional network/classification strategy</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">K</forename>
				<surname>Kim</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Suppl. . 1</note>
</biblStruct>

<biblStruct   xml:id="b6">
	<monogr>
		<title level="m" type="main">Predicting gene function from patterns of annotation</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<forename type="middle">D</forename>
				<surname>King</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Suppl. . 1</note>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Diffusion Kernel-based logistic regression models for protein function prediction</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Lee</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OMICS</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="896" to="904" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting protein functions with message passing algorithms</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Leone</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Pagnani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="239" to="247" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">A combined algorithm for genome-wide prediction of protein function</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">M</forename>
				<surname>Marcotte</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">402</biblScope>
			<biblScope unit="page" from="83" to="86" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">GeneMANIA: a real-time multiple association network integration algorithm for predicting gene function</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Mostafavi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Suppl. . 1</note>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Using the gene ontology hierarchy when predicting gene function</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Mostafavi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Q</forename>
				<surname>Morris</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence<address><addrLine>Oregon</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast integration of heterogeneous data sources for predicting gene function with limited annotation</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Mostafavi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Q</forename>
				<surname>Morris</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1759" to="1765" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Consistent probabilistic outputs for protein function prediction</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Obozinski</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Suppl. . 1</note>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Progress and challenges in the computational prediction of gene function using networks</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Pavlidis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Gillis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">F1000 Res</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">A critical assessment of Mus musculus gene function prediction using integrated genomic evidence</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Penã-Castillo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Suppl. . 1.</note>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Cytoscape 2.8: new features for data integration and network visualization</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Smoot</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="431" to="432" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">BioGRID: a general repository for interaction datasets</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Stark</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="535" to="539" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Suppl. . 1</note>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Successive conjugate gradient methods for structural analysis with multiple load cases</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Suarjana</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">H</forename>
				<surname>Law</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Num. Methods Eng</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="4185" to="4203" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">An en masse phenotype and function prediction system for Mus musculus</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Tasan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Suppl. . 1</note>
</biblStruct>

<biblStruct   xml:id="b21">
	<monogr>
		<title level="m" type="main">A Bayesian framework for combining heterogeneous data sources for gene function prediction</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<forename type="middle">G</forename>
				<surname>Troyanskaya</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>in. Saccharomyces cerevisiae</note>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl. Acad. Sci. USA</title>
		<meeting>. Natl. Acad. Sci. USA</meeting>
		<imprint>
			<biblScope unit="page" from="8348" to="8353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast protein classification with multiple networks</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Tsuda</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="59" to="65" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Suppl. . 2</note>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Random forest similarity for protein-protein interaction prediction from multiple sources</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Qi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pac. Symp. Biocomput</title>
		<imprint>
			<biblScope unit="page" from="531" to="542" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">An integrated probabilistic approach for gene function prediction using multiple sources of high-throughput data</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Biol. Drug Des</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="254" to="274" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Zhou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process Syst</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="328" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using Gaussian fields and harmonic functions</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Zhu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Machine Learning</title>
		<meeting>the Twentieth International Conference on Machine Learning<address><addrLine>Meno Park</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>