
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Trans-species learning of cellular signaling systems with bimodal deep belief networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Lujia</forename>
								<surname>Chen</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Informatics</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<postCode>15237</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Chunhui</forename>
								<surname>Cai</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Informatics</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<postCode>15237</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Vicky</forename>
								<surname>Chen</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Informatics</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<postCode>15237</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Xinghua</forename>
								<surname>Lu</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Informatics</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<postCode>15237</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Trans-species learning of cellular signaling systems with bimodal deep belief networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btv315</idno>
					<note type="submission">Received on September 14, 2014; revised on April 21, 2015; accepted on May 17, 2015</note>
					<note>Systems biology *To whom correspondence should be addressed. Associate Editor: Alfonso Valencia Availability and implementation: The software is available at the following URL: http://pubreview. dbmi.pitt.edu/TransSpeciesDeepLearning/. The data are available through SBV IMPROVER web-site, https://www.sbvimprover.com/challenge-2/overview, upon publication of the report by the organizers. Contact: xinghua@pitt.edu Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Model organisms play critical roles in biomedical research of human diseases and drug development. An imperative task is to translate information/knowledge acquired from model organisms to humans. In this study, we address a trans-species learning problem: predicting human cell responses to diverse stimuli, based on the responses of rat cells treated with the same stimuli. Results: We hypothesized that rat and human cells share a common signal-encoding mechanism but employ different proteins to transmit signals, and we developed a bimodal deep belief network and a semi-restricted bimodal deep belief network to represent the common encoding mechanism and perform trans-species learning. These &apos;deep learning&apos; models include hierarchically organized latent variables capable of capturing the statistical structures in the observed proteomic data in a distributed fashion. The results show that the models significantly outperform two current state-of-the-art classification algorithms. Our study demonstrated the potential of using deep hierarchical models to simulate cellular signaling systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Due to ethical issues, modal organisms such as rat and mouse have been widely used as disease models in studying disease mechanisms and drug actions (<ref type="bibr" target="#b6">Brown, 2011;</ref><ref type="bibr" target="#b17">McGonigle and Ruggeri, 2014</ref>). For example, mouse models have been used to study the disease mechanisms and treatment of type-2 diabetes (<ref type="bibr" target="#b19">Omar et al., 2013</ref>). Since significant differences exist between species in terms of genome, cellular systems and physiology, the success of using model organisms in biomedical research is hinged on the capability to translate/ transfer the knowledge learned from model organisms to humans. For example, when using a rat disease model to screen drugs and investigate the action of drugs, rat cells inevitably exhibit different molecular phenotypes, such as proteomic or transcriptomic responses, when compared with corresponding human cells. Thus, in order to investigate how the drugs act in human cells, it is critical to translate the molecular phenotypes observed in rat cells into corresponding human responses. Recent species-translation challenges organized by the Systems Biology Verification combined with Industrial Methodology for Process Verification in Research (SBV IMPROVER, 2013) provided an opportunity for the research community to assess the methods for trans-species learning in systems biology settings (<ref type="bibr" target="#b22">Rhrissorrakrai et al., 2015</ref>). One challenge task was to predict human cells' proteomic responses to distinct stimuli based on the observed proteomic response to the same stimuli in rat cells. More specifically, during the training phase, participants were provided with data thatmeasured the phosphorylation states of a common set of signaling proteins in primary cultured bronchial cells collected from rats and humans treated with distinct stimuli (<ref type="bibr" target="#b21">Poussin, 2014</ref>). In the testing phase, the proteomic data of rat cells treated with unknown stimuli were provided, and the task is to predict the proteomic responses of human cells treated with the same stimuli (<ref type="figure">Fig. 1</ref>). To address the trans-species learning task, a simplistic approach is to train regression/classification models that use the phosphorylation data from rat cells as input features and treat the phosphorylation status of an individual protein from human cells (treated with the same stimulus) as a target class. In this way, predicting the proteomic profile of human cells can be addressed as a series of independent classification tasks or within a multi-label classification framework (<ref type="bibr" target="#b14">Jin et al., 2008;</ref><ref type="bibr" target="#b27">Tsoumakas and Katakis, 2007</ref>). However, most contemporary multi-label classification methods treat the target classes as independent or are incapable of learning the covariance structure of classes, which apparently does not reflect biological reality. In cellular signaling systems, signaling proteins often form pathways in which the phosphorylation of one protein will affect the phosphorylation state of others in a signaling cascade, and cross-talk between pathways can also lead to coordinated phosphorylation of proteins in distinct pathways (<ref type="bibr" target="#b1">Alberts et al., 2008</ref>). Another shortcoming of formulating trans-species learning as a conventional classification problem is that contemporary classifiers, such as the support vector machine (<ref type="bibr" target="#b3">Bishop, 2006</ref>) or regularized regression/classification (<ref type="bibr" target="#b10">Friedman et al., 2010</ref>), concentrate on deriving mathematical representations that separate the cases, whereas the real goal of trans-species learning is to capture the common signaling mechanisms employed by cells from both model organisms and humans in response to a common stimuli. Indeed, the cornerstone hypothesis underpinning trans-species learning is that there is a common encoding mechanism shared by cells from different species, but distinct signaling molecules are employed by different species to transmit the signals responding to the same environmental stimuli. Therefore, it is important to explore models that are compatible to the above hypothesis. Recent advances in deep hierarchical models, commonly referred to as 'deep learning' models (<ref type="bibr" target="#b2">Bengio et al., 2012;</ref><ref type="bibr" target="#b13">Hinton et al., 2006;</ref><ref type="bibr" target="#b13">Hinton and Salakhutdinov, 2006</ref>), provide an intriguing opportunity to model the common encoding mechanism of cellular signaling systems. These models represent the signals embedded in observed data using multiple layers of hierarchically organized hidden variables, which can be used to simulate a cellular signaling system because the latter is also organized as a hierarchical network such that signaling proteins at different levels compositionally encode signals with different degrees of complexity. For example, activation of the epidermal growth factor receptor (EGFR) will lead to a broad change of cellular functions including the activation of multiple signaling molecules such as Ras and MAP kinases (<ref type="bibr" target="#b1">Alberts et al., 2008</ref>), which in turn will activate different transcription factors, e.g. Erk-1 and c-Jun/c-Fos complex, with each responsible for the transcription of a subset of genes responding to EGFR treatment. The signals encoded by signaling molecules become increasingly more specific, and they share compositional relationships. Therefore, deep hierarchical models, e.g. the deep belief network (DBN) (<ref type="bibr" target="#b13">Hinton et al., 2006</ref>), are particularly suitable for modeling cellular signaling systems. In this paper, we present novel deep hierarchical models based on the DBN model to represent a common encoding system that encodes the cellular response to different stimuli, which was developed after the competition in order to overcome the shortcomings of the conventional classification approaches we employed during competition. We applied the model to the data provided by the SBV IMPROVER challenge and systematically investigated the performance. Our results indicate that, by learning better representations of cellular signaling systems, deep hierarchical models perform significantly better on the task of trans-species learning. More importantly, this study leads to a new direction of using deep networks to model large 'omics' data to gain in depth knowledge of cellular signaling systems under physiological and pathological conditions, such as cancer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In this study, we investigated using the DBN model (<ref type="bibr" target="#b13">Hinton et al., 2006</ref>) to represent the common encoding system of the signal transduction systems of human and rat bronchial cells. A DBN contains one visible layer and multiple hidden layers (<ref type="figure" target="#fig_1">Fig. 2A</ref>). An efficient training algorithm was introduced by (<ref type="bibr" target="#b13">Hinton et al., 2006;</ref><ref type="bibr" target="#b13">Hinton and Salakhutdinov, 2006</ref>), which treats a DBN as a series of restricted Boltzmann machines (RBM;<ref type="figure" target="#fig_1">Fig. 2B</ref>) stacked on top of each other. For example, the visible layer v and the first hidden layer, h (1) , can be treated as a RBM, and the first and second hidden layers, h (1) and h (2) , form another RBM with h (1) as the 'visible' layer. The inference of the hidden node states and learning of model parameters are first performed by learning the RBM stacks bottom-up, which is followed by a global optimization of generative parameters using the back-propagation algorithm. In certain cases, edges between visible variables can be added in a RBM to capture the relationship of the visible variables, which leads to a semi-restricted RBM (<ref type="figure" target="#fig_1">Fig. 2C</ref>). In the following sub-sections, we will first introduce the models and their inference algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Restricted Boltzmann Machines (RBMs)</head><p>A RBM is an undirected probabilistic graphical model consisting of a layer of stochastic visible binary variables (represented as nodes in the graph) v 2 f0; 1g D and a layer of stochastic hidden binary<ref type="figure">Fig. 1</ref>. Trans-species learning task specification. The objective of the SBV challenge was to predict the phosphorylation states of a set of proteins in human cells treated with different stimuli, based on the observed phosphorylation states of the same set of proteins in rat cells treated with the same stimuli. The blocks labled as " training " are matrices representing the observed phosphorylation states of proteins under different treatment conditions in human and rat cells. In test phase, the phosphorylation states of the proteins in rat cells treated with a set of unknown stimuli are provided, and the task is to predict the phosphorylation states of the human cells treated with the same stimuli Trans-species learning of cellular signaling systemsvariables h 2 f0; 1g F. A RBM is a bipartite graph in which each visible node is connected to every hidden node (<ref type="figure" target="#fig_1">Fig. 2B</ref>) and vice versa. The statistical structure embedded in the visible variables can be captured by the hidden variables. The RBM model defines the joint distribution of hidden and visible variables using a Boltzmann distribution as follows:</p><p>Prðv; h; hÞ ¼ 1 ZðhÞ expðÀEðv; h; hÞÞ</p><formula>(1)</formula><p>The energy function E of the state fv; hg of the RBM is defined as follows:</p><formula>Eðv; h; hÞ ¼ Àa &gt; v À b &gt; h À v &gt; Wh ¼ À X D i¼1 a i v i À X F j¼1 b j h j À X D i¼1 X F j¼1 v i h j w ij (2)</formula><p>where v i is the binary state of visible variable i; h j is the binary state of hidden variable j; h ¼ fa; b; Wg are the model parameters. a i represents the bias for visible variable i and b j represents the bias for hidden variable j. w ij represents the weight between visible variable i and hidden variable j. The 'partition function', Z, is derived by summing over all possible states of visible and hidden variables:</p><formula>ZðhÞ ¼ X v;h expðÀEðv; h; hÞÞ (3)</formula><p>The marginal distribution of visible variables is</p><formula>Prðv; hÞ ¼ X h Prðv; h; hÞ ¼ 1 ZðhÞ X h expðÀEðv; h; hÞÞ (4)</formula><p>2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning parameters of the RBM model</head><p>Learning parameters of a RBM model can be achieved by updating the weight matrix and biases using a gradient descend algorithm (delta methods;<ref type="bibr" target="#b13">Hinton and Salakhutdinov, 2006</ref>).</p><formula>w tþ1 ¼ w t þ Dw (5) DW ij ¼ @logPrðvÞ @W ij ¼ ð&lt; v i h j &gt; data À &lt; v i h j &gt; model Þ (6)</formula><p>where is the learning rate; &lt; v i h j &gt; data is the expected product of the observed data and inferred hidden variables conditioning on observed variables; &lt; v i h j &gt; model is the expected product of the model-predicted v and h. One approach to derive &lt; v i h j &gt; model is to obtain samples of v and h from a model-defined distribution using Markov chain Monte Carlo (MCMC) methods and then average the product of the samples, which may take a long time to converge. Representing the &lt; v i h j &gt; model derived MCMC chain after convergence as &lt; v i h j &gt; 1 , one updates the model parameter w ij as follows:</p><formula>DW ij ¼ ð&lt; v i h j &gt; data À &lt; v i h j &gt; 1 Þ (7)</formula><p>To calculate &lt; v i h j &gt; 1 , one can alternatively sample the states of hidden variables given visible variables and then sample the states of visible variables given hidden variables (<ref type="bibr" target="#b23">Salakhutdinov et al., 2007</ref>) based on the following equations.</p><formula>Prðh j ¼ 1jvÞ ¼ rðb j þ X n i¼1 W ij v i Þ (8) Prðv i ¼ 1jhÞ ¼ rða i þ X m j¼1 W ij h j Þ (9)</formula><p>where rðxÞ is the logistic function 1=ð1 þ expðÀxÞÞ.</p><p>The convergence of a MCMC chain may take a long time. Thus, to make RBM learning more efficient, we adopted a learning algorithm called contrastive divergence (CD) (<ref type="bibr" target="#b28">Welling and Hinton, 2002</ref>). Instead of running a MCMC chain for a very large number of steps, CD learning just runs the chain for a small number n of steps and minimizes the divergence between Kullback–Leibler divergence KLðp 0 jj p 1 Þ and KLðp n jjp 1 Þ to approximate &lt; v i h j &gt; model (Carreira<ref type="bibr" target="#b7">Perpinan and Hinton, 2005</ref>). Therefore, the updating algorithm for a parameter of a RBM can be rewritten as follows:</p><formula>DW ij ¼ ð&lt; v i h j &gt; data À &lt; v i h j &gt; model Þ ¼ ð&lt; v i h j &gt; Prðhjv;wÞ À &lt; v i h j &gt; n Þ (10) Da i ¼ ð&lt; v i &gt; data À &lt; v i &gt; n Þ (11) Db j ¼ ð&lt; h j &gt; data À &lt; h j &gt; n Þ (12)</formula><p>The pseudocode for training a RBM is as follows:</p><p>Repeat for t iterations:</p><p>1) Infer state of hidden units h j0 given visible units</p><formula>v 0 Prðh j0 jv 0 Þ Prðh j0 ¼ 1jv 0 Þ ¼ rðb j t þ X n i¼1 W ij t v i0 Þ ¼ &lt; h j0 &gt;</formula><p>2) Gibbs Sampling &lt; h j0 &gt;! binary matrix h j0 3) Infer state of visible units v i1 given hidden units</p><formula>h 0 Prðv i1 jh 0 Þ Prðv i1 ¼ 1jh 0 Þ ¼ rða i t þ X m j¼1 W ij t &lt; h j0 &gt;Þ ¼ &lt; v i1 &gt;</formula><p>4) Infer state of hidden units h j1 given visible units</p><formula>v 1 Prðh j1 jv 1 Þ Prðh j1 ¼ 1jv 1 Þ ¼ rðb j t þ X n i¼1 W ij t &lt; v i1 &gt;Þ ¼&lt; h j1 &gt;</formula><p>5) Update parameters (weight between visible i and hidden j, bias of visible and bias of hidden)</p><formula>W ij tþ1 ¼ W ij t þ ð&lt; v i0 T h j0 &gt; À &lt; v i1 T h j1 &gt;Þ ¼ W ij t þ ð&lt; v i0 &gt; T &lt; h j0 &gt; À &lt; v i1 &gt; T &lt; h j1 &gt;Þ a i tþ1 ¼ a i t þ ð&lt; v i0 &gt; À &lt; v i1 &gt;Þ b j tþ1 ¼ b j t þ ð&lt; h j0 &gt; À &lt; h j1 &gt;Þ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning a Deep Belief Network</head><p>Unlike a RBM, which captures the statistical structure of data using a single layer of hidden nodes, a DBN strives to capture the statistical structure using multiple layers in a distributed manner, such that each layer captures the structure of different degrees of abstraction. Training a DBN involves learning two sets of parameters: (i) a set of recognition weight parameters for the upward propagation of information from the visible layer to the hidden layers, and (ii) a set of generative weight parameters that can be used to generate data corresponding to the visible layer. The learning of recognition weights can be achieved by treating a DBN as a stack of RBMs and progressively performing training in a bottom-up fashion (<ref type="bibr" target="#b13">Hinton et al., 2006;</ref><ref type="bibr" target="#b13">Hinton and Salakhutdinov, 2006</ref>). For example, one can treat the visible layer v and the first hidden layer h (1) as a RBM, and then we can treat hidden layer h (1) as a visible layer and form a RBM with the hidden layer h (2). Following the stack-wise learning of RBMs weight parameters and instantiation of hidden variables in the top layer, learning the generative weights across all layers can be achieved by a backpropagation algorithm as in training standard neural networks. The pseudo-code for training a 4-layered DBN is as follows:</p><p>Input: Binary data matrix Output: recognition and generative weights 1. Randomly initialize parameters 2. Train RBM for layer 1 3. Train RBM for layer 2 4. Train RBM for layer 3 5. Train RBM for layer 4 6. Backpropagation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Bimodal DBN (bDBN)</head><p>A traditional DBN assumes that data are from one common distribution, and the task is to use distributed hidden layers to capture the structure of this distribution. However, our task of transferring the knowledge learned from rat cells to human cells deviates from the traditional assumption in that humans and rats may use different pathways and signaling molecules to encode the response to a common stimulus. Thus our task is to learn a common encoding system that governs two distributions, which may each have its own mode, hence a bimodal problem. Inspired by the bimodal deep Boltzmann machine model and multimodal deep learning (<ref type="bibr" target="#b16">Liang, 2015;</ref><ref type="bibr" target="#b18">Ngiam, 2011;</ref><ref type="bibr" target="#b25">Srivastava and Salakhutdinov, 2012</ref>), which uses a multi-layered deep network to model the joint distribution of images and associated text, we designed a modified variant of bimodal DBN (bDBN) to capture the joint distribution of rat and human proteomic data. Our hypothesis is that rat and human cells share a common encoding system that respond to a common stimulus, but utilize different proteins to carry out the response to the stimulus. Thus, we can use the hidden layers to represent the common encoding system, which regulates distinct human protein phosphorylation and rat phosphorylation responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Training</head><p>Traditional bimodal models dealing with significantly different input modalities such as audio and video (<ref type="figure" target="#fig_0">Fig. 3A</ref>) (<ref type="bibr" target="#b18">Ngiam, 2011;</ref><ref type="bibr" target="#b25">Srivastava and Salakhutdinov, 2012</ref>) usually require one or more separate hidden layers to first capture the statistical structure of each type of data and then model their joint distribution with common high level hidden layers. However, in our setting, although rat and human proteomic data have their own modalities, they are not drastically different. Therefore, instead of using two separate hidden layers, we devised a modified bimodal DBN, in which a rat training case and a human training case treated with a common stimulus are merged into a joint input vector for the bDBN and connected to a common hidden layer h (1) (<ref type="figure" target="#fig_0">Fig. 3B</ref>). In this model, the training procedure is the same as training a conventional DBN using the algorithm described in Section 2.3, but the prediction is carried out in a bimodal manner. Under this setting, the hidden layers are forced to encode the information that can be used to generate both rat and human data, i.e. the hidden layers behave as a common encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Prediction</head><p>When using a trained bDBN to predict human cell response to a specific stimulus based on the observed rat cell response to the same stimulus, we only used the rat data to update the states of nodes in the first hidden layer, Pr h ð1Þ jv rat , with doubled edge weights (2 Â W (1) Rat ) from rat variables to hidden variables (red edges in<ref type="figure" target="#fig_2">Fig.  4</ref>). Then the upper hidden layers were updated using the same method as in a conventional DBN using the recognition weights. When the top hidden layer h (4) was updated using rat data, the bDBN propagated the information derived from rat data downwards to h (1) using generative weights as in a feed forward neural network to predict the human data (<ref type="figure" target="#fig_2">Fig. 4A</ref>). We finally predicted the human cell response Pr v human jh ð1Þ with weights only from hidden variables in h (1) to human visible variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Semi-restricted bimodal deep belief network (sbDBN)</head><p>Since signaling proteins in a phosphorylation cascade have regulatory relationships among themselves, we further modified the bottom Boltzmann machine, consisting of h (1) and v, into a semi-restricted Boltzmann (<ref type="bibr" target="#b26">Taylor and Hinton, 2009</ref>), in which edges between proteins from a common species are allowed (<ref type="figure" target="#fig_0">Fig. 3C</ref>). In this model, the hidden variables in h (1) capture the statistical structure of the 'activated regulatory edges' between signaling proteins, instead of 'activated protein nodes'. In this model, each humanTrans-species learning of cellular signaling systemsprotein was connected to other human proteins, and the same rule was applied to each rat protein. However, we didn't allow interactions between human proteins and rat proteins. The interaction between proteins, which is represented as I, was added into the negative phase shown below:</p><formula>Prðv i ¼ 1jhÞ ¼ rða i þ X m j¼1 W ij h j þ I i Þ (13) I i ¼ X n k6 ¼i v k Ãp ik (14)</formula><p>where I is the influence of the phosphorylation states of other proteins on that of the ith protein.</p><p>Dp ik ¼ ð&lt; v i v k &gt; data À &lt; v i v k &gt; model Þ where k 6 ¼ i (15)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Performance evaluation</head><p>We adopted the evaluation metrics that were used to evaluate and compare the performance of submitted models in the SBV IMPROVER challenge, which include AUROC (area under receiver operator characteristic;<ref type="bibr" target="#b4">Bradley, 1997</ref>), AUPRC (area under the precision-recall curve;<ref type="bibr" target="#b8">Davis and Goadrich, 2006;</ref><ref type="bibr" target="#b11">Goadrich et al., 2004</ref>), Jaccard Similarity (<ref type="bibr">Dombek et al., 2000</ref>), Matthews correlation coefficient (<ref type="bibr" target="#b20">Petersen et al., 2011</ref>), Spearman correlation (<ref type="bibr" target="#b5">Brott et al., 1989</ref>) and Pearson correlation (<ref type="bibr" target="#b0">Adler and Parmryd, 2010</ref>), to measure the accuracy of the prediction. In all metrics except for Jaccard Similarity, the higher the score, the more accurate the model is. We performed a series of cross-validation experiments, in which we held out the three repeated experiments corresponding to one stimulus of both rat and human cells, performed model training, and test the performance using the held-out samples. All results discussed in the paper were derived from these cross-validation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Model selection</head><p>When training a deep hierarchical model, often the first task is to determine the structure of the model, i.e. the number of layers and the number of hidden nodes per layer. However, currently there is no well-established method for model selection when training deep learning models. Therefore, we performed a series of cross-validation experiments to search for an 'optimal' structure for bimodal and semi-restricted bimodal DBNs. We set the initial structure of both bDBN and sbDBN to the following ranges:setting, we performed a leave-one-out experiment to assess the performance of a model. In such an experiment, we held out both human and rat data treated by a common stimulus as the test case, trained models with data treated by the rest of stimuli, and then we predicted the states of human phosphoproteins using the held-out rat data as illustrated in<ref type="figure" target="#fig_2">Figure 4</ref>. By doing this, we predicted human data treated by all stimuli, and we evaluated and compared the performance using the AUROC of different models and retained the model structure that led to the best performance. Note, during leave-one-out training of a model with a given structure, the parameters associated with each model can be different, and therefore the results reflect the fitness of the model with a particular structure after averaging out the impact of individual parameters, an approach closely related to Bayesian model selection (<ref type="bibr" target="#b3">Bishop, 2006</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Baseline predictive models</head><p>As a comparison to bDBN and sbDBN, we formulated the task of predicting human cell response based on rat cell response to a common stimulus as a classification problem, and we employed two current state-of-the-art classification models, a support vector machine (SVM) (<ref type="bibr" target="#b3">Bishop, 2006</ref>) with a Gaussian kernel (<ref type="bibr" target="#b15">Karatzoglou et al., 2004</ref>) and an elastic-net regularized generalized linear model (GLMNET) (<ref type="bibr" target="#b10">Friedman et al., 2010</ref>) to predict human cell responses. In this setting, we trained a classification model (SVM or GLMNET) for one human protein using a vector of rat proteomic data collected under a specific condition as input features (independent variables) and the human protein response under the same condition as a binary class variable (dependent variables). We trained one such classifier for each human protein class. We performed leave-one-out cross-validation using SVM and GLMNET models respectively. The results predicted by SVM and GLMNET were then compared with the results predicted by DBN and sbDBN using the metrics discussed in Section 2.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The data</head><p>The protein phosphorylation response data in this study was provided by SBV IMPROVER (SBV IMPROVER, 2013). The data contains the phosphorylation status of 16 proteins collected after exposing rat and human cells to 26 different stimuli (<ref type="figure" target="#tab_1">Table 1</ref>). Each stimulus was repeated 3 times. The SBV IMPROVER organizers preprocessed the proteomic data into binary values to represent if a protein was phosphorylated under a specific condition. We directly utilized the binary input for our DBN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model selection results</head><p>In order to identify the 'optimal' model structure that perform well, we examined the performance of each model with a specific structure configuration stated in Section 2.7. For a given model, we performed a leave-one-out cross validation experiment and calculated the AUROC for the model. The average of the AUROCs for 120 bDBN models was 0.80, and the highest one is 0.86. The bDBN structure yielding the best AUROC consisted of four hidden layers with the following numbers of nodes 35, 30, 30 and 20, from h (1) to(B) Prediction with sbDBN. When predicting human phosphoprotein states, information derived from rat phosphoprotein states is propagated upward using weights represented by red arrows and then propagated downwards using the weights represented by blue arrows to predict human phosphoprotein states h (4) respectively. For sbDBN, the mean of the AUROCs for 120 candidate models is 0.86 and the highest one was 0.93. The number of nodes for the four layers for the best sbDBN model was 30, 30, 30 and 20, from h (1) to h (4) respectively. A tentative explanation for the different numbers in h (1) between bDBN and sbDBN is that the edges between the visible variables in the sbDBN partially captured the statistical structures of the visible variables, which reduced the need for additional nodes in the layer h (1). In the following sections, we report the results derived from bDBN and sbDBN with these two specific structures with the highest AUROCs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Hyper parameters used for model training</head><p>The weights were updated using a learning rate of 0.1, momentum of 0.9 and a weight decay of 0.0002. The weights were initialized with random values sampled from a standard normal distribution multiplied by 0.1. Contrastive divergence learning was started with n ¼ 1 and increased in small steps during training.<ref type="figure" target="#tab_2">Table 2</ref>shows the comparisons between different predictive models in terms of 6 evaluation metrics. We highlighted the best value for each metric using bold face letters. When comparing bDBN with SVN and GLMNET, the results show that bDBN performs better in terms of AUROC and Spearman's correlation, but underperformed in terms of AUPRC, Jaccard similarity, and Pearson correlation. This is potentially due to the fact that we performed model selection mainly using AUROC as the criteria. Strikingly, with the addition of protein-protein edges in the visible layer, the 4-layered sbDBN performs much better than all other models measured in all metrics. Based on the AUROC value, the performance of the 4-layered sbDBN &gt; 4-layered bDBN &gt; SVM &gt; GLMNET. However, ranking varies depending on the scoring method. It is known that models pursuing optimal area under the ROC curve is not guaranteed to optimize the area under the Precision-Recall curve (<ref type="bibr" target="#b8">Davis and Goadrich, 2006</ref>). Indeed, we noted that the AUROC for the 4-layered DBN is better than the one for GLMNET. However, the AUPRC for the 4-layered DBN is worse than the one for GLMNET (<ref type="figure" target="#tab_2">Table 2</ref>;<ref type="figure" target="#fig_3">Fig. 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison among different models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Biological interpretation of learned edges between proteins in sbDBN</head><p>The best predictive power of the sbDBN reflects the importance of capturing the correlation between signaling proteins. We then investigated whether the learned correlations between signaling proteins are biologically sensible, although it should be noted that Boltzmann machine models cannot infer causal relationships. For each protein, we picked the top 3 strongest interaction edges for rat and human respectively, and we organized the results as shown in<ref type="figure">Figure 6</ref>. In this figure, if the interaction between a pair of proteins exists in both rat and human data, the edge is colored green. If the interaction is rat only, there is a blue line between the two proteins. If the interaction is human only, there is a red line between the two proteins. The results indicate that, while some common correlations are shared between rat and human cells, different covariance structure exists in different proteomic data. Due to the fact that signal transduction in live cells are dynamic events, it is difficult to thoroughly evaluate the accuracy of inferred interactions even through further experimentations. Conventional evaluation metrics such as sensitivity and specificity are difficult to assess in this study. Since it is possible that the signal transduction between a pair of proteins known to have a regulatory relationship may not be present under the experimental conditions of this study, accurately assessing sensitivity is challenging; similarly, since there are seldom reports or databases stating that signal transduction never occurred between a pair of proteins, it is challenging to assess if the lack of an edge between a pair of proteins in our model really represents a true negative outcome. As such, conventional metrics such as AUROC cannot be applied in our evaluation. However, we noted that we were able to assess with reasonable confidence the positive predictive value (PPV) of the model, i.e. the percentage of the predicted signal transduction interactions that is known in literature. We performed a comprehensive literature review and cited the references supporting the predicted regulatory relationship and known protein-protein interactions in Supplementary Tables. The results indicate that most of the predicted regulatory relationshipsStimuli 5AZA, AMPHIREGULIN, BETAHISTINE, BISACODYL, CHOLESTEROL, CLENBUTEROL, EGF, EGF8, FLAST, FORSKOLIN, HIGHGLU, IFNG, IGFII, IL4, MEPYRAMINE, NORETHINDRONE, ODN2006, PDGFB, PMA, PROKINECITIN2, PROMETHAZINE, SEROTONIN, SHH, TGFA, TNFA, WISP3, DME Proteins AKT1, CREB1, FAK1, GSK3B, HSPB1, IKBA, KS6A1, KS6B1, MK03, MK09, MK14K11, MP2K1, MP2K6, PTN11, TF65, WNK1are supported by the literature or have evidence of physical interactions between the proteins. Thus, the results support the notion that the sbDBN correctly captured the correlation (thereby signal transduction or cross talks) between phosphoproteins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>In this study, we investigated the utility of novel deep hierarchical models in a trans-species learning setting. To our knowledge, this is the first report using deep hierarchical models to address this type of problem. Our results indicate that, by learning to represent a common encoding system for both rat and human cells, the deep learning models outperform contemporary state-of-the-art classifiers in this trans-species learning task. The empirical success of deep hierarchical models may be attributed to the following advantages. First, the DBN is capable of learning novel representations of the data that are salient to the task at hand. The DBN models are more compatible to the biological systems that generate the observed data. The hidden variables at the different layers of the DBN models can capture information with different degrees of abstraction, thus allowing the models to capture a more complex covariance structure of the observed variables. It is possible that hidden nodes at lower layers, e.g. h (1) , directly capture the covariance of the observed protein phosphorylation states, whereas the higher layers can capture the crosstalk between signaling pathways that only occur in response to specific stimuli. Thus, shallow models that only concentrate on the covariance at the level of observed variables, such as SVM and elastic network, would have difficulties capturing such a high-level covariance structure of the data. It is now well appreciated that feature-learning methods, such as DBN, tend to outperform feature selection methods in complex domains, such as image classification and speech recognition (<ref type="bibr" target="#b2">Bengio et al., 2012;</ref><ref type="bibr" target="#b13">Hinton et al., 2006;</ref><ref type="bibr" target="#b13">Hinton and Salakhutdinov, 2006</ref>). Second, DBN strives to learn the common encoding system for both human and rat data, and it naturally performs multi-label classification by taking into account the covariance of the class variables. However, a conventional classifier, such as a SVM, can only predict one human protein as the class variable in an independent manner, thus failing to capture the covariance of class variables and yielding inferior performance. The sbDBN model developed in this study provides a novel approach capable of simultaneously learning interactions and predicting the state of phosphoproteins. Interestingly, the model assigns differential weights to the edges between phosphoproteins when comparing those from rat and human cells, which potentially indicates that different parts of signaling pathways are preferentially utilized in a species-specific manner. However, this hypothesis still needs to be experimentally tested in a relatively larger dataset. Deep hierarchical models are particularly suited for modeling cellular signaling systems, because signaling molecules in cells are organized as a hierarchical network and information in the system is compositionally encoded. Our results indicate that DBNs were capable of capturing the complex information embedded in proteomic data. Interestingly, in contrast to the training of deep learning models in a machine learning setting such as object recognition in image analysis where usually a large number of training cases is required, our results show that the DBN models performed very well given a moderate size of training cases. This indicates that biological data tend to have strong signals that can be captured by DBNs with relative ease. Our study demonstrates the feasibility of using deep hierarchical models to simulate cellular signaling systems in general, and we foresee that deep hierarchical models will be widely used in systems biology. For example, one can use deep hierarchical models to study how cells encode the signals regulating gene expression, to detect which signaling pathway is perturbed in a specific pathological condition, e.g. cancer. Finally, models like our bDBN and sbDBN provide a novel approach to simultaneously model multiple types of 'omics' data in an 'integromics' fashion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.3.</head><figDesc>Fig. 3. Training DBN models. (A) A diagram of a conventional bimodal DBN. The green and orange nodes represent different input modalities, e.g. audio and video inputs, and each type is first modeled with a separate hidden layer, and the joint distribution is modeled with a common higher layer hidden nodes. (B) A 4-layered bimodal DBN for modeling rat and human proteomic data. The blue and red nodes represent human and rat phosphoproteins respectively. The bottom layer consists of observed variables. Upward arrows represent recognition weights and downward arrows represent generative weights. C) A sbDBN. Additional edges between proteins from the same species are added</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.2.</head><figDesc>Fig. 2. Graph representation of the Deep Belief Network and related models. (A) The graph representation of a 4-layered deep belief network. The double circles represent visible variables, and the single circles represent hidden variables. (B) The graph representation of a restricted Boltzmann machine. (C) The graph representation of a semi-restricted Boltzmann machine in which visible variables are connected</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.4.</head><figDesc>Fig. 4. Prediction with bDBN and sbDBN models. (A) Prediction with bDBN. (B) Prediction with sbDBN. When predicting human phosphoprotein states, information derived from rat phosphoprotein states is propagated upward using weights represented by red arrows and then propagated downwards using the weights represented by blue arrows to predict human phosphoprotein states</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.5.</head><figDesc>Fig. 5. ROC and RPC curves of different models. (A) Performance results of four models in terms of AUROC. (B) Performance results of four models in terms of AUPRC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>V C The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 3008 Bioinformatics, 31(18), 2015, 3008–3015 doi: 10.1093/bioinformatics/btv315 Advance Access Publication Date: 20 May 2015 Original Paper</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>h (1) : 30–50; h (2) : 25–40; h (3) : 20–30; and h (4) : 20–25. We iteratively modified the structure of the model by changing the number of hidden nodes within a layer using a step size of 5 and explored all combinations in the range stated above. In this case, the total number of models tested is 120 (5*4*3*2) for both bDBN and sbDBN. Under each particular</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>Table 1. Proteins and stimuli involved in this study</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>Table 2.</figDesc><table>Leave-one-out accuracy scores of models 

AUPRC 
AUROC 
Jaccard. 
Similarity 

Matthews. 
Correlation. Coefficient 

Speaman. 
Correlation 

Pearson. 
Correlation 

4-layered bDBN 
0.417 
0.859 
0.750 
0.373 
0.323 
0.235 
4-layered sbDBN 
0.632 
0.936 
0.531 
0.616 
0.391 
0.460 
SVM 
0.493 
0.724 
0.692 
0.411 
0.231 
0.392 
GLMNET 
0.444 
0.709 
0.717 
0.374 
0.194 
0.282 

Trans-species learning of cellular signaling systems</table></figure>

			<note place="foot">at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">L.Chen et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Quantifying colocalization by correlation: the Pearson correlation coefficient is superior to the Mander&apos;s overlap coefficient</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Adler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Parmryd</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cytom Part A</title>
		<imprint>
			<biblScope unit="page" from="77" to="733" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Molecular Biology of the Cell</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Alberts</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Garland Science</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Taylor &amp; Francis Group, LLC</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<monogr>
		<title level="m" type="main">Representation learning: a review and new perspectives</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Bengio</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>arXiv. .org</note>
</biblStruct>

<biblStruct   xml:id="b3">
	<monogr>
		<title level="m" type="main">Pattern Recognition and Machine Learning</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">M</forename>
				<surname>Bishop</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">The use of the area under the roc curve in the evaluation of machine learning algorithms</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">P</forename>
				<surname>Bradley</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1145" to="1159" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Measurements of acute cerebral infarction—a clinical examination scale</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Brott</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stroke</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="864" to="870" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Disease model discovery and translation. Introduction</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">D</forename>
				<surname>Brown</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mammalian Genome Off. J. Int. Mammalian Genome Soc</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">361</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Carreira-Perpinan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">E</forename>
				<surname>Hinton</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">) On Contrastive Divergence Learning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">The relationship between Precision-Recall and ROC curves</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Davis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Goadrich</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning</title>
		<meeting>the 23rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Protein correlation network learned from the 4-layered sbDBN. Ovals represent the proteins. A green line represents a common edge shared between human proteins and rat proteins; a red line represents an edge between human proteins; a blue line represents an edge between rat proteins Use of repetitive DNA sequences and the PCR to differentiate Escherichia coli isolates from human and animal sources</title>
		<author>
			<persName>
				<surname>Fig</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Environ. Microb</title>
		<editor>Dombek,P.E. et al.</editor>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">66</biblScope>
			<biblScope unit="page" from="2572" to="2577" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Regularization paths for generalized linear models via coordinate descent</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Friedman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Softw</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning ensembles of first-order clauses for recallprecision curves: a case study in biomedical information extraction</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Goadrich</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lect. Notes Artif. Int</title>
		<imprint>
			<biblScope unit="volume">3194</biblScope>
			<biblScope unit="page" from="98" to="115" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">E</forename>
				<surname>Hinton</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">E</forename>
				<surname>Hinton</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">R</forename>
				<surname>Salakhutdinov</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-label literature classification based on the Gene Ontology graph</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Jin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">525</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">kernlab—an S4 package for kernel methods in R</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Karatzoglou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Softw</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Integrative data analysis of multi-platform cancer data with a multimodal deep learning approach</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Liang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. Biol. Bioinf</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">99</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Animal models of human disease: challenges in enabling translation</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Mcgonigle</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Ruggeri</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biochem. Pharmacol</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="162" to="171" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Ngiam</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Enhanced beta cell function and anti-inflammatory effect after chronic treatment with the dipeptidyl peptidase-4 inhibitor vildagliptin in an advanced-aged diet-induced obesity mouse model</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">A</forename>
				<surname>Omar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diabetologia</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1752" to="1760" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">SignalP 4.0: discriminating signal peptides from transmembrane regions</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">N</forename>
				<surname>Petersen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="785" to="786" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">The species translation challenge—a systems biology perspective on human and rat bronchial epithelial cells</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Poussin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding the limits of animal models as predictors of human biology: lessons learned from the sbv IMPROVER Species Translation Challenge</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Rhrissorrakrai</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="31" to="471" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Restricted Boltzmann Machines for Collaborative Filtering</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Salakhutdinov</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="791" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<monogr>
		<title level="m" type="main">SBV IMPROVER: Species Translation Challenge Overview</title>
		<author>
			<persName>
				<forename type="first">Sbv</forename>
				<surname>Improver</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<monogr>
		<title level="m" type="main">Multimodal learning with deep Boltzmann machines</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Srivastava</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Salakhutdinov</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2231" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Factored conditional restricted Boltzmann Machines for modeling motion style</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">W</forename>
				<surname>Taylor</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">E</forename>
				<surname>Hinton</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1025" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-label classification: an overview</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Tsoumakas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Katakis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Warehousing Mining</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">A new learning algorithm for Mean Field Boltzmann Machines</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Welling</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">E</forename>
				<surname>Hinton</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lect. Notes Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">2415</biblScope>
			<biblScope unit="page" from="351" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<monogr>
		<title level="m" type="main">Trans-species learning of cellular signaling systems</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>