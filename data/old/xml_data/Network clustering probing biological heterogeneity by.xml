
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining Network clustering: probing biological heterogeneity by sparse graphical models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011">2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Sach</forename>
								<surname>Mukherjee</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Centre for Complexity Science</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<postCode>CV4 7AL</postCode>
									<settlement>Coventry</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Steven</forename>
								<forename type="middle">M</forename>
								<surname>Hill</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Statistics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Centre for Complexity Science</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<postCode>CV4 7AL</postCode>
									<settlement>Coventry</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining Network clustering: probing biological heterogeneity by sparse graphical models</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">27</biblScope>
							<biblScope unit="issue">7</biblScope>
							<biblScope unit="page" from="994" to="1000"/>
							<date type="published" when="2011">2011</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btr070</idno>
					<note type="submission">Received on June 8, 2010 ; revised on February 2, 2011 ; accepted on February 3, 2011</note>
					<note>[13:22 17/3/2011 Bioinformatics-btr070.tex] Page: 994 994–1000 Associate Editor: John Quackenbush Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Networks and pathways are important in describing the collective biological function of molecular players such as genes or proteins. In many areas of biology, for example in cancer studies, available data may harbour undiscovered subtypes which differ in terms of network phenotype. That is, samples may be heterogeneous with respect to underlying molecular networks. This motivates a need for unsupervised methods capable of discovering such subtypes and elucidating the corresponding network structures. Results: We exploit recent results in sparse graphical model learning to put forward a &apos;network clustering&apos; approach in which data are partitioned into subsets that show evidence of underlying, subset-level network structure. This allows us to simultaneously learn subset-specific networks and corresponding subset membership under challenging small-sample conditions. We illustrate this approach on synthetic and proteomic data. Availability:</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Networks and pathways are important in understanding the collective biological function of molecular components such as genes or proteins. The importance of network-based thinking in current biology has motivated a rich body of work in bioinformatics on modelling approaches for biological networks, including networks involved in gene regulation and protein signalling (including<ref type="bibr" target="#b5">Friedman, 2004;</ref><ref type="bibr" target="#b7">Husmeier, 2003;</ref><ref type="bibr" target="#b12">Mukherjee and Speed, 2008;</ref><ref type="bibr" target="#b18">Sachs et al., 2005;</ref><ref type="bibr" target="#b19">Schäfer and Strimmer, 2005;</ref><ref type="bibr" target="#b20">Segal et al., 2003;</ref><ref type="bibr" target="#b26">Yip and Gerstein, 2009;</ref><ref type="bibr" target="#b27">Yu et al., 2004;</ref><ref type="bibr" target="#b28">Zhu et al., 2007</ref>). Most studies have focused on the case in which biochemical data are treated as homogeneous with respect to an underlying biological network structure. That is, the assumption is made that a single network model is broadly appropriate for a given set of data. This is a reasonable approach in many cases where it is clear that biological samples share overall phenotype, or where data can be filtered into roughly homogeneous groups using appropriate markers. However, in many scenarios available data may contain subtypes which differ * To whom correspondence should be addressed. in terms of underlying network structure. A topical example comes from cancer biology. Molecular studies have revealed a number of subtypes of cancer (<ref type="bibr" target="#b15">Perou et al., 2000;</ref><ref type="bibr" target="#b22">Sørlie et al., 2001</ref>) that have been found to differ markedly in terms of both biology and response to treatment. The genomic aberrations harboured by such subtypes may in turn be manifested in terms of differences in gene regulatory or protein signalling networks. In settings where subtypes have already been characterized, this observation may simply motivate stratification by subtype in advance of network modelling or supervised network-based approaches (e.g.<ref type="bibr" target="#b1">Chuang et al., 2007</ref>). However, when such biological heterogeneity has not been characterized a priori this is not possible. Moreover, if there are hitherto unknown subtypes which differ at the level of network structure, discovering the subtypes and elucidating their corresponding networks itself becomes an important biological goal. This article develops an unsupervised analysis called network clustering, which seeks to partition data that is heterogeneous in the sense described above into subsets showing evidence of shared underlying network structure. This is an approach which we believe has broad utility. For example, in the study of human diseases it is becoming clear that traditional diagnostic classifications may, in many cases, understate underlying molecular heterogeneity, often with therapeutic consequences. Indeed, in the cancer arena, the task of discovering cancer subtypes and developing treatment regimes that are subtype specific is emerging as one of crucial importance. Of course, biological subtypes may differ dramatically at the level of mean response, say in terms of gene or protein expression. In such cases, existing clustering methods are well suited to probing heterogeneity (and have successfully done so in many studies) and indeed a small number of key biomarkers may be sufficient to discover and discriminate subtypes. But when differences are truly at the network or pathway level, conventional clustering approaches are likely to prove inadequate, for reasons we discuss below. However, the network clustering problem stated here is a challenging one. Network inference—from a single, homogeneous dataset—itself remains a non-trivial task and the subject of much ongoing work. The learning of network structure within an unsupervised framework is doubly challenging, because multiple rounds of network inference are likely to be needed, often at small sample sizes, in the same way that a standard clustering method typically involves multiple rounds of estimation on data subsets (or 'soft' equivalents thereof). We discuss below how we address these concerns by using undirected graphical models and exploiting recent work on 1-penalized inference. The manner in which molecular players vary in concert, i.e. their covariance, is central to the statistical description of the networks<ref type="bibr">[</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network clustering</head><p>they constitute. From a statistical perspective, a focus on networks rather than individual genes/proteins corresponds to a shift from a mean-to a covariance-centred point of view. Yet, widely used clustering methods either do not consider cluster-specific covariance (e.g. K-means, hierarchical clustering) or do so via estimators which are inapplicable or ill behaved under the conditions of dimensionality and sample size typical of molecular applications (conventional, full-covariance Gaussian mixture models). We emphasize that this is not a general weakness of these powerful and widely used methods, but an important limitation in the network context of interest here. The remainder of this article is organized as follows. We first introduce notation and core ideas in the learning of undirected graphical models and go on to describe the network clustering approach. We then present empirical results on fully synthetic data, synthetic data combined with phospho-proteomic data pertaining to cell signalling, and proteomic data obtained from cancer cell lines that are part of the NCI60 panel. We close with a discussion of some of the finer points and shortcomings of our work and point to some directions for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>..X N ]</head><p>, X i ∈ R p represent a data matrix, comprising N data vectors each containing measurements on p molecular players. Clustering is a form of unsupervised learning in which observations are partitioned into groups, called clusters, within which data points are related in some sense. Let k ∈{1,...,K} index clusters, and C :{1,...,N} →{1,...,K} be a cluster assignment function such that C(i) is the cluster to which the i-th data point is assigned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Gaussian Markov Random Fields</head><p>Graphical models (<ref type="bibr" target="#b9">Jordan, 2004;</ref><ref type="bibr" target="#b10">Koller and Friedman, 2009;</ref><ref type="bibr" target="#b11">Lauritzen, 1996</ref>) are a class of statistical models in which a graph, comprising vertices and linking edges, is used to describe probabilistic relationships between random variables. Markov Random Fields (MRFs) (<ref type="bibr" target="#b2">Dempster, 1972;</ref><ref type="bibr" target="#b17">Rue and Held, 2005</ref>;<ref type="bibr" target="#b23">Speed and Kiiveri, 1986</ref>) are graphical models which use an undirected graph G whose vertices are identified with variables under study. Each variable in such a model is conditionally independent of all the others given its neighbours in the graph. In the case in which the data are jointly Gaussian (i.e. all variables taken together have a multivariate Gaussian probability distribution), model structure is closely related to the covariance matrix of the Gaussian. In particular, zero entries in the inverse covariance or precision matrix (= −1 ) correspond to missing edges in the graph G. That is, ij = 0 ⇐⇒ (i,j) / ∈ E(G), where E(G) denotes the edge set of the graph G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sparse Gaussian MRFs</head><p>The structure of a Gaussian MRF may be learned by estimating the pattern of zeros in the inverse covariance matrix. The asymptotic properties of standard covariance estimators assure recovery of such structure in the large sample limit. However, at small-to-moderate sample sizes the estimation problem is known be challenging. This is especially true in the sample size/dimensionality regimes typical of multivariate data in molecular biology. While small-sample estimation of covariance structure is hard, under an assumption of sparsity, i.e. that there are relatively few edges in the graph G, effective learning can still be possible. In the last few years, a number of authors (including<ref type="bibr" target="#b0">Banerjee et al., 2008;</ref><ref type="bibr" target="#b6">Friedman et al., 2008;</ref><ref type="bibr" target="#b11">Meinshausen and Bühlmann, 2006;</ref><ref type="bibr" target="#b16">Ravikumar et al., 2010</ref>) have shown how 1-penalized approaches can be used to learn sparse MRFs under challenging conditions. Bayesian (<ref type="bibr" target="#b3">Dobra et al., 2004;</ref><ref type="bibr" target="#b8">Jones et al., 2005</ref>) and shrinkage (<ref type="bibr" target="#b19">Schäfer and Strimmer, 2005</ref>) approaches have also been proposed in the literature. The computational efficiency and emphasis on sparsity of 1-penalized approaches make them attractive in the present setting. Here, we follow in particular the approach of<ref type="bibr" target="#b6">Friedman et al. (2008) and</ref><ref type="bibr" target="#b0">Banerjee et al. (2008)</ref>. If is the precision matrix specifying a Gaussian MRF, an 1 penalty is employed to give the following penalized log-likelihood (as a function of precision matrix ):</p><formula>L() = log||−Tr(S)−ρ 1 (1)</formula><p>where, S is the sample covariance matrix and ρ a parameter that controls the overall sparsity of the solution. The desired estimate is given byˆ=byˆ byˆ= argmax L(). Then, the learning problem amounts to optimizing L over (positive definite) matrices. This is a non-trivial optimization problem. Recently,<ref type="bibr" target="#b0">Banerjee et al. (2008)</ref>introduced efficient optimization procedures for this purpose. Briefly, their approach involves deriving a dual and then exploiting work due to Nesterov (2005) on non-smooth optimization to develop an efficient algorithm. We use this algorithm here and refer the interested reader to the references for full technical details. The 1 penalty in Equation (1) resembles a matrix analogue to the Lasso which is widely used to learn sparse, high-dimensional regression models (<ref type="bibr" target="#b25">Tibshirani, 1996</ref>). Just as in the Lasso, the penalty term encourages sparse solutions which have some regression coefficients that are exactly zero; here the penalty on the inverse covariance encourages solutions which are sparse in the sense of having zero entries in and therefore relatively few edges in the corresponding graphical model. In the 'large p, small n' regime that is typical of molecular biology applications, sparsity can be statistically useful in reducing variance. In many cases, sparsity may also be a biologically valid assumption; for example, protein phosphorylation networks typically have on the order of p edges (see e.g.<ref type="bibr" target="#b24">Tan et al., 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">1-penalized network clustering</head><p>We aim to (i) discover, in an unsupervised manner, subsets of the data which share underlying network structure and (ii) characterize such subset-level network structure. Our proposal is simple: we capture subset-level network structure using 1-penalized Gaussian MRFs and carry out clustering or subset identification by an iterative scheme. Our algorithm is as follows:</p><p>1. Initialize: set t = 0, s = s max. Randomly partition data {X i } into K clusters subject to min k |{i : C(i) = k}| ≥ n min , where C(i) ∈{1...K} denotes the cluster assignment for the i-th data point, n min is a parameter controlling smallest permitted cluster size and s max is a positive constant giving an upper bound on model score (here, set to 10 10 ). 2. Estimate: estimate cluster-specific parameters { ˆ µ k , ˆ k } and store current model log-likelihood in s:</p><formula>ˆ µ k ← 1 |{i : C(i) = k}| i:C(i)=k X i ˆ k ← argmax log||−Tr(S k )−ρ 1 s old ← s s ← n i=1 log Normal(X i | ˆ µ C (i ) , ˆ −1 C (i ) )</formula><p>where S k denotes the sample covariance matrix for cluster k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Re</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.Mukherjee and S.M.Hill</head><p>algorithm is not stopped in 4), and store smallest cluster size in m:</p><formula>C tmp (i) ← argmax k∈{1...K} Normal X i | ˆ µ k , ˆ −1 k m ← min k |{i : C tmp (i) = k}| t ← t +1</formula><p>4. Iterate or terminate: stop if reach maximum number of iterations T , cluster size becomes too small or relative change in log-likelihood model score is below a threshold τ:</p><p>If t ≤ T AND m ≥ n min AND |s/s old −1| ≥ τ,</p><formula>C ← C tmp Repeat 2, 3 Else Stop Output C, ˆ k , ˆ µ k , s</formula><p>In all experiments below, we set T =100, n min =4 and τ = 10 −8. The penalty parameter ρ is set at the start of the</p><formula>ρ = (max i&gt;jˆσ i&gt;jˆ i&gt;jˆσ i ˆ σ j ) t N−2 (α) N −2+t 2 N−2 (α)</formula><formula>(2)</formula><p>wherê σ i is the sample variance for variable i and t N−2 (α), α ∈ [0,1] is the quantile function of the Student's t-distribution with N −2 degrees of freedom. Lower α gives sparser solutions (and a more biased estimate); we use a conservative choice of α = 0.1 in all experiments below. Optimization is carried out using the Banerjee/Nesterov procedure. The worst-case time complexity of our algorithm is O(TKp 4.5 //) where &gt;0 is desired accuracy for the Nesterov optimization procedure (<ref type="bibr" target="#b0">Banerjee et al., 2008</ref>). This overall approach can be thought of as analogous to a standard iterative clustering, but with sparse graphical models used to define clusters. In all our experiments, we used the best result (i.e. the highest overall model score s) over 100 random initializations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>We show results on synthetic data, mixed data in which phosphoproteomic data were combined with synthetic data and on proteomic data obtained from cancer cell lines that are part of the NCI-60 panel. We are interested in examining the ability of 1-penalized network clustering to carry out two related tasks. First, to discover data subsets defined by network structure, i.e. to cluster with respect to underlying covariance structure. Second, to recover such subsetspecific network structure. We assess the performance on both tasks below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Synthetic data</head><p>Our simulation strategy was as follows. For each of two clusters, we generated data having known sparse inverse covariance structure. We then withheld cluster labels and analysed the data in an unsupervised manner to generate the results shown here. Sparse covariance structures were specified following a procedure described in<ref type="bibr" target="#b0">Banerjee et al. (2008)</ref>. A specified number of non-zero entries were randomly, symmetrically placed to create a precision matrix (corresponding to a graphical model with a specified number of edges) and then data were generated from the corresponding Gaussian model. The number of non-zero entries was set to equal the number of variables p. In the experiments below, we set p=10 and used per-cluster sample sizes of n=20,30,40,50. To mimic the case in which biological subtypes differ subtly in terms of network structure, the cluster-specific network structures shared 6 out of 10 edges and each variable was set to have unit within-cluster variance. We are interested in the challenging case in which clusters differ at the network level, but are similar at the level of mean response, we therefore set the cluster means to differ by a small amount (0.75 in all experiments below).<ref type="figure" target="#fig_4">Figure 1</ref>shows results for 100 simulated datasets, comparing 1-penalized network clustering with (i) Kmeans; (ii) a recently introduced, message-passing-based algorithm called affinity propagation (<ref type="bibr" target="#b4">Frey and Dueck, 2007</ref>); (iii) diagonalcovariance Gaussian mixture model (GMM) using expectation– maximisation (EM); (iv) full-covariance GMM using EM; and (v) network clustering using an alternative network learning method due to<ref type="bibr" target="#b19">Schäfer and Strimmer (2005)</ref>[this is rooted in shrinkage estimation; algorithm was as above with shrinkage estimation as described in<ref type="bibr" target="#b19">Schäfer and Strimmer (2005)</ref>used in place of<ref type="bibr">]</ref>. We used the function kmeans within the MATLAB statistics toolbox (with K =2 and 1000 random initialisations). Affinity propagation was used with default settings (pair-wise similarities equal to negative Euclidean distance and scalar self-similarity set to median similarity). EM was performed using the gmmb_em function within the MATLAB GMMBAYES toolbox (<ref type="bibr" target="#b14">Paalanen et al., 2006</ref>) (with K =2, 100 random initializations and maximum number of iterations set to T =1000; other stopping criteria were the same as network clustering, namely n min =4 and τ = 10 −8 ). All computations were carried out in MATLAB R2009a. We show boxplots over the Rand index with respect to the true cluster labels (the Rand index is a standard measure of similarity between clusterings; 1 indicates perfect agreement and 0 complete disagreement). The proposed 1-penalized network clustering shows consistent gains relative to the other approaches. At all but the smallest sample size, it is able to closely approximate the correct clustering, with high Rand indices, even when other methods do not perform well. The shrinkage-based network clustering approach also performs well, but is less effective than 1-penalization at all but the largest sample size. In contrast, K-means, affinity propagation and diagonal-covariance GMMs do not perform well, even in the largest sample case. This reflects the fact that these methods are not able to describe the cluster-level covariance structure that is crucial in this setting. A full-covariance GMM provides gains over the aforementioned three methods, but is both less effective and more variable than network clustering. This is due to the fact that conventional full covariance estimators can be ill-behaved at small sample sizes (indeed, in the n = 20 case a conventional full-covariance GMM did not yield valid covariance matrices and therefore could not be employed).<ref type="figure" target="#fig_1">Figure 2</ref>compares true, clusterspecific sparsity patterns (i.e. network structures) with those inferred by network clustering. We show results for network clustering (both 1-penalized and shrinkage-based) and from a GMM. Following the suggestion of a referee, we also show results obtained from the application of 1-penalized network learning to data from the clusters obtained by a GMM and K-means. We are motivated by settings in which underlying biological heterogeneity at the network level has remained unrecognized. Then, the default approach is to carry out network inference on the entire, heterogeneous dataset, without clustering. Results are shown also for this case; to allow a fair comparison, we use the same 1-penalized network inference method as is used for network clustering. We quantified the ability of network clustering to recover clusterlevel structure by computing the SHD between true and estimated cluster-specific graphs. The SHD is equal to the number of edges that must be changed (added or deleted) to recover the true graph; lower scores indicate a closer match to the true structure.<ref type="figure" target="#fig_1">Figure 2</ref>shows SHD, as a function of sample size, for the various approaches described above. At all sample sizes, network clustering provides reductions in SHD relative to the other approaches. Shrinkage network clustering performs well in this regard at larger sample sizes, while 1 network clustering does better at the smallest sample size. In contrast, inference without clustering is unable to recover network structure, and displays no improvements with sample size, despite the use of state-of-the-art sparse 1-penalized network inference. The GMM is outperformed by network clustering methods at all sample sizes, yet provides gains over networkinference without clustering at all but the smallest sample size. (In all cases, we generated graphs from inverse covariance matrices by thresholding to induce p edges; our main conclusions were robust to the precise threshold used.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Clustering</head><formula>(a) ( b) ( c) ( d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Network reconstruction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Phospho-proteomic and synthetic data</head><p>We applied network clustering to a proteomic dataset from<ref type="bibr" target="#b18">Sachs et al. (2005)</ref>pertaining to cell signalling. We used the Sachs et al. dataset to generate network heterogeneity in the following way. First, we subsampled n=40 data points (without replacement) from the complete data (baseline data only, total of 853 samples, p = 11 phospho-proteins). Then, we merged these subsampled data with data generated using a known network structure (as described above; as above within-cluster marginal variances were unity). This gave a single, heterogeneous dataset with a total sample size of N =80. To challenge the analysis and reflect the case in which biological subtypes differ with respect to underlying signalling networks but not in terms of mean phospho-protein abundance, we set the cluster means to differ by a small amount relative to the unit marginal variance (0.75, as above).<ref type="figure" target="#fig_2">Figure 3</ref>shows clustering results obtained from the Sachs et al. phospho-proteomic data. As before, we compare 1-penalized network clustering with K-means, affinity propagation, diagonal-covariance and full-covariance GMMs and shrinkagebased network clustering (settings as described above). We show boxplots over the Rand index with respect to the true cluster labels over 100 datasets generated by subsampling as described above.<ref type="figure" target="#fig_3">Figure 4a</ref>shows SHD (computed as described above) between estimated and correct graphs 1 (these were induced from corresponding inverse covariance matrices by thresholding to return p edges; the result did not depend on precise threshold) over 100 iterations of subsampling. Network clustering provides reductions in SHD (i.e. more accurate network reconstruction) relative to the other methods used.<ref type="figure" target="#fig_3">Figure 4b</ref>–d compares the correct inverse covariance for the proteomic data 2 to estimates of it. Network clustering is able to recover the overall inverse covariance pattern or network structure. In contrast, applying 1-penalized network learning directly to the small-sample, heterogeneous data, without clustering, we find that the correct structure is obscured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Clustering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 998 994–1000</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.Mukherjee and S.M.Hill</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Network reconstruction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cancer proteomic data</head><p>We applied our network clustering method to proteomic data obtained from cell lines belonging to the National Cancer Institute's NCI-60 panel (<ref type="bibr" target="#b21">Shankavaram et al., 2007</ref>). Specifically, we used proteomic data from the 10 melanoma cell lines and 8 renal cell lines within the panel. This gave a combined dataset with two subsets corresponding to cancer type, and a total of N = 18 samples. We used data for p = 147 proteins which did not show strong differential expression between the cancer types (raw P-values under a Welch two-sample t-test all exceed 0.001; total number of proteins in original dataset was 162; in addition to the P-value threshold, 7 proteins with erroneous zero values were discarded). These proteins cover a broad range of signalling pathways (see Supplementary<ref type="figure" target="#tab_1">Table S1</ref>). These data represent a high-dimensional setting in which cancer type labels are known (but withheld from the algorithms), thereby allowing objective assessment of clustering accuracy with respect to the labels.<ref type="figure" target="#tab_1">Table 1</ref>shows clustering results obtained from these data.</p><p>Methods and settings are as described above. Despite the high dimensionality and low sample size of the data, 1-penalized network clustering is able to make a good approximation to the subtype labels. Indeed, over 100 iterations, the single highest model score s gave the correct clustering. Supplementary Figures S1 and S2 show the estimated inverse covariances for the two cancer types. We note that while 1-penalized network clustering provides gains relative to the other methods, the shrinkage-based approach does not. This is likely due to the small sample size, and echoes simulation results above. We note that a conventional full-covariance GMM approach was not applicable here as, due to the high dimensionality with respect to sample size, it does not yield valid (symmetric and positive definite) covariance matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION AND FUTURE WORK</head><p>In this article, we addressed the question of probing molecular heterogeneity in settings where (hitherto uncharacterized) biological subtypes differ with respect to network structure. We did so by introducing a network clustering approach which takes advantage of the recent developments in efficient, 1-penalized learning of undirected graphical models. The approach proposed permits both the discovery of biological subtypes which differ at the network level and inference about underlying network structures. The ideas presented here are closely related to a number of classical multivariate methods. We can think of network clustering as an unsupervised quadratic discriminant analysis with clusterconditional models having sparse inverse covariances. Unfavourable conditions of dimensionality and sample size are exacerbated in clustering-type analyses because the limiting factor becomes not just the overall sample size (which may already be small), but rather the size of the smallest cluster (or, from a mixture model point of view, the smallest mixing coefficient). For this reason, some form of sparsity, regularization or shrinkage is crucial if fullcovariance models are to be used for network clustering. From this point of view, the approach taken here, of emphasizing sparsity by the use of 1-penalization, is just one of several possible choices, but in the biological setting, where sparsity is arguably a reasonable assumption, an attractive one. An alternative approach to the 'hard' assignments used here would be a EM algorithm; this would yield in effect a mixture model formulation with sparse covariance structure. We also note that since the approach presented here builds on a well-understood, probability model-based clustering framework, many existing diagnostic procedures (e.g. for statistical significance, determining number of clusters, etc.) are readily applicable. The network clustering problem is challenging, but one that we believe will become increasingly important as emphasis shifts from individual genes/proteins to networks, motivating covariancerather than mean-centred formulations. In this article, we sought to formulate and address this problem in a tractable way and Page: 999 994–1000</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network clustering</head><p>(a) Structural Hamming Distance (b) Correct sparsity pattern (c) NC: l 1 (d) All data (no clustering)-l 1<ref type="figure">5</ref>. Simulated data, clustering results for K = 3,4 clusters. Boxplots over the Rand index with respect to true cluster membership are shown for data consisting of (a) K = 3 clusters and (b) K = 4 clusters, with per cluster sample size of n = 50. Data were generated from known sparse network models (for details see text), with 25 iterations carried out at each sample size. Results shown for K-means (KM), affinity propagation (AP), diagonalcovariance Gaussian mixture model<ref type="bibr">[</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S.Mukherjee and S.M.Hill</head><p>of gene regulatory and cell signalling networks and have been well developed for these applications in recent years. For this reason, we think that a promising line of research will be in developing BN-based network clustering approaches which extend the work presented here towards directed models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.2.</head><figDesc>Fig. 2. Simulated data, network reconstruction. Distance between true and inferred networks, in terms of the number of edge differences or 'Structural Hamming Distance (SHD; smaller values indicate closer approximation to true network) for simulated data at sample sizes of n = 20,30,40,50 per cluster. Results shown for: 1-penalized network inference applied to complete data, without clustering ('All Data &amp; L 1 '); K-means clustering followed by 1-penalized network inference applied to the clusters discovered ('KM &amp; L 1 '); clustering using a (full covariance) Gaussian mixture model followed by 1-penalized network inference ['GMM (full) &amp; L 1 ']; full covariance GMM ['GMM (full)']; network clustering using 1-penalized network inference ('NC:L 1 '); and network clustering using shrinkage-based network inference ('NC:shrink'). Mean SHD over 100 iterations are shown, and error bars indicate SEM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.3.</head><figDesc>Fig. 3. Phospho-proteomic and synthetic data, clustering results. Boxplots over the Rand index with respect to true cluster membership (score of unity indicates perfect agreement with the true clustering). Data with a known, gold-standard cluster assignment were created using phosphoproteomic data from Sachs et al. (2005) as described in text. Boxplots are over 100 subsampling iterations; per-cluster sample size was n=40; algorithms used were K-means (KM), affinity propagation (AP), diagonalcovariance Gaussian mixture model [GMM (diag)], full-covariance GMM [GMM (full)], network clustering using shrinkage-based network inference (NC:shrink) and 1-penalized network clustering (NC:L 1 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.4.</head><figDesc>Fig. 4. Phospho-proteomic and synthetic data, network reconstruction. (a) SHD between correct and inferred networks. Results shown for 1-penalized network inference applied to complete data, without clustering ('All Data &amp; L 1 '); K-means clustering followed by 1-penalized network inference applied to the clusters discovered ('KM &amp; L 1 '); clustering using a (full covariance) Gaussian mixture model followed by 1-penalized network inference [GMM (full) &amp; L 1 ']; clustering and network inference using a GMM ['GMM (full)']; network clustering using 1-penalized network inference ('NC:L 1 '); and network clustering using shrinkage-based network inference ('NC:shrink'). Mean SHD over 100 subsampling iterations were shown, and error bars indicate SDs; (b) Correct sparsity pattern. Correct, large-sample sparsity pattern for proteomic data of Sachs et al. (2005); (c) NC:l 1. Inverse covariance recovered from small-sample, heterogenous data by 1-penalized network clustering and (d) All data (no clustering)-l 1. Inverse covariance from 1-penalized network inference applied directly to the complete, heterogenous data (see text for full details; per-cluster sample size n=40; red and blue indicate negative and positive values, respectively).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Table1.</head><figDesc>Clustering results for proteomic data from cancer cell lines</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.</head><figDesc>Fig. 5. Simulated data, clustering results for K = 3,4 clusters. Boxplots over the Rand index with respect to true cluster membership are shown for data consisting of (a) K = 3 clusters and (b) K = 4 clusters, with per cluster sample size of n = 50. Data were generated from known sparse network models (for details see text), with 25 iterations carried out at each sample size. Results shown for K-means (KM), affinity propagation (AP), diagonalcovariance Gaussian mixture model [GMM (diag)], full-covariance GMM [GMM (full)], network clustering using shrinkage-based network inference (NC:shrink) and 1-penalized network clustering (NC:L 1 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>assign: re-assign data points to clusters using a temporary cluster assignment function C tmp (assignment becomes permanent if</figDesc><table>Page: 996 994–1000 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>procedure following Banerjee et al. (2008) [see also Meinshausen and Bühlmann (2006) for further details],</figDesc><table></table></figure>

			<note place="foot">© The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="1"> We note that since the models used here are continuous but undirected they differ in formulation to the discrete, directed Bayesian network models used in Sachs et al. (2005). Moreover, we used only baseline proteomic data without any perturbations. As a result, the network structure shown here is similar but not identical to the directed graphs shown in the original reference. 2 Since the true covariance structure is not known for these proteomic data, we obtained the structure labelled as &apos;correct&apos; by applying network inference to the full sample of 853 data points. On account of the large sample size, this should closely approximate the true (population) covariance.</note>

			<note place="foot">Mean Rand indices ±SD for K-means (KM), affinity propagation (AP), diagonalcovariance Gaussian mixture model [GMM (diag)], shrinkage-based network clustering (NC:shrink) and 1-penalized network clustering (NC: 1 ); results shown over 100 iterations, each with 100 random initializations (*except for AP which is a deterministic algorithm). Small sample proteomic data, obtained from melanoma and renal cell lines from the NCI60 panel, were clustered as described in Main Text (p = 147 proteins, full list in Supplementary Table S1; total sample size N = 18). Rand indices were calculated with respect to labels indicating the known cancer type (melanoma or renal) of each sample. therefore focused on the simplest case of two clusters. However, the approaches introduced here apply also to the general K-cluster case. Figure 5 shows network clustering applied to simulated data from K = 3,4 clusters [simulation regime as above, with all clusters sharing 6 out of 10 edges and cluster-specific means differing by a small amount (0.75, as above) from a zero mean cluster]. We based our approach on continuous, undirected graphical models, exploiting recent results in computationally efficient, optimization-based learning for these models. Directed graphical models, including Bayesian networks (BNs) and their variants, have been popular in the study of biological networks (including Husmeier, 2003; Mukherjee and Speed, 2008; Sachs et al., 2005; Segal et al., 2003; Yu et al., 2004). However, structural inference for BNs remains challenging and for networks of even moderate size, is typically computationally intensive. Since in the network clustering setting, efficient inference is essential to render the overall analysis tractable, we chose to eschew BNs here. (For example, for the cancer proteomic data, a single round of 1-penalized network inference for p = 147 variables required 7 s on a standard desktop computer; BN-based network inference for p = 147 variables would require minutes to many hours depending on the precise approach employed.) However, BNs are in many ways well suited to the study</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Banerjee</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="485" to="516" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Network-based classification of breast cancer metastasis</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">Y</forename>
				<surname>Chuang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Syst. Biol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Article. 140</note>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Covariance selection</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">P</forename>
				<surname>Dempster</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="157" to="175" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Sparse graphical models for exploring gene expression data</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Dobra</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Multivar. Anal</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="196" to="212" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Clustering by passing messages between data points</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">J</forename>
				<surname>Frey</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Dueck</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">315</biblScope>
			<biblScope unit="page" from="972" to="976" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Inferring cellular networks using probabilistic graphical models</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Friedman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">303</biblScope>
			<biblScope unit="page" from="799" to="805" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Sparse inverse covariance estimation with the graphical lasso</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Friedman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="432" to="441" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Reverse engineering of genetic networks with Bayesian networks</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Husmeier</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biochem. Soc. Trans</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1516" to="1518" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Experiments in stochastic computation for high-dimensional graphical models</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Jones</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Sci</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="388" to="400" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Graphical models</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">I</forename>
				<surname>Jordan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Sci</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="140" to="155" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<monogr>
		<title level="m" type="main">Probabilistic Graphical Models: Principles and Techniques</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Koller</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Friedman</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Graphical Models High-dimensional graphs and variable selection with the Lasso</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">L</forename>
				<surname>Lauritzen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Meinshausen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Bühlmann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1436" to="1462" />
			<date type="published" when="1996" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Network inference using informative priors</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Mukherjee</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">P</forename>
				<surname>Speed</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="14313" to="14318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Smooth minimization of non-smooth functions</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Nesterov</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Prog</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="127" to="152" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature representation and discrimination based on Gaussian mixture model probability densities–practices and algorithms</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Paalanen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1346" to="1358" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Molecular portraits of human breast tumours</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Perou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">406</biblScope>
			<biblScope unit="page" from="747" to="752" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">High-dimensional Ising model selection using 1-regularized logistic regression</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ravikumar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1287" to="1319" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<monogr>
		<title level="m" type="main">Gaussian Markov Random Fields: Theory and Applications</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Rue</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Held</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Chapman &amp; Hall/CRC</publisher>
			<pubPlace>Boca Raton, FL, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Causal protein-signaling networks derived from multiparameter single-cell data</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Sachs</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="page" from="523" to="529" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">A shrinkage approach to large-scale covariance matrix estimation and implications for functional genomics</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Schäfer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Strimmer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Appl. Genet. Mol. Biol</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Module networks: identifying regulatory modules and their condition-specific regulators from gene expression data</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Segal</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="166" to="176" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Transcript and protein expression profiles of the NCI60 cancer cell panel: an integromic microarray study</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Shankavaram</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Cancer Ther</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="820" to="832" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Gene expression patterns of breast carcinomas distinguish tumor subclasses with clinical implications</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Sørlie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA, 98</title>
		<meeting>. Natl Acad. Sci. USA, 98</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="10869" to="10874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Gaussian Markov distributions over finite graphs</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">P</forename>
				<surname>Speed</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">T</forename>
				<surname>Kiiveri</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="138" to="150" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Comparative analysis reveals conserved protein phosphorylation networks implicated in multiple diseases</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Tan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Signal</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Training set expansion: an approach to improving the reconstruction of biological networks from limited and uneven reliable interactions</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Yip</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Gerstein</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="243" to="250" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Advances to Bayesian network inference for generating causal networks from observational biological data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Yu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="3594" to="3603" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Getting connected: analysis and principles of biological networks</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Zhu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genes Dev</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1010" to="1024" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>