
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining Unsupervised discovery of information structure in biomedical documents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Douwe</forename>
								<surname>Kiela</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Laboratory</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<postCode>CB3 0FD</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Yufan</forename>
								<surname>Guo</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Laboratory</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<postCode>CB3 0FD</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Ulla</forename>
								<surname>Stenius</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Environmental Medicine</orgName>
								<orgName type="institution">Karolinska Institutet</orgName>
								<address>
									<postCode>SE-171 77</postCode>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Anna</forename>
								<surname>Korhonen</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Laboratory</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<postCode>CB3 0FD</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining Unsupervised discovery of information structure in biomedical documents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btu758</idno>
					<note type="submission">Received on May 14, 2014; revised on October 24, 2014; accepted on November 10, 2014</note>
					<note>*To whom correspondence should be addressed. Associate Editor: Jonathan Wren Contact: alk23@cam.ac.uk</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Information structure (IS) analysis is a text mining technique, which classifies text in biomedical articles into categories that capture different types of information, such as objectives, methods, results and conclusions of research. It is a highly useful technique that can support a range of Biomedical Text Mining tasks and can help readers of biomedical literature find information of interest faster, accelerating the highly time-consuming process of literature review. Several approaches to IS analysis have been presented in the past, with promising results in real-world bio-medical tasks. However, all existing approaches, even weakly supervised ones, require several hundreds of hand-annotated training sentences specific to the domain in question. Because bio-medicine is subject to considerable domain variation, such annotations are expensive to obtain. This makes the application of IS analysis across biomedical domains difficult. In this article, we investigate an unsupervised approach to IS analysis and evaluate the performance of several un-supervised methods on a large corpus of biomedical abstracts collected from PubMed. Results: Our best unsupervised algorithm (multilevel-weighted graph clustering algorithm) performs very well on the task, obtaining over 0.70 F scores for most IS categories when applied to well-known IS schemes. This level of performance is close to that of lightly supervised IS methods and has proven sufficient to aid a range of practical tasks. Thus, using an unsupervised approach, IS could be applied to support a wide range of tasks across sub-domains of biomedicine. We also demonstrate that unsupervised learning brings novel insights into IS of biomedical literature and discovers information categories that are not present in any of the existing IS schemes. Availability and Implementation: The annotated corpus and software are available at</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent developments in the areas of natural language processing, machine learning and data mining have led to considerable progress in Biomedical Text Mining (Bio-TM) (<ref type="bibr" target="#b4">Chapman and Cohen, 2009;</ref><ref type="bibr" target="#b16">Harmston et al., 2010;</ref><ref type="bibr" target="#b24">McDonald et al., 2012;</ref><ref type="bibr" target="#b32">Simpson and Demner-Fushman, 2012</ref>), producing highly useful techniques that enable users to retrieve and extract information from scientific text easily and efficiently. Information structure (IS) analysis is a highly useful TM technique that has attracted increasing attention recently. It aims to classify units of text (typically sentences) into a fixed number of categories, which capture different types of information in text (<ref type="bibr" target="#b14">Guo et al., 2011b;</ref><ref type="bibr" target="#b43">Webber et al., 2011</ref>). For instance, IS categories could capture the aim of the study or a type of the study (e.g. animal, human and in vitro), the design of experiments (e.g. exposure length, V C The Author 2014. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com dose and group size), the results obtained (e.g. end points, positive or negative results), the conclusions drawn and so forth. To date, several IS schemes have been proposed for the analysis of scientific documents. There are coarse-grained schemes that stem from typical section headings seen in academic journals (<ref type="bibr" target="#b0">Agarwal and Yu, 2009;</ref><ref type="bibr" target="#b35">Sollaci and Pereira, 2004</ref>), as well as finer-grained schemes. Among the latter are those based on the rhetorical structure and scientific argumentation of a scientific paper [namely argumentative zones (AZs)] (<ref type="bibr" target="#b12">Guo et al., 2010;</ref><ref type="bibr" target="#b26">Mizuta et al., 2006;</ref><ref type="bibr" target="#b39">Teufel et al., 1999</ref><ref type="bibr" target="#b40">Teufel et al., , 2009</ref>), the qualitative dimensions or the properties of factual information (e.g. focus, polarity, certainty, evidence and directionality) (<ref type="bibr" target="#b44">Wilbur et al., 2006</ref>), the distinction between different types of evidence in empirical studies (e.g. explicit versus implicit claims, correlations, comparisons and observations) (<ref type="bibr" target="#b2">Blake, 2009</ref>) and the content and conceptual framework for a scientific investigation (<ref type="bibr" target="#b20">Liakata et al., 2010</ref>), among others. Automatic labeling of biomedical text according to IS has many practical applications. It has been shown to support a variety of BioTM tasks (e.g. information retrieval, information extraction and text summarization) (<ref type="bibr" target="#b5">Contractor et al., 2012;</ref><ref type="bibr" target="#b26">Mizuta et al., 2006;</ref><ref type="bibr" target="#b30">Ruch et al., 2007;</ref><ref type="bibr" target="#b37">Tbahriti et al., 2006;</ref><ref type="bibr" target="#b38">Teufel and Moens, 2002</ref>) and manual study of biomedical literature. For example,<ref type="bibr" target="#b13">Guo et al. (2011a)</ref>reported a speed reading test where IS annotations significantly speeded up literature review in cancer risk assessment. However, the usefulness of IS analysis has been limited by the fact that existing approaches suffer from poor portability. Biomedicine is subject to considerable sub-domain variation at different levels of linguistic description (<ref type="bibr" target="#b22">Lippincott et al., 2011</ref>). Application of IS analysis to different sub-domains of biomedicine (e.g. chemistry, molecular biology and cancer research) has required the development of domain-specific training and evaluation data (<ref type="bibr" target="#b12">Guo et al., 2010;</ref><ref type="bibr" target="#b40">Teufel et al., 2009</ref>). Created by hand, such data are very expensive to develop and are only available for a handful of sub-domains. Recent research has investigated reducing the need for annotations via lightly supervised learning (e.g. active learning) of IS. This research has produced useful results—accurate enough to support high-speed literature review (<ref type="bibr" target="#b14">Guo et al., 2011b</ref><ref type="bibr" target="#b15">Guo et al., , 2013</ref>). However, such an approach still requires several hundreds of annotated sentences for optimal performance. This is unrealistic as biomedical literature is not only varied in terms of domains but also highly dynamic (<ref type="bibr" target="#b25">Miha ˘ ila ˘ et al., 2012</ref>). This article investigates whether unsupervised methods could be realistic for IS analysis. Such methods have the distinct advantage of removing the need for any IS annotation. Not only would this allow for easily applying the approach to the many (sub-)domains where annotated data are not readily available, but it could also lead to the discovery of novel, undefined information categories emerging from the data. We experiment with a large corpus of biomedical abstracts annotated according to two different IS schemes: section names (SN) and AZs. We apply to this corpus two canonical clustering algorithms— spherical k-means (<ref type="bibr" target="#b45">Zhong, 2005</ref>) and Expectation MaximizationGaussian Mixture Model (EM-GMM) (<ref type="bibr" target="#b7">Dempster et al., 1977</ref>)—as well as the-state-of-the-art multilevel-weighted graph clustering algorithm (<ref type="bibr" target="#b11">Dhillon et al., 2007</ref>), which is an efficient approximation of the very popular spectral clustering algorithm. The latter algorithm has the added advantage that it avoids expensive eigenvector computation by exploiting weighted kernel k-means to locally optimize graph clustering objectives (<ref type="bibr" target="#b9">Dhillon et al., 2004</ref>). Using a selection of clustering evaluation metrics, we evaluate these algorithms on the two IS schemes, comparing them against fully supervised and weakly supervised algorithms. Our unsupervised approach performs surprisingly well, especially when multilevel-weighted graph clustering is applied to the SN scheme: we obtain over 0.70 F scores for most categories—performance which is close to that of lightly supervised methods and which has proven sufficient to aid practical tasks in biomedicine. We also demonstrate that unsupervised learning brings novel insights into IS of biomedical literature and discovers information categories that are not present in any of the existing IS schemes. There is only one previous study in unsupervised IS analysis, which focused on detecting SNs (<ref type="bibr" target="#b42">Varga et al., 2012</ref>). They used probabilistic graphical models and reported an F score of 35%, which is rather low and unlikely to be used in real-world tasks. Our results suggest that if we use more sophisticated unsupervised techniques and better features, IS analysis could realistically be used to benefit a much wider range of tasks in biomedicine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>We experiment with two well-known and widely used IS schemes. The first is the SN scheme, which is grounded on SNs found in some scientific abstracts. We use the four-way classification from<ref type="bibr" target="#b17">Hirohata et al. (2008)</ref>, where abstracts are divided into Objective-, Method-, Result-and Conclusion-type sentences. The second is argumentative zoning (AZ)—a scheme originally introduced by Teufel and Moens (2002). AZ provides an analysis of the argumentative structure of a document, following the knowledge claims made by authors. We use the version of AZ developed for biology papers (<ref type="bibr" target="#b26">Mizuta et al., 2006</ref>), with the same modifications as in<ref type="bibr" target="#b12">Guo et al. (2010)</ref>. Our experiments use the dataset by<ref type="bibr" target="#b12">Guo et al. (2010)</ref>, which consists of 1000 biomedical abstracts with a total of 7985 sentences, annotated according to both the SN and AZ schemes.<ref type="bibr" target="#b12">Guo et al. (2010)</ref>reported a high inter-annotator agreement of j ¼ 0:85 for their three annotators: one linguist, one computational linguist and one domain expert. These two IS annotation schemes were selected because they are good examples of a coarse-grained scheme that typically relies on section headings (SN) and a more fine-grained scheme based on scientific rhetorical structure (AZ), respectively. The two schemes with their respective categories are listed in<ref type="figure" target="#tab_1">Table 1</ref>.<ref type="figure" target="#tab_2">Table 2</ref>presents the distribution of scheme-annotated sentences in the corpus. Although there is a subsumption relation between the schemes (<ref type="bibr" target="#b12">Guo et al., 2010</ref>), SN as a coarser-grained version of AZ is still worth investigating because it is widely used in academic writing most notably in the field of biomedicine (<ref type="bibr" target="#b35">Sollaci and Pereira, 2004</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Automatic classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Feature selection</head><p>To apply our algorithms, we need to first select a set of features which may indicate which category in our scheme is appropriate. We follow<ref type="bibr" target="#b12">Guo et al. (2010)</ref>in implementing a set of features that have proved successful in related works (e.g.<ref type="bibr" target="#b38">Teufel and Moens 2002;</ref><ref type="bibr" target="#b14">Guo et al. 2011b</ref><ref type="bibr" target="#b15">Guo et al. , 2013</ref><ref type="bibr" target="#b17">Hirohata et al. 2008;</ref><ref type="bibr" target="#b21">Lin et al. 2006;</ref><ref type="bibr" target="#b27">Mullen et al. 2005</ref>):@BULLET Part-of-Speech (POS): the POS tag of each verb [Penn Treebank tagset (<ref type="bibr" target="#b31">Santorini, 1990)]</ref>, as an indicator of verb tense. @BULLET Grammatical relation (GR): subject, direct object, indirect object and second object relations between verbs and nouns. @BULLET Subj/Obj: the subjects/objects appearing with any verbs. @BULLET Voice: the voice of verbs.</p><p>These features were extracted from the corpus using a number of tools, including a tokenizer that detects sentence boundaries and performs basic tokenization (designed specifically for handling complex biomedical terms, e.g. 2-amino-3,8-diethylimidazo<ref type="bibr">[4,5f]</ref>quinoxaline), the C&amp;C tools (<ref type="bibr" target="#b6">Curran et al., 2007</ref>) for POS tagging and parsing (the output was used for extracting aforementioned lexical and syntactic features) and the Morpha lemmatizer (http://svn.ask.it.usyd.edu.au/trac/candc) for lemmatizing the lexical items for all the features. Verb classes were obtained automatically using unsupervised spectral clustering (<ref type="bibr" target="#b36">Sun and Korhonen, 2009</ref>). To reduce data sparsity, we removed words and GRs with fewer than two occurrences and bi-grams with fewer than five occurrences. The combination of these features leads to the large number of 26 459 components per feature vector. We do not include the feature vectors of surrounding sentences, as some previous studies have done. The feature vectors are binary and tend to be sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Unsupervised algorithms</head><p>We use two canonical clustering algorithms in the form of k-means and EM-GMM and one state-of-the-art algorithm that is an efficient approximation of the very popular spectral clustering algorithm in the form of multilevel-weighted graph clustering. We have selected three algorithms that can be argued to be particularly well suited for the task at hand: they have been shown to perform well on text data and are efficient enough to compute with such large vectors (which is not the case, e.g. spectral clustering). Spherical k-means The k-means algorithm (<ref type="bibr" target="#b23">MacQueen, 1967</ref>) is very well known. Here we apply a version called spherical k-means (<ref type="bibr" target="#b45">Zhong, 2005</ref>), which differs from traditional k-means in having a different objective function: instead of minimizing Euclidean distance, we maximize cosine similarity, which is defined as follows:</p><formula>cosineðv 1 ; v 2 Þ ¼ v 1 Á v 2 jv 1 jjv 2 j (1)</formula><p>where v 1 and v 2 are feature vectors. The objective function thus becomes:</p><formula>argmax S X k i¼1 X xj2Si cosineðl i ; x j Þ (2)</formula><p>where S are the clusters, k is the number of clusters, x j is a data point and l i is a centroid. It has been found that cosine similarity is a much better metric on large amounts of sparse data, which tends to be the case for text mining features, than Euclidean distance (<ref type="bibr" target="#b8">Dhillon and Modha, 2001</ref>). EM-GMM The EM algorithm (<ref type="bibr" target="#b7">Dempster et al., 1977</ref>) has a long history and is also well known in the unsupervised learning literature. In a probabilistic clustering scenario, we assume that the underlying distribution is a GMM and employ the EM algorithm to obtain the maximum (log-)likelihood estimate of the parameters:</p><formula>ln pðxjp; l; RÞ ¼ X N n¼1 ln X K i¼1 p i N ðxjl i ; R i Þ ( )</formula><formula>(3)</formula><p>where x is a D-dimensional vector, p i ; i ¼ 1;. .. ; K are the mixture weights and N defines the component Gaussian densities. This approach allows us to capture uncertainty in cluster assignments. Final assignments are then computed according to the highest probability for each of the data points. Multilevel-Weighted Graph Clustering Two clustering methods that have recently gained attraction (especially for data that is not linearly separable) are kernel k-means and spectral clustering. It has been found that they have equivalent objective functions, i.e. a general weighted kernel k-means objective is mathematically equivalent to a weighted graph clustering objective (<ref type="bibr" target="#b11">Dhillon et al., 2007</ref>). This fact can be exploited by a multilevel algorithm that directly optimizes various weighted graph clustering objectives, such as the popular ratio cut, normalized cut and ratio association criteria (Dhillon et al.,2005). The resultant algorithm is very fast: it eliminates the need for any eigenvector computation for graph clustering problems, which can be prohibitive for very large graphs, as is the case with our dataset. We used this algorithm as an approximation of spectral clustering (<ref type="bibr" target="#b11">Dhillon et al., 2007</ref>), for which the objective function is:</p><formula>max ~ Y traceð ~ Y T W À1=2 AW À1=2 ~ YÞ</formula><p>where A is an adjacency (similarity) matrix, W is a diagonal matrix of the weight/degree of each cluster and ~ Y is an orthonormal matrix that indicates the cluster membership and that is proportional to W 1=2. The problem can be solved using an efficient local search algorithm with good scalability. We used an open source implementation of the algorithm called Graclus (http://www.cs.utexas.edu/users/dml/Software/graclus.html) for multilevel-weighted graph clustering. For the other two algorithms, we used MATLAB implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Evaluation</head><p>Evaluation of clustering results is difficult. In the case of supervised or weakly supervised learning, evaluation is relatively easy: we have a one-to-one mapping of predicted labels to expected labels. In the case of unsupervised learning, however, we have a set of cluster assignments on the one hand and a set of expected labels on the other. The evaluation is thus complicated by the lack of an adequate mapping between the two sides, for how to decide whether a given cluster assignment is the 'right' cluster for an expected label in the gold standard. To mitigate this problem, we elected to evaluate using a variety of metrics and to choose the best clustering based on the combination of these metrics: V measure The V measure is calculated on the basis of two criteria: homogeneity and completeness (<ref type="bibr" target="#b29">Rosenberg and Hirschberg, 2007</ref>). To satisfy the homogeneity criterion, a clustering must assign only those data points that are members of a single class to a single cluster. That is, a perfectly homogeneous clustering has an injective mapping C ! K, where C is the set of classes and K is the set of clusters. To satisfy the completeness criterion, a clustering must assign all of those data points that are members of a single class to a single cluster. That is, a perfectly complete clustering has an injective mapping K ! C. Homogeneity is defined as:</p><formula>hðnÞ ¼ 1 HðC; KÞ ¼ 0 1 À HðC j KÞ HðCÞ else 8 &gt; &lt; &gt; : (4)</formula><p>where</p><formula>HðC j KÞ ¼ À X jKj k¼1 X jCj c¼1 a ck N log a ck X jCj c¼1 a ck (5)</formula><p>a ck is the number of data points that are members of class c and cluster k and</p><formula>HðCÞ ¼ À X jCj c¼1 X jKj k¼1 n log X jK k¼1 n (6)</formula><p>Completeness is then defined as the 'reciprocal' of homogeneity:</p><formula>cðnÞ ¼ 1 HðK; CÞ ¼ 0 1 À HðK j CÞ HðKÞ else 8 &gt; &lt; &gt; : (7)</formula><p>The V measure is simply the harmonic mean between homogeneity and completeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Purity</head><p>Purity is a simple and transparent evaluation measure that is often used for clustering (<ref type="bibr" target="#b1">Amigó et al., 2009</ref>). To compute it, each cluster is assigned to the class that is most frequent in the cluster and then the accuracy of this assignment is measured by counting the number of correctly assigned classes and divided by the total number of data points:</p><formula>PurityðC; KÞ ¼ 1 N X K j¼1 max Ci2C ðjK j \ C i jÞ (8)</formula><p>F measure The F measure is traditionally defined as the weighted harmonic mean between precision and recall (van Rijsbergen, 1974). In our case, we do not have a one-to-one mapping between predicted and expected labels, so instead we take the weighted mean of the highest combined precision and recall per class-cluster mapping, which we called the macro-F (<ref type="bibr" target="#b1">Amigó et al., 2009</ref>):</p><formula>F macro ðC; KÞ ¼ X C i¼1 jC i j N max Kj2K FðC i ; K j Þ (9)</formula><p>where</p><formula>FðC i ; K j Þ ¼ 2 Â RecallðC i ; K j Þ Â PrecisionðC i ; K j Þ RecallðC i ; K j Þ þ PrecisionðC i ; K j Þ (10)</formula><p>As the F measure is probably the best known amongst our evaluation metrics, especially outside of the unsupervised learning community, we will focus on the macro-F measure. When we report F scores for individual class-cluster mappings, we will call these micro-F scores to distinguish them.<ref type="figure" target="#tab_3">Table 3</ref>presents the results for our three unsupervised algorithms, compared with a baseline that maximizes homogeneity by assigning all sentences to the same cluster, which is a much better baseline than chance. Experiments were done with 2 k 20. We report the results for the highest performing k per algorithm. With k set to the number of categories (which would allow for a true 1-1 mapping), we observe the same performance per algorithm as described below. For the SN scheme, multilevel-weighted graph clustering performs best, with a macro F measure of 0.65, which is very high for unsupervised learning. Spherical k-means comes second, judging by V measure and Purity but is not as accurate when measured by macro-F. EM-GMM does not do particularly well, which we suspect is due to sparsity issues. For AZs, we observe a similar pattern when we look at the macro-F score: multilevel-weighted graph clusteringUnsupervised discovery of information structureperforms best, followed by spherical k-means, followed by EM-GMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Class-based analysis</head><p>To investigate the clustering results more closely, we look into the dominant class per cluster. Through a winner takes all approach (i.e. each cluster is labeled with its dominant class), we can calculate micro-F scores for each of the individual classes.<ref type="figure" target="#tab_4">Table 4</ref>presents the F scores for the best performing algorithm with the optimal k for the two schemes. The micro-F scores per class indicate that we are able to obtain very high scores for some of our classes, which are comparable to the weakly supervised scores obtained by Guo et al. (2011b). With the exception of the Method class, the SN scheme F scores are all over 0.70. Although all available classes are identified for the SN scheme, in the AZ scheme, only four out of the seven available classes are ever dominant in a cluster. However, we have found that increasing k allows the other classes to become dominant in at least one of the clusters as well, although identifying Future work and Related work is still a bit challenging, which is not surprising as there are relatively few sentences with these labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature-based analysis</head><p>As explained in Section 2.2.1, we made use of a collection of features, which have been shown to perform well in previous supervised and lightly supervised experiments. We conducted further analysis to investigate which of these feature types are the most (and the least) useful for unsupervised learning. We took the best-performing algorithm and conducted a leave-one-out analysis of the features.<ref type="figure" target="#tab_5">Table 5</ref>presents the F scores per excluded feature type. We can clearly observe that the most important feature types are location, word and voice. It makes sense that word is so important as it is the basic unit of a text corpus. Location and voice are good indicators of certain information categories as explained in Section 2.2.1. Bi-grams seem to be the least useful probably due to the problem of sparse data. These findings are generally in line with those reported in<ref type="bibr">Guo et al. (2011a, b</ref>) on the same dataset, except for the word feature that plays a less important role in fully or lightly supervised IS analysis. A possible explanation could be that features such as verb, subj and obj are actually subsets of the word feature that may compensate for the absence of word features when provided with highly informative annotations. We can now combine the leave-one-out analysis with the dominant class-based analysis to see which features are more or less important for each of the class labels. To save space, we only report results for the SN scheme, on which we have obtained the highest scores.<ref type="figure" target="#tab_6">Table 6</ref>lists the results. Interestingly, we observe that the importance of the location and word features depends on the class: location seems to be particularly important for the Objective class, whereas word features are very important for the Method class. The verb feature type is very important for the Objective class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Error analysis</head><p>In this section, we examine the errors that the algorithm makes more closely. In the case of AZs, it is unsurprising that we have difficulty with the Future-work and Related-work classes for the obvious reason that they are relatively infrequent in the corpus (as presented in<ref type="figure" target="#tab_2">Table 2</ref>). Furthermore, they are arguably very fine-grained classes that could be said to overlap with the Background class present in AZs. The more interesting question concerns the Method class: why do we get scores above 0.70 for all other classes but a much lower score for this class?<ref type="figure" target="#tab_7">Table 7</ref>presents the confusion matrix for the best-performing algorithm. We can clearly observe that the Method class is confused disproportionately often with the Result class. In fact, out of the 1300 sentences that should have been labeled Method, almost half of them have been labeled Result. This problem is not unique to our approach: the same phenomenon can be observed in weakly supervised and even in supervised learning.<ref type="figure" target="#tab_8">Table 8</ref>presents some example method sentences and how they were classified. One might say that the errors are understandable, considering the misleading key words (e.g. 'detected') or the various numerical values that are frequently seen in the Result class.A possible solution to this problem would be to assign higher weights to verb features that are more crucial to the meaning of a sentence and that can possibly make a better distinction between the Method and Result sentences.<ref type="figure" target="#tab_9">Table 9</ref>lists other common errors such as CON being misclassified as RES and OBJ being misclassified as METH. These errors often occur in complex sentences where different clauses/constituents carry different types of information. Under the current schemes, a unique information category is assigned to a full sentence according to the distribution or the priority of information contained in the sentence. However, in the case of machine learning, a computer may be confused by the contradictory evidence in a complex sentence (e.g. 'increased' for RES versus 'may be related' for CON, 'investigated' for OBJ versus 'treated' for METH). A potential solution would be to split the text into smaller units than sentences for more accurate IS analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Algorithm performance</head><p>The best performing algorithm is undoubtedly multilevel-weighted graph clustering. It achieves a macro-F score of .65, which is very high for an unsupervised approach. In comparison, the approach by<ref type="bibr" target="#b14">Guo et al. (2011b)</ref>, which was weakly supervised but still required a large proportion of the data (thousands of labeled sentences) for optimal performance, achieved a macro-F score of 0.81. Multilevelweighted graph clustering is preferable to the other unsupervised algorithms not only because of accuracy but also because it is faster than any of the others [unlike EM or k-means that works on a random initialization, the multilevel algorithm ensures a good initial clustering at each level and its convergence rate is better (<ref type="bibr" target="#b11">Dhillon et al., 2007)]</ref>. It is perhaps slightly surprising that EM-GMM does not perform very well. We speculate that this is due to sparsity issues: it has been commented in the past that EM-GMM cannot deal very well with sparseness (<ref type="bibr" target="#b28">Neal and Hinton, 1999</ref>). Our data are particularly sparse, because all features are binary. Running EM-GMM on a subset of the features, such as location, voice and POS-tag feature types, yields better performance but is still not comparable to that of multilevel-weighted graph clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discovering novel IS categories</head><p>As mentioned above, unsupervised learning techniques may also yield novel insight into the appropriate information categories for biomedical literature. Interestingly, one of the clusters appears to be very 'noisy' compared with the others, which might indicate that the clustering finds an information category that is not present in our labeling scheme. Cluster 1, as we have called it, contains 889 sentences, of which 225 are Objective (25%), 350 are Result (39%), 204 are Method (23%) and 83 are Conclusion (9%). Using the winner-takes-all approach, we would assign this cluster the dominant label Result. However, these percentages show that this is not really a Result cluster at all. To further examine the properties of this cluster, we looked at the sentences closest to the cluster centroid. Most sentences describe experimental data, either as reporting the outcome of an experiment (Result), the goal or rationale of an experiment (Objective) or the methods used in an experiment (Method). Some example sentences are listed in<ref type="figure" target="#tab_0">Table 10</ref>. This raises the distinct possibility thatin vivo were identified by comparison with the products formed from the reaction of the individual epoxides with 2'-deoxyguanosine (dG). 2 RES METH B6C3F1 lacI transgenic mice were exposed to air or to 62.5, 625 or 1250 ppm BD for 4 weeks (6 h/day, 5 days/week) and euthanized 14 days after the last exposure. 2 RES METH Groups of control and exposed animals (n ¼ 4–12/group) were necropsied at multiple time points after exposure and the T-cell cloning assay was used to measure Hprt mutant frequencies in lymphocytes isolated from spleen. 5 METH METH F344 rats were given a single i.p. injection of diethylnitrosamine (200 mg/kg body weight) and subjected to two-thirds partial hepatectomy at week 3. 5 METH METH Female Wistar rats were administered NDEA (200 ppm) through drinking water (5 days/week) for 4 weeks. 5 METH METH Six-week-old male F344 rats were given a single dose of diethylnitrosamine (DEN, 200 mg/kg b.w., i.p.).Example sentence 6 RES CON Treatment with DMDTC significantly increased the protein carbonyl contents of hepatic microsomes compared with that of controls, a finding that may be related to DMDTC's activity as a prooxidant. 5 METH OBJ Potential modifying effects of epoprostenol sodium administration on liver carcinogenesis were investigated in male F344/DuCrj rats initially treated with N-nitrosodiethylamine (DEN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised discovery of information structure</head><p>performance would be improved by adding an 'Experiment' label to the SN scheme—it seems that the clustering algorithm prefers clustering these types of sentences together instead of with their respective gold standard label categories. To delve even further into cluster properties, we looked at words that frequently occur in cluster sentences. Specifically, we are interested in words that occur disproportionately frequently in a particular cluster. Hence, we use a weighting scheme similar to, e.g. TFIDF (<ref type="bibr" target="#b18">Jones, 1972</ref>), where we treat each cluster as a document and weight the words based on the following formula, where N w c is the number of sentences containing word w in cluster c, N c is the number of sentences in a cluster and c 0 is the set of clusters excluding cluster c (i.e. all clusters except for the current one):</p><formula>weightðw; cÞ ¼ N w c Â N c 0 N c Â N w c 0 (11)</formula><p>This weighting scheme allows us to gauge the relative importance of words, compared with the sentences in the cluster and sentences in the other clusters. An alternative is to use the same scheme as in Equation (11) but to upweight common words by multiplying with the number of clusters that the word occurs in. To illustrate how well this approach can work in giving insight into cluster properties, consider some of the top words for the Conclusion and Experiment clusters for both of these weighting schemes (frequency based and common word based) in<ref type="figure" target="#tab_1">Table 11</ref>. Using an unsupervised approach to discover novel or optimal IS categories can be particularly useful when applying IS analysis to new tasks, domains and datasets (e.g. full text biomedical articles) where we can expect a higher degree of IS variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Applicability</head><p>With respect to the usefulness of unsupervised IS analysis, a natural question is whether the results are good enough to support a realworld task in biomedicine. There are two issues that need to be addressed separately: first, how to obtain class labels and second, whether the performance would still be good enough to be useful. The former question is a side effect of the approach being unsupervised: we only have cluster assignments, without the class labels. We propose that this can be solved by presenting the user with one or more sentences that are closest to each of the centroids. The label that the user picks for those sentences is given to all the sentences in the same cluster. As illustrated in<ref type="figure" target="#tab_1">Table 12</ref>, a user should be able to choose the appropriate label based on the sentences nearest to the centroid, especially when more than one sentences are presented. The latter question is beyond the scope of this first paper on unsupervised IS—however, we expect good results since previous research on weakly supervised IS showed that performances very similar to those obtained for our best categories (around or above 0.70 F) were sufficient to significantly improve the efficiency of literature review in a real-life user test (<ref type="bibr" target="#b14">Guo et al., 2011b</ref><ref type="bibr" target="#b15">Guo et al., , 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>To summarize, in this article, we investigate canonical and stateof-the-art clustering algorithms for IS analysis of biomedical literature. This is the first study of unsupervised IS analysis that uses sophisticated unsupervised learning techniques that may beover the average control value of 0.96 6 0.51 (SD)x10(À6) (P ¼ 0.004). METH We then compared Hprt mutant frequencies (MFs) among these groups. OBJ The higher levels of these two DNA-reactive metabolites in mice compared with rats probably contribute to the species differences in carcinogenic effects of BD between mice and rats. CON These results suggest that EB causes mutation primarily by base substitution and that the spectrum of these mutations closely resembles that of BD.Sentence closest to centroid 1 RES In vitro activation of p-cresol with horseradish peroxidase produced six DNA adducts with a relative adduct level of 8.03 6 0.43 x 10(À7). 2 RES There was no significant effect from individual GSTM1, GSTT1 or mEH genotypes in workers exposed to &lt;150 p.p.b. 3 OBJ 1,3-Butadiene (BD), which is used to make styrene-butadiene rubber, is a potent carcinogen in mice and a probable carcinogen, associated with leukemia, in humans. 4 CON These results suggest that the alteration of the NMPs may be involved in DENinduced hepatocarcinogenesis. 5 METH Two weeks after a single dose of DEN (200 mg/kg, intraperitoneally), rats were given annatto extract at dietary levels of 0, 0.03, 0.1 and 0.3% or phenobarbital sodium at 0.05% as a positive control for 6 weeks. 6 RES The total number of thyroid-follicular cells was not significantly increased by MEI treatment. applicable in real-world tasks. We obtain high performance especially for identifying objective, result and conclusion sentences. We further demonstrate that unsupervised learning brings new insights into IS analysis and allows for the possibility of discovering novel information categories. We have demonstrated the potential of clustering and have shown that clustering can detect valuable new information in data. In the future, we would like to pursue unsupervised learning further and learn feature representations as well, rather than pre-defining them [e.g. similar to<ref type="bibr" target="#b33">Socher et al. (2011</ref><ref type="bibr" target="#b34">Socher et al. ( , 2013</ref>. Other areas we want to explore are probabilistic clustering algorithms such as LDAstyle topic models (<ref type="bibr" target="#b3">Blei et al., 2003</ref>) and clustering algorithms specifically designed for high-dimensional data (<ref type="bibr" target="#b19">Kriegel et al., 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This work was supported by the Royal Society (UK), the Swedish Research Council, FAS (Sweden) and an Engineering and Physical Sciences Research Council (EPSRC) doctoral training grant (to D.K.). Conflict of Interest: none declared.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>@BULLET</head><figDesc>Location: each abstract is divided into 10 equal parts, measured by the number of words. The location feature is defined by the parts where a sentence begins and ends. @BULLET Word: all the words in the corpus. @BULLET Bi-gram: any combination of two adjacent words in the corpus. @BULLET Verb: all the verbs in the corpus. @BULLET Verb class: sixty verb classes appearing in biomedical journal articles (e.g. the experiment class includes verbs such as 'measure' and 'inject').</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>METH A way of doing research, especially according to a defined and regular plan; a special form of procedure or characteristic set of procedures employed in a field of study as a mode of investigation and inquiry Result RES The effect, consequence, issue or outcome of an experiment; the quantity, formula, etc. obtained by calculation Conclusion CON A judgment or statement arrived at by any reasoning process; an inference, deduction, induction; a proposition deduced by reasoning from other propositions; the result of a discussion or examination of a question, final determination, decision, resolution, final arrangement or agreement Related work REL A comparison between the current work and the related work Future work FUT The work that needs to be done in the future</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>Table 1. The section-based and AZ annotation schemes in the corpus of Guo et al. (2010) Scheme Category Abbreviation Definition and example Section Objective OBJ The background and the aim of the research Method METH The way to achieve the goal Result RES The principal findings Conclusion CON Analysis, discussion and the main conclusions AZ Background BKG The circumstances pertaining to the current work, situation or its causes, history, etc. Objective OBJ A thing aimed at or sought, a target or goal Method</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>Table 2. Distribution of sentences in the scheme-annotated corpus</figDesc><table>Scheme 
OBJ 
METH 
RES 
CON 
BKG 
REL 
FUT 

Section 
27% 
17% 
40% 
16% 
– 
– 
– 
AZ 
8% 
18% 
40% 
14% 
18% 
1% 
1% </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 3. Results for both schemes using our evaluation metrics</figDesc><table>Algorithm 
k 
V measure 
Purity 
F measure 

SN 
Baseline 
4 
0.00 
0.38 
0.43 
Spherical k-means 
9 
0.29 
0.68 
0.50 
EM-GMM 
5 
0.09 
0.46 
0.43 
Multilevel-weighted graph 
6 
0.31 
0.69 
0.65 
AZ 
Baseline 
7 
0.00 
0.40 
0.38 
Spherical k-means 
9 
0.31 
0.64 
0.51 
EM-GMM 
5 
0.09 
0.43 
0.40 
Multilevel-weighted graph 
6 
0.30 
0.62 
0.58 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>Table 4. Micro-F score per category for best performing in both schemes</figDesc><table>Scheme 
OBJ 
METH 
RES 
CON 
BKG 
REL 
FUT 

SN 
0.70 
0.48 
0.74 
0.70 
— 
— 
— 
AZ 
— 
0.46 
0.75 
0.73 
0.59 
— 
— 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>Table 5. Macro F score per excluded feature type (LC, location; WO, word; BI, bi-gram; VE, verb; VC, verb class; SU, subject; OB, object; VO, voice) for both schemes. The lower the score, the higher the impact of the feature type Scheme All LC WO BI VE VC POS GR SU OB VO</figDesc><table>SN 
0.65 0.45 0.44 0.58 0.53 0.53 0.51 0.50 0.50 0.51 0.46 
AZ 
0.58 0.42 0.43 0.52 0.49 0.48 0.52 0.46 0.47 0.52 0.41 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><figDesc>Table 6.</figDesc><table>Micro F score per excluded feature type per class for the 
SN scheme. The lower the score, the higher the impact of the fea-
ture type for the given class 

Class 
Conclusion 
Method 
Objective 
Result 

Location 
0.57 
0.46 
0.38 
0.60 
Word 
— 
0.38 
0.58 
0.58 
Bi-gram 
0.55 
0.42 
0.60 
0.67 
Verb 
0.52 
0.44 
0.37 
0.71 
Verb Class 
— 
0.44 
0.54 
0.68 
POS 
0.53 
0.43 
0.43 
0.65 
GRs 
0.61 
— 
0.60 
0.71 
Subj 
0.59 
— 
0.52 
0.68 
Obj 
0.53 
0.43 
0.43 
0.65 
Voice 
0.55 
— 
0.60 
0.66 
All 
0.70 
0.48 
0.70 
0.74 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><figDesc>Table 7.</figDesc><table>Confusion matrix for best performing. Rows are the gold 
standard classes and columns are the winner-takes-all class 
assignments 

Conclusion 
Method 
Objective 
Result 

Conclusion 
817 
3 
114 
442 
Method 
5 
474 
177 
614 
Objective 
85 
126 
1419 
600 
Result 
57 
103 
118 
2734 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><figDesc>Table 8. Example Method sentences and how they were classified in the winner-takes-all assignments</figDesc><table>Cluster 
Dominant 
label 

Gold standard 
label 

Example sentence 

1 
RES 
METH 
The metabolites were determined by HPLC. 
1 
RES 
METH 
These four reaction products were used as analytical standards for kinetic studies of the reaction 
of valinamide with BMO at physiological pH (7.4) and temperature (37 C). 
1 
RES 
METH 
Adducts detected </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><figDesc>Table 9. Examples of CON misclassified as RES and OBJ misclassified as METH</figDesc><table>Cluster 
Dominant 
label 

Gold 
standard label 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="true"><figDesc>Table 10. Sentences closest to the centroid in the 'Experiment' cluster</figDesc><table>Gold standard 
label 

Sentence 

RES 
In addition, the average Hprt MF in mice exposed 
to 3 ppm BD [1.54 6 0.82 (SD) x 10(À6)] was 
significantly increased by 1.6-fold </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><figDesc>Table 11.</figDesc><table>Weighted contribution of words in clusters 1 
(Experiment) and 3 (Conclusion) 

Cluster 3 (Conclusion) 
Cluster 1 (Experiment) 

Freq-weight 
Common-weight Freq-weight 
Common-weight 

Add 
Suggest 
Exhaled 
hprt 
Understanding Useful 
Differed 
Frequency 
Implication 
Indicate 
Passive 
Exposed 
Determinant 
May 
Bioactivated 
Genotype 
Need 
Result 
Globin 
Spleen 
Improve 
Demonstrate 
Linearly 
Inhalation 
Causative 
Suggests 
Cross-sectional Mutant 
Counteract 
Together 
Leukemia 
bdo2 
Conjunction 
Finding 
Collection 
ppb 
Considering 
Taken 
Matched 
laci 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><figDesc>Table 12.</figDesc><table>Sentences closest to each of the clusters' centroids 

Cluster 
Dominant 
label 

</table></figure>

			<note place="foot">Bioinformatics, 31(7), 2015, 1084–1092 doi: 10.1093/bioinformatics/btu758 Advance Access Publication Date: 18 November 2014 Original Paper at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">D.Kiela et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatically classifying sentences in full-text biomedical articles into introduction, methods, results and discussion</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Agarwal</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Yu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="3174" to="3180" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">A comparison of extrinsic clustering evaluation metrics based on formal constraints</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Amigó</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="461" to="486" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Beyond genes, proteins, and abstracts: Identifying scientific claims from full-text biomedical articles</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Blake</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Inform</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="173" to="189" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">M</forename>
				<surname>Blei</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Current issues in biomedical text mining and natural language processing</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Chapman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">B</forename>
				<surname>Cohen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Inform</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="757" to="759" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Using argumentative zones for extractive summarization of scientific articles</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Contractor</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics (COLING), ACL</title>
		<meeting>the International Conference on Computational Linguistics (COLING), ACL<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="663" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Linguistically motivated large-scale nlp with c&amp;c and boxer</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">R</forename>
				<surname>Curran</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL &apos;07 Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Dempster</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Concept decompositions for large sparse text data using clustering</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<forename type="middle">S</forename>
				<surname>Dhillon</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">M</forename>
				<surname>Modha</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learn</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="143" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Kernel k-means, spectral clustering and normalized cuts</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Dhillon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="551" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">A fast kernel-based multilevel algorithm for graph clustering</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Dhillon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 11th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Chicago, IL</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="629" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Weighted graph cuts without eigenvectors: a multilevel approach</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Dhillon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1944" to="1957" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Identifying the information structure of scientific abstracts: an investigation of three different schemes</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Guo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BioNLP, ACL 2010 in</title>
		<meeting>BioNLP, ACL 2010 in<address><addrLine>Uppsala, Sweden, ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="99" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">A comparison and user-based evaluation of models of textual information structure in the context of cancer risk assessment</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Guo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Weakly-supervised learning of information structure of scientific abstracts–is it accurate enough to benefit real-world tasks in biomedicine?</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Guo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3179" to="3185" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Active learning-based information structure analysis of full scientific articles and two applications for biomedical literature review</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Guo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1440" to="1447" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">What the papers say: text mining for genomics and systems biology</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Harmston</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum. Genomics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="17" to="29" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Identifying sections in scientific abstracts using conditional random fields</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Hirohata</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 3rd International Joint Conference on Natural Language Processing. Asian Federation of Natural Language Processing</title>
		<meeting>3rd International Joint Conference on Natural Language Processing. Asian Federation of Natural Language Processing<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="381" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">A statistical interpretation of term specificity and its application in retrieval</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Jones</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Doc</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Clustering high-dimensional data: A survey on subspace clustering, pattern-based clustering, and correlation clustering</title>
		<author>
			<persName>
				<forename type="first">H.-P</forename>
				<surname>Kriegel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowledge Discov. Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="58" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Corpora for the conceptualisation and zoning of scientific papers</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Liakata</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation (LREC). European Language Resources Association</title>
		<meeting>the International Conference on Language Resources and Evaluation (LREC). European Language Resources Association<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2054" to="2061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Generative content models for structural analysis of medical abstracts</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Lin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL BioNLP Workshop on Linking Natural Language and Biology. ACL</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploring subdomain variation in biomedical language</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Lippincott</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">212</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">B</forename>
				<surname>Macqueen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the fifth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<editor>Le Cam,L.M. and Neyman,J.</editor>
		<meeting>. of the fifth Berkeley Symposium on Mathematical Statistics and Probability</meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1967" />
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Value and benefits of text mining</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Mcdonald</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JISC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Analysing entity type variation across biomedical subdomains</title>
		<author>
			<persName>
				<forename type="first">˘</forename>
				<surname>Miha ˘ila</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Building and Evaluating Resources for Biomedical Text Mining</title>
		<editor>Ananiadou,S. et al.</editor>
		<meeting>the Third Workshop on Building and Evaluating Resources for Biomedical Text Mining</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Zone analysis in biology articles as a basis for information extraction</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Mizuta</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Med. Inform</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="468" to="487" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">A baseline feature set for learning rhetorical zones using full articles in the biomedical domain</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Mullen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Process. Text Mining</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="52" to="58" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<monogr>
		<title level="m" type="main">A view of the EM algorithm that justifies incremental, sparse, and other variants) Learning in Graphical Models</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Neal</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Hinton</surname>
			</persName>
		</author>
		<editor>M.I.,Jordan, (ed.</editor>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="355" to="368" />
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">V-measure: A conditional entropybased external cluster evaluation measure</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Rosenberg</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Hirschberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), ACL</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), ACL<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="410" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Using argumentation to extract key sentences from biomedical abstracts</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Ruch</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Med. Inform</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="195" to="200" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<monogr>
		<title level="m" type="main">Part-of-speech tagging guidelines for the penn treebank project (3rd revision)</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Santorini</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<monogr>
		<title level="m" type="main">Biomedical text mining: a survey of recent progress (eds) Mining Text Data</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Simpson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Demner-Fushman</surname>
			</persName>
		</author>
		<editor>Aggarwal,C.C. and Zhai,C.X.</editor>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="465" to="517" />
			<pubPlace>ScienceþBusiness Media LLC, New York, Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Socher</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 28th International Conference on Machine Learning (ICML)</title>
		<meeting><address><addrLine>Bellevue, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Socher</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">The introduction, methods, results, and discussion (IMRAD) structure: a fifty-year survey</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">B</forename>
				<surname>Sollaci</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">G</forename>
				<surname>Pereira</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Libr. Assoc</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="364" to="367" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving verb clustering with automatically acquired selectional preference</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Sun</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Korhonen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), ACl</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP), ACl<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="638" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b37">
	<analytic>
		<title level="a" type="main">Using argumentation to retrieve articles with similar citations</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Tbahriti</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Med. Inform</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="488" to="495" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b38">
	<analytic>
		<title level="a" type="main">Summarizing scientific articles: experiments with relevance and rhetorical status</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Teufel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Moens</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="409" to="445" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b39">
	<analytic>
		<title level="a" type="main">An annotation scheme for discourse-level argumentation in research articles</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Teufel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (EACL), ACL</title>
		<meeting>the Conference of the European Chapter of the Association for Computational Linguistics (EACL), ACL<address><addrLine>Bergen, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="110" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards domain-independent argumentative zoning: Evidence from chemistry and computational linguistics</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Teufel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), ACL</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP), ACL<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1493" to="1502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b41">
	<analytic>
		<title level="a" type="main">Foundation of evaluation</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Van Rijsbergen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Doc</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="365" to="373" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised document zone identification using probabilistic graphical models</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Varga</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation (LREC), European Language Resources Association</title>
		<meeting>the International Conference on Language Resources and Evaluation (LREC), European Language Resources Association<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1610" to="1617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b43">
	<analytic>
		<title level="a" type="main">Discourse structure and language technology</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Webber</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="437" to="490" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b44">
	<analytic>
		<title level="a" type="main">New directions in biomedical text annotation: definitions, guidelines and corpus construction</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">J</forename>
				<surname>Wilbur</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">356</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient online spherical k-means clustering</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Zhong</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Joint Conference on Neural Networks (IJCNN 2005)</title>
		<meeting>IEEE International Joint Conference on Neural Networks (IJCNN 2005)<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3180" to="3185" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>