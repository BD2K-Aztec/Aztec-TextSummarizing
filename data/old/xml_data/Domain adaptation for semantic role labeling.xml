
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining Domain adaptation for semantic role labeling in the biomedical domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010">2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Daniel</forename>
								<surname>Dahlmeier</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">NUS Graduate School for Integrative Sciences and Engineering</orgName>
								<address>
									<postCode>117456</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Hwee</forename>
								<forename type="middle">Tou</forename>
								<surname>Ng</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">NUS Graduate School for Integrative Sciences and Engineering</orgName>
								<address>
									<postCode>117456</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>117417</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining Domain adaptation for semantic role labeling in the biomedical domain</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">26</biblScope>
							<biblScope unit="issue">8</biblScope>
							<biblScope unit="page" from="1098" to="1104"/>
							<date type="published" when="2010">2010</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btq075</idno>
					<note type="submission">Received on September 22, 2009; revised on January 24, 2010; accepted on February 18, 2010</note>
					<note>Copyedited by: PSB MANUSCRIPT CATEGORY: ORIGINAL PAPER Page: 1098 1098â€“1104 Associate Editor: Alfonso valencia</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Semantic role labeling (SRL) is a natural language processing (NLP) task that extracts a shallow meaning representation from free text sentences. Several efforts to create SRL systems for the biomedical domain have been made during the last few years. However, state-of-the-art SRL relies on manually annotated training instances, which are rare and expensive to prepare. In this article, we address SRL for the biomedical domain as a domain adaptation problem to leverage existing SRL resources from the newswire domain. Results: We evaluate the performance of three recently proposed domain adaptation algorithms for SRL. Our results show that by using domain adaptation, the cost of developing an SRL system for the biomedical domain can be reduced significantly. Using domain adaptation, our system can achieve 97% of the performance with as little as 60 annotated target domain abstracts. Availability: Our BioKIT system that performs SRL in the biomedical domain as described in this article is implemented in Python and C and operates under the Linux operating system. BioKIT can be downloaded at http://nlp.comp.nus.edu.sg/software. The domain adaptation software is available for download at</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Advances in biology and life sciences have led to an exponential growth in the amount of biomedical literature. Thus, automatic information retrieval (IR) and information extraction (IE) methods become more and more important to help researchers to keep track of the latest developments in their field. Current IR is still mostly limited to keyword search and unable to infer the relationship between two entities in a text. A system that is able to understand how words in a sentence are related could greatly increase the quality of IE and would allow IR to handle more complex user queries. Semantic role labeling (SRL) is a shallow semantic processing task that has become increasingly popular in the natural language processing (NLP) community over the last few years. The task is to * To whom correspondence should be addressed. identify all parts of a sentence that represent arguments for a given predicate and subsequently label each argument with a semantic role. Roughly speaking, SRL can be thought of as the task of finding the words that answer simple questions of the form Who did what to whom when and where? The input to the SRL system is a single sentence and a predicate in that sentence. The output is the same sentence, but with labeled semantic roles. Consider the following example: Input: Transcription factor GATA-3<ref type="bibr">[stimulates]</ref>In this example, the semantic role Arg0 is the cause of stimulate and the semantic role Arg1 is the thing stimulated (see Section 2 for a detailed description of semantic roles). This information is most valuable for IE (<ref type="bibr" target="#b14">Surdeanu et al., 2003</ref>) and other tasks such as question answering and summarization. Traditionally, most work in SRL has focused on documents from the newswire domain. While SRL works well on test sentences from the same domain, the models show a sharp performance drop when they are tested on a different domain (<ref type="bibr" target="#b11">Pradhan et al., 2008</ref>). Although there have been a number of efforts to apply SRL to the biomedical domain in recent years, the development of state-of-the-art SRL systems for the biomedical domain is hampered by the lack of large biomedical corpora that are labeled with semantic roles. The creation of such corpora is time consuming and expensive. In this article, we address SRL on biomedical text as a domain adaptation problem. The goal is to adapt an SRL system for the newswire domain (where a large annotated corpus is available) to the biomedical domain (where only a small amount of annotated text is available). This way, we can leverage existing corpora from the newswire domain and significantly reduce the cost of developing an SRL system for the biomedical domain. The main contributions of this article are: @BULLET it is the first work that performs a comparative evaluation of the performance of three recently proposed domain adaptation algorithms on the task of SRL for biomedical text; @BULLET it is the first work that investigates the extent of manual annotation needed to port an SRL system trained on newswire text to biomedical text, by explicitly determining theTo our knowledge, this is the first detailed study of domain adaptation for SRL in biomedical text, and our work demonstrates that domain adaptation can greatly reduce the cost of developing biomedical SRL systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SRL</head><p>The task of SRL is to find all arguments for a given predicate in a sentence and label them with semantic roles. The first step is to parse the sentence into a syntactic parse tree. The parse tree consists of the words in the sentence, their part-of-speech tags (e.g. NN, VBZ, etc.) and nodes with syntactic categories (e.g. S, NP, VP, etc.).<ref type="figure">Figure 1</ref>shows the syntactic parse tree for the example sentence from Section 1 (the labels Arg0 and Arg1 are not part of the syntactic parse tree). The next step is the argument identification step, where the SRL system has to find the boundaries for all the arguments in the sentence. The annotation standard for semantic roles demands that the boundaries align with nodes in the syntactic parse tree. Thus, argument identification is equivalent to deciding which nodes in the parse tree, including the part-of-speech tags, span arguments. As shown by the example in<ref type="figure">Figure 1</ref>, the system should find that the NP node that dominates Transcription factor GATA-3 and the NP node that dominates HIV-1 expression span arguments and all other nodes do not. Finally, the system has to determine the semantic role for all identified nodes. This step is called argument classification. In our example, the first identified NP node should be labeled Arg0 and the second should be labeled Arg1, as shown in<ref type="figure">Figure 1</ref>. For the predicate stimulate, Arg0 and Arg1 represent the cause of stimulate and the thing stimulated, respectively. In general, Arg0 refers to the agent and Arg1 refers to the theme of the predicate. Each of the semantic roles Arg2-5 does not have a general meaning that stays consistent across different predicates. The semantic role Arg2, for example, is the instrument for the predicate stimulate, but for the predicate increase, Arg2 is the amount increased. The semantic roles Arg0-5 are called core arguments, because they represent the essential arguments of a predicate. A predicate and the semantic roles that can appear with it are called a predicate argument structure (PAS) or proposition. Additional to its core arguments, a predicate can appear with any number of adjunctive arguments. Adjunctive arguments express general properties such as time, location, manner, etc. They are labeled with ArgM plus a functional tag, e.g. ArgM-LOC, ArgM-TMP or ArgM-MNR. The combined</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FEATURES AND MACHINE LEARNING METHODS</head><p>This section describes how SRL can be solved by supervised machine learning algorithms. First, the input sentence has to be parsed into a syntactic parse tree. In this article, we assume that this step has already been solved and that the correct syntactic parse tree is available to us. The next step is to learn classifiers for the argument identification and argument classification step. The classifier for argument identification performs a binary classification for every node in the parse tree to decide whether the node spans an argument or not. The classifier for argument classification performs a multiclass classification to predict the semantic role for a node in the parse tree, given that the node spans an argument. By casting SRL as a machine learning problem, there are two key decisions that have to be made: the choice of features and the choice of the machine learning algorithm. In this article, we adopt the features used in other stateof-the-art SRL systems, which include the seven baseline features from the original work of<ref type="bibr">Gildea</ref><ref type="figure" target="#tab_1">Table 1</ref>lists the features that we use for easy reference. The machine learning algorithm in our experiments is a maximum entropy (maxent) classifier. 1 Since their introduction by<ref type="bibr" target="#b1">Berger et al. (1996)</ref>, maxent classifiers have successfully been applied to many NLP problems, including SRL (<ref type="bibr" target="#b15">Toutanova et al., 2008;</ref><ref type="bibr" target="#b18">Xue and Palmer, 2004</ref>). Maximum entropy classifiers do not require any independence assumptions, which allow great flexibility in encoding linguistic knowledge via features. The model takes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.Dahlmeier and H.T.Ng</head><formula>the form P(y|x) = 1 Z Â·exp N i=1 Î» i f i (x,y) (1)</formula><p>where y is a semantic role, x is an input vector, f i are feature functions, Î» i are the weights that are learned during training and Z is a normalization term. A detailed description of maximum entropy classifiers can be found in Ratnaparkhi (1998).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DOMAIN ADAPTATION ALGORITHMS</head><p>The task of domain adaptation is to adapt a classifier that is trained on some source domain to a new target domain. Domain adaptation algorithms can be divided into two categories: unsupervised and supervised domain adaptation algorithms. Unsupervised algorithms only use unlabeled instances from the target domain, while supervised algorithms assume that there is a small amount of labeled target instances available during training. The algorithms that we evaluate in this article are all supervised. The algorithms are presented below:</p><p>@BULLET Instance weighting (InstWeight): The essential problem when applying a classifier to data from another domain is that the joint distribution P(X,Y ) of features and class labels in the target domain will be different from the source domain. Instance weighting (<ref type="bibr" target="#b5">Jiang and Zhai, 2007</ref>) is a general framework to tune the estimate for P(X,Y ). The probability P(X,Y ) can be factored in the following way:</p><formula>P(X,Y ) = P(Y |X)Ã—P(X) (2)</formula><p>The first component is the likelihood of the class given the features and the second is the prior probability of observing the features. The difference in P(X,Y ) can arise from either P(X) or P(Y |X). InstWeight tries to tackle the difference in the conditional probability P(Y |X). By weighting the instances in the training set, the domain adaptation algorithm can try to adjust the probability estimate for the target domain. Intuitively, if the estimated probability density for an instance does not match the probability density in the target domain very well, then the learning algorithm should give less weight to this instance. To do this, the algorithm weights an instance by the ratio Pt (Y |X) Ps(Y |X) between the probability densities in the target and source domain. @BULLET Augment method (Augment): DaumÃ© III (2007) proposed a domain adaptation strategy that is based on feature space augmentation. The algorithm takes each feature vector and maps it to a feature space of a higher dimension. The mapping depends on whether the instance is from the source or from the target domain. Assume that x âˆˆ X is a feature vector in the original feature space. We define mappings s and t for the source and target domain, respectively:</p><formula>s (x) = &lt; x,x,0 &gt; (3) t (x) = &lt; x,0,x &gt; (4)</formula><p>where 0 is a zero vector of length |x|. The transformation can be interpreted in the way that it takes each feature vector and makes three versions out of it: one 'general' version, one 'source-specific' version and one 'target-specific' version. The algorithm is surprisingly easy to implement and is independent of the machine learning algorithm that is used. @BULLET Instance pruning (InstPrune): instance pruning (<ref type="bibr" target="#b5">Jiang and Zhai, 2007</ref>) trains a classifier on the target domain instances and uses this classifier to predict class labels for all instances from the source domain. The top N instances that are predicted wrongly, ranked by prediction confidence, are removed from the source domain. The intuition here is that these instances are very different from the target domain and will confuse the classifier during training. The remaining instances from the source domain are then used to train the classifier. Instance pruning is actually another form of instance weighting where the weight for a wrongly predicted source instance is set to zero. InstPrune depends on the parameter N. Setting N too low will hurt the performance, because it leaves too many confusing source instances in the training set. Setting N too high will also result in poor performance, because all information from the source domain is pruned away.</p><p>In addition to the above domain adaptation algorithms, we implement the following three baseline algorithms: @BULLET Source only (SrcOnly): this baseline simply ignores the target domain data and trains a classifier on only the source domain data. @BULLET Target only (TrgtOnly): at the other extreme, the TrgtOnly baseline trains a classifier on the target domain data only, ignoring any source domain data that are available. @BULLET Source and Target (All): the simplest way to combine source and target domain data is to train a classifier on the combined dataset from both domains. This we call the All baseline. The potential problem with this algorithm is that when the source domain dataset is much larger than the target domain dataset, the learning algorithm might regard the target domain instances as 'noise' and essentially ignore them.</p><p>We evaluate all six algorithms for the SRL task on biomedical text. The details of our experiments are given in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>This section presents the details of the datasets that we used in our experiments. The source domain data comes from the PropBank corpus (<ref type="bibr" target="#b9">Palmer et al., 2005</ref>), which is the most commonly used corpus for SRL. The corpus is built from financial news articles from the Wall Street Journal and is available through the Linguistic Data Consortium (http://www.ldc.upenn.edu). We use sections 2â€“21, which form the standard training set used in SRL evaluations, as our source domain dataset. The source domain dataset has a total of 36 090 annotated sentences with their syntactic parse trees and over 90 000 annotated PAS. The target domain dataset consists of the BioProp corpus (<ref type="bibr" target="#b16">Tsai et al., 2007</ref>). The corpus is created from 500 MEDLINE article abstracts. The articles were selected based on the keywords human, blood cells and transcription factor. To our knowledge, BioProp is the only resource for biomedical SRL that uses full syntactic parse trees. The parse trees are taken from the Genia Treebank (GTB;<ref type="bibr" target="#b6">Kim et al., 2003</ref>). The GTB is available for download from the Genia project web site ( http://www-tsujii.is.s.utokyo.ac.jp/GENIA/). During preprocessing of the data, we found that nine abstracts from the BioProp were missing in GTB and that another 45 abstracts did not contain any annotated PAS. The remaining 446 abstracts contain 1635 sentences with a total of 1982 PAS. The statistics of the datasets are given in<ref type="figure" target="#tab_2">Table 2</ref>. It is obvious that BioProp is much smaller than PropBank, not only in terms of the number of sentences, but also in the number of PAS and verbs that are covered. The reason is that the creators of BioProp concentrated on 30 important or frequent verbs from the biomedical domain, while PropBank annotates PAS for all verbs. We also observe that the semantic roles Arg3 and Arg4 are very rare in BioProp and that Arg5 is not used at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental setup</head><p>We investigate the performance of the SRL system on argument identification, argument classification and the combined SRL task. All experiments are conducted using 5-fold cross-validation on the target domain dataset. The 446 abstracts in the target domain dataset are split into five equal portions. Thus, there are four portions with 89 abstracts and one portion with 90 abstracts. The split is done randomly to guard against any selection bias. The SRL system is trained on four of the portions plus the complete source domain data and tested on the remaining portion of the target domain data. This is done for each of the five portions in turn and the results areaveraged over all classified instances. We further ensure that all sentences from one abstract end up in the same portion, to avoid a situation where the classifier is trained and tested on sentences from the same abstract. The evaluation metrics are described in the next section. During our experiments, we gradually increase the number of target domain abstracts that are available during training from 8 to 356 abstracts (357 in the case where the fold with 90 abstracts is used for training). This allows us to assess the impact of the target domain data. The order in which abstracts are added is random, but a particular randomly chosen order of abstracts is used in each experiment where abstracts are added incrementally. In all experiments, we use gold standard syntactic parse trees, including part-of-speech tags, which we take from the PropBank and BioProp corpus. The InstPrune algorithm has a parameter N that needs to be tuned. For each of the five portions of target domain data, we tune N through 4-fold cross-validation on the four portions of target domain data that are used as training data. The classifier is trained on three portions of the target domain data plus the complete source domain data for different values of N and tested on the remaining one portion of target domain that is part of the training data. This is done for each portion of the training data and the results are averaged. The best value of N for each of the five portions is kept. Note that no data from the portion that is used during testing is used to tune the parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain adaptation for SRL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation metrics</head><p>Argument identification and the combined SRL task are evaluated in terms of precision (p), recall (r) and F 1 measure (</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head><p>This section presents the results of our experiments. Before we started experiments on the target domain data, we performed a test on the PropBank corpus to ensure that our SRL system represents a strong baseline. We trained the classifier on sections 2â€“21 and tested on section 23, which is the standard evaluation setting.The results are 95.11% F 1 measure for argument identification, 90.58% accuracy for argument classification, and 86.79% F 1 measure for the combined SRL task. This confirms that our model performs comparably with other state-of-the-art SRL systems (<ref type="bibr" target="#b18">Xue and Palmer, 2004</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Argument identification</head><p>The first experiment examines the system's performance for the argument identification step. The learning curves for the domain adaptation algorithms and the baselines are shown in<ref type="figure" target="#fig_5">Figure 2</ref>. The first observation that can be made is that the SrcOnly baseline achieves a high F 1 measure of 90.49%, only 5% lower than the F 1 measure on PropBank. The TrgtOnly baseline performs poorly in the beginning but improves as more target domain abstracts are added. The All baseline shows no significant improvement over SrcOnly. For the domain adaptation algorithms, InstPrune and Augment perform better than InstWeight and also better than the three baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Argument classification</head><p>The second experiment examines the system's performance for the argument classification step. The learning curves for the domain adaptation algorithms and the baselines are shown in<ref type="figure" target="#fig_6">Figure 3</ref>. The SrcOnly baseline achieves 81.50% accuracy, a drop of over 9% from 90.58% accuracy on PropBank. The TrgtOnly baseline improves quickly with more target domain data. The three domain adaptation algorithms perform similar to or slightly above the TrgtOnly baseline. None of the domain adaptation algorithms can clearly outperform the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Combined SRL task</head><p>The third experiment examines the system's performance for the combined SRL task. The results are shown in<ref type="figure" target="#fig_7">Figure 4</ref>.abstracts from the target domain are added. The All baseline performs decently. Our initial concern that the larger source domain data would dominate the effect of the target domain data appears to be unjustified. The best performing algorithm is InstPrune, followed by Augment. InstPrune shows a consistent improvement over all three baselines for 32 or more target domain abstracts. We recall that InstPrune is actually a version of InstWeight where misclassified source domain instances are weighted with zero weights. Our experiments show that the more aggressive strategy of InstPrune shows better results than InstWeight. We performed the Wald test for statistical significance to determine whether the improvement for InstPrune and Augment could have occurred by chance. The test was always performed against the best performing baseline algorithm. The domain adaptation algorithm performed worse than or equal to the baseline for two data points for InstPrune and four data points for Augment. For InstPrune, the improvement is statistically significant (P &lt; 0.05) for all remaining data points. For Augment, the improvement is statistically significant (P &lt; 0.05) for all remaining data points, except for 16 abstracts. The detailed results of all six algorithms are given in<ref type="figure" target="#tab_3">Table 3</ref>. Overall, we observe that by using PropBank data and domain adaptation algorithms, the SRL system can achieve accurate results with only a fraction of the target domain abstracts. For example, with 60 abstracts (âˆ¼17% of the data) and InstPrune, we can get 97% (= 81.92% 84.44% ) of the performance that we get when using 356 abstracts without domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ANALYSIS</head><p>In this section, we analyze the results to better understand why SRL on BioProp is difficult and why domain adaptation helps. One reason why SRL on Bioprop is difficult is that the vocabularies in the newswire domain and in the biomedical domain are very different, so there are many words in the target domain that the model has not seen during training. Another reason is that a word can have a different dominant meaning in the source and target domain. Consider the following two examples for the predicate increase:</p><p>Source domain:<ref type="bibr">[Sales]</ref>In the first example, increase has an intransitive usage where the subject is Arg1 (thing increasing). This usage can typically be found in the source domain. That is why the SrcOnly baseline wrongly predicts Arg1 for LTB4. In the target domain, we often observe increase with a transitive usage where the subject is Arg0 (causer of increase) and the object is Arg1. With domain adaptation, the system correctly classifies LTB4 as Arg0, for all three domain adaptation algorithms. This example suggests that predicates with different usage in the source and target domain are more difficult to predict than predicates with similar usage in both domains. To quantify this difference, we split the target domain data into two sets: one set contained only instances of predicates that have a similar usage and the other set only contained instances for predicates which have a different usage. To decide which predicates have similar or different usage, we referred to the data provided in<ref type="bibr" target="#b16">Tsai et al. (2007)</ref>. We tested the InstPrune algorithm, which performed best in our previous experiments, on the combined SRL task for the two datasets, using the same experimental setup as before. The results are shown in<ref type="figure" target="#fig_10">Figures 5</ref>and 6. We observe a significant gap between the SrcOnly baseline results in<ref type="figure" target="#fig_10">Figures 5</ref>and 6. This empirically confirms our conjecture that predicates with different usage are more difficult to predict without domain adaptation. When domain adaptation is used, predicates with different usage can be predicted as accurately as predicates with the same usage. Finally, we wanted to find out if domain adaptation only improves the performance for predicates that appear frequently in the target domain, or if infrequent predicates see an improvement as well. We again split the target domain data into two sets, depending on whether the predicate belongs to the most frequent verbs in the target domain or not. The information on which predicates are frequent can be found in<ref type="bibr" target="#b16">Tsai et al. (2007)</ref>. Again, we tested theThe best baseline and domain adaptation algorithm for each column are printed in bold face. Statistically significant improvements for Augment and InstPrune over the best baseline algorithm are marked with an 'asterisk'. All results are obtained using 5-fold cross-validation.InstPrune algorithm on the combined SRL task on the two datasets, using the same experimental setup as before. Our results show that SRL performance for infrequent predicates is about 1â€“2% lower in F 1 measure than the performance for frequent predicates. Thus, even for infrequent predicates, domain adaptation improves SRL performance, although performance is slightly lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain adaptation for SRL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>70</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">RELATED WORK</head><p>In the last few years, there have been a number of efforts to bring SRL to the biomedical domain.<ref type="bibr" target="#b17">Wattarujeekrit et al. (2004)</ref>developed PASBio that contains and analyzes PAS for over 30 verbs and has become a standard for annotating PAS for molecular events. Shah and<ref type="bibr" target="#b13">Bork (2006)</ref>applied SRL in the LSAT system to identify sentences that talk about gene transcripts.<ref type="bibr" target="#b7">Kogan et al. (2005)</ref>analyzed PAS in medical case reports, but they did not present a functioning SRL system.<ref type="bibr" target="#b8">Paek et al. (2006)</ref>applied SRL to abstracts from randomized controlled trial reports, but they limited the scope to five verbs only.<ref type="bibr" target="#b2">Bethard et al. (2008)</ref>presented an SRL system that extracted information about protein movement. They created a corpus for 34 verbs and 4 semantic roles. Their work was more problem-specific than general SRL, as they only focused on a very problem specific set of semantic roles. Most recently,<ref type="bibr" target="#b0">Barnickel et al. (2009)</ref>presented a neural network-based SRL system for relation extraction. Their emphasis was not on accurate SRL but on fast processing speed. All of the above SRL systems use a word-by-word or chunkby-chunk approach instead of a full constituent-by-constituent, syntax-based approach, although the latter approach is the state-ofthe-art in SRL. To our knowledge, the only system for biomedical SRL on full syntactic parse trees is the BIOSMILE system by<ref type="bibr" target="#b16">Tsai et al. (2007)</ref>. They observed that their SRL system did not perform well on BioProp if it was only trained on PropBank. However, their results are not directly comparable with ours, because they only used a smaller portion of PropBank to train their model and they did not use any domain adaptation algorithms. Thus, their work did not investigate how well a state-of-the-art SRL system performs on biomedical text if it is trained on the whole PropBank corpus and uses domain adaptation algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>In this article, we study the effect of domain adaptation for SRL in the biomedical domain. We evaluate three different domain adaptation algorithms on the BioProp corpus using a competitive,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.Dahlmeier and H.T.Ng</head><p>state-of-the-art SRL classifier. We conduct a systematic, detailed comparison of different domain adaptation algorithms for different number of target domain training examples. Our results show that by using just the existing SRL resources and domain adaptation, significant improvements can be achieved with only a small number of annotated target domain data. We believe that our findings will be helpful for applying SRL to new domains in the biomedical field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>and Jurafsky (2002), additional features taken from Pradhan et al. (2005) and feature combinations that are inspired by the system in Xue and Palmer (2004). All features can be extracted from the syntactic parse tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>Copyedited by: PSB MANUSCRIPT CATEGORY: ORIGINAL PAPER [10:44 23/3/2010 Bioinformatics-btq075.tex] Page: 1100 1098â€“1104</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><figDesc>Copyedited by: PSB MANUSCRIPT CATEGORY: ORIGINAL PAPER [10:44 23/3/2010 Bioinformatics-btq075.tex] Page: 1101 1098â€“1104</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><figDesc>F 1 ). Precision measures how accurate the predictions of a classifier are. It is calculated as the number of correct predictions divided by the total number of predictions: p = # correct predictions # predicted instances. Recall is measured as the number of correct predictions divided by the actual number of relevant instances in the test set: r = # correct predictions # actual instances in the test set. F 1 measure combines precision and recall into a single metric by computing the harmonic mean of the two: F 1 = 2Ã— pÃ—r p+r. During argument classification, the boundaries of the relevant instances are already known. In that case, the accuracy (a) of the classifier is reported. Accuracy is defined as the number of correct predictions divided by the total number of instances: a = # correct predictions # total instances .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.2.</head><figDesc>Fig. 2. Argument identification results for InstWeight, Augment and InstPrune. The x-axis denotes the number of target domain abstracts that are available during training. The y-axis denotes the averaged F 1 measure, using 5-fold cross-validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig.3.</head><figDesc>Fig. 3. Argument classification results for InstWeight, Augment and InstPrune. The x-axis denotes the number of target domain abstracts that are available during training. The y-axis denotes the averaged accuracy, using 5-fold cross-validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig.4.</head><figDesc>Fig. 4. Results for InstWeight, Augment and InstPrune on the combined SRL task. The x-axis denotes the number of target domain abstracts that are available during training. The y-axis denotes the averaged F 1 measure, using 5-fold cross-validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><figDesc>Arg1 increased [a more modest 4.8%] Arg2 [in the South] ArgM-LOC. Target domain: [LTB4] Arg0 increased [the expression of the c-fos gene] Arg1 [in a time-and concentration-dependent manner] ArgM-MNR .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><figDesc>Copyedited by: PSB MANUSCRIPT CATEGORY: ORIGINAL PAPER [10:44 23/3/2010 Bioinformatics-btq075.tex] Page: 1103 1098â€“1104</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig.5.</head><figDesc>Fig. 5. Results for InstPrune on the combined SRL task for predicates which have similar usage in source and target domain. The x-axis denotes the number of target domain abstracts that are available during training. The y-axis denotes the averaged F 1 measure, using 5-fold cross-validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig.6.</head><figDesc>Fig. 6. Results for InstPrune on the combined SRL task for predicates which have different usage in source and target domain. The x-axis denotes the number of target domain abstracts that are available during training. The y-axis denotes the averaged F 1 measure, using 5-fold cross-validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><figDesc>Copyedited by: PSB MANUSCRIPT CATEGORY: ORIGINAL PAPER [10:44 23/3/2010 Bioinformatics-btq075.tex] Page: 1104 1098â€“1104</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 1. Features used in our SRL system</figDesc><table>Baseline features (Gildea and Jurafsky, 2002) 
pred 
Predicate lemma 
path 
Path from constituent to predicate 
ptype 
Syntactic category (NP, PP, etc.) 
pos 
Relative position to the predicate 
voice 
Active or passive voice 
hw 
Syntactic head word of the phrase 
sub-cat 
Rule expanding the predicate's parent 

Advanced features (Pradhan et al., 2005) 
hw POS 
POS of the syntactic head word 
PP hw/POS 
Head word and POS of the rightmost NP child if 
the phrase is a prepositional phrase 
first/last word 
First/last word and POS in the constituent 
parent ptype 
Syntactic category of the parent node 
parent hw/POS 
Head word and POS of the parent 
sister ptype 
Phrase type of left and right sister 
sister hw/POS 
Head word and POS of left and right sister 
temporal 
Temporal key words present 
partPath 
Partial path predicate 
proPath 
Projected path without directions 

Feature combinations (Xue and Palmer, 2004) 
pred&amp;ptype 
Predicate and phrase type 
pred&amp;hw 
Predicate and head word 
pred&amp;path 
Predicate and path 
pred&amp;pos 
Predicate and relative position 

SRL task involves argument identification followed by argument 
classification. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>Table 2. Statistics of the source and target domain dataset Arg0 and Arg1 generally refer to the agent and the theme, respectively. The semantic roles Arg2-5 do not have a general meaning. ArgM refers to adjunctive arguments.</figDesc><table>PropBank 
BioProp 

Sentences 
36 090 
1635 
Words 
898 778 
46682 
Unique verbs 
3101 
30 
PAS 
91 122 
1982 

Arg0 
66 329 
1464 
Arg1 
92 958 
2124 
Arg2 
20 547 
325 
Arg3 
3491 
8 
Arg4 
2739 
5 
Arg5 
69 
0 
ArgM 
60 962 
1762 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><figDesc>Table 3. F 1 measure for the combined SRL task for different number of target domain abstracts that were available to the systems during training</figDesc><table>Algorithm 
0 
8 
16 
24 
32 
40 
60 
80 
160 
240 
320 
356 

SrcOnly 
76.46 
â€“ 
â€“ 
â€“ 
â€“ 
â€“ 
â€“ 
â€“ 
â€“ 
â€“ 
â€“ 
â€“ 
TrgtOnly 
â€“ 
57.04 
68.03 
71.84 
73.24 
74.69 
77.77 
78.65 
82.03 
83.11 
84.15 
84.44 
All 
â€“ 
77.65 
78.83 
79.57 
79.93 
80.72 
81.00 
81.51 
81.66 
83.28 
83.74 
84.12 

InstWeight 
â€“ 
67.62 
74.04 
75.50 
76.47 
78.18 
78.99 
80.49 
82.44 
83.33 
84.06 
83.57 
Augment 
â€“ 
76.98 
78.86 
79.04 
78.34 
80.72 
81.60* 
82.04* 
82.73* 
84.01* 
85.07* 
85.03 
InstPrune 
â€“ 
75.49 
78.99* 
79.30 
80.05* 
80.96* 
81.92* 
82.47* 
83.55* 
84.37* 
85.02* 
85.38* 

</table></figure>

			<note place="foot">at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="1"> We use the implementation in the DALR package (Jiang and Zhai, 2007)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank Jing Jiang for allowing us to use her DALR software in our experiments and Wen-Lian Hsu for sharing a prerelease version of the BioProp dataset with us.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Large scale application of neural network based semantic role labeling for automated relation extraction from biomedical texts</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Barnickel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="issue">e6393</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">A maximum entropy approach to natural language processing</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">L</forename>
				<surname>Berger</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="39" to="71" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic role labeling for protein transport predicates</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bethard</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">277</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName>
				<forename type="first">Iii</forename>
				<surname>DaumÃ©</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics ACL</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics ACL<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic labeling of semantic roles</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Gildea</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Jurafsky</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="245" to="288" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Instance weighting for domain adaptation in NLP</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Jiang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Zhai</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics ACL</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics ACL<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Genia corpus â€“ a semantically annotated corpus for biotextmining</title>
		<author>
			<persName>
				<forename type="first">J.-D</forename>
				<surname>Kim</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="180" to="182" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Suppl. . 1</note>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards semantic role labeling &amp; IE in the medical literature</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Kogan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the American Medical Informatics Association (AMIA) Annual Symposium</title>
		<meeting>the American Medical Informatics Association (AMIA) Annual Symposium<address><addrLine>AMIA, Washington DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="410" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Shallow semantic parsing of randomized controlled trial reports</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Paek</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the American Medical Informatics Association (AMIA) Annual Symposium</title>
		<meeting>the American Medical Informatics Association (AMIA) Annual Symposium<address><addrLine>AMIA, Washington DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="604" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">The proposition bank: an annotated corpus of semantic roles</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Palmer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="71" to="106" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Support vector learning for semantic argument classification</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">S</forename>
				<surname>Pradhan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="11" to="39" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards robust semantic role labeling</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">S</forename>
				<surname>Pradhan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="289" to="310" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<monogr>
		<title level="m" type="main">Maximum entropy models for natural language ambiguity resolution</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Ratnaparkhi</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<pubPlace>Philadelphia, PA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">LSAT: learning about alternative transcripts in MEDLINE</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">K</forename>
				<surname>Shah</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Bork</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="857" to="865" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Using predicate-argument structures for information extraction</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Surdeanu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL 2003), ACL</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics (ACL 2003), ACL<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">A global joint model for semantic role labeling</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Toutanova</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="161" to="191" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">BIOSMILE: A semantic role labeling system for biomedical verbs using a maximum-entropy model with automatically generated template features</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">T</forename>
				<surname>Tsai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">-H</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">325</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">PASBio: predicate-argument structures for event extraction in molecular biology</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Wattarujeekrit</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">155</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Calibrating features for semantic role labeling</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Xue</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Palmer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing ACL</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing ACL<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="88" to="94" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>