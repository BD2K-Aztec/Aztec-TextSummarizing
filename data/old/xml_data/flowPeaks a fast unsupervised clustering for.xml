
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining flowPeaks: a fast unsupervised clustering for flow cytometry data via K-means and density peak finding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012">. 15 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Yongchao</forename>
								<surname>Ge</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Neurology and Center of Translational System Biology</orgName>
								<orgName type="department" key="dep2">Mount Sinai School of Medicine</orgName>
								<address>
									<postCode>10029</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Stuart</forename>
								<forename type="middle">C</forename>
								<surname>Sealfon</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Neurology and Center of Translational System Biology</orgName>
								<orgName type="department" key="dep2">Mount Sinai School of Medicine</orgName>
								<address>
									<postCode>10029</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining flowPeaks: a fast unsupervised clustering for flow cytometry data via K-means and density peak finding</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">28</biblScope>
							<biblScope unit="page" from="2052" to="2058"/>
							<date type="published" when="2012">. 15 2012</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/bts300</idno>
					<note type="submission">Received on March 19, 2012; revised on April 27, 2012; accepted on May 14, 2012</note>
					<note>Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [09:59 5/7/2012 Bioinformatics-bts300.tex] Page: 2052 2052–2058 Associate Editor: Jonathan Wren Availability: The R package flowPeaks is available at https:// github.com/yongchao/flowPeaks. Contact: yongchao.ge@mssm.edu Supplementary information: Supplementary data are available at Bioinformatics online</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: For flow cytometry data, there are two common approaches to the unsupervised clustering problem: one is based on the finite mixture model and the other on spatial exploration of the histograms. The former is computationally slow and has difficulty to identify clusters of irregular shapes. The latter approach cannot be applied directly to high-dimensional data as the computational time and memory become unmanageable and the estimated histogram is unreliable. An algorithm without these two problems would be very useful. Results: In this article, we combine ideas from the finite mixture model and histogram spatial exploration. This new algorithm, which we call flowPeaks, can be applied directly to high-dimensional data and identify irregular shape clusters. The algorithm first uses K-means algorithm with a large K to partition the cell population into many small clusters. These partitioned data allow the generation of a smoothed density function using the finite mixture model. All local peaks are exhaustively searched by exploring the density function and the cells are clustered by the associated local peak. The algorithm flowPeaks is automatic, fast and reliable and robust to cluster shape and outliers. This algorithm has been applied to flow cytometry data and it has been compared with state of the art algorithms, including Misty Mountain, FLOCK, flowMeans, flowMerge and FLAME.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In analyzing flow cytometry data, one fundamental question is how to divide the cells into distinct subsets with the phenotypes defined by the fluorescent intensity of the cell surface or intracellular markers. The unsupervised clustering for flow cytometry data is traditionally done by manual gating, where cells are sequentially clustered (gated) in one-dimension (1D) or 2D with the aid of 2D contour plots and 1D histograms. Manual gating has two problems: it is (i) highly subjective, depending on the users' expertise and the sequences of the markers to draw the gates and where to draw the gates and, (ii) tedious, for data consisting of n channels, * To whom correspondence should be addressed. the user needs to check and draw the gates on possibly n 2 pairs of 2D contour plots. The automatic gating of the cells, in machine learning called unsupervised clustering, has become an active research area for the past several years. There are currently two common approaches to address the unsupervised clustering problem, one is based on the finite mixture model (<ref type="bibr" target="#b0">Aghaeepour et al., 2011;</ref><ref type="bibr" target="#b2">Chan et al., 2008;</ref><ref type="bibr" target="#b3">Finak et al., 2009;</ref><ref type="bibr" target="#b15">Lo et al., 2008;</ref><ref type="bibr" target="#b19">Pyne et al., 2009</ref>) and the other is based on spatial exploration of the histograms (<ref type="bibr" target="#b18">Naumann et al., 2010;</ref><ref type="bibr" target="#b20">Qian et al., 2010;</ref><ref type="bibr" target="#b23">Sugar and Sealfon, 2010</ref>). Both approaches have their own weaknesses. The finite mixture model assumes that the data are generated by a mixture of Gaussian distributions, Student's t-distribution or skewed t-distributions. Some of these methods require data transformation to reduce the data asymmetry. There are two issues faced by the finite mixture model: (i) how many components are needed and (ii) the cluster shape is not necessarily the same as what the model assumed. Most authors resort to the Bayesian information criterion (BIC) or some variants to determine the optimum number of components (<ref type="bibr" target="#b3">Finak et al., 2009;</ref><ref type="bibr" target="#b15">Lo et al., 2008;</ref><ref type="bibr" target="#b19">Pyne et al., 2009</ref>), which still leaves ambiguity as there are competing finite mixtures that give similar BIC with completely different partitions of the data. The BIC approach is also computationally very burdensome since it needs to compute the clustering for all possible K and then determine the best K. If the cluster shape is not convex or very asymmetrical, these algorithms are likely to split a single cluster into several small ones. The new-generation algorithms such as Misty Mountain (<ref type="bibr" target="#b23">Sugar and Sealfon, 2010</ref>) and FLOCK (<ref type="bibr" target="#b20">Qian et al., 2010</ref>) try to find the irregular shape and not to rely on K. They are fast and they find the data-dependent cluster shape. However, the new-generation algorithms cannot be applied directly to high-dimensional data. Thus, Misty Mountain needs to first apply principal component analysis to reduce the dimension and FLOCK needs to search a 3D subspace that is optimal for a particular cluster. These dimension reduction techniques may result in information loss. In this article, our goal is to combine these two approaches, allowing us to quickly detect the data-dependent cluster shapes so that the algorithm can be applied directly to high-dimensional data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">What is a cluster</head><p>As said in Jain (2010), there is inherent vagueness in the definition of a cluster. We want to illustrate what a cluster is with a toy example.<ref type="figure" target="#fig_1">Figure 1</ref>shows a density function of two Gaussian distributions when varying the mean of the first distribution. In<ref type="figure">Figure 2</ref>, the means are fixed, and the proportion for the first Gaussian distribution is varied. Most figures show two distinct peaks. However, we can see that the data should be considered</p><formula>function f (x) = w 1 φ(x;μ 1 ,σ 2 1 )+(1−w 1 )φ(x;μ 2 ,σ 2 2 ), where w 1 = 0.7,μ 2 = 4,σ 1 = 2,σ 2 = 1.5, φ(x;μ,σ 2 )</formula><p>is the density function of the Gaussian distribution with mean μ and variance σ 2 , and μ 1 takes the values of −3, −1, and 1, respectively. The two components</p><formula>w k φ(x;μ k ,σ 2 k ) (k = 1,2)</formula><p>are respectively given by the red and green curves (A color version of this figure is available as Supplementary Material)as one cluster in Figures 1C and 2C, because there is only a single peak. An ideal cluster would be such that the corresponding probability density function has a unique peak (mode) and every point can move to the peak following a monotonically nondecreasing path. In this article, we use Kmeans as a building block to estimate the probability density function (see Sections 2.2 and 2.3), which is then used to partition the clusters based on the above consideration (see Section 2.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">K-means</head><p>The K-means algorithm has traditionally been used in unsupervised clustering, and was applied to flow cytometry data as early as in<ref type="bibr" target="#b17">Murphy (1985)</ref>, and as recently as in<ref type="bibr" target="#b0">Aghaeepour et al. (2011)</ref>. In fact, K-means is a special case of a Gaussian finite mixture model where the variance matrix of each cluster is restricted to be the identity matrix. Our use of K-means is not for the final clustering, but for a first partition of the cells, for which we can compute the smoothed density function. In the literature, the most popular K-means implementation is based on Lloyd's algorithm (<ref type="bibr" target="#b14">Lloyd, 1982</ref>). Since there are many local minima, the final clustering depends critically on the initial seeds. We used the seeds generation algorithm from the K-means++ algorithm (<ref type="bibr" target="#b1">Arthur and Vassilvitskii, 2007</ref>). Let x i = (x 1 i ,...,x d i ) be a d-dimensional vector for the measurements of cell i and c h be the seed vector for cluster h. Initially, a random cell is picked and assigned to c 1. To sequentially determine the seed for cluster k (k = 2,··· ,K), we first compute the minimum Euclidean distance for all cells to the previous k −1 seeds by d 2 i = min {h=1,···,k−1} x i −c h 2 ,i = 1,··· ,n.</p><p>A cell x i is selected to be the seed c k of the k-th cluster according to the probability</p><formula>d 2 i /{ n j=1 d 2 j }.</formula><p>After the seeds for all K clusters are assigned, Lloyd's algorithm (<ref type="bibr" target="#b14">Lloyd, 1982)</ref>will iterate with the following two steps: assign each data point with a cluster label according to the smallest distance to the K seeds (cluster membership assignment step) and then recompute the center vector of all data points that are assigned with the same cluster label (center update step). The updated center vectors become the seed vectors for the cluster membership assignment step in the next iteration. We use a k-d tree representation of cells (<ref type="bibr" target="#b12">Kanungo et al., 2002</ref>) for improved computing speed for the implementation of Lloyd's algorithm. After Lloyd's algorithm converged, we further applied the Hartigan and Wong's (1979) algorithm to recompute the cluster centers and cluster membership to decrease the objective function n i=1 x i −c L i 2 , where L i ∈ 1,...,K is the cluster label of x i and c k is the center vector for cluster k ∈ 1,...,K. We could have applied the Hartigan and Wong's algorithm directly to the seeds, but the computation is too slow. In general clustering, it is important to specify a good K in the Kmeans algorithm. For our purpose, a very accurate specification of K is not necessary. However, it is still important that the K can give a smooth density in which the peaks can reveal the clustering structure. This specification of K is similar to the determination of the number of bins in drawing histograms. We adopted the formula of Freedman and Diaconis (1981)</p><formula>K j = (x j (n) −x j (1) )/{2·IQR(x j )·n −1/3 } for j = 1,...,d, (1) where x j (1) ,x j (n)</formula><p>are, respectively, the minimum and maximum of the j-th dimension of the data</p><formula>x j = (x j 1 ,x j 2 ,...,x j n</formula><p>) and IQR(·) is the interquartile range of the data, defined as the difference between the 75th percentile and 25th percentile. Then our K is defined as the median of K j 's, i.e.</p><formula>K ==median(K 1 ,...,K d ), (2)</formula><p>where ·· is the ceiling function that maps a real number to the smallest following integer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Gaussian finite mixture to model the density function</head><p>After K-means, we may approximate the density function f (x) by the Gaussian finite mixture models,</p><formula>f (x) = K k=1 w k ·φ(x;μ k ,, k ),</formula><p>where the proportion w k of the k-th</p><formula>component satisfies 0 ≤ w k ≤ 1 and K k=1 w k = 1 and φ(x;μ k ,, k</formula><p>) is the probability density function of the multivariate normal distribution with mean μ k and variance matrix k. After applying the K-means algorithm of Section 2.2, we have already partitioned the data into K clusters, and for the k-th cluster, we can compute the sample proportion w k , sample mean μ k and sample variance matrix k (a rigorous writing would require the hat notation, which is ignored for the sake of simplicity). However, the estimate k may be too noisy, and we want to smooth the variance matrix by</p><formula>k = λ k ·h k +(1−λ k )·h 0 0 ,</formula><p>where h and h 0 are customized parameters tuned to make the density function smoother or rougher. The default setting in the software is h = 1.5 and h 0 = 1. Here, λ k = nw k /(k +nw k ) so that a greater w k results in a λ k closer to 1; 0 is the variance matrix assuming the data are uniformly distributed in the whole data range and is a diagonal matrix with its (j,j) element j,j</p><formula>0 = {(x j (n) −x j (1) )/k 1/d } 2 for j = 1</formula><p>,...,d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Y.Ge and S.C.Sealfon</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Peak search and merging</head><p>According to our definition, a cluster is defined by the local peak. For all cells, we can use the greatest gradient search (hill climbing) to find which local peak a given cell can reach. This rules out any global optimization strategy such as the conjugate gradient algorithm. It is computationally very time consuming to search all the local maximums of the density function for all cells. Since the cells are pre-grouped by the K-means, we only need to search the local peaks for the centers of the K-means clusters. The hill climbing method searches along the greatest gradient of the density function. If we take the negative of the density function as the optimization function, the hill climbing of peak search can be achieved by the deepest descent algorithm, which is implemented by the GSL library at http://www.gnu.org/ software/gsl/. We also need to restrict the step size in case it steps too far away and jumps to another local peak. When the data move from one Kmeans cluster into another K-means cluster, we can speed it up by moving directly to the center of the other cluster. When two peaks are relatively close, they should be joined together and considered as a single peak. We search the two peaks with the closest Euclidean distance and check if the two clusters may not be too different from a single cluster. The details on the local peak search and peak merging are described in the Appendix. Algorithm 1 gives the summary of the steps to use in K-means and density peak finding in order to cluster the flow cytometry data as implemented in the software flowPeaks. In the end, we will obtain K (≤ K) of merged clusters, each of which consists of one or many K-means clusters.3. Use the k-d tree data representations to apply Lloyd's Kmeans algorithm until it converges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Further apply the Hartigan and Wong's K-means algorithm to</head><p>improve the compactness of the clusters.</p><formula>5. Compute w k ,μ k , k for k = 1</formula><p>,··· ,K using the partitions of the K-means.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Based</head><p>on the density function generated by Gaussian finite mixture model, compute the local peak starting from the centers μ k , k = 1,··· ,K (see Algorithm A1 in the Appendix). 7. Apply Algorithm A2 in the Appendix to merge peaks hierarchically. 8. The K clusters of the final K-means algorithm are regrouped according to the merged peaks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Cluster tightening</head><p>The default setting in the flowPeaks algorithm is to not identify the outliers. Some data points may lie far from the center or cannot be unambiguously classified into a specific cluster. We determine whether a data point is an outlier using the following strategy. Let (x) be the final merged cluster label of data point x. Let ω i and f i (x) (respectively) be the proportion and the probability density function of the i-th final merged cluster. The proportion ω i is the sum of w k 's of the K-means clusters that form the i-th final merged cluster. The density function f i (x) itself is a Gaussian finite mixture based on the K-means clusters that are merged into the i-th final cluster, while the overall density function f (x) is based on all K-means clusters (see Section 2.3) and f (x) = K i=1 ω i f i (x). A point x is an outlier if</p><formula>f (x)/max y {f (y) : (y) = (x)}≤0.01, or ω (x) f (x) (x)/ K i=1 ω i f i (x) ≤ 0.8.</formula><p>The numbers 0.01 and 0.8 can be adjusted in the software settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Barcode data: The data were generated for a barcoding experiment (<ref type="bibr" target="#b13">Krutzik and Nolan, 2006</ref>) with varying concentrations of flurophores (APC and Pacific Blue). The flow cytometry data have 180912 cells and three channels with an additional channel for Alexa. The manual gates for the 20 clusters to be used for assessing cluster algorithm performance were created from flowJo (www. flowjo.com). Simulated concave data: The data were simulated with two distinctive concave shapes based on the idea from the supplemental material of<ref type="bibr" target="#b19">Pyne et al. (2009)</ref>. It has 2729 rows and 2 columns. Both barcode data and simulated concave data along with their gold standard cluster labels are available in the flowPeaks package. GvHD dataset: Graft versus host disease dataset and the manual gates are obtained from<ref type="bibr" target="#b0">Aghaeepour et al. (2011)</ref>. This dataset contains 12 samples, and the cells are stained with four markers, CD4, CD8b, CD3 and CD8. In addition, two channels FS and SS are also measured. These data are mostly analyzed based on the four markers unless specified otherwise. The numbers of cells of the 12 samples range from 12 000 to 32 000. Rituximab data: The flow cytometry data that are obtained from the flowClust package (<ref type="bibr" target="#b16">Lo et al., 2009</ref>). They have 1545 cells and two channels of interest. The data were originally produced by<ref type="bibr" target="#b6">Gasparetto et al. (2004)</ref>. The barcode data, simulated data and GvHD datasets have gold standard cluster labels (either by simulation or manual gating) to assess performance. The rituximab data are used for the purpose of exploration.<ref type="figure" target="#fig_2">Figure 3</ref>displays all four datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Different metrics to assess the cluster algorithm performance</head><p>The most widely used metric to assess how a candidate clustering algorithm compares with the gold standard, for which the correct cluster membership is known, is the adjusted Rand index (<ref type="bibr" target="#b9">Hubert and Arabie, 1983;</ref><ref type="bibr" target="#b21">Rand, 1971</ref>). The Rand index (<ref type="bibr" target="#b21">Rand, 1971</ref>) is based on the percentage of the agreement between the two clustering methods. Let us assume that n data points are labeled differently with two different clustering methods, say Method A and Method B with K A and K B clusters. Let A i ,i = 1,...,n and B i ,i = 1,...,n be the cluster labels for the two methods. The Rand index is defined asIn order to compute the adjusted Rand index, we first define the contingency tables</p><formula>n a,b = n i=1 I(A i = a and B i = b),</formula><p>for a = 1,...,K A , b = 1,··· ,K B. The marginal sums on the contingency tables are then defined as</p><formula>n a,+ = K B b=1 n a,b and n +,b = K A a=1 n a,b .</formula><formula>Note that n = K A a=1 n a,+ = K B b=1 n +</formula><p>,b. The adjusted Rand index can be quickly computed using the following formula (<ref type="bibr" target="#b9">Hubert and Arabie, 1983</ref></p><formula>) a,b n a,b 2 − a n a,+ 2 b n +,b 2 a n a,+ 2 + b n +,b 2 /2− a n a,+ 2 b n +,b 2</formula><p>The F-measure (<ref type="bibr" target="#b5">Fung et al., 2003</ref>) is based on a greedy strategy to match the two clustering. It has been used in 2010s flowCAP I (http://flowcap.flowsite.org/summit2010.html) and in the flowMeans algorithm paper (<ref type="bibr" target="#b0">Aghaeepour et al., 2011</ref>) to assess the performance of different algorithms. The F-measure is defined as</p><formula>F = a n a,+ n max b F(a,b),</formula><formula>where F(a,b) = 2R(a,b)P(a,b) R(a,b)+P(a,b) ,R(a,b) = n a,b n a,+ ,P(a,b) = n a,b n +,b</formula><p>. Rosenberg and<ref type="bibr" target="#b22">Hirschberg (2007)</ref>proposed the V-measure to evaluate the clustering algorithm. This measure uses entropy to assess how much a second clustering provides extra information for the first clustering. For the clustering Method A, the entropy is</p><formula>H(A) =− K A a=1 n a,+ n log n a,+ n</formula><p>and the conditional entropy</p><formula>H(A|B) =− K B b=1 n +,b n K A a=1 n a,b n +,b log n a,b n +,b =− K B b=1 K A a=1 n a,b n log n a,b n +,b .</formula><p>The conditional entropy H(A|B) is always no greater than the entropy H(A). The extra information provided by Method B for Method A is the reduced entropy H(A)−H(A|B). After normalization, we can define</p><formula>h = 1−H(A|B)/H(A)·I(H(A) = 0).</formula><p>In the above equation, by definition h = 1 if H(A) = 0. If we reverse the positions of A and B, we can define c = 1−H(B|A)/H(B)·I(H(B) = 0) If Method B is the candidate clustering to be compared with the gold standard clustering A, h evaluates the homogeneity of clustering for Method B, while c evaluates the completeness. The homogeneity ensures that the gold standard labels (A labels) for all data points of a candidate cluster B are unique. Completeness ensures that for each gold standard cluster (A cluster), data points are all assigned to a single candidate cluster (B cluster). Details can be found in Rosenberg and Hirschberg (2007). The V-measure is a weighed harmonic mean of h and c,</p><formula>V β = (1+β)hc/(βh+c)</formula><p>. In this artice, we will fix β to be 1.<ref type="figure" target="#tab_1">Table 1</ref>displays the running time of all algorithms that are applied to the concave and barcode datasets described in Section 3.1. The algorithms flowPeaks, Misty Mountain (<ref type="bibr" target="#b23">Sugar and Sealfon, 2010</ref>), FLOCK (<ref type="bibr" target="#b20">Qian et al., 2010</ref>) and flowMeans (<ref type="bibr" target="#b0">Aghaeepour et al., 2011</ref>) are falling into a category where the computational time is under several minutes so that they can compete with manual gating, while FLAME (<ref type="bibr" target="#b19">Pyne et al., 2009</ref>) and flowMerge (<ref type="bibr" target="#b3">Finak et al., 2009</ref>) take too much computational time to be practically useful. Among the first four algorithms, a good seeding strategy and k-d tree implementation make flowPeaks a little bit faster than the other algorithms. When we applied the three metrics in Section 3.2 to assess different algorithms, we removed the outliers according to the gold standard. Tables 2 and 3 give the performance of different algorithms to be compared with the gold standard. We see that flowPeaks does quite well for the barcode data and the concave data. Due to the slowThe running time is shown in wall-clock seconds on the same desktop computer except that FLOCK and FLAME were run, respectively, at immport (http://immport. niaid.nih.gov) and gene pattern websites (http:www.broadinstitute.org/cancer/software/ genepattern)speed of flowMerge and FLAME and the difficulty to batch running FLOCK and FLAME, which are only available from a web interface, for performance comparison on the 12 samples in the GvHD dataset, we only selected flowPeaks, Misty Mountain and flowMeans, which are the three best algorithms according to Tables 2 and 3.<ref type="figure" target="#tab_4">Table 4</ref>shows that flowPeaks is better than the other two algorithms for the GvHD dataset. We have displayed the flowPeaks results for the four datasets in Figures 4A, 5A–C. Since rituximab does not have a gold standard, the visual display shows that flowPeaks does a good job revealing the cluster structure of the data.<ref type="figure" target="#fig_5">Figure 5D</ref>displays the application of flowPeaks in the GvHD data when FSC and SSC channels are included. The clustering on 6D highly agrees with 4D with only 0.59% of points classified differently between 6D and 4D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Application</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Y.Ge and S.C.Sealfon</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SOFTWARE</head><p>We have implemented the algorithm in C++ wrapped into an R package named 'flowPeaks'. The following example illustrates how to use the basic functions of this R package library(flowPeaks) data(barcode) fp&lt;-flowPeaks(barcode<ref type="bibr">[,c(1,3)]</ref>) plot(fp,drawlocalpeaks=TRUE)</p><p>The above R script will display<ref type="figure" target="#fig_4">Figure 4A</ref>. In order to identify the outliers to obtain<ref type="figure" target="#fig_4">Figure 4B</ref>, we can proceed further with the following script fpc&lt;-assign.flowPeaks(fp,fp$x) plot(fp,classlab=fpc,drawboundary=FALSE, drawvor=FALSE,drawkmeans=FALSE,drawlab=TRUE)</p><p>For further use of the software flowPeaks, one can consult the package's vignette pdf file and help documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION AND FUTURE WORK</head><p>In this article, we described the algorithm flowPeaks that combines the K-means and density function peak finding to partition the flow cytometry data into distinct clusters. We have compared our algorithm with other state of the art algorithms for real and simulated datasets. Our algorithm is fast and able to detect the non-convex shapes. We should point out that flowPeaks's goal is to find the overall density shape and search for global structure. It will not be able to uncover overlapping clusters as shown in<ref type="figure" target="#fig_1">Figure 1C</ref>or the rare cluster as shown in<ref type="figure">Figure 2C</ref>. The flowPeaks algorithm is based on the geometrical shape of the density function. Prior to apply flowPeaks, data transformation may be necessary to reveal the structure, and irrelevant channels need to be first discarded to avoid the curse of dimensionality. Due to the curse of dimensionality, if the data dimension is too high and the number of cells is too low whereThe same sample in (B) with FSC and SSC channels included. Due to the long running time required for the heatmap, 4000 data points were randomly selected to generate the cluster-tree and the heatmap. The three rows fp4D.lab, fp6D.lab and GS.lab, respectively, display the class labels of the flowPeaks on 4D, flowPeaks on 6D, and the Gold Standard, where different colors indicate different clusters. The signal intensities of all six channels are displayed in the heatmap with the key displayed on the bottom (A color version of this figure is available as Supplementary Material) the density function cannot be reliably estimated by flowPeaks, users should alternatively use the heatmap to visualize the data. As commented in Jain (2010), there is not a single clustering algorithm suitable for all datasets. This is probably true for flow cytometry clustering. There is not a good collection of flow cytometry data with gold standard gates, which makes algorithm comparison very challenging. The comparison in Section 3.3 should not be taken literally. We tend to agree with<ref type="bibr" target="#b18">Naumann et al. (2010)</ref>that 'it is too early for extensive comparisons of automated gating procedure'. The current approach of using the manual gating as a gold standard to compare the automatic gating algorithm is very subjective. We participated with flowPeaks and support vector machine algorithm in 2011's flowCAP II (http:// flowcap.flowsite.org/summit2011.html). Our algorithm gave 100% prediction accuracy for the clinical flow cytometry data, establishing us as one of the best algorithms. We have released our datasets in our flowPeaks package with the gold standard gates so that one can test one's favorite algorithm with our datasets. The source code and windows binary built of the R package flowPeaks is available at https://github.com/yongchao/flowPeaks. The package is in the progress of being permanently hosted at the Bioconductor (<ref type="bibr" target="#b7">Gentleman et al., 2004;</ref><ref type="bibr" target="#b10">Ihaka and Gentleman, 1996</ref>) with open source code for algorithm developers and batching processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mathematical notation</head><p>For the sake of clarity, we will use the following notation. Assume the data consist of n points in d dimension. Let the underlying clusters, obtained by K-means, be labeled as 1,...,K. The density function generated by the finite mixture model is</p><formula>f (x) = K k=1 w k /| k | 1/2 exp{−(x −μ k ) t −1 k (x −μ k )}, where w k ,μ k , ˜ k are</formula><p>the weights, means and the smoothed variance matrix of cluster k, respectively, for k = 1,...,K. The derivative of the density function at x is defined as f (x) = ∂f (x)/∂x. According to the K-means algorithm, the cluster label of x can be defined as</p><formula>L(x) = argmin K k=1 x −μ k 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Searching the local peak starting from a point x</head><p>As we do not want to jump over the local peak, when the data fall into a cluster k, we define the maximum step size</p><formula>β max k = min i=1,...,d i,i k .</formula><p>The detailed computations for the local peak search are described in Algorithm A1. We initially set a small step size β (Step 0), and try to find a step size such that the density function f improves (Step 2 and Step 3). If the same step size improves twice in a row (N suc denote the number of continuous improvements), then we double the step size; otherwise we half the step size. If the point is falling into a new cluster, we want to find out if we can jump to the new center directly (Step 6). The details are described in Algorithm A1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The algorithm on merging local peaks</head><p>When two peaks are close and the density function between the two peaks is relatively flat, the two peaks should be combined into one. For each underlying K-means cluster, we define the nearest neighbor cluster distance by S k = min{{μ k −μ i :i ∈{1,...,K} and i = k}.</p><p>For an arbitrary position x, we can similarly define the function S(x) = S L(x). Let x and y be two points, we define the tolerance that describes how the density function of the line segment that connects x and y can be approximated by a straight line</p><formula>tol = max t∈[0,1] f (z t )− ˆ f (z t ) ˆ f (z t ) · (n L(x) +n L(y) )/2 n/K ,</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [09:59 5/7/2012 Bioinformatics-bts300.tex] Page: 2054 2052–2058</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm1</head><figDesc>Summary of the flowPeaks algorithm 1. Apply the Freedman-Diaconis formula in each dimension of the data to obtain the number K of clusters for K-means [see Equations (1) and (2)]. 2. Use the K-means++ algorithm to generate the initial seeds of the K clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.3.</head><figDesc>Fig. 3. Four example datasets: (A) barcode data with the 20 clusters. (B) Concave data with two clusters. (C) One of the 12 samples in the GvHD dataset. The plot only shows the scatter plot of the first two of the four channels, where different colors indicate different manual gated clusters and black points are outliers. (D) Rituximab data (A color version of this figure is available as Supplementary Material)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><figDesc>Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [09:59 5/7/2012 Bioinformatics-bts300.tex] Page: 2056 2052–2058</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.4.</head><figDesc>Fig. 4. Application of flowPeaks to the barcode data. (A) the bold boundary displays the clusters output by flowPeaks with their centers (⊕), the dotted lines are the boundary for the underlying K-means clusters with their centers (@BULLET). The local peaks are indicated by. (B) The same as in (A) except the outliers have been identified as black points and other secondary information was not displayed, and the clusters are labeled according to their proportions (w k ) (A color version of this figure is available as Supplementary Material)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.5.</head><figDesc>Fig. 5. The application of flowPeaks to different datasets. See Figure 4A for explanations of legends. (A) Concave data. (B) One sample of the GvHD dataset that is shown in Figure 3C. The boundaries between flowPeaks clusters and between K-means clusters are not drawn as two non-overlapping clusters generated in 4D may overlap in a 2D projection. (C) Rituximab dataset. (D) The same sample in (B) with FSC and SSC channels included. Due to the long running time required for the heatmap, 4000 data points were randomly selected to generate the cluster-tree and the heatmap. The three rows fp4D.lab, fp6D.lab and GS.lab, respectively, display the class labels of the flowPeaks on 4D, flowPeaks on 6D, and the Gold Standard, where different colors indicate different clusters. The signal intensities of all six channels are displayed in the heatmap with the key displayed on the bottom (A color version of this figure is available as Supplementary Material)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>for the black curve is the probability density f (x) at x given by the x-axis. The overall density</figDesc><table>Copyedited by: TRJ 

MANUSCRIPT CATEGORY: ORIGINAL PAPER 

[09:59 5/7/2012 Bioinformatics-bts300.tex] 
Page: 2053 2052–2058 

flowPeaks: a unsupervised clustering for flow data 

−10 
0 
5 10 

0.00 

0.05 

0.10 

0.15 

0.20 

A 

µ 1 =−3 

−10 −5 0 
5 10 

0.00 

0.05 

0.10 

0.15 

0.20 

B 

µ 1 =−1 

−5 
0 
5 
10 

0.00 

0.05 

0.10 

0.15 

0.20 

C 

µ 1 =1 

Fig. 1. The overall density with two Gaussian mixture components with 
different choices of mean for the first component. The y-axis </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>index = 1≤i&lt;j≤n I(A i = A j and B i = B j )/ n 2 , where I(·) is the indicator function. The adjusted Rand corrects for chance, and the general form is Index−Expected Index Max Index−Expected Index . Copyedited by: TRJ</figDesc><table>MANUSCRIPT CATEGORY: ORIGINAL PAPER 

[09:59 5/7/2012 Bioinformatics-bts300.tex] 
Page: 2055 2052–2058 

flowPeaks: a unsupervised clustering for flow data 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>Table 1.</figDesc><table>Comparison of the running time of different flow cytometry 
clustering algorithms 

flowPeaks Misty 
FLOCK flowMeans FLAME flowMerge 
mountain 

Concave 0.13 
0.59 
6 
6.2 
1434 
3202 
Barcode 2.3 
24.7 
14 
82.3 
80 952 
132 446 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><figDesc>Table 2. The performance of different algorithms on the barcode data</figDesc><table>flowPeaks Misty 
FLOCK flowMeans FLAME flowMerge 
mountain 

Adj-Rand 
0.998 
0.971 
0.258 
0.998 
0.859 
0.801 
F-measure 0.993 
0.984 
0.341 
0.993 
0.868 
0.887 
V -measure 0.996 
0.967 
0.567 
0.995 
0.946 
0.952 

Adj-Rand is for the adjusted Rand index. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><figDesc>Table 3. Performance of different algorithms on the concave data</figDesc><table>flowPeaks Misty 
FLOCK flowMeans FLAME flowMerge 
mountain 

Adj-Rand 
1.000 
1.000 
0.501 
0.723 
0.232 
0.952 
F-measure 1.000 
1.000 
0.683 
0.884 
0.438 
0.987 
V -measure 1.000 
1.000 
0.667 
0.713 
0.480 
0.932 

Adj-Rand is for the adjusted Rand index. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><figDesc>Table 4.</figDesc><table>The comparison of the performance on the 12 samples of the GvHD 
dataset 

flowPeaks 
Misty mountain 
flowMeans 

Adj-Rand 
0.807 (0.175) 
0.675 (0.287) 
0.573 (0.292) 
F-measure 
0.924 (0.075) 
0.859 (0.146) 
0.848 (0.120) 
V -measure 
0.816 (0.135) 
0.664 (0.205) 
0.639 (0.199) 

Adj-Rand is for the adjusted Rand index. Each entry lists the mean and standard 
deviation. 

</table></figure>

			<note place="foot">© The Author 2012. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank Fernand Hayot, Istvan Sugar and German Nudleman for valuable comments and discussions. We thank Ryan Brinkman and Nima Aghaeepour for providing the GvHD dataset and the associated manual gates. We appreciate the reviewers' insightful comments, resulting in a much improved article.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm A1</head><p>searching the local peak starting from a point x</p><formula>0. Set x 0 := x, k 0 = L(x), β := β max k 0</formula><p>/10, N suc = 0 and n := 0 1. If β is small or ||f (x n )|| is small, stop. If n is too large, stop./10. Check if we can jump directly to the center of the new cluster:</p><formula>2. Let y := x n +βf (x n )/f (x n ) 3. If f (y) &gt; f (x n ),</formula><formula>if f (μ k n+1 ) &gt; f (x n+1 ), then set x n+1 := μ k n+1 .</formula><p>7. Update n := n+1, go to Step 1.</p><p>where z t = x +t(y−x) andˆfandˆ andˆf (z t ) = f (x)+t f (y)−f (x) . The functionˆf functionˆ functionˆf (z t ) is the fitted density function at the position z t by using a straight line to connect the two points (x,f (x) and (y,f (y)). The second term in defining tol corrects for cluster sample sizes. Many K-means centers may reach the same local peak. A local peak can then be represented by a subset P j of {1,...,K} and its location is denoted by ν j , where j = 1,...,N P and N P is the number of distinct local peaks. In other words, for each k in P j , μ k will move to the same ν j by using our local peak algorithm. Initially, set G g ={g},g = 1,...,N P , i.e. each peak set just contains a single peak (Step 0). Two peak sets can be merged only if the two peaks are relative close and the density function between the peaks is relatively flat (Step 1). G g are merged hierarchically (Step 2). The details are given in Algorithm A2. After the algorithm completes, N G is the number of K (see Section 2.4) final clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm A2 peak merging algorithm</head><formula>0. Let G g ={g},g = 1,...,N G , where N G is initially N P. 1. Set (g,h) = argmin (g,h) {d(G g ,G h ) : G g and G h can be merged, g &lt; h}, If (g,h</formula><p>) do not exist, stop; otherwise go to Step 2. G g and G h can be merged only if there exists a p ∈ G g and a</p><formula>p ∈ G h such that tol(ν p ,ν p ) ≤ tol 0 and ν p −ν p ≤2(S(ν p )+ S(ν p )).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Merge G g and G h as</head><p>in the following:</p><formula>a. Update G g := G g ∪G h b. Set G h := G h+1 ,...,G N G −1 = G N G c. Update N G := N G −1 …</formula><p>3. If N G = 1 stop; otherwise go to Step 1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Rapid cell population identification in flow cytometry data</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Aghaeepour</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cytometry A</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="6" to="13" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">k-means++: the advantages of careful seeding</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Arthur</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Vassilvitskii</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms<address><addrLine>New Orleans, SIAM</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Statistical mixture modeling for cell subtype identification in flow cytometry</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Chan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cytometry A</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="693" to="701" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Merging mixture components for cell population identification in flow cytometry</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Finak</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Bioinformatics</title>
		<imprint>
			<biblScope unit="page">247646</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">On the histogram as a density estimator: L 2 theory</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Freedman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Diaconis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Zeitschrift fur Wahrscheinlichkeitstheorie und verwandte Gebiete</title>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="page" from="453" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical document clustering using frequent itemsets</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">C M</forename>
				<surname>Fung</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third SIAM International Conference on Data Mining (SDM)</title>
		<meeting>the Third SIAM International Conference on Data Mining (SDM)<address><addrLine>San Francisco, CA, SIAM</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="59" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Identification of compounds that enhance the anti-lymphoma activity of rituximab using flow cytometric high-content screening</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Gasparetto</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Immunol. Methods</title>
		<imprint>
			<biblScope unit="volume">292</biblScope>
			<biblScope unit="page" from="59" to="71" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Bioconductor: open software development for computational biology and bioinformatics</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Gentleman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">A K-means clustering algorithm</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">A</forename>
				<surname>Hartigan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Wong</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Hubert</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Arabie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Classif</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">R: a language for data analysis and graphics</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Ihaka</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Gentleman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graph. Stat</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="299" to="314" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Data clustering: 50 years beyond K-means</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">K</forename>
				<surname>Jain</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="651" to="666" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">An efficient k-means clustering algorithm: analysis and implementation</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Kanungo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="881" to="892" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Fluorescent cell barcoding in flow cytometry allows high-throughput drug screening and signaling profiling</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Krutzik</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Nolan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="361" to="368" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">P</forename>
				<surname>Lloyd</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="page" from="28" to="129" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Automated gating of flow cytometry data via robust model-based clustering</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Lo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cytometry A</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="321" to="353" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">flowClust: a Bioconductor package for automated gating of flow cytometry data</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Lo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">145</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Automated identification of subpopulations in flow cytometric list mode data using cluster analysis</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">F</forename>
				<surname>Murphy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cytometry</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="302" to="309" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">The curvHDR method for gating flow cytometry samples</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Naumann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Automated high-dimensional flow cytometric data analysis</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Pyne</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl. Acad. Sci. USA</title>
		<meeting>. Natl. Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="8519" to="8524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Elucidation of seventeen human peripheral blood B-cell subsets and quantification of the tetanus response using a density-based method for the automated identification of cell populations in multidimensional flow cytometry data</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Qian</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cytometry B</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="69" to="82" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Objective criteria for the evaluation of clustering methods</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">M</forename>
				<surname>Rand</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="846" to="850" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">V-Mmeasure: a conditional entropy-based external cluster evaluation measure</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Rosenberg</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Hirschberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning for Computational Lingusistics</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning for Computational Lingusistics<address><addrLine>Prague, Association</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="410" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Misty Mountain clustering: application to fast unsupervised flow cytometry gating</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<forename type="middle">P</forename>
				<surname>Sugar</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">C</forename>
				<surname>Sealfon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">502</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>