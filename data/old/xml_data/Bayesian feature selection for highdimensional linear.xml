
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bayesian feature selection for high-dimensional linear regression via the Ising approximation with applications to genomics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Charles</forename>
								<forename type="middle">K</forename>
								<surname>Fisher</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Physics</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<postCode>02215</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Pankaj</forename>
								<surname>Mehta</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Physics</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<postCode>02215</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bayesian feature selection for high-dimensional linear regression via the Ising approximation with applications to genomics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btv037</idno>
					<note type="submission">Received on August 11, 2014; revised on November 13, 2014; accepted on January 16, 2015</note>
					<note>Gene expression *To whom correspondence should be addressed. Associate Editor: Inanc Birol Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Feature selection, identifying a subset of variables that are relevant for predicting a response , is an important and challenging component of many methods in statistics and machine learning. Feature selection is especially difficult and computationally intensive when the number of variables approaches or exceeds the number of samples, as is often the case for many genomic datasets. Results: Here, we introduce a new approach—the Bayesian Ising Approximation (BIA)—to rapidly calculate posterior probabilities for feature relevance in L2 penalized linear regression. In the regime where the regression problem is strongly regularized by the prior, we show that computing the marginal posterior probabilities for features is equivalent to computing the magnetizations of an Ising model with weak couplings. Using a mean field approximation, we show it is possible to rapidly compute the feature selection path described by the posterior probabilities as a function of the L2 penalty. We present simulations and analytical results illustrating the accuracy of the BIA on some simple regression problems. Finally, we demonstrate the applicability of the BIA to high-dimensional regression by analyzing a gene expression dataset with nearly 30 000 features. These results also highlight the impact of correlations between features on Bayesian feature selection. Availability and implementation: An implementation of the BIA in Cþþ, along with data for reproducing our gene expression analyses, are freely available at</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Linear regression is one of the most broadly and frequently used statistical tools. Despite hundreds of years of research on the subject (<ref type="bibr" target="#b15">Legendre, 1805</ref>), modern applications of linear regression to large datasets present a number of new challenges. Modern applications of linear regression, such as Genome Wide Association Studies (GWAS), often consider datasets that have at least as many potential variables (or features) as there are data points (<ref type="bibr" target="#b19">McCarthy et al., 2008</ref>). Applying linear regression to high-dimensional datasets often involves selecting a subset of relevant features, a problem known as feature selection in the literature on statistics and machine learning (<ref type="bibr" target="#b12">Guyon and Elisseeff, 2003</ref>). Even for classical least-squares linear regression, it turns out that the associated feature selection problem is quite difficult (<ref type="bibr" target="#b14">Huo and Ni, 2007</ref>). The difficulties associated with feature selection are especially pronounced in genomics and GWAS. In general, the goal of many genomics studies is to identify a relationship between a small number of genes and a phenotype of interest, such as height or body V C The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com mass index (<ref type="bibr" target="#b3">Burton et al., 2007;</ref><ref type="bibr" target="#b19">McCarthy et al., 2008;</ref><ref type="bibr" target="#b25">Peng et al., 2010;</ref><ref type="bibr" target="#b28">Subramanian et al., 2005;</ref><ref type="bibr" target="#b31">Wu et al., 2009</ref>). For example, many GWAS seek to identify specific genetic mutations (called single nucleotide polymorphisms—SNPs) that best explain the variation of a quantitative trait, such as height or body mass index, in a population (<ref type="bibr" target="#b32">Yang et al., 2012</ref>). Using various techniques, the trait is regressed against binary variables representing the presence or absence of the SNPs in order to find a subset of SNPs that are highly explanatory for the trait (<ref type="bibr" target="#b25">Peng et al., 2010;</ref><ref type="bibr" target="#b31">Wu et al., 2009</ref>). Although the number of individuals genotyped in such a study may be in the thousands or even tens of thousands, this pales in comparison to the number of potential SNPs which can be in the millions (<ref type="bibr" target="#b19">McCarthy et al., 2008</ref>). Moreover, the presence or absence of various SNPs tends to be correlated due to chromosome structure and genetic processes that induce the so-called linkage disequilibrium (<ref type="bibr" target="#b32">Yang et al., 2012</ref>). As a result, selecting the best subset of SNPs for the regression involves a search for the global minimum of a landscape that is both high dimensional (due to the large number of SNPs) and rugged (due to correlations between SNPs). The obstacles that make feature selection difficult in GWAS also occur in many other applications of linear regression to big datasets. In fact, the task of finding the optimal subset of features is proven, in general, to be NP-hard (<ref type="bibr" target="#b14">Huo and Ni, 2007</ref>). Therefore, it is usually computationally prohibitive to search over all possible subsets of features and one has to resort to other methods of feature selection. For example, forward (or backward) selection adds (or eliminates) one feature at a time to the regression in a greedy manner (<ref type="bibr" target="#b12">Guyon and Elisseeff, 2003</ref>). Alternatively, one may use heuristic methods such as Sure Independence Screening (SIS) (<ref type="bibr" target="#b7">Fan and Lv, 2008</ref>), which selects features independently based on their correlation with the response, or Minimum Redundancy Maximum Relevance (<ref type="bibr" target="#b6">Ding and Peng, 2005</ref>), which penalizes features that are correlated with each other. The most popular approaches to feature selection for linear regression, however, are penalized least-squares methods (<ref type="bibr" target="#b5">Candes and Tao, 2007;</ref><ref type="bibr" target="#b13">Hoerl and Kennard, 1970;</ref><ref type="bibr" target="#b29">Tibshirani, 1996;</ref><ref type="bibr" target="#b34">Zou and Hastie, 2005</ref>) that introduce a function that penalizes large regression coefficients. Common choices for the penalty function include an L2 penalty, called 'Ridge' regression (<ref type="bibr" target="#b13">Hoerl and Kennard, 1970</ref>), and an L1 penalty, commonly referred to as LASSO regression (<ref type="bibr" target="#b29">Tibshirani, 1996</ref>). Penalized methods for linear regression typically have natural interpretations as Bayesian approaches with appropriately chosen prior distributions. For example, L2 penalized regression can be derived by maximizing the posterior distribution obtained with a Gaussian prior on the regression coefficients. Similarly, L1 penalized regression can be derived by maximizing the posterior distribution obtained with a Laplace (i.e. double-exponential) prior on the regression coefficients. While penalized regression methods essentially aim to find the features that maximize a posterior distribution they do not allow one to actually compute posterior probabilities, which provide information about confidence in a Bayesian framework. Calculating these posterior probabilities generally requires Monte Carlo methods, which can be very computationally demanding in high dimensions (<ref type="bibr" target="#b9">George and McCulloch, 1993;</ref><ref type="bibr" target="#b11">Guan et al., 2011;</ref><ref type="bibr" target="#b16">Li and Zhang, 2010</ref>). Thus, in order to apply Bayesian approaches to feature selection to highdimensional problems it is necessary to develop approximate methods for computing posterior probabilities that bypass the need for extensive sampling from the posterior distribution. Inspired by the success of statistical physics approaches to hard problems in computer science (<ref type="bibr" target="#b20">Mézard et al., 2002;</ref><ref type="bibr" target="#b21">Monasson et al., 1999</ref>) and statistics (<ref type="bibr" target="#b1">Balasubramanian, 1997;</ref><ref type="bibr" target="#b18">Malzahn and Opper, 2005;</ref><ref type="bibr" target="#b22">Nemenman and Bialek, 2002</ref>), we study high-dimensional regression with 'strongly regularizing' prior distributions. A strongly regularizing prior distribution is one that exerts a significant influence on the posterior distribution even when the sample size goes to infinity. The definition will be made more precise later. In this strongly regularized regime, we show that the marginal posterior probabilities of feature relevance for L2 penalized regression are well-approximated by the magnetizations of an appropriately chosen Ising model—a widely studied model from physics used to describe magnetic materials (<ref type="bibr" target="#b24">Opper and Winther, 2001</ref>). For this reason, we call our approach the Bayesian Ising Approximation (BIA) of the posterior distribution. Using the BIA, the posterior probabilities can be computed without resorting to Monte Carlo simulation using an efficient mean field approximation that facilitates the analysis of very high-dimensional datasets. We envision the BIA as part of a two-stage procedure where the BIA is applied to rapidly screen irrelevant variables, i.e. those that have low rank in posterior probability, before applying a more computationally intensive cross-validation procedure to infer the regression coefficients for the reduced feature set. This study is especially well suited to modern feature selection problems where the number of features, p, is often larger than the sample size, n. Our approach differs significantly from previous methods for feature selection. Traditionally, penalized regression and related Bayesian approaches have focused on the 'weakly regularized regime' where the effect of the prior is assumed to be negligible as the sample size tends to infinity. The underlying intuition for considering the weak-regularization regime is that as long as the prior (i.e. the penalty parameter) is strong enough to regularize the inference problem, a less influential prior distribution should be better suited for feature selection and prediction tasks because it 'allows the data to speak for themselves' (<ref type="bibr" target="#b8">Gelman et al., 2013</ref>). In the machine learning literature, the penalty parameter is usually chosen using cross validation to maximize out-of-sample predictive ability (<ref type="bibr" target="#b29">Tibshirani, 1996;</ref><ref type="bibr" target="#b34">Zou and Hastie, 2005</ref>). A similar esthetic is also reflected in the abundant literature on 'objective' priors for Bayesian inference (<ref type="bibr" target="#b10">Ghosh et al., 2011</ref>). As expected, these weakly regularizing approaches perform well when the sample size exceeds the number of features ðn ) pÞ. However, very strong priors may be required for high-dimensional inference where the number of features can greatly exceed the sample size ðp ) nÞ. Our BIA approach exploits the large penalty parameter in this strongly regularized regime to efficiently calculate marginal posterior probabilities using methods from statistical physics. The article is organized as follows: in Section 2.1, we review Bayesian linear regression; in Section 2.2, we derive the BIA using a series expansion of the posterior distribution and describe the associated algorithm for variable selection; and in Section 3.1, we present analytical results and simulations on the performance of the BIA using features with a constant correlation, in Section 3.2 we analyze a real dataset for predicting bodyfat percentage from 12 different body measurements and in Section 3.3 we analyze a real dataset for predicting a quantitative phenotypic trait from data on the expression of 28 395 genes in soybeans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bayesian linear regression</head><p>In this section, we briefly review the necessary aspects of Bayesian linear regression. This entire section follows standard arguments, the details of which can be found in many textbooks on Bayesian statistics (see e.g. O'<ref type="bibr">Hagan et al., 2004</ref>). The goal of linear regression is to infer the set of coefficients b j for j ¼ 1;. .. ; p that describe the relationship y ¼ x T b þ g from n observations ðy i ; x i Þ for i ¼ 1;. .. ; n. Here, x is a (p Â 1Þ vector of features and g $ N ð0; r 2 Þ is a Gaussian distributed random variable with unknown variance r 2. Without loss of generality, we will assume throughout this article that the data are standardized with</p><formula>X i y i ¼ 0; X i y 2 i ¼ n; X i ðx i Þ j ¼ 0 and X i ðx i Þ 2</formula><p>j ¼ n so that it is not necessary to include an intercept term in the regression. Penalized least-squares methods estimate the regression coefficients by minimizing a convex objective function in the form of:</p><formula>UðbÞ ¼ X i ðy i À x T i bÞ 2 þ kf ðbÞ; (1)</formula><p>where f ðbÞ is a function that penalizes large regression coefficients and k is the strength of the penalty. Common choices for the penalty function include f ðbÞ ¼ X j b 2 j for L2 penalized or 'Ridge' regression (<ref type="bibr" target="#b13">Hoerl and Kennard, 1970</ref>), and f ðbÞ ¼ X j jb j j for L1 penalized or LASSO regression (<ref type="bibr" target="#b29">Tibshirani, 1996</ref>). The standard least-squares (and maximum likelihood) estimate ^ b ¼ ðX T XÞ À1 X T y is recovered by setting k ¼ 0, where X is the ðn Â pÞ design matrix with columns x i. Adding a penalty to the least-squares objective function mitigates instability that results from computing the inverse of the X T X matrix. In the case of the L1 penalty, many of the regression coefficients end up being shrunk exactly to 0 resulting in a type of automatic feature selection (<ref type="bibr" target="#b5">Candes and Tao, 2007;</ref><ref type="bibr" target="#b29">Tibshirani, 1996;</ref><ref type="bibr" target="#b34">Zou and Hastie, 2005</ref>). Bayesian methods combine the information from the data, described by the likelihood function, with a priori knowledge, described by a prior distribution, to construct a posterior distribution that describes one's knowledge about the parameters after observing the data. In the case of linear regression, the likelihood function is a Gaussian</p><formula>Pðyjb; r 2 Þ ¼ 1 ffiffiffiffiffiffiffiffiffiffi ffi 2pr 2 p n exp À ðy À XbÞ T ðy À XbÞ 2r 2 ! :</formula><p>In this work, we will use standard conjugate prior distributions for b and r 2 given byPðb; r 2 jsÞ ¼ Pðr 2 ÞPðbjr 2 ; sÞwhere</p><formula>Pðr 2 Þ / ðr 2 Þ Àða0þ1Þ exp ðÀb 0 =r 2 Þ Pðbjr 2 ; sÞ / Y j ð1 À s j Þdðb j Þ þ ð1 þ s j Þ ffiffiffiffiffiffiffiffiffiffi ffi k 2pr 2 r exp À kb 2 j 2r 2 ! " # :</formula><p>These distributions were chosen because they ensure that the posterior distribution can be obtained in closed-form (O'<ref type="bibr">Hagan et al., 2004</ref>). Here, we have introduced a vector (s) of indicator variables so that b j ¼ 0 if s j ¼ À1 and b j 6 ¼ 0 if s j ¼ þ1. We also have to specify a prior for the indicator variables, which we will set to a flat prior PðsÞ / 1 for simplicity. In principle, a 0 , b 0 and the penalty parameter on the regression coefficients, k, are free parameters that must be specified ahead of time to reflect our prior knowledge. We will discuss these parameters in the following section. We have set up the problem so that identifying which features are relevant is equivalent to identifying those features for which s j ¼ þ1. Therefore, we need to compute the posterior distribution for s, which can be determined from Bayes' theorem:</p><formula>log P k ðsjyÞ þ C ¼ log ð dbdr 2 Pðyjb; r 2 ÞPðb; r 2 jsÞPðsÞ ¼ 1 2 lnjkIj À 1 2 lnjkI þ X T s X s j À a 0 þ n 2 ln b 0 þ 1 2 E s ðkÞ ;</formula><formula>(2)</formula><p>where C is a constant and E s ðkÞ is the sum of the squared residual errors. In this expression, q ¼ X j ð1 þ s j Þ=2 is the number of variables with s j ¼ þ1, I is the ðq Â qÞ identity matrix and X s is a ðn Â qÞ restricted design matrix which only contains columns corresponding to features where s j ¼ þ1. The sum of the squared residual errors is given by E s ðkÞ ¼ y T y À y T X s b s ðkÞ, where b s ðkÞ ¼ ðkI þ X T s X s Þ À1 X T s y is the Bayesian estimate for the regression coefficients corresponding to those variables for which s j ¼ þ1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Ising approximation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Strongly regularized expansion</head><p>In principle, one can directly use Equation (2) to estimate the relevance of each feature using two different approaches. First, we could find the s that maximizes the posterior probability distribution. Alternatively, we could compute the marginal probabilities of feature relevance, P k ðs j ¼ þ1jyÞ ¼ ð1 þ hs j iÞ=2, where hs j i is the expectation value of s j with respect to the posterior distribution, and select the features with the largest P k ðs j ¼ þ1jyÞ. In the Bayesian setting, these two point estimates result from the use of different utility functions (<ref type="bibr" target="#b2">Berger, 1985</ref>). Here, we will focus on computing the latter, i.e., the expected value of s. The expectation values cannot be evaluated analytically due to the cumbersome restriction of the design matrix to those variables for which s j ¼ þ1. Moreover, although the computation of the expectation values can be performed using Monte Carlo methods (<ref type="bibr" target="#b9">George and McCulloch, 1993;</ref><ref type="bibr" target="#b16">Li and Zhang, 2010</ref>), the numerical calculations often take a long time to converge for high-dimensional inference problems. Our main result—which we call the BIA of the posterior distribution for feature selection—is that a second-order series expansion of Equation (2) in k À1 corresponds to an Ising model described by logP k ðsjyÞ '</p><formula>n 2 4k X i h i ðkÞs i þ 1 2 X i;j;i6 ¼j J ij ðkÞs i s j ! (3)</formula><p>with an error that is O k À3 Tr½ðX T s X s Þ 3  where Tr½Á is the matrix trace operator and the external fields and couplings are defined as</p><formula>h i ðkÞ ¼ r 2 ðy; x i Þ À 1 n þ X j J ij ðkÞ (4) J ij ðkÞ ¼ k À1 r 2 ðx i ; x j Þ À n k rðx i ; x j Þrðy; x i Þrðy; x j Þ À 1 2 r 2 ðy; x i Þr 2 ðy; x j Þ :</formula><formula>(5)</formula><p>Here, rðz 1 ; z 2 Þ is the Pearson correlation coefficient between variables z 1 and z 2. In writing this expression, we have assumed that the hyperparameters a 0 and b 0 are small enough to neglect, though this assumption is not necessary. A detailed derivation of this result is presented in the Supporting Information. The series expansion converges as long as k k &gt; Tr½ðX T s X s Þ k  for all s and integer powers k!1, which defines the regime that we call 'strongly regularized'. Since X s is the restricted design matrix for standardized data, we can relate Tr½ðX T s X s Þ k  to the covariances between x j 's. In particular, Gershgorin's Circle Theorem (<ref type="bibr" target="#b30">Varga, 2010</ref>) implies that the series will converge as long as k &gt; nð1 þ pr~Þ where r~</p><formula>¼ 1 p inf i X j6 ¼i</formula><p>jrðX i ; X j Þj (see Supporting Information). For large p, we can replace r~ by the root-mean-squared correlation between features,</p><formula>r ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi p À1 ðp À 1Þ À1 X i6 ¼j r 2 ðX i ; X j Þ s .</formula><p>This defines a natural scale k Ã ¼ nð1 þ prÞ:</p><formula>(6)</formula><p>for the penalty parameter at which the BIA is expected to breakdown. We expect the BIA to be accurate when k ) k Ã and to breakdown when k ( k Ã. Because higher-order terms in the series can be neglected, the strongly regularized expansion allows us to remove any references to the restricted design matrix, and maps the posterior distribution to the Ising model, which has been studied extensively in the physics literature. Moreover, the magnitude of the couplings (J ij ) scales as k À1 , ensuring that the couplings are weak, which will allow us to compute posterior probabilities analytically. To perform feature selection, we are interested in computing marginal probabilities P k ðs j ¼ 1jyÞ ' ð1 þ m j ðkÞÞ=2, where we have defined the magnetizations m j ðkÞ ¼ hs j i. While there are many techniques for calculating the magnetizations of an Ising model, we focus on the mean field approximation which leads to a self-consistent equation (<ref type="bibr" target="#b24">Opper and Winther, 2001</ref>):</p><formula>m i ðkÞ ¼ tanh n 2 4k h i ðkÞ þ 1 2 X j6 ¼i J ij ðkÞm j ðkÞ ! " # : (7)</formula><p>This mean field approximation provides a computationally efficient tool that approximates Bayesian feature selection for linear regression, requiring only the calculation of the Pearson correlations and solution of Equation (7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Computing the feature selection path</head><p>As with other approaches to penalized regression, our expressions depend on a free parameter (k) that determines the strength of the prior distribution. As it is usually difficult, in practice, to choose a specific value of k ahead of time it is often helpful to compute the feature selection path; i.e. to compute m j ðkÞ over a wide range of k's. Indeed, computing the variable selection path is a common practice when applying other feature selection techniques such as LASSO regression. To obtain the mean field variable selection path as a function of ¼ 1=k, we notice that lim !0 m j ðÞ ¼ 0 and so define the recursive formula</p><formula>m i ð þ dÞ % tanh ð þ dÞn 2 4 h i ð þ dÞ þ 1 2 X j6 ¼i J ij ð þ dÞm j ðÞ ! " #</formula><p>with a small step size d ( 1=k Ã ¼ n À1 ð1 þ prÞ À1. We have set d ¼ 0:05=k Ã in all of the examples presented below. We note that our implementation of the BIA is an example of homotopy algorithm and could potentially be improved by applying more advanced methods (<ref type="bibr" target="#b0">Allgower and Georg, 2003</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Remarks</head><p>The BIA provides a computationally efficient framework to calculate posterior probabilities of feature relevance as a function of k without Monte Carlo simulations. We have used a simple, unoptimized Cþþ implementation of the BIA method. Using this code, computing the entire feature selection path for a genomics dataset with almost 30 000 features took $15 min on a desktop computer with 24 GB of RAM and two 2.4 GHz 6-core Intel Xeon processors. The bulk of the computational effort—in terms of both processing power and memory usage—is expended computing the ðp Â pÞ correlation matrix. For example, computing the feature selection path for the genomics dataset with our naive implementation required $15 GB of RAM. However, any method designed for efficiently computing large correlation matrices could be applied to improve the computational performance of the BIA. For example, adaptive thresholding estimators could be used to obtain a sparse correlation matrix that requires less memory (<ref type="bibr" target="#b4">Cai and Liu, 2011</ref>). In any case, we have left the optimization of the code for future research. To first order in ¼ k À1 , the posterior distribution corresponds to an Ising model with fields and couplings given by h i ¼ r 2 ðy; x i Þ À1=n and J ij ¼ 0. That is, the indicator variables representing feature relevance are independent, and the probability that a feature is relevant is only a function of its squared correlation with the response. Specifically, m j ðkÞ!0 if jrðy; x j Þj &gt; 1= ffiffiffi n p and m j ðkÞ 0 if jrðy; x j Þj &lt; 1= ffiffiffi n p. Therefore, the BIA demonstrates that methods that rank features by their squared correlation with the response, such as SIS (<ref type="bibr" target="#b7">Fan and Lv, 2008</ref>), are actually performing a first-order approximation to Bayesian feature selection in the strongly regularized limit. The couplings between the spin variables representing feature relevance enter into the BIA with the second-order term in ¼ k À1. A positive coupling between spins i and j favors models that include both features i and j, whereas a negative coupling favors models that include one feature or the other, but not both. In general, the coupling terms are negative for highly correlated variables which minimizes the redundancy of the feature set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Examples</head><p>We have chosen three examples to illustrate different characteristics of the BIA for Bayesian feature selection. (A) First, we consider regression problems with p features that have a constant correlation r. We present some simple analytic expressions in the large p limit that illustrates how different aspects of the problem affect feature selection performance, and study some simulated data. (B) Next, we analyze a dataset on the prediction of bodyfat percentage from various body measurements. The number of features (p ¼ 12) is small enough that we can compute the exact posterior probabilities and, therefore, directly assess the accuracy of the BIA for these data. (C) Finally, we demonstrate the applicability of the BIA for feature selection on high-dimensional regression problems by examining a dataset relating the expression of p ¼ 28 395 genes to the susceptibility of soybean plants to a pathogen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Features with a constant correlation</head><p>Correlations between features are detrimental to feature selection. For example, suppose that we observe a response variable y given by y ¼ bx 1 þ g. A second feature x 2 that is strongly correlated with x 1 will also be correlated with y. Thus, identifying which feature, x 1 or x 2 , is the relevant one is not an easy task. Of course, the reasoning becomes more complicated in high dimensions, but similar effects are observed in high-dimensional regression with the LASSO (<ref type="bibr" target="#b29">Tibshirani, 1996;</ref><ref type="bibr" target="#b34">Zou and Hastie, 2005</ref>). Given these observations, we use this section to analyze a simple model of BIA feature selection that allows us to examine many of the characteristics that influence feature selection performance. Specifically, we consider a simple, analytically tractable, model in which we are given p features that are correlated with each other with a constant Pearson correlation coefficient, r. The response, y~, is a linear function of the first p~ p variables, which have equal true regression coefficients b j ¼ b for j p~. That is, y~¼ b X j¼p~ j¼1 x j þg~ where g~$ N ð0; r~ 2 Þ is a Gaussian noise. We are interested in studying the behavior of this model when the number of features is large (p ) 1). To simplify analytic expressions, it is helpful to define the number of samples as n ¼ hp, and the number of relevant features as p~¼ /p. Furthermore, we assume that the correlation between features scales as r ¼ ap À1 so that the correlation between y and x j stays constant in the large p limit.<ref type="figure" target="#fig_0">Figure 1a</ref>presents an example feature selection path computed using the BIA for a simulation of this model. This variable selection path was generated for data simulated from a linear model using p ¼ 200 features with a constant correlation r ¼ 2=p, n ¼ 100, p~¼ 10 and x 2 ¼ r~ 2 =b 2 ¼ 1.<ref type="figure" target="#fig_0">Figure 1a</ref>demonstrates that all but one of the relevant features (red) have higher posterior probabilities than the irrelevant features (black) as long as k &gt; k Ã. In fact, there is a clear gap in posterior probability separating the relevant and irrelevant features, and the correct features can be easily selected by visible inspection of the feature selection path in<ref type="figure" target="#fig_0">Fig. 1a</ref>. The BIA breaks down beyond the threshold of the penalty parameter and the feature selection performance of the BIA deteriorates, as demonstrated by the mixing of the probabilities for the relevant (red lines) and irrelevant (black lines) features in<ref type="figure" target="#fig_0">Fig. 1a</ref>. The indicator variables characterizing the feature selection problem can be divided into two groups: relevant features with j p~ and magnetization m ðþÞ , and irrelevant features with j &gt; p~ and magnetization m ðÀÞ. Note that an algorithm that performs perfect variable selection will have m ðþÞ ¼ þ1 and m ðÀÞ ¼ À1. The Pearson correlation coefficient of a relevant feature (j p~) with the standardized response y ¼ y~= ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi VARðy~Þ p is given by</p><formula>rðy; x j¼1 ... p~ Þ r ðþÞ ¼ 1 þ rðp~À 1Þ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi x 2 þ p~ðrp~þ 1 À rÞ p ;</formula><p>where x 2 ¼ r~ 2 =b 2 $ Oð1Þ is an inverse signal-to-noise ratio. Similarly, the Pearson correlation coefficient of an irrelevant variable (j &gt; p~) with the standardized response is</p><formula>rðy; x j¼p~þ1; ... ;p Þ r ðÀÞ ¼ rp~ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi x 2 þ p~ðrp~þ 1 À rÞ p :</formula><p>Note that correlations make this problem incredibly difficult when the number of true features is large, i.e. r ðÀÞ =r ðþÞ ! 1 as p~! 1 for r &gt; 0. If we choose k ¼ hp 2 to ensure that the problem is always in the strongly regularized regime, the magnetizations can be computed explicitly to order 1=p giving</p><formula>m ðþÞ % h À /ð1 À ahÞ 4/ 1 p þ O 1 p 2 ; m ðÀÞ % À 1 þ a/ À a 2 /h 4ð1 þ a/Þ 1 p þ O 1 p 2 :</formula><p>In general, we say that feature selection performance is good, on average, as long as m ðÀÞ &lt; 0 &lt; m ðþÞ , because relevant features have Pðs j ¼ þ1jyÞ &gt; 1=2 and irrelevant features have Pðs j ¼ þ1jyÞ &lt; 1=2.<ref type="figure" target="#fig_0">Figure 1b</ref>shows that the average feature selection performance is good in this sense within a large volume of the phase space. Specifically,</p><formula>m ðÀÞ &lt; 0 &lt; m ðþÞ when 1 1 þ a/ &lt; h / &lt; 1 þ a/ ð/aÞ 2 :</formula><p>However, m ðÀÞ &lt; m ðþÞ even if the stronger statement m ðÀÞ &lt; 0 &lt; m ðþÞ is not satisfied. As a result, there is always a gap between the posterior probabilities of the relevant and irrelevant features. Nevertheless, the gap between the relevant and irrelevant features shrinks with increasing correlations, suggesting that feature selection performance will be strongly affected by sample-to-sample fluctuations, which we have neglected here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bodyfat percentage</head><p>Bodyfat percentage is an important indicator of health, but obtaining accurate estimates of bodyfat percentage is challenging. For example, underwater weighing is one of the most accurate methods for measuring bodyfat percentage but it requires special equipment, e.g. a pool. Here, we analyze a well-known dataset obtained from StatLib (http://lib.stat.cmu.edu/datasets/) on the relationship between bodyfat percentage and various body measurements from n ¼ 252 men (<ref type="bibr" target="#b26">Penrose et al., 1985</ref>). The p ¼ 12 features included in our regression are age and body mass index (height=mass 2 ), as well as circumference measurements of the neck, chest, waist, hip, thigh, knee, ankle, upper arm, forearm and wrist. All of the data were standardized to have mean 0 and variance 1. Therefore, there are 2 12 ¼ 4096 potential combinations of features. For our purposes, the most interesting part about the bodyfat dataset is that the number of features is small enough to compute the posterior probabilities exactly using Equation (2) by enumerating all of the 4096 feature combinations. The exact posterior probabilities as a function of k À1 are shown in<ref type="figure" target="#fig_1">Fig. 2a</ref>.The posterior probabilities computed from recursive solution of the BIA are shown in<ref type="figure" target="#fig_1">Fig. 2b. Comparing</ref><ref type="figure" target="#fig_1">Fig. 2a</ref>with<ref type="figure" target="#fig_1">Fig. 2b</ref>demonstrates that the posterior probabilities computed from the BIA are very accurate for k ) k Ã , with k Ã ¼ nð1 þ prÞ and r the root-mean-squared correlation between features. However, the approximation breaks down for k ( k Ã as expected.<ref type="figure" target="#fig_1">Figure 2c</ref>provides another representation of the breakdown of the BIA upon approaching the breakdown point of the penalty (k Ã ). The root-mean-squared error given by RMSE ðkÞ ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi</p><formula>p À1 X j ðP exact k ðs j ¼ 1jyÞ À P BIA k ðs j ¼ 1jyÞÞ 2 s is sig</formula><p>moidal, with an inflection point close to k Ã. In the strongly regularized regime with k ) k Ã , the exact Bayesian probabilities and those computed using the BIA both rank waist and chest circumference as the most relevant features. Below the breakdown point of the penalty parameter, however, the BIA suggests solutions that are too sparse. That is, it underestimates many of the posterior probabilities describing whether or not the features are relevant. Far below the breakdown point of the penalty parameter (beyond the range of the graph in<ref type="figure" target="#fig_1">Fig. 2</ref>), the BIA ranks age and body mass index as the most relevant variables even thoughwhere m ðÀÞ &lt; 0 &lt; m ðþÞ computed with k ¼ hp 2 these have some of the smallest correlations with the response. Age and body mass index also become increasingly important for small k's in the exact calculation; though, they are never ranked as the most relevant variables. The change in the rankings of the features as a function of k highlights the importance of the coupling terms (J ij ðkÞ) that punish correlated features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Gene expression</head><p>In 2010, the Dialogue for Reverse Engineering Assessments and Methods (DREAM) (<ref type="bibr" target="#b27">Prill et al., 2010</ref>) initiative issued a challenge to predict the response of soybean plants to a pathogen from data on gene expression (<ref type="bibr" target="#b33">Zhou et al., 2009</ref>). These DREAM5 training data consist of a response of n ¼ 200 different soybean plants to a pathogen (specifically, the response is a measure of the amount of pathogen in an infected tissue sample) along with the expressions of p ¼ 28 395 genes. The team (<ref type="bibr" target="#b17">Loh et al., 2011</ref>) that achieved the highest rank correlation on a blind test set of 30 other soybean plants trained their model using elastic net regression to predict the ranks of the responses in the training set. The ranks were used rather than the actual values of the responses to mitigate the effects of outliers, and the value of the penalty parameter was chosen using cross validation.<ref type="bibr" target="#b17">Loh et al. (2011)</ref>found that their cross-validation procedure for elastic net regression favored sparse models with only a few features, and they highlighted 12 of these features that were frequently chosen by their procedure. However, even the best teams achieved only modest performance on the test data (<ref type="bibr" target="#b17">Loh et al., 2011</ref>). Nevertheless, the soybean gene expression dataset presents a good benchmark to compare Bayesian feature selection with the BIA to feature selection using cross-validated penalized regression for a very high-dimensional inference problem. We used the BIA to compute the posterior probabilities for all p ¼ 28 395 features as a function of k À1 using the ranks of the responses of the soybean plants to the pathogen as our y variable. As before, all of the data were standardized to have mean 0 and variance 1.<ref type="figure" target="#fig_2">Figure 3a</ref>compares the posterior probabilities of the 12 features highlighted by<ref type="bibr" target="#b17">Loh et al. (2011)</ref>(red lines) to the distribution of posterior probabilities for all of the features (gray area). Visual inspection of<ref type="figure" target="#fig_2">Fig. 3a</ref>suggests that the 12 features identified by<ref type="bibr" target="#b17">Loh et al. (2011)</ref>have some of the highest posterior probabilities among all 28 395 features. Similarly,<ref type="figure" target="#fig_2">Fig. 3b</ref>shows that only a small percentage of features have higher posterior probabilities than those identified by<ref type="bibr" target="#b17">Loh et al. (2011)</ref>, demonstrating that there is generally a pretty good agreement between features that are predictive (i.e. those that perform well in cross validation) and those with high posterior probabilities computed with the BIA. Although our analyses of the soybean gene expression data identify similar features as cross-validated elastic net regression, the posterior probabilities all fall in the range P k ðs j jyÞ ¼ 1=260:001. The small range of posterior probabilities around the value representing random chance (P k ðs j jyÞ ¼ 1=2) is consistent with the highly variable out-of-sample performance discussed by<ref type="bibr" target="#b17">Loh et al. (2011)</ref>. One reason for the generally poor performance of feature selection on these data, aside from the underdetermined nature of the problem, is that the expressions of the genes are significantly correlated (r % 0:29). To demonstrate this, we constructed synthetic datasets with varying numbers of relevant and irrelevant genes and computed the rate at which true features were identified by the BIA (<ref type="figure">Fig. 4</ref>and Supporting Information). Like the original data, these synthetic datasets each contained n ¼ 200 distinct samples. The true positive rate (or sensitivity) was defined as the fraction of true features among the q features with the highest BIA posterior probabilities at k ¼ 0:5k Ã. Comparing the true positive rates of BIA feature selection on synthetic data using genes with a strong correlation (r % 0:28,<ref type="figure">Fig. 4a</ref>) and synthetic data with a weak correlation obtained by randomly shuffling the genes (r % 0:07,<ref type="figure">Fig. 4b</ref>) clearly demonstrates the dramatic effect that interfeature correlations have on feature selection performance. This highlights the importance of strong</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>To summarize, we have shown that Bayesian feature selection for L2 penalized regression, in the strongly regularized regime, corresponds to an Ising model, which we call the BIA. Mapping the posterior distribution to an Ising model that has simple expressions for the local fields and couplings using a controlled approximation opens the door to analytical studies of Bayesian feature selection using the vast number of techniques developed in physics for studying the Ising model. It will be interesting to see if our analyses can be generalized to study Bayesian feature selection for many statistical techniques other than linear regression, as well as other prior distributions. From a practical standpoint, the BIA provides an algorithm to efficiently compute Bayesian feature selection paths for L2 penalized regression. Using our approach, it is possible to compute posterior probabilities of feature relevance for very high-dimensional datasets such as those typically found in genomic studies. Unlike most previous work of feature selection, the BIA is ideally suited for large genomic datasets where the number of features can be much greater than the sample size, p ) n. The underlying reason for this is that we work in strongly regularized regime where the prior always has a large influence on the posterior probabilities. This is in contrast to previous works on penalized regression and related Bayesian approaches that have focused on the 'weakly regularized regime' where the effect of the prior is assumed to be small. Moreover, we have identified a sharp threshold for the regularization parameter k Ã ¼ nð1 þ prÞ where the BIA is expected to break down. This threshold depends on the sample size, n, number of features, p, and root-mean-squared correlation between features, r. The threshold at which the BIA breaks down occurs precisely at the transition from the strongly regularized to the weakly regularized regimes where the prior and the likelihood have a comparable influence on the posterior distribution. This study also highlights the importance of accounting for correlations between features when assessing statistical significance in large datasets. When the number of features is large, even small correlations can cause a huge reduction in the posterior probabilities of features. For example, our analysis of a dataset including the expression of 28 395 genes demonstrates that the resulting posterior probabilities of gene relevance may be very close to value representing random chance P k ðs j jyÞ ¼ 1=2 when p ) n and the genes are moderately correlated, e.g. r % 0:29. This is likely to have important implications for assessing the results of GWAS studies where such correlations are often ignored. Moreover, we suggest that it is generally not reasonable to choose a posterior probability threshold for judging significance on very high-dimensional problems. Instead, the BIA can be used as part of a two-stage procedure, where the BIA is applied to rapidly screen irrelevant variables, i.e. those that have low rank in posterior probability, before applying a more computationally intensive crossvalidation procedure to infer the regression coefficients. The computational efficiency of the BIA and the existence of a natural threshold for the penalty parameter where the BIA works make this procedure ideally suited for such two-stage procedures.<ref type="figure">Fig. 4</ref>. True positive rates for feature selection with (a) correlated and (b) uncorrelated genes. Features were selected by taking the q genes with highest posterior probability at k ¼ 0:5k Ã. The uncorrelated genes were created by randomly shuffling the correlated genes. The root mean squared correlation among the correlated genes was r ¼ 0.28 compared with r ¼ 0.7 for the uncorrelated genes</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. Performance of BIA feature selection. (a) An example variable selection path as a function of decreasing regularization. The relevant variables are red, and the irrelevant variables are black. The dashed vertical line is at k ¼ k Ã ¼ nð1 þ rpÞ, which is the estimated breakdown point of the approximation. Simulations were performed with p ¼ 200, n ¼ 100, $ p $ ¼ 10; r ¼ 2=p and x 2 ¼ 1. (b) A phase diagram illustrating the regions of parameter space where m ðÀÞ &lt; 0 &lt; m ðþÞ computed with k ¼ hp 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.2.</head><figDesc>Fig. 2. Comparison of exact Bayesian marginal probabilities to the BIA for the bodyfat data. (a) Exact Bayesian marginal probabilities for decreasing regularization. (b) BIA approximations of the marginal probabilities for decreasing regularization. (c) RMSE between the exact and BIA probabilities as a function of decreasing regularization. The dashed vertical line is at k ¼ k Ã ¼ nð1 þ rpÞ, which is the estimated breakdown point of the approximation. The variables have been color coded (blue to red) by increasing squared Pearson correlation coefficient with the response (bodyfat percentage)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.3.</head><figDesc>Fig. 3. Feature selection path for the gene expression data. The problem is severely under-determined, involving the prediction of a quantitative phenotype from the expressions of p ¼ 28 395 genes given a sample size of n ¼ 200 and, therefore, the posterior probabilities remain close to Pkðs j ¼ 1jyÞ ¼ 1=2. (a) Features selected in a previous study (red lines) by cross validation with the elastic net have high ranking posterior probabilities. Gray area represents the 1–99% quantiles, and the black area represents the 25–75% quantiles. (b) The median (solid black line) and mean (dashed red line) percentage of features with higher posterior probabilities than those identified by Loh et al. (2011). The vertical axis is a logarithmic scale. The dashed vertical line is at the breakdown point k ¼ k Ã ¼ nð1 þ rpÞ</figDesc></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">C.K.Fisher and P.Mehta at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Alex Lang, Javad Noorbakhsh and David Schwab for comments on this manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<monogr>
		<title level="m" type="main">Introduction to Numerical Continuation Methods</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">L</forename>
				<surname>Allgower</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Georg</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<pubPlace>SIAM, Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Statistical inference, Occam&apos;s razor, and statistical mechanics on the space of probability distributions</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Balasubramanian</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="349" to="368" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<monogr>
		<title level="m" type="main">Statistical Decision Theory and Bayesian Analysis</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">O</forename>
				<surname>Berger</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Genome-wide association study of 14,000 cases of seven common diseases and 3,000 shared controls</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">R</forename>
				<surname>Burton</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">447</biblScope>
			<biblScope unit="page" from="661" to="678" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive thresholding for sparse covariance matrix estimation</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Cai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Liu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="672" to="684" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">The dantzig selector: statistical estimation when p is much larger than n</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Candes</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Tao</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2313" to="2351" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Minimum redundancy feature selection from microarray gene expression data</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Ding</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Peng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Bioinform. Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="185" to="205" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Sure independence screening for ultrahigh dimensional feature space</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Fan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Lv</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="849" to="911" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<monogr>
		<title level="m" type="main">Bayesian Data Analysis</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Gelman</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CRC Press</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Variable selection via Gibbs sampling</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">I</forename>
				<surname>George</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">E</forename>
				<surname>Mcculloch</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="881" to="889" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Objective priors: an introduction for frequentists</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ghosh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Sci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="187" to="202" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Bayesian variable selection regression for genome-wide association studies and other large-scale problems</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Guan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1780" to="1815" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">An introduction to variable and feature selection</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Guyon</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Elisseeff</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learning Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1157" to="1182" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Ridge regression: biased estimation for nonorthogonal problems</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">E</forename>
				<surname>Hoerl</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">W</forename>
				<surname>Kennard</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">When do stepwise algorithms meet subset selection criteria?</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Huo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Ni</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="870" to="887" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">M</forename>
				<surname>Legendre</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1805" />
			<pubPlace>Paris</pubPlace>
		</imprint>
	</monogr>
	<note>Nouvelles. Méthodes Pour la Détermination des Orbites des Cometes. F. Didot</note>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Bayesian variable selection in structured high-dimensional covariate spaces with applications in genomics</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<forename type="middle">R</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="1202" to="1214" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Phenotype prediction using regularized regression on genetic data in the dream5 systems genetics b challenge</title>
		<author>
			<persName>
				<forename type="first">P.-R</forename>
				<surname>Loh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">29095</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">A statistical physics approach for the analysis of machine learning algorithms on real data</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Malzahn</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Opper</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Mech.: Theory Exp</title>
		<imprint>
			<biblScope unit="page">11001</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Genome-wide association studies for complex traits: consensus, uncertainty and challenges</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">I</forename>
				<surname>Mccarthy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Genet</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="356" to="369" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Analytic and algorithmic solution of random satisfiability problems</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Mézard</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">297</biblScope>
			<biblScope unit="page" from="812" to="815" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Determining computational complexity from characteristic phase transitions</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Monasson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">400</biblScope>
			<biblScope unit="page" from="133" to="137" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Occam factors and model independent Bayesian learning of continuous distributions</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Nemenman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Bialek</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="page" from="65" to="026137" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">O &apos;</forename>
				<surname>Hagan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Bayesian Inference. Arnold</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<monogr>
		<title level="m" type="main">2 from naive mean field theory to the tap equations In: Advanced Mean Field Methods: Theory and Practice</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Opper</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Winther</surname>
			</persName>
		</author>
		<editor>Saad,D.</editor>
		<imprint>
			<date type="published" when="2001" />
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="7" to="20" />
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Regularized multivariate regression for identifying master predictors with application to integrative genomics study of breast cancer</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Peng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Stat</title>
		<imprint>
			<biblScope unit="page" from="4" to="53" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Generalized body composition prediction equation for men using simple measurement techniques</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">W</forename>
				<surname>Penrose</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Sci. Sports Exerc</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">189</biblScope>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards a rigorous assessment of systems biology models: the dream3 challenges</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">J</forename>
				<surname>Prill</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9202</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Gene set enrichment analysis: a knowledgebased approach for interpreting genome-wide expression profiles</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Subramanian</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="15545" to="15550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<monogr>
		<title level="m" type="main">Geršgorin and His Circles</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">S</forename>
				<surname>Varga</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer Science &amp; Business</publisher>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Genome-wide association analysis by lasso penalized logistic regression</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">T</forename>
				<surname>Wu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="714" to="721" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Conditional and joint multiple-SNP analysis of GWAS summary statistics identifies additional variants influencing complex traits</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Yang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="369" to="375" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Infection and genotype remodel the entire soybean transcriptome</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Zhou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Genomics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">49</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Zou</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hastie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>