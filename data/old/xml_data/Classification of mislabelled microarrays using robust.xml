
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gene expression Classification of mislabelled microarrays using robust sparse logistic regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013">7 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Jakramate</forename>
								<surname>Bootkrajang</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Birmingham</orgName>
								<address>
									<addrLine>Edgbaston</addrLine>
									<postCode>B15 2TT</postCode>
									<settlement>Birmingham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Ata</forename>
								<surname>Kabá</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Birmingham</orgName>
								<address>
									<addrLine>Edgbaston</addrLine>
									<postCode>B15 2TT</postCode>
									<settlement>Birmingham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gene expression Classification of mislabelled microarrays using robust sparse logistic regression</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">29</biblScope>
							<biblScope unit="page" from="870" to="877"/>
							<date type="published" when="2013">7 2013</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btt078</idno>
					<note type="submission">Received on December 21, 2012; revised on February 6, 2013; accepted on February 9, 2013</note>
					<note>BIOINFORMATICS ORIGINAL PAPER Associate Editor: Martin Bishop Contact: J.Bootkrajang@cs.bham.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Previous studies reported that labelling errors are not uncommon in microarray datasets. In such cases, the training set may become misleading, and the ability of classifiers to make reliable inferences from the data is compromised. Yet, few methods are currently available in the bioinformatics literature to deal with this problem. The few existing methods focus on data cleansing alone, without reference to classification, and their performance crucially depends on some tuning parameters. Results: In this article, we develop a new method to detect mislabelled arrays simultaneously with learning a sparse logistic regression classifier. Our method may be seen as a label-noise robust extension of the well-known and successful Bayesian logistic regression classifier. To account for possible mislabelling, we formulate a label-flipping process as part of the classifier. The regularization parameter is automatically set using Bayesian regularization, which not only saves the computation time that cross-validation would take, but also eliminates any unwanted effects of label noise when setting the regularization parameter. Extensive experiments with both synthetic data and real microarray datasets demonstrate that our approach is able to counter the bad effects of labelling errors in terms of predictive performance, it is effective at identifying marker genes and simultaneously it detects mislabelled arrays to high accuracy. Availability: The code is available from</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>High-throughput microarray technologies make it possible to measure the expression levels of thousands of genes. Our ability to use these data to reliably predict the presence of a certain disease and to better understand the biological mechanisms underlying the development of disease is of fundamental importance from the perspective of treatment and prevention. Statistical machine learning methods have already shown a lot of promise towards these goals, and methods that can deal with high dimensional and low sample size settings have been the subject of considerable research efforts over the last decade. However, the classical machinery of learning a classifier relies on a set of labelled examples, and the quality of a classifier depends crucially on the accurate labelling of these data. Unfortunately, the task of labelling is complex and not without ambiguities. As a result, there is no guarantee that the class labels are all correct; in fact, there is an increasing realization that labelling errors are not uncommon in microarray data—see<ref type="bibr" target="#b23">Malossini et al. (2006) and</ref><ref type="bibr" target="#b31">Zhang et al. (2009)</ref>. The presence of class label noise in training sets has been reported to deteriorate the performance of the existing classifiers in a broad range of classification problems (<ref type="bibr" target="#b16">Krishnan and Nandy, 1990;</ref><ref type="bibr" target="#b18">Lawrence and Schoïkopf, 2001;</ref><ref type="bibr" target="#b23">Malossini et al., 2006;</ref><ref type="bibr" target="#b29">Yang et al., 2012;</ref><ref type="bibr" target="#b30">Yasui et al., 2004</ref>). Although, the problem posed by the presence of class label noise is acknowledged, often it is naively ignored in practice. Part of the reason may be that symmetric label noise can be relatively harmless—however, asymmetric noise inevitably deteriorates the performance, as it changes the decision boundary between the true classes (<ref type="bibr" target="#b12">Chhikara and McKeon, 1984;</ref><ref type="bibr" target="#b17">Lachenbruch, 1974;</ref><ref type="bibr" target="#b20">Lugosi, 1992</ref>). Various approaches have been devised in the machine learning literature to address the issue of learning from samples with label noise. The seemingly straightforward approach is by means of data preprocessing where any suspect samples are removed or relabelled (<ref type="bibr" target="#b8">Barandela and Gasca, 2000;</ref><ref type="bibr" target="#b10">Brodley and Friedl, 1999;</ref><ref type="bibr" target="#b14">Jiang and Zhou, 2004;</ref><ref type="bibr" target="#b22">Maletic and Marcus, 2000;</ref><ref type="bibr" target="#b24">Muhlenbach et al., 2004;</ref><ref type="bibr" target="#b26">Sa´nchezSa´nchez et al., 2003</ref>). However, these approaches hold the risk of removing useful data too, which is unsuitable in microarray classification, as the number of training examples is limited. In sharp contrast with the multitude of methods for microarray classification, there are few attempts to address the problem of label noise in the bioinformatics literature.<ref type="bibr" target="#b23">Malossini et al. (2006)</ref>pointed out the difference between mislabelled arrays and outliers, and proposed two methods to detect mislabellings based on data perturbation.<ref type="bibr" target="#b31">Zhang et al. (2009)</ref>developed this work further and obtained improved precision and recall in both synthetic and real data settings. Both of these works are based on data perturbation, and their main focus is to detect suspects that are potentially mislabelled. These methods can help repairing the labels, so we can imagine a two-stage procedure of creating a repaired training set first and feed this to existing classifiers in a second stage. However, one must be aware that any errors made in separate stages of analysis will necessarily accumulate. In this article, we address the above problems by developing an integrated approach where the ambiguity of the given label assignments is modelled explicitly during the training of a *</p><p>To whom correspondence should be addressed.</p><p>classifier. This allows us to build on classifiers that have been successful for microarray classification by developing an extension to account for possible label noise. Specifically, here we will harness the sparse Bayesian logistic regression (BLogReg) model proposed by<ref type="bibr" target="#b11">Cawley and Talbot (2006)</ref>with a robustness against label noise. From our model formulation, we then derive a new algorithm that alternates between training the classifier and estimating the label noise probabilities. Straightforward calculations further provide the posterior probability of mislabelling for each of the training points. This enables us to detect the suspect samples for possible follow-up study. In addition, our experimental validation results, using both synthetic and real microarray datasets, demonstrate that the proposed method improves on traditional algorithms and achieves a reduced classification error rate. A variant of our approach appears in Bootkrajang and Kaba´nKaba´n (2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A model for label-noise robust logistic regression</head><p>We now describe our label-noise robust Logistic Regression (RLogReg) model. We will use the term 'robust' to differentiate this from traditional logistic regression. Consider a set of training data S ¼ fðx 1 , ~ y 1 Þ,. .. , ðx D , ~ y D Þg, where x i 2 &lt; M and ~ y i 2 f0, 1g, where ~ y i denotes the observed label of x i. As in the classical scenario for binary classification, we start with defining the log likelihood:</p><formula>LðwÞ ¼ X D i¼1 ~ y i log ðpð ~ y i ¼</formula><p>1jx i , wÞÞ þ ð1 À ~ y i Þ log ðpð ~ y i ¼ 1jx i , wÞÞ ð1Þ where w is the weight vector orthogonal to the decision boundary and it determines the orientation of the separating hyperplane. If the labels were presumed to be correct, then for a point x i we would take pð ~ y i ¼ 1jx i , wÞ ¼ ðw T x i Þ ¼ 1 1 þ e ðÀw T xiÞ ð2Þ and whenever this is above 0.5 we would decide that x i belongs to class 1. However, when there is label noise present, making predictions in this way is no longer valid. Instead, we will introduce a latent variable y to represent the true label, and we rewrite pð ~ y i ¼ kjx i , wÞ as the following:</p><formula>pð ~ y i ¼ kjx i , wÞ ¼ X 1 j¼0 pð ~ y i ¼ kjy ¼ jÞpðy ¼ jjx i , wÞ ¼ def S k i ð3Þ</formula><p>In Equation (3), pð ~ y ¼ kjy ¼ jÞ ¼ def jk represents the probability that the label has flipped from the true label j to the observed label k. These parameters form a transition table, which we will call the 'gamma table', À, and these label flipping probabilities may be estimated. Using this model, instead of Equation (2) we will have:</p><formula>pðy ¼ 1jx i , wÞ ¼ ðw T x i Þ ¼ 1 1 þ e ðÀw T xiÞ ð4Þ</formula><p>We decide that x belongs to class 1 whenever pðy ¼ 1jx, wÞ ! 0:5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sparsity prior</head><p>Microarray data are high dimensional with more features than observations while only a subset of the features is relevant to the target. A vast literature demonstrates that sparsity-inducing regularization approaches are effective in such cases (<ref type="bibr" target="#b11">Cawley and Talbot, 2006;</ref><ref type="bibr" target="#b21">MacKay, 1995;</ref><ref type="bibr" target="#b27">Shevade and Keerthi, 2003</ref>). Hence, we now incorporate sparsity in our model described in the previous section. Following Shevade and Keerthiwhere is the Lagrange multiplier (or regularization parameter) that balances between fitting the data well and having small parameter values. The L1-norm in the regularization term is defined as,</p><formula>k w k 1 ¼ X M d¼1 jw d j ð 6Þ</formula><p>Now, the regularization parameter needs be determined. We cannot use cross-validation, not only for its computational demand, but primarily because it would need a validation set with trusted correct labels, which may be not available. Hence, we adopt the Bayesian regularization approach of Cawley and<ref type="bibr" target="#b11">Talbot (2006)</ref>, which bypasses the need for cross-validation and determines automatically by putting a Jeffrey's prior on and integrating it out from the model. This yields the following (see<ref type="bibr" target="#b11">Cawley and Talbot, 2006</ref>, for details):</p><formula>¼ N P N d¼0 jw d j ð7Þ</formula><p>where N denotes the number of non-zero parameters, i.e. those with w d 6 ¼ 0 —so N M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Parameter estimation</head><p>It now remains to estimate w and À. Notice that Equation (5) is not differentiable at the origin. Shevade and<ref type="bibr" target="#b27">Keerthi (2003)</ref>proposed a simple, yet effective, algorithm to optimize the non-smooth but convex objective function of sparse logistic regression (SLogReg) using the Gauss-Seidel method and using coordinate-wise descent. We will create a modification of this approach to make it applicable to our non-convex objective. Define F d ¼ @LðwÞ @wd , where w d¼0 is the bias term that is usually left unregularized. The optimality conditions for Equation (5), which are the same as in<ref type="bibr" target="#b27">Shevade and Keerthi (2003)</ref>and Cawley and Talbot (2006) can be stated algebraically as the following:</p><formula>F d ¼ 0 if d ¼ 0 F d ¼ if w d 40, d40 F d ¼ À if w d 50, d40 À F d if w d ¼ 0, d40</formula><p>Accordingly, the violation from optimality of w d may be summarized as:</p><formula>viol d ¼ jF d j if d ¼ 0 ¼ j À F d j if w d 40, d40 ¼ j þ F d j if w d 50, d40 ¼ maxðF d À , À À F d , 0Þ if w d ¼ 0, d40</formula><p>We start optimizing the component w d that makes the largest violation to an optimality condition. At this point, if the objective function was convex then it would be possible to use gradient information to bracket the region where the optimal w d lies by specifying upper and lower limits (H and L). For example, Shevade and Keerthi (2003) identify 10 different cases for their sparse logistic regression model. However, since our likelihood term is non-convex, the cases identified there are not applicable because the sign of gradients give no information about the interval where the optimal solution resides. Therefore we introduce a simple modification by performing two searches: one in the range R þ [ f0g and another in the range R À [ f0g. We then choose the solution that returns a higher value of the objective function. This modified searching approach is more general and will work on any locally differentiable function at the expense of a slight increase in computation time. In practice, L and H are finite—provided that the design matrix is standardized and appropriate regularization is imposed on the solution, it is sufficient to search in the (0, 1000) and (À1000, 0) intervals. Finally, having completed the optimization of w, it remains to derive the update rule for the label-flipping probabilities. Conveniently, these can be estimated via fixed point update equations. By introducing a Lagrange multiplier to ensure that the probabilities in each row of the À table sum to 1 and solving the stationary equations, we obtain the following update equations (for details see Bootkrajang and Kaba´nKaba´n, 2012):</p><formula>00 ¼ g 00 g 00 þ g 01 , 01 ¼ g 01 g 00 þ g 01 ð8Þ 10 ¼ g 10 g 10 þ g 11 , 11 ¼ g 11 g 10 þ g 11 ð9Þ where g 00 ¼ 00 X D i¼1 ð1 À ~ y i Þ S 0 i ð1 À ðw T x i ÞÞ , g 11 ¼ 11 X D i¼1 ~ y i S 1 i ðw T x i Þ g 01 ¼ 01 X D i¼1 ~ y i S 1 i ð1 À ðw T x i ÞÞ , g 10 ¼ 10 X D i¼1 ð1 À ~ y i Þ S 0 i ðw T x i Þ</formula><p>Derivation details are given in the Supplementary Material. The optimization of the log-likelihood is then to alternate between optimizing w along with updating according to Equation (7) until convergence is reached, and we alternate this with the fixed point update equations of the label-flipping probabilities. The entire optimization procedure is summarized in Algorithms 1–2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Main loop</head><p>Input: Training examples. Initialize w 0, 0, I nz fw 0 g, I z fw d, d2f1, ng g, À. while Optimality violator exists in I z do Find the greatest optimality violator, , in I z repeat Optimize w using Algorithm 2</p><formula>I z I z nfw g I nz I nz [ fw g</formula><p>Find the maximum optimality violator, , in I nz until No violator exists in I nz Update the entries of À by Equations (8) and (9) Update regularization parameter, by Equation (7) end while Output: Optimized weight vector, w. Optimized À.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Detecting mislabelled points</head><p>For an observation ðx i , ~ y i Þ, the probability of it being mislabelled can be computed as the following:</p><formula>pðy 6 ¼ ~ y i jx i Þ ¼ X 1 j¼0, j6 ¼ ~ yi pðy ¼ jjx i Þ ð 10Þ</formula><p>This may be thought of as the models 'degree of belief' that x i 's label is incorrect. We may use it either in this form, or in a hard-thresholded form (i.e. predict that the point x i is mislabelled if pðy 6 ¼ ~ y i jx i Þ ! 0:5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">A note on low sample size, high dimensional data</head><p>Since additional parameters À are being estimated from the data, we expect that RLogReg will require more training examples to deliver its full potential. In microarray datasets, the training set size is often of the order of tens only. A possible workaround in such cases is to guide the algorithm by presetting the gamma table from domain knowledge about the likely proportion of mislabelled data. When such knowledge exists, the values of gamma may either be fixed throughout the optimization process or they may be seeded initially and then optimized.<ref type="bibr" target="#b27">Shevade and Keerthi, 2003</ref>) where BLogReg was shown to be superior. We shall demonstrate that our proposed robust extension of BLogReg performs better than the original BLogReg in terms of classification performance when there is label noise present in the training set. Moreover, our model can be used to identify mislabelled arrays for potential follow-on study. Before proceeding, we should comment that symmetric and asymmetric label flipping have very different consequences in classification. Symmetric or uniform flipping means that each class is affected by label flipping in the same proportion. In contrast, asymmetric or non-uniform flipping is when the label flips from one class to another more often than vice-versa. The latter type of label flipping has been theoretically shown (<ref type="bibr" target="#b20">Lugosi, 1992</ref>) to degrade the performance of an algorithm to a much larger degree, as it modifies the decision boundary between the true classes. Our empirical study (<ref type="bibr" target="#b9">Bootkrajang and Kaba´nKaba´n, 2012</ref>) also demonstrated this. Therefore, we will mainly focus our attention on datasets with asymmetric label noise and indeed expect the advantages of our approach to be most apparent in that setting. To demonstrate the benefit of having a label noise model embedded in the classifier, we start with experiments on synthetic data where labels were asymmetrically flipped at the rate of 30%. The use of synthetic data for controlled experiments is standard in bioinformatics (see e.g.<ref type="bibr" target="#b31">Zhang et al., 2009</ref>), as it allows us assess the performance of a new approach against a ground truth. We shall then move on to analysing real microarray datasets where label noises have not been injected artificially. These datasets have been previously reported to contain wrongly labelled samples. Finally, we shall assess the ability of our proposed approach to identifying mislabelled arrays using Receiver Operating Characteristics (ROC) analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiment setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Datasets</head><p>We generate synthetic data by sampling points from a standard Gaussian distribution where the class label associated with each point is assigned by a logistic function with a predefined weight vector w having only three relevant features, w 1 ¼ w 2 ¼ w 3 ¼ 10=3, w i ¼ 0, 8i43, following Ng (2004). We create sets with 500 training points and sets with 100 training points together with independent test sets of 100 points each time, and call these datasets Synth-500 and Synth-100, respectively. The dimensionality of the synthetic datasets ranges from 100 up to 1000. Asymmetric label noise was artificially injected into each synthetic dataset at the 30% rate. Further, we use two real microarray datasets: Colon cancer (<ref type="bibr" target="#b7">Alon et al., 1999</ref>) and Breast cancer (<ref type="bibr" target="#b28">West et al., 2001</ref>)—both of which are known to contain some mislabelled arrays. No artificial label flipping is injected in these data. We standardize these datasets so the rows of the D Â M design matrix (where D is the number of observations and M is the dimensionality) of the input sample will have zero mean and unit variance.<ref type="figure" target="#tab_1">Table 1</ref>summarizes the characteristics of all of these datasets used. Additional datasets and results are given in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Error measures While</head><p>in the case of synthetic data the true labels can be used to validate the predictive accuracy of our algorithm, in the real microarray data there is no absolute ground truth. Since the labels given in the datasets may be incorrect, the issue of what should count as a misclassification must be defined. We define two variants for measuring out-of-sample error rates:</p><p>Corrected (CRT): Count misclassification errors against the 'corrected' labels where corrections are made cf. the mislabellings reported in the literature.</p><p>Cleansed (CLN): Exclude any mislabelled suspects (known in the literature) from the test sets for the purpose of evaluation, so these are always placed into the training set instead; then count the misclassification errors on test sets in the usual way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results and analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Results on synthetic data</head><p>The average misclassification error rates on the Synth-500 and Synth-100 datasets are shown in<ref type="figure">Figure 1</ref>as the data dimension is varied. Each point on these plots represents the average misclassification rate on the test sets, where the average is taken over 500 independent repetitions of the experiment. The error bars are too small to be visible. We see that RLogReg achieves significantly lower error rates than BLogReg on the datasets that contain more training examples (Synth-500). This clearly demonstrates the advantage of modelling the label noise process. On the smaller size dataset (Synth-100), however, the performance gain becomes marginal—this is because the accurate estimation of the additional parameters (label-flipping probabilities) requires sufficient training data for our approach to achieve its full potential. Nevertheless, it is should be noticed that even in the small sample setting, RLogreg performs no worse than BLogReg on all the datasets tested (additional results are given in the Supplementary Material.). More importantly, the rightmost plot shows that we can counter the problem of small sample sizes by using prior knowledge about the extent of label noise, e.g. by pre-defining the gamma table. We denote this version as RLogReg-F in the figure, and we see this significantly improves the classification accuracy in the small sample setting. Beyond classification performance, it is of interest to evaluate the methods' ability to identify the relevant predictive genes.<ref type="figure">Figure 2</ref>shows the estimated weight vectors as obtained by BLogReg and RLogReg respectively from 100-dimensional synthetic data with only the first three features being relevant. The classifiers were trained on 250 training examples per class that were subjected to 30% asymmetric label flipping. We see that RLogReg achieved a more accurate estimation of the weight vector, while BLogReg became confused by the noisy labels and selected too many false non-zero weights. This is an important advantage of RLogReg over BLogReg when it comes to finding a small set of predictive marker genes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Results on colon cancer</head><p>The colon cancer classification task aims to distinguish between normal tissue and tumour. According to<ref type="bibr" target="#b7">Alon et al. (1999)</ref>, there is biological evidence that the samples T2, T30, T33, T36, T37, N8, N12, N34, N36 may be mislabelled. The proportion of mislabelling in the two classes is unequal; hence, this is a case of asymmetric labelflipping that can distort the correct decision boundary of the classes. The limited number of training observations implies that a good estimate of the gamma table may be difficult to obtain from the data alone (as we have seen in the previous section), nevertheless prior knowledge of the noise proportions may still allow us to exploit the advantages of having a noise model as integral part of our classifier. Therefore, we include RLogReg-F in our experiments, with the gamma table set to the true label-flipping proportions.<ref type="figure" target="#tab_2">Table 2</ref>reports the leave-one-out (LOO) errors in terms of the error measures defined in Section 3.2.1, and we also give the average number of genes selected by the three methods considered. The results confirm the expectations. RLogReg that attempts to estimate the gamma table along with all other parameters is marginally worse than BLogReg (although not statistically significantly so, according to the unpaired t-test), while RLogReg-F improves over BLogReg in all validation criteria used, and it also selects a smaller fraction of relevant features.<ref type="figure">Figure 3</ref>shows the average magnitude of each gene according to BLogReg and RLogReg-F, respectively. These are averages of w estimates across 1000 bootstrap repetitions to inspect possible systematic differences. These average weights turned out to be quite similar for BLogReg and RLogReg-F, with the exception of a few genes that had been ranked differently by the two methods. To see this, a summary of top ten selected genes and their estimated weights are given in Tables 3 and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Results on breast cancer</head><p>We further apply the proposed model on the Breast Cancer dataset from<ref type="bibr" target="#b28">West et al. (2001)</ref>. The aim is to discriminate between oestrogen-positive and oestrogen-negative observations. According to<ref type="bibr" target="#b28">West et al. (2001)</ref>, there is biological evidence that the arrays 11, 14, 16, 31, 33, 40, 43, 45, 46 are mislabelled. However, unlike the Colon dataset, we observe the nature of label flipping in the Breast cancer dataset is rather close to symmetric. As a consequence, mislabelling might do less harm to traditional classifiers in terms of class prediction on future arrays.<ref type="figure" target="#tab_5">Table 5</ref>summarizes LOO error rates together with the numbers of genes selected by the classifiers. The picture is quite similar to what we have seen in the case of Colon, although the differences tend to be smaller, as the label noise here is more symmetric. We also see that RLogReg did pretty well with a limited amount of training data, but of course the difficulty of accurate estimation of the gamma table from such few points remains an issue. In fact, the estimated gamma table of RLogReg may converge to identity in such conditions, which statistically will result in a weight vector that is identical to that of BLogReg. As previously, knowledge of the extent of noise can be used here, resulting in a slight improvement for RLogReg-F. Finally, as somewhat expected, the average magnitude of gene weights from BLogreg and RLogreg-F look similar, as shown in<ref type="figure" target="#fig_3">Figure 4</ref>, which was expected by the symmetric nature of the label noise in this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Computation time</head><p>We should give an indication of the added computation overhead required by our noise modelling relative to the existing BLogReg. One LOO loop on all datasets considered took on average 4 s for RLogReg, while BLogReg required roughly 0.2 s on an Intel's Core-i5 3.2 GHz machine. We believe this extra computation time is most worthwhile especially when the training set size is sufficiently large to exploit the full potential of the presented approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Detecting mislabelled instances</head><p>One of the most appealing features of our proposed algorithm is the possibility to detect mislabelled examples from the data, in addition to classification and gene selection. There are two types of possible errors: (i) a false positive is when a sample is believed to be mislabelled despite it is in fact labelled correctly; and (ii) a false negative is when a sample is believed to be labelled correctly despite its label is in fact incorrect. A good way to summarize both, while also making use of the probabilistic outputs given by the sigmoid function, is byconstructing the ROC curves. The area under the ROC curve signifies the probability that a randomly drawn and mislabelled example would be flagged by the proposed algorithms.<ref type="figure">Figure 5</ref>shows the ROC curves for Synth-500 and Colon cancer datasets. Superimposed for reference, we also plotted the ROC curves that correspond to BLogReg. BLogReg considers that all points have the correct labels, and it has not been designed to spot mislabelled points. The best we can do is to take that mistakes made on the training points are mislabelling predictions. From<ref type="figure">Fig. 5</ref>. Average ROC curves for BLogReg, RLogReg and RLogReg-F on Synth-500 and colon cancer benchmarks. For consistency with classification result bootstrap is performed on Synth-500 while LOO is used to obtain the result for colon cancer. The prediction is based on hard-thresholded rule<ref type="figure">Figure 5</ref>, we see the gap between the two curves is significant and well apparent in the experiment on Synth-500. This quantifies the gain that our modelling approach is able to obtain. The gain for Colon is smaller but still significant, despite the dataset size is so limited, provided that RLogReg incorporates knowledge about the proportion of mislabelling (i.e. RLogReg-F).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Comparison with previous findings</head><p>In addition to comparisons that quantify the benefits of having a noise model, we compare our results with previously identified mislabelling in the Colon cancer samples. We conduct 100 bootstrap repetitions drawing subsets of size 50 from the total of 62 points randomly while imposing that none of the suspects from the literature are left out. In<ref type="figure" target="#tab_6">Table 6</ref>, after quoting the previous detections from the literature, we report the mislabelling detections obtained by BLogReg-F and BLogReg respectively, in two forms: (i) from the run that returned the largest number of detections, and (ii) the percentage that a particular array was flagged up as a mislabelling during the 100 repetitions. It is interesting to note that RLogReg-F was able to identify up to seven mislabelled points, and these also agree with the majority of previously reported detections using other algorithms (i.e. for T30, T33, T36, N34 and N36). BLogReg is also able to find up to seven mislabelled samples but with fewer true positives and more false positives. From both figures, we see that RLogReg-F is able to identify mislabelled arrays more often than BLogReg can.The detections for RLogReg-F are based on the hard threshold rule (pð ~ y 6 ¼ yjx, wÞ ! 0:5). The first line is the 'gold standard' that is backed up by biological evidence in the literature.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.4.</head><figDesc>Fig. 4. Comparison of the average weights of features selected by BLogReg and RLogReg-F on the Breast Cancer dataset over 1000 bootstrap repeats (39 train/10 test)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 1. Characteristics of the datasets used in the reported experiments</figDesc><table>Dataset 
No. of samples 
No. of genes No. of wrong labels 

Class 1 
Class 2 
Class 1 
Class 2 

Synth-500 250 
250 
100–1000 
0 
75 
Synth-100 50 
50 
100–1000 
0 
15 
Colon 
40 (T) 
22 (N) 
2000 
5 
4 
Breast 
25 (ERþ) 24 (ERÀ) 7129 
4 
5 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>Table 2.</figDesc><table>LOO misclassification (%) on Colon Cancer dataset 

Algorithm 
LOO-CRT 
LOO-CLN 
No. of genes 

BLogReg 
8.06 AE 0.44 
7.55 AE 0.64 
11.94 AE 0.41 
RLogReg 
9.68 AE 0.48 
9.43 AE 0.66 
11.85 AE 0.41 
RLogReg-F 
4.83 AE 0.35 
1.88 AE 0.54 
9.21 AE 0.45 

The average number of selected genes (AEstandard deviation) was computed from 
the CLN runs. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><figDesc>Table 3. Relative importance of top 10 genes selected by the BLogReg algorithm</figDesc><table>(a) 
( b) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><figDesc>Table 5.</figDesc><table>LOO misclassification (%) on Breast Cancer dataset 

Algorithm 
LOO-CRT 
LOO-CLN 
No. of genes 

BLogReg 
18.37 AE 0.79 
2.50 AE 0.40 
9.22 AE 0.58 
RLogReg 
18.37 AE 0.79 
2.50 AE 0.40 
9.10 AE 0.63 
RLogReg-F 
16.33 AE 0.76 
0.00 AE 0.29 
7.58 AE 0.50 

The average number of selected genes (AEstandard deviation) was computed from 
the CLN runs. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><figDesc>Table 6. Identifying mislabelled samples in colon cancer dataset</figDesc><table>Source 
Suspects identified 
</table></figure>

			<note place="foot">ß The Author 2013. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">Robust sparse logistic regression at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">J.Bootkrajang and A.Kabá n at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="4"> CONCLUSIONS We proposed a robust extension of sparse Bayesian logistic regression for classification in the presence of labelling errors. The numerical experiments suggest that our approach is superior to its traditional counterpart when the training data contains labelling errors, and more significantly so when the label-flipping distribution is asymmetric. Simultaneously, our methods are effective in identifying marker genes and detecting mislabelled data. Since our robust model needs to estimate the label-flipping probabilities together with the parameters of the classifier, it does require more training data to achieve its full potential. However, in our experience, RLogReg performs statistically no worse than BLogReg even when the training set sizes are small. The need for more data can also be relaxed by incorporating knowledge about the extent of label noise. Funding: J.B. is supported by the Royal Thai Government. A.K. acknowledges the MRC Discipline Hopping Award G0701858 (ID no. 85545). Conflict of Interest: none declared.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Gene number Gene annotation Average magnitude 765 Human cysteine-rich protein (CRP) gene, exons 5 and 6 0.4289 377 H.sapiens mRNA for GCAP-II/uroguanylin precursor 0</title>
	</analytic>
	<monogr>
		<title level="m">C4-DICARBOXYLATE TRANSPORT SENSOR PROTEIN DCTB (Rhizobium leguminosarum) 0.2618 1870 PEPTIDYL-PROLYL CIS-TRANS ISOMERASE, MITOCHONDRIAL PRECURSOR (HUMAN) À0</title>
		<imprint>
			<date type="published" when="1644" />
			<biblScope unit="page">2435</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName>
				<surname>Human Desmin Gene</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2416 1346 60S RIBOSOMAL PROTEIN L24 (Arabidopsis thaliana) À0.2376 1772 COLLAGEN ALPHA 2(XI) CHAIN (Homo sapiens) À0.2292 1024 ATP SYNTHASE A CHAIN (Trypanosoma brucei) À0.1356</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<monogr>
		<title level="m" type="main">Human spermidine synthase gene, complete cds 0.1290 1641 Human enkephalin B (enkB) gene, exon 4 and 3 0 flank and complete cds À0</title>
		<imprint>
			<date type="published" when="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<monogr>
		<title level="m" type="main">Table 4 Relative importance of the top 10 genes selected by RLogReg-F algorithm Gene number Gene annotation Average magnitude 765 Human cysteine-rich protein (CRP) gene, exons 5 and 6 0.3988 377 H.sapiens mRNA for GCAP-II/uroguanylin precursor 0</title>
		<imprint>
			<biblScope unit="page">3273</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName>
				<surname>Human Desmin Gene-Dicarboxylate</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Transport</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Sensor</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Dctb</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">leguminosarum) 0.2883 1870 PEPTIDYL-PROLYL CIS-TRANS ISOMERASE, MITOCHONDRIAL PRECURSOR (HUMAN) À0.2852 1346 60S RIBOSOMAL PROTEIN L24 (A.thaliana) À0.2420 1024 ATP SYNTHASE A CHAIN (T.brucei) À0.2220 1993 Human hormone-sensitive lipase (LIPE) gene, complete cds À0.2114 493 MYOSIN HEAVY CHAIN, NONMUSCLE (Gallus gallus) 0.1325 1772 COLLAGEN ALPHA 2(XI) CHAIN (H.sapiens)</title>
		<imprint>
			<biblScope unit="page" from="1644" to="1648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<monogr>
		<title level="m" type="main">Colon cancer Synth-500</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">REFERENCES</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Alon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="6745" to="6750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Decontamination of training samples for supervised pattern recognition methods</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Barandela</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Gasca</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Pattern Recognition</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Label-noise robust logistic regression and its applications</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Bootkrajang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kaba´nkaba´n</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of Machine Learning and Knowledge Discovery in Databases-European Conference, ECML-PKDD 2012</title>
		<meeting>eeding of Machine Learning and Knowledge Discovery in Databases-European Conference, ECML-PKDD 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="143" to="158" />
		</imprint>
	</monogr>
	<note>Part. I</note>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Identifying mislabeled training data</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">E</forename>
				<surname>Brodley</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Friedl</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="131" to="167" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Gene selection in cancer classification using sparse logistic regression with Bayesian regularization</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">C</forename>
				<surname>Cawley</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<forename type="middle">L</forename>
				<surname>Talbot</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2348" to="2355" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Linear discriminant analysis with misallocation in training samples</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">S</forename>
				<surname>Chhikara</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Mckeon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="899" to="906" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Support vector machine classification and validation of cancer tissue samples using microarray expression data</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">S</forename>
				<surname>Furey</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="906" to="914" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Editing training data for k-NN classifiers with neural network ensemble</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Jiang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<forename type="middle">H</forename>
				<surname>Zhou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="356" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Detecting outlying samples in microarray data: a critical assessment of the effect of outliers on sample classification</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Kadota</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chem. Bio. Inform. J</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="30" to="45" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficiency of discriminant analysis when initial samples are classified stochastically</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Krishnan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">C</forename>
				<surname>Nandy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="529" to="537" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Discriminant analysis when the initial samples are misclassified II: non-random misclassification models</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">A</forename>
				<surname>Lachenbruch</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="419" to="424" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimating a kernel fisher discriminant in the presence of label noise</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<forename type="middle">D</forename>
				<surname>Lawrence</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Schoïkopf</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Machine Learning</title>
		<meeting>the 18th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="306" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Gene assessment and sample classification for gene expression data using a genetic algorithm/k-nearest neighbor method</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Li</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comb. Chem. High Throughput Screen</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="727" to="739" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning with an unreliable teacher</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Lugosi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Probable networks and plausible predictions-a review of practical Bayesian methods for supervised neural networks</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">J</forename>
				<surname>Mackay</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="469" to="505" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Data cleansing: beyond integrity analysis</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">I</forename>
				<surname>Maletic</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Marcus</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Information Quality</title>
		<meeting>the Conference on Information Quality</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Detecting potential labeling errors in microarrays by data perturbation</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Malossini</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2114" to="2121" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Identifying and handling mislabelled instances</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Muhlenbach</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="89" to="109" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature selection, L1 vs. L2 regularization, and rotational invariance</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">Y</forename>
				<surname>Ng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Machine Learning</title>
		<meeting>the 21st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="78" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Analysis of new techniques to obtain quality training sets</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">S</forename>
				<surname>Sa´nchezsa´nchez</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1015" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">A simple and efficient algorithm for gene selection using sparse logistic regression</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">K</forename>
				<surname>Shevade</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">S</forename>
				<surname>Keerthi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2246" to="2253" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Predicting the clinical status of human breast cancer by using gene expression profiles</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>West</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA, 98</title>
		<meeting>. Natl Acad. Sci. USA, 98</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="11462" to="11467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Multiple kernel learning from noisy labels by stochastic programming</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Yang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning, ICML &apos;1202</title>
		<meeting>the 29th International Conference on Machine Learning, ICML &apos;1202</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Partially supervised learning using an EM-boosting algorithm</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Yasui</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="199" to="206" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Methods for labeling error detection in microarrays based on the effect of data perturbation on the regression model</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2708" to="2714" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>