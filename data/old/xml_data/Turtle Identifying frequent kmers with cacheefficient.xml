
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequence analysis Turtle: Identifying frequent k-mers with cache-efficient algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014">2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Rajat</forename>
								<surname>Shuvro</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Ecology, Evolution and Natural Resources</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Marine and Coastal Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Roy</forename>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Debashish</forename>
								<surname>Bhattacharya</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Ecology, Evolution and Natural Resources</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Marine and Coastal Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Alexander</forename>
								<surname>Schliep</surname>
							</persName>
							<affiliation key="aff3">
								<orgName type="department">BioMaPS Institute for Quantitative Biology</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>08901</postCode>
									<settlement>New Brunswick</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sequence analysis Turtle: Identifying frequent k-mers with cache-efficient algorithms</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">30</biblScope>
							<biblScope unit="issue">14</biblScope>
							<biblScope unit="page" from="1950" to="1957"/>
							<date type="published" when="2014">2014</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btu132</idno>
					<note type="submission">Received on April 29, 2013; revised on February 25, 2014; accepted on March 4, 2014</note>
					<note>BIOINFORMATICS ORIGINAL PAPER Associate Editor: Michael Brudno Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Counting the frequencies of k-mers in read libraries is often a first step in the analysis of high-throughput sequencing data. Infrequent k-mers are assumed to be a result of sequencing errors. The frequent k-mers constitute a reduced but error-free representation of the experiment, which can inform read error correction or serve as the input to de novo assembly methods. Ideally, the memory requirement for counting should be linear in the number of frequent k-mers and not in the, typically much larger, total number of k-mers in the read library. Results: We present a novel method that balances time, space and accuracy requirements to efficiently extract frequent k-mers even for high-coverage libraries and large genomes such as human. Our method is designed to minimize cache misses in a cache-efficient manner by using a pattern-blocked Bloom filter to remove infrequent k-mers from consideration in combination with a novel sort-and-compact scheme, instead of a hash, for the actual counting. Although this increases theoretical complexity, the savings in cache misses reduce the empirical running times. A variant of method can resort to a counting Bloom filter for even larger savings in memory at the expense of false-negative rates in addition to the false-positive rates common to all Bloom filter-based approaches. A comparison with the state-of-the-art shows reduced memory requirements and running times. Availability and implementation: The tools are freely available for download at</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>K-mers play an important role in many methods in bioinformatics because they are at the core of the de Bruijn graph structure (<ref type="bibr" target="#b20">Pevzner et al., 2001</ref>) that underlies many of today's popular de novo assemblers (<ref type="bibr" target="#b24">Simpson et al., 2009;</ref><ref type="bibr" target="#b27">Zerbino and Birney 2008</ref>). They are also used in assemblers based on the overlaplayout-consensus paradigm like Celera (<ref type="bibr" target="#b17">Miller et al., 2008</ref>) and Arachne (<ref type="bibr" target="#b10">Jaffe et al., 2003</ref>) as seeds to find overlap between reads. Several read correction tools (<ref type="bibr" target="#b11">Kelley et al., 2010;</ref><ref type="bibr" target="#b13">Liu et al., 2012;</ref><ref type="bibr" target="#b15">Medvedev et al., 2011</ref>) use k-mer frequencies for error correction. Their main motivation for counting k-mers is to filter out or correct sequencing errors by relying on k-mers that appear multiple times and can thus be assumed to reflect the true sequence of the donor genome. In contrast, k-mers that appear only once are assumed to contain sequencing errors. Melsted and Pritchard (2011) and Marcais and Kingsford (2011) make a more detailed compelling argument about the importance of k-mer counting. In a genome of size g, we expect up to g unique k-mers. This number can be smaller because of repeated regions (which produce the same k-mers) and small k, as smaller k-mers are less likely to be unique, but is usually close to g for reasonable values of k. However, depending on the amount of sequencing errors, the total number of k-mers in the read library can be substantially larger than g. For example, in the DM dataset (<ref type="figure" target="#tab_2">Table 2</ref>), the total number of 31-mers is $289.20 M, whereas the number of 31-mers occurring at least twice is $131.82 M. The size of the genome is 122 Mb (megabase pairs). This is not surprising because one base call error in a read can introduce up to k false kmers. Consequently, counting the frequency of all k-mers, as done by Jellyfish (<ref type="bibr" target="#b14">Marcais and Kingsford, 2011</ref>), which is limited to k 31, requires O(N) space where N is the number of k-mers in the read library. This makes the problem of k-mer frequency counting time and memory intensive for large read libraries like human. We encounter similar problems for large libraries while using Khmer (<ref type="bibr" target="#b19">Pell et al., 2012</ref>), which uses a Bloom filter-based (<ref type="bibr" target="#b5">Bloom, 1970</ref>) approach for counting frequencies of all k-mers. Ideally, the frequent k-mer identifier should use O(n) space where n is the number of frequent k-mers (n ( N). The approach taken by BFCounter (<ref type="bibr" target="#b16">Melsted and Pritchard, 2011</ref>) achieves something close to this optimum by ignoring the infrequent k-mers with a Bloom filter and explicitly storing only frequent k-mers. This makes BFCounter more memory-efficient compared with Jellyfish. However, the running time of BFCounter is large for two reasons. First, it is not multi-threaded. Second, both the Bloom filter and the hash table used for counting incur frequent cache misses. The latter has recently been identified as a major obstacle to achieving high performance on modern architectures, motivating the development of cache-oblivious algorithms and data structures (<ref type="bibr" target="#b4">Bender et al., 2005</ref>), which optimize the cache behavior without relying on information of cache layout and sizes. Additionally, BFCounter is also limited to a count range of 0â€“255, which will often be exceeded in single-cell experiments because of the large local coverage produced by whole genome amplification artifacts. A different approach is taken by DSK (<ref type="bibr" target="#b22">Rizk et al., 2013</ref>) to improve memory efficiency. DSK makes many passes over the read file and uses temporary disk space to trade off the memory requirement. Although<ref type="bibr" target="#b22">Rizk et al. (2013)</ref>claimed DSK to be faster than BFCounter, on our machine *To whom correspondence should be addressed. using an 18 TB Raid-6 storage system; DSK required more wallclock time compared with BFCounter. Therefore, we consider DSK without dedicated high-performance disks, e.g. solid state, and BFCounter to be too slow for practical use on large datasets. A disk-based sorting and compaction approach is taken by KMC (<ref type="bibr" target="#b8">Deorowicz et al., 2013</ref>), which was published very recently, and it is capable of counting k-mers of large read libraries with a limited amount of memory. However, in our test environment, we found it to be slower than the method described here. We present a novel approach that reduces the memory footprint to accommodate large genomes and high-coverage libraries. One of our tools (scTurtle) can report frequent 31mers with counts (with a very low false-positive rate) from a human read set with 135.3 Gb using 109 GB of memory in 52 h using 19 worker threads. Like BFCounter, our approach also uses a Bloom filter to screen out k-mers with frequency one (with a small false-positive rate), but in contrast to BFCounter, we use a pattern-blocked Bloom filter (<ref type="bibr" target="#b21">Putze et al., 2010</ref>). The expected number of cache misses for each inquiry/update in such a Bloom filter is one. The frequency of the remaining k-mers is counted with a novel sorting and compaction-based algorithm. Our compaction step is similar to run-length encoding (<ref type="bibr" target="#b23">Salomon, 1997</ref>). Note that this is similar to the strategy of KMC, which was developed as a concurrent and independent work. Though the complexity of sorting in our compression step is OÃ°n log nÃž, it has sequential and localized memory access that helps in avoiding cache misses and will run faster than an O(n) algorithm that has O(n) cache misses as long as log n is much smaller than the penalty issued by a cache miss. For larger datasets, where O(n) space is not available, the aforementioned method will fail. We show that it is possible to get a reasonable approximate solution to this problem by accepting small false-positive and false-negative rates. The method is based on a counting Bloom filter implementation. The error rates can be made arbitrarily small by making the Bloom filter larger. Because the count is not maintained in this method, it reports only the k-mers seen more than once (with a small false-positive and false-negative rate), but not their frequency. We call the first tool scTurtle and the second one cTurtle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">scTurtle</head><p>2.1.1 Outline By a k-mer, we always refer to a k-mer and/or its reverse complement. Our objective is to separate the frequent k-mers from the infrequent ones and count the frequencies of the frequent k-mers. To achieve this, first, we use a Bloom filter to identify the k-mers that were seen at least twice (with a small false-positive rate). To count the frequency of these k-mers, we use an array of items containing a k-mer and its count. These are the two main components of our tool. Once the counts are computed, we can output the k-mers having a frequency greater than the chosen cutoff. For the sake of cache efficiency, the Bloom filter is implemented as a pattern-blocked Bloom filter (<ref type="bibr" target="#b21">Putze et al., 2010</ref>). It localizes the bits set for an item to a few consecutive bytes (block) and thus reduces cache misses. The basic idea is as follows: when a k-mer is seen, the Bloom filter is checked to decide whether it has been seen before. If that is the case, we store the k-mer in the array with a count of 1. When the number of items in the array crosses a threshold, it is sorted in place, and a linear pass is made, compressing items with the same k-mer (which lie in consecutive positions of the sorted array) to one item. The counts add up to reflect the total number of times a k-mer was seen. Note that this strategy is similar to run-length encoding (<ref type="bibr" target="#b23">Salomon, 1997</ref>) of the items. Our benchmarking (<ref type="figure" target="#tab_1">Table 1</ref>) shows that this simple approach of storing items and their frequencies is faster than a hash tablebased implementation. An outline of the algorithm is given in Algorithm 1. More details are provided in the following subsections. Note that the improved efficiency of sort and compaction also suggests that it can speed up the k-mer counting step for all de Bruijn graph-based assemblers where k-mer counting is required for building the graph. We found that ABySS (<ref type="bibr" target="#b24">Simpson et al., 2009</ref>) and SPAdes (<ref type="bibr" target="#b3">Bankevich et al., 2012</ref>) require 3660 and 2144 s, respectively, for k-mer counting on the DM dataset (see<ref type="figure" target="#tab_2">Table 2</ref>). But the sort and compaction method takes only 523.41 s. We provide a single-threaded preliminary tool called aTurtle that implements this method for counting all k-mers and their frequencies.</p><p>Algorithm 1 scTurtle outline 1. Let S be the stream of k-mers coming from the read library, BF be the Bloom filter, A be the array to store k-mers with counts and t be the threshold when we apply sorting and compaction. 2. for all k-mer 2 S do 3. if k-mer present in BF then 4. Add k-mer to A 5. end if 6. if jAj ! t then 7. Apply sorting and compaction on A 8. end if 9. end for 10. Apply sorting and compaction on A. 11. Report all k-mers in A with their counts as frequent k-mers and their counts. 2.1.2 k-mer extraction and bit encoding For space efficiency, kmers are stored in a bit-encoded form where 2-bits represent a nucleotide. This is possible because k-mers are extracted out of reads by splitting them on 'N's (ambiguous base calls) and hence contain only A, C, G and T. Because we consider a k-mer and its reverse complement to be two representations of the same object, whenever we see a k-mer, we also compute the bit representation of the reverse complement and take the numerically smaller value as the unique representative of the k-mer/reverse complement pair.Note: Jellyfish is a highly optimized hash table-based implementation for the k-mer counting problem. We also compare against general purpose tools that uses Google sparse/dense hash maps for storing k-mers and their counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Identification</head><p>of frequent k-mers with pattern-blocked Bloom filter A Bloom filter is a space-efficient probabilistic data structure, which, given an item, can identify whether this item was seen before with some prescribed, small false-positive rate. We use this property of the Bloom filter to identify k-mers that were seen at least twice. An ordinary Bloom filter works as follows: a large bit-array (B) of size L is initialized to 0. Given an item x, k hash values (h 1 , h 2 ,. .. , h k ) using kindependent hash functions {within the range Â½0, Ã°L Ã€ 1ÃžÂŠ} are computed. We now check all the bits BÂ½h 1 ÂŠ,. .. , BÂ½h k ÂŠ. If they are all set to 1, with high probability, this item has been seen at least once before. If not, it is certainly the first appearance of this item, and we set all of BÂ½h 1 ÂŠ,. .. , BÂ½h k ÂŠ to one. For all subsequent appearance(s) of this item, the Bloom filter will report that it has been seen at least once before. In this way, the Bloom filter helps us to identify frequent k-mers. Note that if the bit locations are randomly distributed, because of the large size of the Bloom filter, each bit inspection and update is likely to incur one cache miss. Thus, the total number of cache misses per item would be k. On the contrary, if the bit locations are localized to a few consecutive bytes (a block), each item lookup/update will have a small number of cache misses. This can be done by restricting h 1 ,. .. , h k to the range Â½h 1 Ã°xÃž, h 1 Ã°xÃž Ã¾ bÂŠ where b is a small integer. The bit pattern for each item can also be precomputed. This is called the pattern-blocked Bloom filter.<ref type="bibr" target="#b21">Putze et al., (2010)</ref>observe that the increase in false-positive rate because of this localization and precomputed patterns can be countered by increasing L by a few percent. To summarize, we first select a block for an item (using a hash function), select h 1 5h 2 5. .. 5h k from a set of precomputed random numbers such that all of them lie within the block and update/inquire them sequentially. Note that Bloom filters are widely used in many applications like assembly (<ref type="bibr" target="#b6">Chikhi and Rizk, 2012;</ref><ref type="bibr" target="#b19">Pell et al., 2012</ref>), and we believe using a more optimized version of this data structure (like the pattern-blocked Bloom filter) will benefit such applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Counting frequencies with sorting and compaction Our next</head><p>objective is to count the frequencies of the frequent k-mers. The basic idea is to store the frequent k-mers in an array A of size4n, where n is the number of frequent items. When this array fills up, we sort the items by the k-mer values. This places the items with the same k-mer next to each other in the array. Now, by making a linear traversal of the array, we can replace multiple items with the same k-mer with one item where a count field represents how many items were merged, which is equal to how many times this k-mer was seen; see<ref type="figure" target="#fig_1">Figure 1</ref>. Note that this is similar to run-length encoding. Here is a toy example: say A Â¼ Â½.. . , Ã°i, 1Ãž, .. . ,. .. , Ã°i, 1Ãž,. .. .. . , Ã°i, 1ÃžÂŠ. After sorting A Â¼ Â½.. . ,. .. , Ã°i, 1Ãž, Ã°i, 1Ãž, Ã°i, 1Ãž,. .. .. .ÂŠ and compressing results in A Â¼ Â½.. . , Ã°i, 3Ãž,. . .ÂŠ. We have to repeat these steps until we have seen all items. To reduce the number of times, we sort the complete array, and we apply the following strategies. We select a threshold n5t5jAj. We start with an unsorted k-mer array. It is sorted and compacted (Phase-0 Sorted and Compacted array or Phase-0 SAC). We progress in phases as follows. At phase i, a certain number of items in the beginning of the array are already sorted and compressed [Phase-(iâ€“1) SAC]. The new incoming k-mers are stored as unsorted items in the empty part of the array. Let m be the total number of items in the array. When m4t, we sort the unsorted items. Many of these k-mers are expected to exist in Phase-(iâ€“1) SAC. We make a linear traversal of the array replacing k-mers present in both Phase-(iâ€“1) SAC and the newly sorted part with one item in Phase-(iâ€“1) SAC. The k-mers not present in Phase-(i â€“ 1) SAC are represented with one item in the newly sorted part. The counts are added up to reflect the total number of times a k-mer was seen. This takes O(m) time. Note that this compaction has sequential and localized memory access, which makes it cache-efficient. After a few such compaction steps, m4t, and we sort and compress all the items in the array to produce Phase-i SAC. By repeatedly applying this mechanism on the frequent items, we ultimately get the list of frequent k-mers with their counts decremented by 1. This is due to the fact that when inserted into the array for the first time, an item was seen at least twice unless it is a false-positive finding. To offset this, we simply add 1 to all counts before writing them out to a file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.5">Parallelization</head><p>We implemented a one-producer, multiple-consumer model with a pool of buffers. The producer extracts k-mers from the reads and distributes them among the consumers. Each consumer has its own Bloom filter. Because a k-mer should always pass through the same Bloom filter, we distribute the k-mers to the consumers using the modulo operation, which is one of the cheapest hash functions available. Because modulo a prime number shows better hash properties compared with non-primes, it is recommended that one uses a prime (or at least an odd) number of threads because this spreads out the k-mers more evenly among the consumers, which is helpful for speeding up the parallelization. The k-mers are stored in buffers, and only when the buffers fill up are they transferred to the consumer. Because consumers consume the kmers at an uneven rate, having the same fixed buffer size for all consumers may cause the producer to block if the buffer for a busy consumer fills up. To reduce such blocking, we have a pool of buffers, and the number of buffers is more than the number of consumers. If a consumer is taking longer to consume its items, the producer has extra buffers to store its k-mers in. This improves the speedup. With many consumers (usually413), the producer becomes the bottleneck. Therefore, it is important to make the producer more efficient. The two most expensive parts of the producer are converting reads to k-mers and the modulo operation required to determine which consumer handles a particular k-mer. Modern computers support SSE (<ref type="bibr" target="#b18">Patterson and Hennessey, 1998</ref>) instructions that operate on 128-bit registers and can perform arithmetic/logic operations in parallel on multiple variables. We used Streaming SIMD Extensions (SSE) instructions for speeding up bit encoding of k-mers. It is also possible to design approximate modulo functions that execute much faster than regular modulo instruction for some numbers (e.g. 5, 7, 9, 10, 63) (<ref type="bibr" target="#b26">Warren, 2012</ref>). But each of these functions has to be custom designed. If we restrict the number of consumers to the numbers that have efficient modulo function, it is possible to improve the producer's running time even further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.6">Running time analysis We first analyze the sort and compress</head><p>algorithm. Let the total number of frequent k-mers (those with frequency !2) be N, and let n be the number of distinct frequent k-mers. We use an xn, x41, sized array A for storing the frequent k-mers and their counts. First, consider the following simplified version of our algorithm: Ã°x Ã€ 1Ãžn new items are loaded into the array, and they are sorted and compacted. Because there are n distinct k-mers, at least xn Ã€ n Â¼ Ã°x Ã€ 1Ãžn locations will be empty after sorting and compaction. We again load Ã°x Ã€ 1Ãžn items and perform sorting and compaction. We iterate until all N items have been seen. Each iteration takes OÃ°xn log xn Ã¾ xnÃž time, and we have at most N=Ã°x Ã€ 1Ãžn such iterations. Thus, the total time required is:</p><formula>O N Ã°x Ã€ 1Ãžn Ã°xn log xn Ã¾ xnÃž Â¼ O x Ã°x Ã€ 1Ãž Ã°N log xn Ã¾ NÃž O x Ã°x Ã€ 1Ãž Ã°N log N Ã¾ NÃž</formula><p>As discussed earlier, to reduce the number of times sorting is performed, which is more expensive than compaction, we implemented a modified version of the aforementioned method, which delays sorting at the expanse of more compactions. Our benchmarking shows this to be faster than the naive method. The algorithm we implemented progresses in phases as follows. At the beginning of phase i, the array is filled up with unsorted elements. They are sorted and compacted<ref type="bibr">[</ref>The total cost of a lazy compaction is therefore upper bounded by OÃ°xn log xn Ã¾ xnÃž. This again creates empty locations at the end of the array, which allows us to perform another round of lazy compression. We assume that the incoming items are uniformly distributed, and every lazy compaction stage reduces the size of the empty part by an approximately constant fraction 1/c. Therefore, on average, we expect to have c lazy compaction stages. This completes Phase-i, the expected cost of which is upper bounded by:</p><p>OÃ°xn log xn Ã¾ xn Ã¾ cÃ°xn log xn Ã¾ xnÃžÃž Â¼ OÃ°Ã°c Ã¾ 1ÃžÃ°xn log xn Ã¾ xnÃžÃž</p><p>To compute how many phases are expected to consume all N items, we observe that, at every phase, the lazy compaction steps consume a total of at least Ã°x Ã€ 1Ãžnf1 Ã¾ Ã°1 Ã€ 1 c Ãž Ã¾ Ã°1 Ã€ 2 c Ãž Ã¾. .. Ã¾ Ã°1 Ã€ cÃ€1 c Ãžg Â¼ Ã°x Ã€ 1Ãž nÃ°c Ã¾ 1Ãž=2 items. So, on average, each phase consumes at least Ã°c Ã¾ 1ÃžnÃ°x Ã€ 1Ãž=2 items, and hence the expected number of phases is at most 2N=nÃ°c Ã¾ 1ÃžÃ°x Ã€ 1Ãž. Therefore, the total expected cost would be: 2N Ã°c Ã¾ 1ÃžnÃ°x Ã€ 1Ãž O xnÃ°c Ã¾ 1Ãž log xn Ã¾ xnÃ°c Ã¾ 1Ãž Ã° Ãž Â¼ 2x Ã°x Ã€ 1Ãž O N log xn Ã¾ N Ã° Ãž O x Ã°x Ã€ 1Ãž Ã°N log N Ã¾ NÃž Note that we obtained the same expression for the naive version of sorting and compaction. It is surprising that this expression is independent of c. As an intuitive explanation, observe that more lazy compactions within a phase result in more items being consumed by a phase, which in turn decreases the number of phases. This inverse relationship between c and the number of phases makes the running time independent of c. We found the naive version to be slower than the implemented version in empirical tests and therefore believe our bound to be an acceptable approximation.We now analyze the performance of sorting and compaction-based strategy against a hash table-based strategy for counting frequency of items. Let p be the cache miss penalty, h be the hashing cost, s be the comparison and swapping cost for sort and compress and b be the number of items that fits in the cache. The cost of frequency counting in the hash-based method will be (p Ã¾ h)N because each hash update incurs one cache miss. For sorting and compress, we will have one cache miss for every b operation, and thus the cost for sorting and compaction will be Ã°p=b Ã¾ sÃžaÃ°N log N Ã¾ NÃž, where a Â¼ x Ã°xÃ€1Ãž. To compute the value of N for which sorting and compaction will be faster than a hashbased method, we write:</p><formula>Ã°p Ã¾ hÃžN ! Ã°p=b Ã¾ sÃžaÃ°N log N Ã¾ NÃž log N Ã°p Ã¾ hÃž Ã°p=b Ã¾ sÃža Ã€ 1</formula><p>Let a comparison and swap be one unit of work. A conservative set of values like s Â¼ 1, p Â¼ 160 (<ref type="bibr" target="#b12">Levinthal, 2008</ref>), h Â¼ 8, b Â¼ 256 (assuming 8 byte items and 2 KB cache), a Â¼ 2 results in N 2 50. Therefore, for a large range of values of N, with a fast and moderate-sized cache, the sorting and compaction-based method would run faster than a hashbased method. Because every observed k-mer has to go through the Bloom filter, the time required in the Bloom filter is O(M) where M is the total number of k-mers in the read library. Thus, the total running time that includes the Bloom filter checks and the sorting and compression of the frequent items is OÃ°MÃž Ã¾ OÃ°N log N Ã¾ NÃž. Our measurements on the datasets used show that the total time is dominated by the Bloom filter updates [i.e. OÃ°MÃž4OÃ°N log N Ã¾ NÃž].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">cTurtle</head><p>When there are so many frequent k-mers that keeping track of the k-mers and their counts explicitly is infeasible, we can obtain an approximate set of frequent k-mers by using a counting Bloom filter. Note that the number of bits required for a Bloom filter for n items is O(n), but the constants are small. For example, it may be shown that for a 1% falsepositive rate, the Bloom filter size is recommended to be $9.6n bits (<ref type="bibr" target="#b9">Fan et al., 2000</ref>). On the other hand, with a k-mer size of 32 and counter size of 1 byte, the memory required by a naive method that explicitly keeps track of the k-mers and their count is at least 9n bytes or 72n bits. With data compression techniques like prefix of k-mers being implied from the context of the data structure as in Jellyfish and KMC, this is 59n bytes but, from the memory comparison between Jellyfish and cTurtle presented in<ref type="figure" target="#tab_3">Table 3</ref>, we believe it still remains considerably higher than the 9.6n bits required by the Bloom filter. The basic idea of our counting Bloom filter is to set k bits in the Bloom filter when we see an item for the first time. When seen for the second time, the item is identified as a frequent k-mer and written to the disk. To record this writing, k 0 more bits are set in the Bloom filter. For all subsequent sightings of this item, we find the (k Ã¾ k 0 ) bits set and know that this is a frequent k-mer that has already been recorded. For cache efficiency, we implement the counting Bloom filter as a pattern-blocked counting Bloom filter as follows. We take a larger Bloom filter (B) of size L. When an item x is seen, k values (h 1 , h 2 ,. .. , h k ) within the range Â½hÃ°xÃž, hÃ°xÃž Ã¾ bÂŠ, where h(x) is a hash function and b is the block size, are computed using precomputed patterns. If this is the first appearance of x, with high probability, not all of the bits BÂ½h 1 ÂŠ,. .. , BÂ½h k ÂŠ are set to 1, and so we set all of them. When we see the same item again, we will find all of BÂ½h 1 ÂŠ,. .. , BÂ½h k ÂŠ set to 1. We then compute another set of locations (h kÃ¾1 , h kÃ¾2 ,. .. , h kÃ¾k 0 ) within the range Â½hÃ°xÃž Ã¾ b, hÃ°xÃž Ã¾ 2bÂŠ using precomputed patterns. Again, with high probability, not all of BÂ½h kÃ¾1 ÂŠ,. .. , BÂ½h kÃ¾k 0 ÂŠ are set to 1, and so we set all of them. At the same time, we write this k-mer to the disk as a frequent k-mer. For all subsequent observations of this k-mer, we will find all of BÂ½h 1 ÂŠ,. .. , BÂ½h kÃ¾k 0 ÂŠ set to 1 and will avoid writing it to the disk. Note that a false-positive rate in the second stage means that we do not write the k-mer out to file and thus have a false-negative rate.Currently, cTurtle reports k-mers with frequency 41. But this strategy can be easily adopted to report k-mers of frequency greater than c41. We argue that for most libraries with reasonable uniform coverage, c Â¼ 1 is sufficient. Let C be the average nucleotide coverage of a read library with read length R. Then, the average k-mer coverage is C k Â¼ CÃ°RÃ€kÃ¾1Ãž R (<ref type="bibr" target="#b27">Zerbino and Birney, 2008</ref>). Suppose we have an erroneous k-mer with one error. The probability that the same error will be reproduced is 1 3k where 1/k is the probability of choosing the same position, and 1/3 is the probability of making the same base call error. Therefore, the expected frequency of that erroneous k-mer is 1 Ã¾ CkÃ€1 3k. For R Â¼ 100 and k Â¼ 31, this expression is 1 Ã¾ 0.0075C. Therefore, we need C4132:85 at a location for an erroneous 31-mer to have a frequency42. Because most large libraries are sequenced at a much lower depth (560x), such high coverage is unlikely except for exactly repeated regions, and therefore our choice of frequency cutoff will provide a reasonable set of reliable kmers. However, this does not hold for single-cell libraries, which exhibit uneven coverage (<ref type="bibr" target="#b7">Chitsaz et al., 2011</ref>). Note that frequent k-mers are considered reliable only for uniform coverage libraries, and thus singlecell libraries are excluded from our consideration. The parallelization strategy is the same as that for scTurtle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">COMPARISONS WITH K-MER COUNTERS</head><p>The datasets we use to benchmark our methods are presented in<ref type="bibr">et al., 2013</ref>) is the fastest open-source k-mer counter. DSK (<ref type="bibr" target="#b22">Rizk et al., 2013</ref>) is also memory-efficient but is slow. Khmer (<ref type="bibr" target="#b19">Pell et al., 2012</ref>) and BFCounter (<ref type="bibr" target="#b16">Melsted and Pritchard, 2011</ref>) use Bloom filter-based methods for reducing memory requirements. We have a similar strategy for memory reduction but achieve a much better computational efficiency. We decided not to report times for any tool that required 410 h of wall-clock time, and therefore some data are missing in<ref type="figure" target="#tab_3">Table 3</ref>. KMC was able to perform k-mer counting for all the datasets but was slower than Turtle for the larger datasets. Note that for the sake of comparison, we allowed KMC to use the same amount of memory that Turtle used, but it is capable of performing the computation with smaller amount of memory. Unexpectedly, on large datasets (ZM and HS), BFCounter required more memory than scTurtle (for 128 versus 109 GB). We suspect this is due to the memory overhead required to reduce collusions in the hash table used for storing frequent kmers, which we avoid using in our sort and compaction algorithm.<ref type="bibr" target="#b22">Rizk et al. (2013)</ref>claimed DSK to be faster than BFCounter, but on our machine, which had an 18 TB Raid-6 storage system of 2 TB SATA disks, it proved to be slower (1591 versus 1012 min for the GG dataset).<ref type="bibr" target="#b22">Rizk et al. (2013)</ref>reported performance using more efficient storage systems (solid-state disks). This might explain DSK's poor performance in our experiments. The detailed results are presented in<ref type="figure" target="#tab_3">Table 3</ref>for multithreaded Khmer, KMC, scTurtle and cTurtle. Because BFCounter (single threaded) and DSK (4 threads) do not allow a variable number of threads, we present their results separately in<ref type="figure" target="#tab_4">Table 4</ref>.</p><p>Jellyfish's (<ref type="bibr" target="#b14">Marcais and Kingsford, 2011</ref>) performance was inconsistent on the AMD machine (details not shown). For example, while running with 17 worker threads, it started with a near-perfect central processing unit (CPU) utilization of $1700% but steadily declined to $100%, resulting in an average CPU utilization of only 290%. The computation required 16 h and 56 min of wall-clock time and 238 GB of memory. This is inconsistent with Jellyfish's performance on Intel machines reported by<ref type="bibr" target="#b14">Marcais and</ref><ref type="bibr" target="#b14">Kingsford (2011) and</ref><ref type="bibr" target="#b22">Rizk et al. (2013)</ref>. Therefore, to compare our tools with Jellyfish, we ran additional experiments on the Intel machine.<ref type="figure" target="#tab_5">Table 5</ref>presents the wall-clock times for Jellyfish, KMC, scTurtle and cTurtle run with 19 worker threads on the Intel machine for all the datasets. OnNote: The input of a run is a single read file (FASTA or FASTQ format), and the output is a text file containing k-mers and their frequencies (FASTA or tab delimited format). The k-mer size is 31. Each tool was run six times with 19 worker threads, and the average was reported.this machine, Jellyfish's count step had a .5% for 19 worker threads. We found KMC to be the fastest tool for the small datasets (DM and GG), but for the two large datasets ZM and HS, Jellyfish and cTurtles, respectively, were the fastest. Jellyfish had the highest memory requirements for all datasets. To support our claim that the wall-clock time (and therefore parallelization) may be improved by speeding up the producer, we made special versions of scTurtle and cTurtle, which use 31 worker threads and a fast approximate modulus-31 function. For the largest library tested (HS), on average, the special version of scTurtle (counting only) produces frequent 31-mers in $73 min compared with $87 min by the regular version (a 19% speedup). As we use 64-bit integers for storing k-mers of length 32 and less and 128-bit integers for storing k-mers of length in the range 33â€“ 64, the memory requirement for larger k-mers was also investigated. Again, for the largest dataset tested (HS), we found that scTurtle's memory requirement increased from 109 GB for 05k 31 to 172 for 32 k 64 (a 58% increase). Note that the Turtles require less memory for up to 64-mers than Jellyfish for 31-mers. Detailed results of all the datasets for the Turtles are presented in<ref type="figure" target="#tab_6">Table 6</ref>. We also examined the error rates for our tools and BFCounter. Note that, just like BFCounter, scTurtle has falsepositive rates only, and cTurtle has both false-positive and falsenegative rates. We investigated these rates for the two small datasets (see<ref type="figure" target="#tab_7">Table 7</ref>) and found error rates for all tools to be 51%. For the large datasets, because of memory requirements, we could not get the exact counts for all k-mers and therefore could not compute these rates. The error rates also increase if the expected number of frequent k-mers is underestimated. As discussed in the introduction, for a genome of size g, we expect to observe approximately g frequent k-mers in the read library. To get an understanding of how the underestimation of g drives up the error rates, we tested the DM dataset with expected number of frequent k-mers to be g, 0.9g and 0.85g and found the false-positive rates to be 1.87, 1.99 and 2%, respectively. However, the true number of frequent k-mers in the DM library is % 1:07g. Therefore, we recommend setting this parameter to % 1:1g. Software versions, commands and parameters used for producing the results presented in this article are provided in Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>Identifying correct k-mers out of the k-mer spectrum of a read library is an important step in many methods in bioinformatics. Usually, this distinction is made by the frequency of the k-mers. Fast tools for counting k-mer frequencies exist, but for large read libraries, they may demand a significant amount of memory, which can make the problem computationally unsolvable on machines with moderate amounts of memory resource ( 128 GB or even with 256 GB for large datasets). Simple memory-efficient methods, on the other hand, can be timeconsuming. Unfortunately, there is no single tool that achieves a reasonable compromise between memory and time. Here we present a set of tools that make some compromises and simultaneously achieve memory and time requirements that are matching the current state-of-the-art in both aspects. With our first tool (scTurtle), we achieve memory efficiency by filtering k-mers of frequency one with a Bloom filter. Our pattern-blocked Bloom filter implementation is more time-efficient compared with a regular Bloom filter. We present a novel strategy based on sorting and compaction for storing frequent kmers and their counts. Because of its sequential memory access pattern, our algorithm is cache-efficient and achieves good running time. However, because of the Bloom filters, we incur a small false-positive rate. The second tool (cTurtle) is designed to be more memory-efficient at the cost of giving up the frequency values and allowing both false-positive and false-negative rates. The implementationNote: The tools ran with fast mod and 31 worker threads. Each reported number is an average of five runs.Note: For the large datasets, because of memory constraints, the exact counts for all k-mers could not be obtained, and therefore, these rates could not be computed. is based on a counting Bloom filter that keeps track of whether a k-mer was observed and whether it has been stored in external media. This tool does not report the frequency count of the kmers. Both tools allow a k-mer size of up to 64. They also allow the user to decide how much memory should be consumed. Of course, there is a minimum memory requirement for each dataset, and the amount of memory directly influences the running time and error rate. However, we believe, with the proper compromises, the approximate frequent k-mer extraction problem is now computationally feasible for large read libraries within reasonable wall-clock time using a moderate amount of memory.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>OÃ°xn log xn Ã¾ xnÃž]. This is called the Phase-(iâ€“1) SAC. Let e be the number of empty locations after each complete sorting and compaction step. Then, Ã°x Ã€ 1Ãžn e xn. The new incoming k-mers are stored as unsorted items in the empty locations. When the empty part is full, we sort the new items [OÃ°xn log xnÃž]. Many of these k-mers are expected to exist in Phase-(iâ€“1) SAC. We make a linear traversal of the array replacing k-mers present in both Phase-(iâ€“1) SAC and the newly sorted part with one item in Phase-(iâ€“1) SAC. The k-mers not present in Phase-(iâ€“1) SAC are represented with one item in the newly sorted part. The counts are added up to reflect the total number of times a k-mer was seen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.1.</head><figDesc>Fig. 1. The Sorting and compaction mechanism. We start with an unsorted k-mer array. It is sorted and compacted (Phase-0 SAC). The empty part is filled with unsorted k-mers, sorted and compacted. After repeating this step several times, the compacted new part almost fills up the whole array. Then, all items are sorted and compacted to produce Phase-1 SAC. This cycle repeats until all k-mers have been seen</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><figDesc>Table 1. Comparison of sort and compress and hash table-based implementations for counting items and their frequencies</figDesc><table>Method 
Number of insertions/updates 

458 M 
2.2 B 

Time (s) Space (GB) Time (s) Space (GB) 

Sort and compress 
153.37 
2.70 
523.41 
7.10 
Jellyfish 
296.49 
2.40 
1131.70 
7.20 
Google dense hash 
626.77 
20.47 
6187.95 
40.38 
Google sparse hash 1808.48 
7.44 
28069.18 
10.60 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><figDesc>Table 2. Descriptive statistics about the datasets used for benchmarking</figDesc><table>Set ID 
Organism 
Genome size (Mb) 
Read library 
Bases (Gb) 

DM 
Drosophila melanogaster 
122 
SRX040485 
3.7 
GG 
Gallus gallus 
1 Ã‚ 10 3 
SRX043656 
34.7 
ZM 
Zea mays 
2:9 Ã‚ 10 3 
SRX118541 
95.8 
HS 
Homo sapiens 
3:3 Ã‚ 10 3 
ERX009609 
135.3 

Note: The library sizes range from 3.7 to 135.3 Gb, and the genome size ranges from 122 Mb to 3.3 Gb. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>Table 3.</figDesc><table>Comparative results of wall-clock time and memory between Khmer, KMC, scTurtle and cTurtle on a machine with 48 cores (AMD 
OpteronTM 6174 Processors, clocked at 2.2 GHz) and 256 GB memory for 5â€“19 worker threads 

Set ID 
Tool 
Multi-worker-threaded wall-clock time (min:sec) 
Memory (GB) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>Table 2.</figDesc><table>The library sizes range from 3.7 to 135.3 Gb for gen-
omes ranging from 122 Mb to 3.3 Gb. Experiments were per-
formed on a 48-core computer with AMD OpteronTM 6174 
processors, clocked at 2.2 GHz, 256 GB of memory and 18 TB 
Raid-6 storage system, and an 80-core Intel machine with Intel Ã• 
Xeon Ã• CPU E7-4870, clocked at 2.40 GHz and 1 TB of memory. 
According to our experiments, with limited memory, KMC 
(Deorowicz </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><figDesc>Table 5. Comparative results of wall-clock time between Jellyfish, KMC, scTurtle and cTurtle on a SMP server with 80 cores (Intel Ã• Xeon Ã• CPU E7-4870 clocked at 2.40 GHz) and 1 TB of memory</figDesc><table>Set ID 
Tool 
Wall-clock time (min:s) 
Memory (GB) 

DM 

Jellyfish 
3:01 
7.4 
KMC 
1:48 
5.6 
scTurtle 
2:12 
5.3 
cTurtle 
1:59 
4.2 

GG 

Jellyfish 
32:03 
81.9 
KMC 
17:58 
46.8 
scTurtle 
22:39 
44.9 
cTurtle 
21:31 
23.9 

ZM 

Jellyfish 
42:44 
158.2 
KMC 
90:36 
82.0 
scTurtle 
60:15 
82.1 
cTurtle 
58:43 
51.6 

HS 

Jellyfish 
197:42 
238.0 
KMC 
103:27 
108.8 
scTurtle 
88:31 
109.5 
cTurtle 
85:39 
68.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><figDesc>Table 4. Performance of BFCounter and DSK (4 threads) for 31-mers</figDesc><table>Set ID 
Tool 
Wall-clock 
time (min:s) 

CPU utilization 
(%) 

Space 
(GB) 

DM 
BFCounter 
78:35 
99 
3.24 
DSK 
170:37 
318 
4.86 
GG 
BFCounter 
1011:51 
99 
29.26 
DSK 
1590:54 
290 
48.59 
ZM 
BFCounter 
42289:00 
NA 
4166.00 
DSK 
42923:00 
NA 
NA 
HS 
BFCounter 
43840:00 
NA 
4128.00 
DSK 
41367:00 
NA 
NA 

Note: Some of the results are not available because those computations could not be 
completed within a reasonable time. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><figDesc>Table 6. Performance of scTurtle (counting only) and cTurtle for 64-mers k-mer size Set ID</figDesc><table>Tool 
Wall-clock 
time (min:sec) 

CPU 
utilization (%) 

Space 
(GB) 

31 

DM scTurtle 
02:18 
2335.8 
5.50 
cTurtle 
1:28 
580.0 
4.20 
GG scTurtle 
24:48 
2388.0 
47.10 
cTurtle 
28:51 
413.2 
29.90 
ZM scTurtle 
55:57 
1838.0 
82.15 
cTurtle 
74:46 
756.0 
51.60 
HS 
scTurtle 
73:24 
1563.0 
109.53 
cTurtle 
98:24 
512.0 
68.55 

48 

DM scTurtle 
2:33 
1790.0 
8.30 
cTurtle 
2:30 
871.4 
4.76 
GG scTurtle 
25:11 
1373.8 
70.69 
cTurtle 
25:29 
693.8 
29.34 
ZM scTurtle 
90:16 
1125.0 
129.09 
cTurtle 
81:28 
782.0 
52.35 
HS 
scTurtle 112:11 
953.4 
172.11 
cTurtle 
105:10 
657.6 
69.29 

64 

DM scTurtle 
1:40 
948.0 
8.30 
cTurtle 
1:28 
580.0 
4.76 
GG scTurtle 
31:60 
825.8 
70.69 
cTurtle 
28:51 
413.2 
29.35 
ZM scTurtle 
79:90 
1037.0 
129.09 
cTurtle 
74:46 
756.0 
52.35 
HS 
scTurtle 
79:56 
952.0 
172.11 
cTurtle 
98:24 
512.0 
69.29 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><figDesc>Table 7. False-positive and false-negative rates of scTurtle and cTurtle</figDesc><table>Set ID 
scTurtle 
(FP only) (%) 

BFCounter 
(FP only) (%) 

cTurtle 

FP (%) 
FN (%) 

DM 
0.178 
0.300 
1:9 Ã‚ 10 Ã€4 
2:3 Ã‚ 10 Ã€4 
GG 
0.848 
0.027 
0.31 
0.08 

</table></figure>

			<note place="foot">ÃŸ The Author 2014. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">Turtle at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">R.S.Roy et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="5"> 7 9 1 1 1 3 1 5 1 7 1 9 DM</note>

			<note place="foot">Note: The input of a run is a single read file (FASTA or FASTQ format), and the output is a text file containing k-mers and their frequencies (FASTA or tab delimited format). Khmer does not provide a tool for dumping. Therefore, its run times are for counting only. Each reported number is an average of 5 runs. Runs requiring410 h were not reported. The k-mer size is 31. Recall that KMC, scTurtle and Khmer report k-mers and their counts, whereas cTurtle only reports the k-mers with count 41.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>The authors thank Mohammad Pavel Mahmud for helpful discussions regarding the run-time analysis and implementation of our method. Funding: RSR was partially supported by a grant from the National Science Foundation (DEB-1004213) to DB. Conflict of Interest: none declared.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">GG Khmer</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="40" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ZM Khmer</title>
		<imprint>
			<biblScope unit="volume">482</biblScope>
			<biblScope unit="issue">870</biblScope>
			<biblScope unit="page" from="3048" to="8255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">Hs</forename>
				<surname>Khmer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">!</forename>
			</persName>
		</author>
		<imprint>
			<biblScope unit="page" from="47" to="12114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Spades: a new genome assembly algorithm and its applications to single-cell sequencing</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Bankevich</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="455" to="477" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Cache-oblivious b-trees</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Bender</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="341" to="358" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Space/time trade-offs in hash coding with allowable errors</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">H</forename>
				<surname>Bloom</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="422" to="426" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Space-efficient and exact de Bruijn graph representation based on a Bloom filter</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Chikhi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Rizk</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics Lecture Notes in Computer Science</title>
		<editor>Raphael,B. and Tang,T.</editor>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="volume">7534</biblScope>
			<biblScope unit="page" from="236" to="248" />
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient de novo assembly of single-cell bacterial genomes from short-read data sets</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Chitsaz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="915" to="921" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Disk-based k-mer counting on a PC</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Deorowicz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">160</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Summary cache: a scalable wide-area web cache sharing protocol</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Fan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Netw</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="281" to="293" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Whole-genome sequence assembly for mammalian genomes: Arachne 2</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">B</forename>
				<surname>Jaffe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="91" to="96" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Quake: quality-aware detection and correction of sequencing errors</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">R</forename>
				<surname>Kelley</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">116</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<monogr>
		<title level="m" type="main">Performance analysis guide for Intel Core i7 processor and intel xeon 5500 processors</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Levinthal</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Musket: a multistage k-mer spectrum based error corrector for illumina sequence data</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Liu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="308" to="315" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">A fast, lock-free approach for efficient parallel counting of occurrences of k-mers</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Marcais</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Kingsford</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="764" to="770" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Error correction of high-throughput sequencing datasets with non-uniform coverage</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Medvedev</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="137" to="141" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient counting of k-mers in DNA sequences using a Bloom filter</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Melsted</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">K</forename>
				<surname>Pritchard</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">333</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Aggressive assembly of pyrosequencing reads with mates</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">R</forename>
				<surname>Miller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2818" to="2824" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<monogr>
		<title level="m" type="main">Computer Organization and Design: the Hardware/Software Interface</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">A</forename>
				<surname>Patterson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">L</forename>
				<surname>Hennessey</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Morgan Kaufmann Publishers, Inc</publisher>
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd. edn</note>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Scaling metagenome sequence assembly with probabilistic de Bruijn graphs</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Pell</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA, [Epub ahead of print</title>
		<meeting>. Natl Acad. Sci. USA, [Epub ahead of print</meeting>
		<imprint>
			<date type="published" when="2012-06-26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">An Eulerian path approach to DNA fragment assembly</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">A</forename>
				<surname>Pevzner</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA, 98</title>
		<meeting>. Natl Acad. Sci. USA, 98</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="9748" to="9753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Cache-, hash-, and space-efficient Bloom filters</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Putze</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Algorithmics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="4" to="4" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">DSK: k-mer counting with very low memory usage</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Rizk</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="652" to="653" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<monogr>
		<title level="m" type="main">Data Compression: The Complete Reference</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Salomon</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<monogr>
		<title level="m" type="main">Abyss: a parallel assembler for short read sequence data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">T</forename>
				<surname>Simpson</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1117" to="1123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<monogr>
		<title level="m" type="main">Hackers Delight</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">S</forename>
				<surname>Warren</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Addison-Wesley Professional</publisher>
		</imprint>
	</monogr>
	<note>2nd. edn</note>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Velvet: algorithms for de novo short read assembly using de Bruijn graphs</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">R</forename>
				<surname>Zerbino</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Birney</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="821" to="829" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>