
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Disk-based compression of data from genome sequencing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Szymon</forename>
								<surname>Grabowski</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Applied Computer Science</orgName>
								<orgName type="institution">Lodz University of Technology</orgName>
								<address>
									<addrLine>Al. Politechniki 11, 90-924 ŁódzŁódz´</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Sebastian</forename>
								<surname>Deorowicz</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Informatics</orgName>
								<orgName type="institution">Silesian University of Technology</orgName>
								<address>
									<addrLine>Akademicka 16</addrLine>
									<postCode>44-100</postCode>
									<settlement>Gliwice</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Łukasz</forename>
								<surname>Roguski</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department">Polish-Japanese Institute of Information Technology</orgName>
								<address>
									<addrLine>Koszykowa 86</addrLine>
									<postCode>02-008</postCode>
									<settlement>Warszawa</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Centro Nacional de Aná lisis Genó mico (CNAG)</orgName>
								<address>
									<postCode>08-028</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Disk-based compression of data from genome sequencing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btu844</idno>
					<note type="submission">Received on September 19, 2014; revised on November 27, 2014; accepted on December 17, 2014</note>
					<note>Sequence analysis *To whom correspondence should be addressed. Associate Editor: John Hancock Contact: sebastian.deorowicz@polsl.pl Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: High-coverage sequencing data have significant, yet hard to exploit, redundancy. Most FASTQ compressors cannot efficiently compress the DNA stream of large datasets, since the redundancy between overlapping reads cannot be easily captured in the (relatively small) main memory. More interesting solutions for this problem are disk based, where the better of these two, from Cox et al. (2012), is based on the Burrows–Wheeler transform (BWT) and achieves 0.518 bits per base for a 134.0 Gbp human genome sequencing collection with almost 45-fold coverage. Results: We propose overlapping reads compression with minimizers, a compression algorithm dedicated to sequencing reads (DNA only). Our method makes use of a conceptually simple and easily parallelizable idea of minimizers, to obtain 0.317 bits per base as the compression ratio, allowing to fit the 134.0 Gbp dataset into only 5.31 GB of space. Availability and implementation: http://sun.aei.polsl.pl/orcom under a free license.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It is well known that the growth of the amount of genome sequencing data produced in the last years outpaces the famous Moore's law predicting the developments in computer hardware (<ref type="bibr" target="#b7">Deorowicz and Grabowski, 2013;</ref><ref type="bibr" target="#b15">Kahn, 2011</ref>). Confronted with this deluge of data, we can only hope for better algorithms protecting us from drowning. Speaking about big data management in general, there are two main algorithmic concerns: faster processing of the data (at preserved other aspects, like mapping quality in de novo or referential assemblers) and more succinct data representations (for compressed storage or indexes). In this article, we focus on the latter concern. Raw sequencing data are usually kept in FASTQ format, with two main streams: the DNA symbols and their corresponding quality scores. Older specialized FASTQ compressors were lossless, squeezing the DNA stream down to about 1.5–1.8 bpb (bits per base) and the quality stream to 3–4 bpb, but more recently it was noticed that a reasonable solution for lossy compression of the qualities has negligible impact on further analyzes, for example, referential mapping or variant calling performance (<ref type="bibr" target="#b3">Cá novas et al., 2014;</ref><ref type="bibr">Illumina, 2012;</ref><ref type="bibr" target="#b24">Wan et al., 2012</ref>). This scenario became thus immediately practical, with scores lossily compressed to about 1 bpb (<ref type="bibr" target="#b13">Janin et al., 2014</ref>) or less (<ref type="bibr" target="#b27">Yu et al., 2014</ref>). Note also that Illumina software for their HiSeq 2500 equipment contains an option to reduce the number of quality scores (even to a few), since it was shown that the fraction of discrepant single nucleotide polymorphisms grows slowly with diminishing number of quality scores in Illumina's CASAVA package (http://support.illumina.com/sequencing/sequencing_software/casava.ilmn). It is easy to notice that now the DNA stream becomes the main compression challenge. Even ifhigher order modeling (<ref type="bibr" target="#b0">Bonfield and Mahoney, 2013</ref>) or LZ77-style compression (<ref type="bibr" target="#b6">Deorowicz and Grabowski, 2011</ref>) can lead to some improvement in DNA stream compression, we are aware of only two much more promising approaches. Both solutions are disk based. Yanovsky (2011) creates a similarity graph for the dataset, defined as a weighted undirected graph with vertices corresponding to the reads of the dataset. For any two reads s 1 and s 2 the edge weight between them is related to the 'profitability' of storing s 1 and the edit script for transforming it into s 2 versus storing both reads explicitly. For this graph, its minimum spanning tree (MST) is found. During the MST traversal, each node is encoded using the set of maximum exact matches between the node's read and the read of its parent in the MST. As a backend compressor, the popular 7zip is used. ReCoil compresses a dataset of 192 M Illumina 36 bp reads (http://www.ncbi.nlm.nih.gov/sra/SRX001540), with coverage below 3-fold, to 1.34 bpb. This is an interesting result, but ReCoil is hardly scalable; the test took about 14 h on a machine with 1.6 GHz Intel Celeron CPU and four hard disks. More recently, Cox et al. (2012) took a different approach, based on the Burrows–Wheeler transform (BWT). Their result for the same dataset was 1.21 bpb in less than 65 min, on a Xeon X5450 (Quad-core) 3 GHz processor. The achievement is however more spectacular if the dataset coverage grows. For 44.5-fold coverage of real human genome sequence data, the compressed size improves to as little as 0.518 bpb (Actually in<ref type="bibr" target="#b5">Cox et al. (2012)</ref>the authors report 0.484 bpb, but their dataset is seemingly no longer available and in our experiments we use a slightly different one.) allowing to represent the 134.0 Gbp of input data in 8.7 GB of space. Note that if the reference sequence is available, either explicit or can be reconstructed ('presumed', in the terminology of Cá novas and Moffat 2013), then compressing DNA reads is much easier and high compression ratios are possible. Several FASTQ or SAM/BAM compressors make use of a reference sequence, to name Quip (<ref type="bibr" target="#b14">Jones et al., 2012</ref>), Fastqz and Fqzcomp (<ref type="bibr" target="#b0">Bonfield and Mahoney, 2013</ref>) in one of their modes, SlimGene (<ref type="bibr" target="#b16">Kozanitis et al., 2011</ref>), CRAM (<ref type="bibr" target="#b9">Fritz et al., 2011</ref>), Goby (<ref type="bibr" target="#b1">Campagne et al., 2013</ref>), DeeZ (<ref type="bibr" target="#b11">Hach et al., 2014</ref>) and FQZip (<ref type="bibr" target="#b28">Zhang et al., 2015</ref>). Several techniques for compressing SAM files, including mapping reads to a presumed reference sequence, were also explored in Cá novas and Moffat (2013). In this article, we present a new reference-free compressor for FASTQ data, Overlapping Reads COmpression with Minimizers (ORCOM), achieving compression ratios surpassing the best known solutions. For the two mentioned human datasets it obtains the compression ratios of 1.005 and 0.317 bpb, respectively. ORCOM is also fast, producing the archives in about 8 and 77 min, respectively, using eight threads on an AMD Opteron 6136 2.4 GHz machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and methods</head><p>Let s ¼ s½0s½1. .. s½n À 1 be a string of length n over a finite alphabet R of size r. We use the following notation, assuming 0 i j &lt; n: s½i denotes the ði þ 1Þth symbol of s, s½i. .. j the substring s½is½i þ 1. .. s½j (called a factor of s), and s t the concatenation of strings s and t. Our algorithm, ORCOM, follows the ancient paradigm of external algorithms: distribute the data into disk bins and then process (i.e. compress) each bin separately. Still, the major problem with this approach in reads compression concerns the bin criterion: how to detect similar (overlapping) reads, in order to pass them into the same bin? Our solution makes use of the idea of minimizers (<ref type="bibr" target="#b19">Roberts et al., 2004</ref>), a late bloomer in bioinformatics, cf.</p><p>(<ref type="bibr" target="#b4">Chikhi et al., 2014;</ref><ref type="bibr" target="#b8">Deorowicz et al., 2014;</ref><ref type="bibr" target="#b17">Li et al., 2013;</ref><ref type="bibr" target="#b18">Movahedi et al., 2012;</ref><ref type="bibr" target="#b25">Wood and Salzberg, 2014</ref>). Minimizers are a simple yet ingenious notion. The minimizer for a read s of length r is the lexicographically smallest of its all ðr À p þ 1Þ p-mers; usually it is assumed that p ( r. This smallest p-mer may be the identifier of the bin into which the read is then dispatched. Two reads with a large overlap are likely to share the same minimizer. In the next paragraphs we present the details of our solution. Assume the alphabet size r ¼ 5 (ACGTN). A reasonable value of p is about 10, but sending each bin to a file on disk would require 5 10 ¼ 9:77 M files, which is way too much. Reducing this number to 4 10 þ 1 (all minimizers containing at least one symbol N, which are rare, are mapped to a single bin, labeled N) is still not satisfactory. We solved this problem in a radical way, using essentially one (In fact, there is one extra file, with metadata, yet this one is of minor overall importance and we skip further description.) temporary file for all the bins, using large output buffers. To fetch the bin data in a further stage, the file has to be opened to read and the required reads extracted to memory from several locations of the file. As DNA sequences can be read in two directions: forwards and backwards (with complements of each nucleotide), we also process each read twice, in its given and reverse-complemented form. Additionally, we introduce a 'skip zone', that is, do not look for minimizers in read suffixes of (default) length z ¼ 12 symbols. This allows for some improvement in read ordering in the next step. The minimizers are thus sought over 2ðr À z À p þ 1Þ resulting p-mers. We call them canonical minimizers. However, a problem with strictly defined minimizers is uneven bin distribution. This not only increases the peak memory use, but also hampers parallel execution as the requirement for load balancing is harder to fulfill. To mitigate these problems we forbid some canonical minimizers, namely those that contain any triple AAA, CCC, GGG or TTT or at least one N (some of them, especially minimizers with runs of three or more As, are frequent). The allowed canonical minimizers are further called signatures, a term that we also used (with a slightly different definition) for the minimizers used in KMC 2, a k-mer counting algorithm (<ref type="bibr" target="#b8">Deorowicz et al., 2014</ref>). In the next step, when the disk bins are built, we reordered the reads in bins to move overlapping reads possibly close to each other. From a few simple sort criteria tried out, the one that worked best was to sort the reads s i , for all i, according to the lexicographical order of the string s i ½j. .. r À 1 s i ½0. .. j À 1, where j is the beginning position of the signature for the read s i. Such a reordering has a major positive impact on further compression. The reason is that overlaping reads are with high probability close to each other in the reordered array. The size of the skip zone should be chosen carefully. When too small, some signatures will be found close to the end of the read and the first factor of the sorting criterion, s i ½j. .. r À 1, will be too short to have a good chance of placing the read among those that overlap it in the genome. On the other hand, with the zone being too long many truly overlapping reads will be forbidden. The last phase is the backend compression on bin-by-bin basis. We devised a specialized processing method, which produces several (interleaving) streams of data, finally compressed with either a well-known context-based compressor PPMd (<ref type="bibr" target="#b23">Shkarin, 2002</ref>) or a variant of arithmetic coder (<ref type="bibr" target="#b21">Salomon and Motta, 2010</ref>) (We use a popular and fast arithmetic coding variant by Schindler (http:// www.compressconsult.com/rangecoder/), also known as a RC.). How we process the bins in detail, including careful mismatch handling, is presented in the next paragraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lengths</head><p>Read lengths are stored here (1 byte per length in the current implementation, but a simple byte code can be used to handle the general case). The five streams: lettersN, lettersA, lettersC, lettersG, lettersT (These are used only if 'Flag' is f ex ; f mis or f oth .) 'LettersN' stores (i) all mismatching symbols from the current read where at the corresponding position of the referenced read there is symbol N, and (ii) all trailing symbols from the current read beyond the match (i.e. C and C in the example above). 'LettersX', for X 2 fA; C; G; Tg, stores all mismatching symbols from the current read where at the corresponding position of the referenced read there is symbol X (in our running example, the mismatching C would be encoded in the stream 'lettersG'. Note that the alphabet size for any 'LettersX' stream is 4, that is, fA; C; G; T; NgnfXg. Prev (Used only if 'Flag' is f ex ; f mis or f oth .) Stores the location (id) of the referenced read from the buffer. Shift (Used only if 'Flag' is f ex, f mis or f oth. ) Stores the offsets of the current reads against their referenced read. The offset may be negative. For our running example, the offset is þ 2. Matches (Used only if 'Flag' is f oth. ) Stores information on mismatch positions. For our running example, the matching area has 13 symbols, but 4 of them belong to the signature and can thus be omitted (as the signature's position in the current read is known from the corresponding value in the stream 'Shift'). A form of RLE (run-length encoding) is used here. Namely, each run of matching positions (of length at least 1) is encoded with its length on 1 byte, and if the byte value is less than 255 and there are symbols left yet, we know that there must be a mismatch at the next position, so it is skipped over. 'Unpredicted' mismatches are encoded with 0. For our example, we obtain the sequence: 1, 7 (match of length 1; omitted mismatch; match of length 11, which is 7 plus 4 for the covered signature's area). HReads (Used only if 'Flag' is f diss .) Here the 'hard' reads (not similar enough to any read from the buffer) are dispatched. They are stored almost verbatim: the only change in the representation is to replace the signature with an extra symbol (.). This helps a little for the compression ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rev</head><p>Contains binary flags telling if each read is processed directly or first reverse-complemented. Some of the streams are compressed with a strong general-purpose compressor, PPMd (http://compression.ru/ds/ppmdj1.rar), using switches-o4-m16m (order-4 context model with memory use up to 16 MB), others with our range coder (RC), also of order-4. Namely, the streams 'Flags' and 'Rev' are compressed with order-4 RC, the stream 'LettersX' with order-4 RC, where the context is formed of the four previous symbols, and all the other streams are compressed with PPMd. The description presented above is somewhat simplified. We took some effort to achieve high processing performance. In particular, the input data (read from FASTQ files, possibly gzipped) are processed in 256 MB blocks (block size configurable as an input parameter) and added to a queue. Several worker threads find signatures in them, perform the necessary processing and add to an output queue, whose data are subsequently written to the temporary file. Also further bin processing is parallelized, to maximize the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We tested our algorithm versus several competitors on real and simulated datasets, detailed in<ref type="figure" target="#tab_1">Table 1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Real datasets</head><p>The experimental results with real read data are presented in the upper parts of Tables 2 and 3. Apart from the proposed compressor Disk-based compression of data from genome sequencingORCOM, we tested DSRC 2 (<ref type="bibr" target="#b20">Roguski and Deorowicz, 2014</ref>), Quip (<ref type="bibr" target="#b14">Jones et al., 2012</ref>), FQZComp (<ref type="bibr" target="#b0">Bonfield and Mahoney, 2013</ref>), Scalce (<ref type="bibr" target="#b10">Hach et al., 2012</ref>), SRcomp (<ref type="bibr" target="#b22">Selva and Chen, 2013</ref>) and BWT-SAP (<ref type="bibr" target="#b5">Cox et al., 2012</ref>). All these competitors, with the exception of BWT-SAP and SRcomp, are FASTQ compressors, and all of them present compression results of the separate streams. In<ref type="figure" target="#tab_2">Table 2</ref>we also present the result of ReCoil (<ref type="bibr" target="#b26">Yanovsky, 2011</ref>) on one dataset, copied from<ref type="bibr" target="#b5">Cox et al. (2012)</ref>. ReCoil is too slow to be run on all our data in reasonable time. As we can see in<ref type="figure" target="#tab_2">Table 2</ref>, ORCOM wins on all datasets, in an extreme case (Musa balbisiana) with almost twice better compression ratio than the second best compressor, BWT-SAP.<ref type="figure" target="#fig_3">Figure 1</ref>confirms the intuition that the performance of our algorithm improves with growing coverage.<ref type="figure" target="#tab_3">Table 3</ref>presents the compression times and RAM consumptions. Our compressor is also usually the fastest, with rather moderate memory usage (up to 14 GB). We point out that the first phase, distributing the data into bins, is not very costly, for example, it takes less than 25% of the total time for the largest dataset, Homo sapiens 2. In the memory use the most frugal is BWT-SAP (which is another disk-based software), spending only 3 MB for each dataset. One should remember that compression times are related to the number of used threads: Quip, FQZComp, SRcomp and BWT-SAP are sequential (one thread), whereas DSRC 2, Scalce and ORCOM are parallel and use eight threads here. Moreover, ORCOM, BWT-SAP and SRcomp compress the DNA stream only, whereas the remaining compressors have full FASTQ files on the input (with fake remaining streams in case of simulated reads presented in Section 3.2), what hampers their performance in compression speed and memory use (the compression ratios are however given for the DNA stream only). For these reasons, the results from<ref type="figure" target="#tab_3">Table 3</ref>should not be taken too seriously; they are given mostly to point out promising performance of our software. ORCOM's compression performance depends on two parameters: the signature length and the skip zone length. How varying these parameters affects the compression ratio is shown in<ref type="figure" target="#fig_4">Figures 2</ref>and 3, respectively, on the example of two datasets. It seems that choosing the signature length from {6, 7, 8} is almost irrelevant for the compression ratio, but with longer signature the ratio starts to deteriorate. The impact of the skip zone length depends somewhat on the chosen signature length, yet from our results we can say that any zone length between 8 and 16 is almost equally good. We also show how replacing the straight minimizers with signatures affects the compression ratio and memory consumption (<ref type="figure" target="#tab_4">Table 4</ref>). The compression gain is slight, up to 2.3% on real data (H.sapiens 2) and up to 6.9% on simulated data (H.sapiens 3). Fortunately, the improvement is greater in memory reduction, sometimes exceeding factor 2. As we can see, in two cases using signatures required more memory than with minimizers, but this is for two relatively small datasets. Using signatures generally leads to more even data distribution across bins. The bin size distribution is still far from perfect though, as shown in<ref type="figure" target="#fig_7">Figure 4</ref>. Finally, in<ref type="figure" target="#tab_5">Table 5</ref>the sizes of individual streams after compression, for three datasets, are given. As one can see, the stream of hard reads is hardest to compress, which we think justifies its name.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Simulated datasets</head><p>In the experiments for simulated datasets the reads were obtained by randomly sampling H.sapiens reference genome (HG 37.3). The number of non-N-symbols in the reference is approximately 2859 M. The H.sapiens 3 dataset contains 1.25 G reads of length 100 bp reads (to have the genome coverage like for H.sapiens 2). Half of them were obtained directly from the reference and another half were reverse complemented. The reads in H.sapiens 4 dataset were obtained from H.sapiens 3 dataset by modifying each base with a probability 1% (the probability is independent from the base position). It is important to stress that such a simulation of errors is far from what happens in real experiments (e.g. in most sequencers the quality of bases depends on the base position). Nevertheless, this simple error model allows us to compute the theoretically possible compression ratio and compare theNotes: Compressed ratios, in bits per base. The results of our approach are presented in the rightmost column. The best results are in bold. 'NS' means that the compressor was not examined as it does not support variable-length reads in a dataset. ReCoil was not examined in our experiments due to very long running times [the only result comes from<ref type="bibr">Cox et al. (2012) paper]</ref>. results of existing compressors with the estimated optimum and check what improvement is still possible in the reads compression area. The obtained compression ratios are presented in two bottom rows of<ref type="figure" target="#tab_2">Table 2</ref>. We note that 'standard' FASTQ compressors achieve compression ratios similar to the ones on real reads, which is perhaps no surprise. BWT-SAP achieves a substantial improvement on H.sapiens 3, as the noise in the real data must have broken many long runs in the BWT-related sequence and have hampered the compression. Yet, even more improvement, close to 2-fold, is observed for ORCOM. This can be explained by the local search for similar reads in our solution: once an error affects a read's signature area, the read is moved to another bin. On the noisy H.sapiens 4 dataset all compression ratios are, as expected, inferior. It is also not surprising that the standard FASTQ compressors, unable to eliminate most of the redundancy of the DNA reads, lose less here than ORCOM and Scalce do. The compression of clean data (H.sapiens 3) is also faster (<ref type="figure" target="#tab_3">Table 3</ref>). It is interesting to compare the ratios with estimations on how good compression ratio is possible. In theory, it is possible to perform a de novo assembly and to reproduce the genome from the reads. The reads from H.sapiens 3 dataset can be reordered according to the position of the read in the assembled genome. Thus, to encode the dataset it is sufficient to encode the assembled genome and for each read also: @BULLET the position of the read in the assembled genome, @BULLET the length (in a case of variable-length reads); for our data it is unnecessary as all reads are of length 100 bp, @BULLET the read orientation, that is, whether the read maps the genome directly or must be reverse complemented.Notes: Times are in thousands of seconds. RAM consumptions are in GBs. The best results are in bold. 'NS' means that the compressor was not examined as it does not support variable-length reads in a dataset.The assembled genome can be encoded using 2 bits per base (there is no N symbols), so for 2859 Mb we need 5718 Mbit. Then, for each read it is necessary to use 1 bit for the orientation, that is, 1250 Mbit in total. The read positions are ordered but are not unique, so to encode the positions we need to store a multisubset of size 1250 M from a set of size 2859 M, which is equivalent to storing 1250 M unique and ordered integers from the range h0; 1250M þ 2859MÞ ¼ h0; 4109MÞ. The number of bits necessary to encode m unique and ordered integers from the range h0; nÞ is</p><formula>0.5 z Compression ratio [bpb] G. gallus, p = 6 H. sapiens 2, p = 6 G. gallus, p = 8 H. sapiens 2, p = 8 G. gallus, p = 10 H. sapiens 2, p = 10</formula><formula>log 2 n m ! % nlog 2 n À ðn À mÞlog 2 ðn À mÞ À mlog 2 m:</formula><formula>(1)</formula><p>For n ¼ 4109 Â 10 6 and m ¼ 1250 Â 10 6 we obtain 3642.12 Mbit. Thus, in total we need 10 610.12 Mbit, so the compression ratio expressed in bits per base is 0.085. In case of reads with 1% of wrong bases we need to encode also the bases in the reads and the positions of the differences between the reads and the assembled genome. There are 1250 Mbases to encode, but this time it is enough to use log 2 3 bits per base as we are sure that the actual base differs to the base in the genome, so the total size of these data is 1981.20 Mbit. To encode the positions, we can conceptually concatenate the reads and encode the ordered and unique positions of wrong bases. We now apply Equation (1) with parameters n ¼ 125 Â 10 9 and m ¼ 0:01n and obtain 10 099.14 Mbit. Thus, in total, to encode 1.25 G reads of length 100 bp with 1% of wrong bases, we need 22 690.46 Mbit, which translates into 0.182 bpb. We can notice that the obtained results with simulated reads are much worse (roughly, by factor 2 for H.sapiens 3 and factor 3 for H.sapiens 4) than the estimated lower bounds. This is basically due to two reasons. One is that the proposed read grouping method belongs to crisp ones, that is, one read belongs to one and only one bin. In this way, reads with relatively small overlaps are likely to be scattered to different bins and their cross-correlation cannot be exploited. Moreover, even reads with a large overlap has some (albeit rather small) chance of landing in different bins. This harmful effect of separating similar reads is stronger for noisy data. The other reason is the simplicity of our modeling, in which read alignment is performed only in pairs of reads and thus some long matches may be prematurely truncated. Overcoming these limitations of our algorithm is an interesting topic for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and future work</head><p>We presented ORCOM, a lightweight solution for grouping and compressing overlapping reads in DNA sequencing data. We showed that the obtained compression ratio for large datasets is much better the one from the previously most successful, BWT-based, approach. Our algorithm is based on the recently popular idea of minimizers. For the human dataset comprising reads of 100 bp, with 44.5-fold coverage, we obtain 0.317 bpb compression ratio. This means that the 134.0 Gb dataset can be stored in as little as 5.31 GB. Also for the other tested datasets ORCOM attains, to our knowledge, compression ratios better than all previously reported. ORCOM, as a tool, may be improved in a number of ways. Its performance depends, albeit mildly, on two parameters: signature length and skip zone length. Currently their default values are set ad hoc, while in the future we are going to work out quite a robust automated parameter selection procedure. Also, our plans include fine-tuning the backend modeling (e.g. the distance function between reads is rather crude now). More importantly, perhaps, a memory-only mode can be added, convenient for powerful machines, but with more compact internal data representations affordable also for standard PCs, at least on small to moderate sized genomes. On the other hand, in the diskbased mode the memory use may be reduced, at least as a trade-off (less RAM, but also fewer threads, thus slower compression and/or somewhat worse compression ratio). From the algorithmic point, a more interesting challenge would be to come closer to the compression bounds estimated in Section 3.2. Finally, our ideas could be incorporated in a full-fledged FASTQ compressor, together with recent advances in lossy compression of the quality data, to obtain unprecedented compression ratios for FASTQ inputs in an industryoriented massively parallel implementation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>. The test machine was a server equipped with four 8-core AMD Opteron 6136 2.4 GHz CPUs, 128 GB of RAM and a RAID-5 disk matrix containing 6 HDDs. We use decimal multiples for the units, that is, 'M' (mega) is 10 6 , 'G' (giga) is 10 9 , etc., for the file sizes and memory uses reported in this section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>Notes: Approximate genome lengths are in Mbases according to http://www.ncbi.nlm.nih.gov/genome/. Homo sapiens 3 and 4 datasets are artificial reads produced by sampling reference human genome. In H.sapiens 3 the sampled reads are exact and in H.sapiens 4 bases are modified with probability 1%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.1.</head><figDesc>Fig. 1. Compression ratio for various coverage factors for M.balbisiana dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.2.</head><figDesc>Fig. 2. Compression ratio for various signature lengths for Gallus gallus and H.sapiens 2-trim datasets (z ¼ 10)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.3.</head><figDesc>Fig. 3. Compression ratio for various skip zone lengths for G.gallus and H.sapiens 2 datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig.4.</head><figDesc>Fig. 4. Bin sizes for G.gallus dataset. Note that bin id must be at least 1024, since 1024 corresponds to AACAAAAA, and the prefix AAA is forbidden. Some samples of bin id to signature mappings: 2000—AACTTCAA, 3000—AAGTGTGA, 4000—AATTGGAA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>V C The Author 2014. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 1389 Bioinformatics, 31(9), 2015, 1389–1395 doi: 10.1093/bioinformatics/btu844 Advance Access Publication Date: 22 December 2014 Original Paper</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><figDesc>Table 1. Characteristics of the datasets used in the experiments</figDesc><table>Dataset (Organism) 
Genome length 
Number of Gbases 
Number of Mreads 
Average read length 
Accession number 

G.gallus 
1040 
34.7 
347 
100 
SRX043656 
M.balbisiana 
472 
56.9 
569 
101 
SRX339427 
H.sapiens 1 
3093 
6.9 
192 
36 
SRX001540 
H.sapiens 2 
3093 
135.3 
1340 
101 
ERA015743 
H.sapiens 2-trim 
3093 
134.0 
1340 
100 
ERA015743 
H.sapiens 3 
3093 
125.0 
1250 
100 
— 
H.sapiens 4 
3093 
125.0 
1250 
100 
— 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 2. Compression ratios for various datasets</figDesc><table>Dataset 
DSRC 2 
Quip 
FQZComp 
Scalce 
SRcomp 
ReCoil 
BWT-SAP 
ORCOM 

G.gallus 
1.820 
1.715 
1.419 
0.824 
1.581 
— 
0.630 
0.433 
M.balbisiana 
1.838 
1.196 
0.754 
0.342 
0.522 
— 
0.208 
0.110 
H.sapiens 1 
1.857 
1.773 
1.681 
1.263 
1.210 
1.34 
1.246 
1.005 
H.sapiens 2 
1.821 
1.665 
1.460 
1.117 
NS 
— 
NS 
0.327 
H.sapiens 2-trim 
1.839 
1.682 
1.474 
0.781 
Failed 
— 
0.518 
0.317 
H.sapiens 3 
1.832 
1.710 
1.487 
0.720 
Failed 
— 
0.410 
0.174 
H.sapiens 4 
1.902 
1.754 
1.568 
1.022 
Failed 
— 
0.810 
0.562 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><figDesc>Table 3. Compression times and memory usage of compressors</figDesc><table>Dataset 
DSRC 2 
Quip 
FQZComp 
Scalce 
SRcomp 
BWT-SAP 
ORCOM 

Time 
RAM 
Time 
RAM 
Time 
RAM 
Time 
RAM 
Time 
RAM 
Time 
RAM 
Time 
RAM 

G.gallus 
1.3 
5.2 
9.7 
0.8 
12.3 
4.2 
4.6 
5.5 
4.2 
34.3 
62.3 
0.003 
1.1 
9.6 
M.balbisiana 
2.2 
5.4 
14.4 
0.8 
18.0 
4.2 
9.7 
5.4 
3.6 
55.5 
99.1 
0.003 
2.1 
5.2 
H.sapiens 1 
0.3 
6.1 
1.6 
0.8 
2.3 
4.2 
1.2 
5.5 
0.3 
6.9 
6.0 
0.003 
0.5 
9.7 
H.sapiens 2 
9.7 
2.8 
39.0 
0.8 
47.4 
4.3 
18.5 
5.5 
NS 
NS 
5.6 
13.8 
H.sapiens 2-trim 
9.7 
2.8 
39.0 
0.8 
45.9 
4.2 
18.5 
5.5 
Failed 
267.8 
0.003 
4.6 
12.5 
H.sapiens 3 
2.2 
5.2 
35.3 
0.8 
45.0 
4.2 
12.8 
5.4 
Failed 
246.1 
0.005 
2.8 
11.7 
H.sapiens 4 
2.8 
5.6 
42.7 
0.8 
48.7 
4.2 
16.0 
5.5 
Failed 
278.0 
0.006 
5.0 
13.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><figDesc>Table 4. Comparison of compression ratios and memory usage between minimizers and signatures in ORCOM</figDesc><table>Dataset 
Compression ratio 
RAM 

Minimizers Signatures Minimizers Signatures 

G.gallus 
0.443 
0.433 
9.7 
9.6 
M.balbisiana 
0.112 
0.110 
8.3 
9.6 
H.sapiens 1 
1.007 
1.005 
7.1 
9.7 
H.sapiens 2 
0.338 
0.327 
29.7 
13.8 
H.sapiens 2-trim 
0.330 
0.317 
28.7 
12.5 
H.sapiens 3 
0.188 
0.175 
26.5 
11.7 
H.sapiens 4 
0.565 
0.562 
26.9 
13.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><figDesc>Table 5. Stream sizes for selected datasets used in the experiments</figDesc><table>Stream 
G.gallus 
M.balbisiana 
H.sapiens 2 

Flags 
64 
53 
280 
Lengths 
0 
0 
157 
LettersX 
364 
141 
1,152 
Prev 
59 
51 
343 
Shift 
136 
68 
536 
Matches 
148 
72 
582 
HReads 
1072 
326 
2316 
Rev 
42 
63 
165 

Notes: Sizes are given in Mbytes (1 Mbyte ¼ 10 6 bytes). </table></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">S.Grabowski et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from We maintain a buffer (sliding window) of m previous reads, storing also the position of the signature in each read. For each read, we seek the read from the buffer which maximizes the overlap. The distance between a pair of considered reads depends on the number of elementary operations transforming one into another. For example, if the pair of reads is: AACGTXXXXCGGCAT, CCTXXXXCGGCATCC, where XXXX denotes a signature, we match them after a (conceptual) alignment: AACGTXXXXCGGCAT, CCTXXXXCGGCATCC, to find that they differ with one mismatch (G versus C) and 2 end symbols of the second read have to be inserted, hence the distance is c m Â 1 þ c i Â 2, where c m and c i are the mismatch and the insert cost, respectively. The default values for the parameters are: c m ¼ 2 and c i ¼ 1, and they were chosen experimentally. In our example, the final distance is thus 2 Â 1 þ 1 Â 2 ¼ 4. The read among the m previous ones that minimizes such a distance, and is not greater than max dist, set by default to a half of read length, is considered a reference for the current read. Next, the referential matching data are sent into a few streams. Flags Values from ff copy ; f diss ; f ex ; f mis ; f oth g, with the following meaning: • f copy —the current read is identical to the previous one, • f diss —the read is not similar to any read from the buffer; more precisely, the similarity distance exceeds a specified threshold max dist , • f ex —the read overlaps with some read from the buffer without mismatches (only its trailing symbols are to be encoded), • f mis —the read overlaps with some read from the buffer with exactly one mismatch at the last position of the referenced read, • f oth —the read overlaps with some read from the buffer, but not in a way corresponding to flags f ex or f mis.</note>

			<note place="foot">S.Grabowski et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors thank the reviewers for helpful comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Compression of FASTQ and SAM format sequencing data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">K</forename>
				<surname>Bonfield</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">V</forename>
				<surname>Mahoney</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">59190</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Compression of structured high-throughput sequencing data</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Campagne</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">79871</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Practical compression for multi-alignment genomic files</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Cá Novas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Moffat</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding ACSC&apos;13 Proceedings of the Thirty-Sixth Australasian Computer Science Conference. Darlinghurst, Australia</title>
		<editor>B. Thomas</editor>
		<meeting>eeding ACSC&apos;13 eedings of the Thirty-Sixth Australasian Computer Science Conference. Darlinghurst, Australia</meeting>
		<imprint>
			<publisher>Australian Computer Society, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Lossy compression of quality scores in genomic data</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Cá Novas</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2130" to="2136" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<monogr>
		<title level="m" type="main">On the representation of de Bruijn graphs. arXiv preprint arXiv</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Chikhi</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1401" to="5383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-scale compression of genomic sequence databases with the Burrows–Wheeler transform</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Cox</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1415" to="1419" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Compression of DNA sequence reads in FASTQ format</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Deorowicz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Grabowski</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="860" to="862" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Data compression for sequencing data</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Deorowicz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Grabowski</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<monogr>
		<title level="m" type="main">KMC 2: Fast and resource-frugal k-mer counting. arXiv preprint</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Deorowicz</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient storage of high throughput DNA sequencing data using reference-based compression</title>
		<author>
			<persName>
				<forename type="first">M.-Y</forename>
				<surname>Fritz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="734" to="740" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">SCALCE: boosting sequence compression algorithms using locally consistent encoding</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Hach</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3051" to="3057" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Deez: reference-based compression by local assembly</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Hach</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1082" to="1084" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Reducing whole-genome data storage footprint</title>
	</analytic>
	<monogr>
		<title level="j">Illumina</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive reference-free compression of sequence quality scores</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Janin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="24" to="30" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Compression of next-generation sequencing reads aided by highly efficient de novo assembly</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Jones</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">171</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">On the future of genomic data</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">D</forename>
				<surname>Kahn</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">331</biblScope>
			<biblScope unit="page" from="728" to="729" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Compressing genomic sequence fragments using SlimGene</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Kozanitis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="401" to="413" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Memory efficient minimum substring partitioning</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Li</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Very Large Data Bases. VLDB Endowment</title>
		<meeting>the 39th International Conference on Very Large Data Bases. VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="169" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">De novo co-assembly of bacterial genomes from multiple single cells</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<forename type="middle">S</forename>
				<surname>Movahedi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: BIBM. IEEE Computer Society. pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Reducing storage requirements for biological sequence comparison</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Roberts</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="3363" to="3369" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">DSRC 2—industry-oriented compression of FASTQ files</title>
		<author>
			<persName>
				<forename type="first">Ł</forename>
				<surname>Roguski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Deorowicz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2213" to="2215" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<monogr>
		<title level="m" type="main">Handbook of Data Compression</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Salomon</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Motta</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">SRComp: Short read sequence compression using burstsort and elias omega coding</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">J</forename>
				<surname>Selva</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Chen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>article. no. e81414</note>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">PPM: one step to practicality</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Shkarin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Compression Conference (DCC). Snowbird, UT</title>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Transformations for the compression of FASTQ quality scores of next-generation sequencing data</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Wan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="628" to="635" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Kraken: ultrafast metagenomic sequence classification using exact alignments</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">E</forename>
				<surname>Wood</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">L</forename>
				<surname>Salzberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">46</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Recoil—an algorithm for compression of extremely large datasets of DNA data</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Yanovsky</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Traversing the k-mer landscape of NGS read datasets for quality score sparsification</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Yu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Molecular Biology Lecture Notes in Computer Science</title>
		<editor>R. Sharan</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="volume">8394</biblScope>
			<biblScope unit="page" from="385" to="399" />
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">FQZip: lossless reference-based compression of next generation sequencing data in FASTQ format</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th Asia Pacific Symposium on Intelligent and Evolutionary Systems</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>