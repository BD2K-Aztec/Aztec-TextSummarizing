
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining An efficient method to estimate the optimum regularization parameter in RLDA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Daniyar</forename>
								<surname>Bakir</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Electronics Engineering</orgName>
								<orgName type="institution">Nazarbayev University</orgName>
								<address>
									<postCode>010000</postCode>
									<settlement>Astana</settlement>
									<country key="KZ">Kazakhstan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Alex</forename>
								<surname>Pappachen</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Electronics Engineering</orgName>
								<orgName type="institution">Nazarbayev University</orgName>
								<address>
									<postCode>010000</postCode>
									<settlement>Astana</settlement>
									<country key="KZ">Kazakhstan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">James</forename>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Electronics Engineering</orgName>
								<orgName type="institution">Nazarbayev University</orgName>
								<address>
									<postCode>010000</postCode>
									<settlement>Astana</settlement>
									<country key="KZ">Kazakhstan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Amin</forename>
								<surname>Zollanvari</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Electronics Engineering</orgName>
								<orgName type="institution">Nazarbayev University</orgName>
								<address>
									<postCode>010000</postCode>
									<settlement>Astana</settlement>
									<country key="KZ">Kazakhstan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining An efficient method to estimate the optimum regularization parameter in RLDA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btw506</idno>
					<note type="submission">Received on April 26, 2016; revised on July 4, 2016; accepted on July 21, 2016</note>
					<note>*To whom correspondence should be addressed Associate Editor: Jonathan Wren Supplementary information: Supplementary materials are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: The biomarker discovery process in high-throughput genomic profiles has presented the statistical learning community with a challenging problem, namely learning when the number of variables is comparable or exceeding the sample size. In these settings, many classical techniques including linear discriminant analysis (LDA) falter. Poor performance of LDA is attributed to the ill-conditioned nature of sample covariance matrix when the dimension and sample size are comparable. To alleviate this problem, regularized LDA (RLDA) has been classically proposed in which the sample covariance matrix is replaced by its ridge estimate. However, the performance of RLDA depends heavily on the regularization parameter used in the ridge estimate of sample covari-ance matrix. Results: We propose a range-search technique for efficient estimation of the optimum regulariza-tion parameter. Using an extensive set of simulations based on synthetic and gene expression microarray data, we demonstrate the robustness of the proposed technique to Gaussianity, an assumption used in developing the core estimator. We compare the performance of the technique in terms of accuracy and efficiency with classical techniques for estimating the regularization parameter. In terms of accuracy, the results indicate that the proposed method vastly improves on similar techniques that use classical plug-in estimator. In that respect, it is better or comparable to cross-validation-based search strategies while, depending on the sample size and dimensionality, being tens to hundreds of times faster to compute. Availability and Implementation: The source code is available at https://github.com/</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ridge estimation is a type of shrinkage and traces back to the pioneering work of<ref type="bibr">Hoerl and Kennard (Hoerl, 1962;</ref><ref type="bibr" target="#b14">Hoerl and Kennard, 1970a</ref>,b) on estimating regression parameters. They considered the standard linear model y ¼ Xb þ e ;</p><formula>(1)</formula><p>where y is the n-dimensional observation vector, X is a known n Â p matrix, b ¼ ½b 1 ; b 2 ;. .. ; b p  T is a p-dimensional parameter vector to be estimated, and e is the n-dimensional error vector with mean 0 and covariance matrix r 2 I p. If we assume X is a full (column) rank matrix (p &lt; n), the ordinary least-square solution to this familiar linear model is given by b b ¼ ðX T XÞ À1 X T y:</p><formula>(2)</formula><p>However, when p &gt; n, the solution (2) does not exist because X T X becomes degenerate. Even the solution obtained by generalized inverse form of matrix X T X is not working well.<ref type="bibr">Hoerl and Kennard (Hoerl, 1962;</ref><ref type="bibr" target="#b14">Hoerl and Kennard, 1970a</ref>,b) then formulated a problem in which the residual sum of squares is replaced by its ' 2-penalized form given by L 2 ðbÞ¢jjy À Xbjj 2 þ kjjbjj 2 ;</p><formula>(3)</formula><p>where k &gt; 0 denotes a penalty factor controlling the length of b. Minimizing L 2 ðbÞ results in the so-called ridge regression given by b b ¼ ðX T X þ kI p Þ À1 X T y:</p><formula>(4)</formula><p>In this way, the inverse of possibly ill-conditioned X T X is stabilized by adding the scalar matrix kI p. This idea was then used by Di Pillo (1976) to replace the estimate of the sample covariance matrix used in linear discriminant analysis (LDA) by its ridge estimate resulting in the so-called regularized LDA (RLDA). The goal is to improve the performance of LDA in situations where dimensionality of observations, p, is larger or comparable to the number of measurements, n. Di Pillo (1979) attempts to determine the optimum value of the optimum regularization parameter in RLDA. On this Di Pillo's study, Peck and Ness (1982) comment that 'He found the analytical solution to this problem intractable, and so used a simulation study to choose an optimum value for k [the regularization parameter]. He concluded that if an algorithm can be found which leads to a value of k near the optimum value, then considerable improvement in the PCC [probability of correct classification] should occur'.<ref type="bibr" target="#b10">Friedman (1989)</ref>suggested the use of cross-validation in finding the optimum value of regularization parameter. In this procedure, cross-validation is used to estimate the true error of RLDA for each value of the regularization parameter selected from a pre-specified set of size 25–50. The estimate of the optimum regularization parameter is then the one that results in the minimum cross-validation estimate of true error. Despite the computational complexity of cross-validation in such a search algorithm [e.g. see comments in<ref type="bibr" target="#b10">Friedman (1989</ref><ref type="bibr" target="#b21">), Sharma et al. (2014</ref>and Tasjudin and<ref type="bibr" target="#b22">Landgrebe (1998)]</ref>, this approach has remained the most popular method in estimating the optimum value of regularization parameter in RLDA—for instance, see<ref type="bibr" target="#b12">Guo et al. (2007</ref><ref type="bibr" target="#b2">), Bandos et al. (2009</ref><ref type="bibr" target="#b25">), Ye et al. (2006</ref><ref type="bibr" target="#b16">), Huang et al. (2009</ref>and Ye and Xiong (2006) to cite just a few articles. Recently, we constructed a generalized consistent estimator of true error of RLDA. In this regard, we proposed an estimator that converges to true error in a double asymptotic sense. In this setting, the estimator converges to the actual parameter in an asymptotic scenario in which dimension and sample size increase in a proportional manner (n ! 1; p ! 1 and p=n ! J &gt; 0) (<ref type="bibr" target="#b31">Zollanvari and Dougherty, 2015</ref>). In developing this estimator, we assumed that the true distributions governing the data follow multivariate Gaussian model. However, the underlying mechanism to develop the estimator was based on double asymptotics and random matrix theory, both of which suggest applicability of the estimator in non-Gaussian settings as well [see p. xii in<ref type="bibr" target="#b11">Girko (1995)</ref>, p. 335 in Bai and Silverstein (2010) and<ref type="bibr" target="#b30">Zollanvari (2015)]</ref>. In this work, we employ this estimator of true error in a one-dimensional search to estimate the optimum regularization parameter of RLDA. As such, we employ data taken from seven gene expression microarray studies as well as synthetically generated Gaussian and non-Gaussian data. We compare the performance (in terms of accuracy and efficiency) of the search technique that uses this estimator with similar search schemes that use cross-validation or plug-in estimators. Using an extensive set of simulations, we observe that the proposed technique is an efficient method that can outperform cross-validation and plugin estimate-based schemes in estimating the optimum regularization parameter of RLDA. Throughout this work, we use boldface lower case letters to denote a column vector. A boldface upper case letter denotes a matrix and tr½: is the trace operator. The identity matrix of p dimension is denoted by I p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Systems and methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">RLDA classifier</head><p>Assume a separate sampling scheme is employed: n ¼ n 0 þ n 1 sample points are collected to constitute the sample S in R p , where n, n 0 and n 1 are non-random and pre-determined and where S 0 ¼ fx 1 ; x 2 ; .. . ; x n0 g and S 1 ¼ fx n0þ1 ; x n0þ2 ;. .. ; x n g are randomly selected from populations P 0 and P 1 , respectively. In this two-class problem, a classifier is a function w n : R p ! f0; 1g. If w n is given by w n ðxÞ ¼ 0 if x 2 R 0 and w n ðxÞ ¼ 1 if x 2 R 1 , where R 0 and R 1 are measurable sets partitioning the sample space, then the true error of w n , denoted by e, is defined to be the probability of misclassification,</p><formula>e ¼ a 0 Ð R1 f ðxj0Þdx þ a 1 Ð R0 f ðxj1Þdx ¼ a 0 e 0 þ a 1 e 1 ; (5)</formula><p>where a i is the prior probability for class i, e i is the error contributed by class i, and f ðxj0Þ and f ðxj1Þ are the class-conditional densities governing P 0 and P 1 , respectively. Separate sampling is very common in biomedical applications, where data from two classes are collected without reference to the other class, for instance, when discriminating two types of tumors or when distinguishing a normal from a pathological phenotype. With separate sampling, the prior probabilities a i cannot be estimated from the sample, an issue with a long history in the study of LDA (<ref type="bibr" target="#b0">Anderson, 1951</ref>). Both classification rules (<ref type="bibr" target="#b9">Esfahani and Dougherty, 2014</ref>) and error estimation rules (<ref type="bibr" target="#b3">Braga-Neto et al., 2014</ref>) need to be adjusted for separate sampling rather than use their usual random-sampling definitions; otherwise, they suffer performance degradation. The adjustment requires that a 0 and a 1 be known, as assumption made in this study. In our case, the adjustment is straightforward because it simply means that we directly use a 0 and a 1 rather than their random-sampling estimates n0 n and n1 n : In practice, the salient point is that given n, n 0 and n 1 are chosen so that n0 n is as close to a 0 as possible (<ref type="bibr" target="#b9">Esfahani and Dougherty, 2014</ref>). Assuming P i follows a multivariate Gaussian distribution Nðl i ; RÞ, for i ¼ 0, 1, where R is the common non-singular covariance matrix of both class, replacing the unknown mean and the covariance matrix of classes in Bayes rule (optimum classifier) results in LDA, which is characterized by Anderson's statistics,</p><formula>W LDA x 0 ; x 1 ; C; x ð Þ ¼ x À x 0 þ x 1 2 T C À1 x 0 À x 1 ð Þ ; (6) where x 0 ¼ 1 n0 P x l 2S0 x l and x 1 ¼ 1 n1 P x l 2S1 x l</formula><p>are the sample means for classes 0 and 1, respectively, and C is the pooled sample covariance matrix,</p><formula>C ¼ n 0 À 1 ð Þ C 0 þ n 1 À 1 ð Þ C 1 n 0 þ n 1 À 2 ; (7)</formula><p>where</p><formula>C i ¼ 1 n i À 1 X x l 2Si x l À x i ð Þx l À x i ð Þ T : (8)</formula><p>In this work, we consider a form of RLDA classifier that is obtained by using ridge estimators of the inverse covariance matrix in W LDA ; that is, by using ðI þ cCÞ À1 and c &gt; 0; in (6), which yields</p><formula>Wð x 0 ; x 1 ; C; xÞ ¼ x À x 0 þ x 1 2 T H x 0 À x 1 ð Þ ; (9)</formula><p>where H ¼ ðI p þ cCÞ À1 :</p><formula>(10)</formula><p>The designed RLDA classifier is then given bywhere c ¼ log 1Àa0 a0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">RLDA true error, optimum regularization and their estimates</head><p>The true error of w RLDA n is given by (5). Given sample S n , for i ¼ 0, 1,</p><formula>e i ¼ PððÀ1Þ i Wð x 0 ; x 1 ; C; xÞ ðÀ1Þ i cjx 2 P i ; x 0 ; x 1 ; CÞ: (12)</formula><p>Under the multivariate Gaussian model, we have</p><formula>e i ¼ U ðÀ1Þ iþ1 Gðl i ; x 0 ; x 1 ; HÞ þ ðÀ1Þ i c ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi Dð x 0 ; x 1 ; H; RÞ p ! ; (13)</formula><p>where Uð:Þ denotes the cumulative distribution function of a standard normal random variable and</p><formula>Gðl i ; x 0 ; x 1 ; HÞ ¼ l i À x 0 þ x 1 2 T H x 0 À x 1 ð Þ ;</formula><p>Dð x 0 ;</p><formula>x 1 ; H; RÞ ¼ ð x 0 À x 1 Þ T HRHð x 0 À x 1 Þ:</formula><formula>(14)</formula><p>Given training data, the optimal choice of c is the value of c, which minimizes the overall true error e as defined by</p><p>(5) and (13); to wit, c opt ¼ argmin c e. However, true error depends on unknown population parameters l i and R, which must be estimated from training data. As such, the optimum regularization parameter depends on unknown distributional parameters and must be estimated from data as well. Even with the assumption of knowing the true distributional parameters, c opt is the solution of a non-linear equation that needs to be solved numerically. To see the latter statement and for simplicity of presentation, let a i ¼ 1 and a 1Ài ¼ 0, i ¼ 0, 1, which means c opt ¼ argmin c e ¼ argmin c e i. By taking the derivative of e i defined in (13) with respect to c, setting the derivative to zero, and after some tedious but straightforward algebraic manipulations we observe that c opt is the unique positive solution of the following equation:</p><formula>Gðl i ; x 0 ; x 1 ; HCHÞ Dð x 0 ; x 1 ; H; HCRÞ ¼ Gðl i ; x 0 ; x 1 ; HÞ Dð x 0 ; x 1 ; H; RÞ ;</formula><formula>(15)</formula><p>where dependency of equation on c is via H defined in (10). The non-linearity of the equation makes a closed form expression of c opt hopeless. As such, a range search strategy is a feasible path forward. The objective in the range search is to determine the c that minimizes the estimate of true error of RLDA. In this regard, a classical estimate of true error is obtained by replacing the unknown parameters by their sample estimate, resulting in standard plug-in estimator of true error, which is given by (<ref type="bibr" target="#b17">McLachlan, 2004</ref>It is straightforward to see that for fixed p, as n i ! 1, we have x i ! l i and C ! R, and therefore, b e P iwhere</p><formula>b d ¼ p n0þn1À2 À tr½H n0þn1À2 c 1 À p n0þn1À2 þ tr½H n0þn1À2 : (18)</formula><p>Using random matrix theory and under double asymptotic conditions, the estimator (17) converges (almost surely) to true error. The double asymptotic conditions are mainly characterized by n 0 ! 1; n 1 ! 1; p ! 1, with the assumption that the following limits exist:</p><formula>p n0 ! J 0 &gt; 0; p n1 ! J 1 &gt; 0, and p n0þn1 ! J &lt; 1.</formula><p>Nevertheless, the readers are referred to<ref type="bibr" target="#b31">Zollanvari and Dougherty (2015)</ref>for the complete list of conditions used in developing (17). We use the following protocol to estimate c opt using a set of benchmark gene expression datasets and, at the same time, compare the performance of the proposed search strategy based on various estimators of error. The estimators that we use are 5-fold crossvalidation with five repetitions (CV5F-5R), leave-one-out (loo), plug-in (b e P ) available from</p><p>(16) and our proposed doubleasymptotic estimator b e D i available from</p><p>(17). The experiments on real data and synthetic data are essentially similar except that in real-data experiments we employ t-test feature selection to reduce the dimensionality to P ¼ 50 and P ¼ 150.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Protocol (Real Data):</head><p>@BULLET Step I: Let r denote the ratio of the total number of sample points in class 0 to the total amount in class 1 in the full dataset. Let n Full denote the sample size in the full dataset. Fix a value n &lt; n Full and let it be the number of training sample points that are randomly taken out of the whole dataset such that n ¼ n 0 þn 1 with n i being the number of training sample points in class i. We choose n 0 ¼ brn 1 c, where b:c is the floor function. This practice resembles a random sampling scheme in which a 0 % n0 n and a 1 % n1 n. Therefore, we use these values of a i to find the overall error rate from</p><p>(5) and the held-out samples. In order to set aside enough sample points for testing (i.e. the n Full À n held-out sample), we restrict the training sample size to n 2 ½30; 100 (for synthetic data, we consider n 2 ½30; 300). @BULLET Step II: For a prescribed value of regularization parameter c in a prescribed range, design the RLDA classifier by (9). We discretize the range with the exponential function 1000 1 10 i for i ¼ fÀ10; À9; À8;. .. ; 10g that covers values from 0.001 to 1000. The above exponential function has been chosen to improve the efficiency of the search. This choice seems to be a reasonable one because a small perturbation in large values of c is a smaller relative change with respect to a similar perturbation in small Optimum regularization parameter: gene expression datavalues of c. This implies that the effect of the former perturbation in changing the true error of the classifier may not be as large as the latter perturbation (although in terms of magnitude both perturbations are the same). In other words, for large values of c having a fine discretization is not as critical as small values. @BULLET Step III: For each value of c in the prescribed set of points, estimate the error of the designed classifier using as estimator of error (CV5F-5R, loo, b e P and b e D ). Obtain the holdout estimate of the true error (taken as the true error) from the test data. @BULLET Step IV: The estimate of the optimum c is the c which results in the smallest error estimate on the prescribed range of c. For the estimated optimum c record the value of true error (available from Step III). @BULLET Step V: Repeat Steps I–IV, 500 times for each n and determine the average expected error of RLDA.Optimum regularization parameter: gene expression data</p><formula>20: (o), (p), (q)</formula><p>and (r) correspond to Gaussian data and Bayes error ¼ 0.332, 0.239, 0.131, 0.066, respectively, whereas (s) and (t) correspond to skewed normal distribution with a 'distance' 2 and skewness factor a ¼ 2; 4, respectively (see Supplementary Section S4 for more information on simulations and parameters regarding skew-normal distribution)</p><p>assumed the Gaussianity of the data, the underlying mechanism to develop the estimator is based on random matrix theory. The universality principle of random matrix theory though suggests applicability of developed estimators in non-Gaussian settings as well<ref type="bibr">[</ref>. In this work, we conducted an extensive set of simulations using both synthetic and gene expression microarray data to compare the performance of our technique in terms of expected error of the constructed RLDA and the compute time to similar search schemes that use classical error estimators (5-fold crossvalidation with five repetitions, leave-one-out and plug-in estimator). We observe that the proposed technique is tens to hundreds of times faster than cross-validation to compute, while at the same time results in a comparable or better classification accuracy of the constructed RLDA. The good accuracy of the proposed technique on non-Gaussian real data and synthetic data used in this study confirms robustness of the estimator to non-Gaussianity of data. The next natural step in this line of work is to estimate the RLDA regularization parameter that minimizes the area under the ROC curve.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.1.</head><figDesc>Fig. 1. Expected (mean of) estimated and true error (vertical axis) as a function of regularization parameter in logarithmic scale (horizontal axis) for real datasets and feature size p ¼ 50. Columns from left to right: the double asymptotic estimator b e D (identified by dasym-est), CV5F-5R, leave-one-out (identified by loo), the plug-in estimator b e P and the true error. Rows from top to bottom: Chen et al. (2004), Desmedt et al. (2007), Natsoulis et al. (2005), Rosenwald et al. (2002), Valk et al. (2004), van de Vijver et al. (2002) and Yeoh et al. (2002) studies. The x-axis denotes the regularization parameter ranging from 10 À3 to 10 3. Note that the range of vertical axis for plug-in estimator differs from other estimators due to substantial difference between magnitudes of plug-in estimates from others</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.2.</head><figDesc>Fig. 2. The performance (mean of true error) of RLDA classifiers with regularization parameter determined using different estimators of true error versus sample size for different dimensionality on real and synthetic data. The estimators used are the double asymptotic estimator b e D (identified by dasym-est), CV5F-5R, leave-one-out (identified by loo) and the plug-in estimator b e P. Plots (a)–(n) show the results of experiments on real data; (a)–(g) and (h)–(n) show results for p ¼ 50 and p ¼ 150, respectively. (a) and (h): Chen et al. (2004); (b) and (i): Desmedt et al. (2007); (c) and (j): Natsoulis et al. (2005); (d) and (k): Rosenwald et al. (2002); (e) and (l): Valk et al. (2004); (f) and (m): van de Vijver et al. (2002); and (g) and (n): Yeoh et al. (2002) studies. Plots (o)–(r) show the results for synthetic data and p ¼ 20: (o), (p), (q) and (r) correspond to Gaussian data and Bayes error ¼ 0.332, 0.239, 0.131, 0.066, respectively, whereas (s) and (t) correspond to skewed normal distribution with a 'distance' 2 and skewness factor a ¼ 2; 4, respectively (see Supplementary Section S4 for more information on simulations and parameters regarding skew-normal distribution)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><figDesc>see p. xii in Girko (1995), p. 335 in Bai and Silverstein (2010) and Zollanvari (2015)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><figDesc>Funding: This work was partially supported by the Nazarbayev University Social Policy Grant (to A.Z.). Conflict of Interest: none declared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 1. Microarray studies used in this work</figDesc><table>Dataset 
Features 
n 0 =n 1 

Chen et al. (2004) 
10 237 
75/82 
Desmedt et al. (2007) 
22 215 
98/77 
Natsoulis et al. (2005) 
8491 
120/61 
Rosenwald et al. (2002) 
5013 
114/89 
Valk et al. (2004) 
22 215 
116/157 
van de Vijver et al. (2002) 
10 237 
180/115 
Yeoh et al. (2002) 
5077 
149/99 

</table></figure>

			<note place="foot">D.Bakir et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="3"> Results and discussion Based on the protocols described in Section 2, we have performed a set of experiments employing both synthetic models and gene expression microarray data to examine the performance of the search scheme based on various estimators. First, we consider seven publicly available datasets on breast cancer (van de Vijver et al., 2002), pediatric acute lymphoblastic leukemia (Yeoh et al., 2002), hepatocellular carcinoma (Chen et al., 2004), toxicants response on rats (Natsoulis et al., 2005), diffuse large B-cell lymphoma (Rosenwald et al., 2002), node-negative breast cancer (Desmedt et al., 2007) and acute myeloid leukemia (Valk et al., 2004). Table 1 provides a summary of these datasets, including the total number of genes and sample size. For a description of the data preparation, the readers are referred to Supplementary Section 1. shows the expected true and estimate of error for RLDA classifier as a function of regularization parameter c for different number of sample points ranging from 30 to 100 chosen from datasets listed in Table 1 with p ¼ 50 and p ¼ 150, respectively. This leaves us with 8 (sample sizes) Â 7 (datasets) Â 2 (dimensionalities)¼112 experiments on real data. As seen in the far right column of these figures, for each sample size, the true error of classifier decreases as a function of c and then increases for increasing c with the optimal c corresponding to the minimum true error at the bottom of the valley. In this regard, in all experiments such a &apos;peaking phenomenon&apos; occurs in the pre-specified range of c 2 ½0:001; 1000 with 75% of times (84 out of 112) happening in the range ½0:1; 100. Notice that this peaking phenomenon is also observed in curves of estimated errors (columns 1–3 in Fig. 1 and Supplementary Fig. S1) except for the plug-in estimator, suggesting that plug-in is not a good estimator of the optimum c. Figure 2(a–n) shows the expected true error of RLDA classifier designed using the estimate of the optimum c (the c that results in the minimum estimated error in Fig. 1 and Supplementary Fig. S1) obtained from various estimators as a function of sample size on each dataset. We observe that an RLDA classifier designed by double asymptotic estimator b e D has a better or comparable performance to RLDA classifiers constructed using plug-in, CV5F-5R and loo estimators. At the same time, we have to note that to compute b e D , we only need to evaluate the closed-form expression presented in (17). Consequently, b e D is tens to hundreds of times faster to compute than cross-validation estimators. To illustrate this point, we have plotted the ratio of average time it takes to compute CV5F-5R and leave-one-out estimators to the time it takes to compute b e P and b e D estimators in experiments related to Chen et al. (2004) (see Fig. 3). The actual average compute time is presented in the Supplementary Section S6. Note that the pre-specified range of c is important to obtain a realistic view of the performance of estimators. For example, if we limit the search range of c to ½0:1; 100, then in the Natsoulis&apos; experiment, the classical plug-in estimator b e P , which is not expected to have a good performance in small-sample situations, outperforms all other estimators (see Supplementary Fig. S2). This behavior is because in this dataset for all examined sample sizes the optimum regularization parameter is larger than or close to the upper limit of the range of c 2 ½0:1; 100. This can be seen from the figure on the third row, fifth column in Figure 1. At the same time in all datasets, b e P points to the upper bound of the range as the estimate of the optimum regularization parameter, which in the Natsoulis&apos; experiment happens to be closer to the actual optimum regularization parameter (see the plot in the third row, fourth column of Fig. 1). We also used synthetic data to compare the performance of estimators in estimating optimum c. Figure 2(o–t) shows the results for a wide range of Bayes (optimum) error and p ¼ 20 for data taken from Gaussian and skew-normal distributions. For the complete set of results along with the protocol used for synthetic experiments, see Supplementary Sections S4 and S5. In most of experiments on synthetic data, b e D uniformly outperforms other estimators of c. The efficiency of the proposed procedure is a direct consequence of having a closed form for the core estimator that we use in the search. The good performance is due to convergence of the core estimator to true error in a double asymptotic regime. Classically, the notion of statistical consistency guarantees the performance of an estimator in situations where the number of measurements unboundedly increases for a fixed dimensionality (n ! 1, p fixed). In a finite sample operating regime, this implies that in order to expect an acceptable performance from an estimator, we need to have many more sample points than variables. However, in a double asymptotic regime the magnitude of p and n are kept comparable (p=n ! J &gt; 0 with J being an arbitrary number) and, as a result, we generally expect an acceptable performance of developed estimators in a wide range of dimension and sample size. We note that both crossvalidation and plug-in estimators are statistically consistent in a classical sense whereas the core estimator that we use in the search is a consistent estimator in a double asymptotic sense. 4 Concluding remarks A recently proposed estimator of true error of RLDA based on double asymptotics is used in a one-dimensional search to optimize the performance of the classifier in terms of regularization parameter. While in developing the core estimator used in the search we have</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Classification by multivariate analysis</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Anderson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="31" to="50" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<monogr>
		<title level="m" type="main">Spectral Analysis of Large Dimensional Random Matrices</title>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<forename type="middle">D</forename>
				<surname>Bai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">W</forename>
				<surname>Silverstein</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Classification of hyperspectral images with regularized linear discriminant analysis</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">V</forename>
				<surname>Bandos</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="862" to="873" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Cross-validation under separate sampling: strong bias and how to correct it</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3349" to="3355" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Novel endothelial cell markers in hepatocellular carcinoma</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Chen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mod. Pathol</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1198" to="1210" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Strong time dependence of the 76-gene prognostic signature for node-negative breast cancer patients in the transbig multicenter independent validation series</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Desmedt</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Cancer Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="3207" to="3214" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">The application of bias to discriminant analysis</title>
		<author>
			<persName>
				<forename type="first">Di</forename>
				<surname>Pillo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">J</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun</title>
		<imprint>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">–</forename>
				<surname>Stat</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="middle">M</forename>
				<surname>Theor</surname>
			</persName>
		</author>
		<imprint>
			<biblScope unit="page" from="843" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Biased discriminant analysis: Evaluation of the optimum probability of misclassification</title>
		<author>
			<persName>
				<forename type="first">Di</forename>
				<surname>Pillo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">J</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Stat – Theor. M</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1447" to="1457" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Effect of separate sampling on classification accuracy</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">S</forename>
				<surname>Esfahani</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="242" to="250" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Regularized discriminant analysis</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Friedman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="165" to="175" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<monogr>
		<title level="m" type="main">Statistical Analysis of Observations of Increasing Dimension</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<forename type="middle">L</forename>
				<surname>Girko</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Regularized discriminant analysis and its application in microarrays</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Guo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostat</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="86" to="100" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Application of ridge analysis to regression problems</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">E</forename>
				<surname>Hoerl</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chem. Eng. Prog</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="54" to="59" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Ridge regression: Applications to nonorthogonal problems</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">E</forename>
				<surname>Hoerl</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">W</forename>
				<surname>Kennard</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="69" to="82" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Ridge regression: Biased estimation for nonorthogonal problems</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">E</forename>
				<surname>Hoerl</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">W</forename>
				<surname>Kennard</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="55" to="59" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Comparison of linear discriminant analysis methods for the classification of cancer based on gene expression data</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Clin. Cancer Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<monogr>
		<title level="m" type="main">Discriminant Analysis and Statistical Pattern Recognition</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Mclachlan</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Classification of a large microarray data set: algorithm comparison and analysis of drug signatures</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Natsoulis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="724" to="736" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">The use of shrinkage estimators in linear discriminant analysis</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Peck</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">V</forename>
				<surname>Ness</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="409" to="424" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">The use of molecular profiling to predict survival after chemotherapy for diffuse large-b-cell lymphoma</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Rosenwald</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N. Eng. J. Med</title>
		<imprint>
			<biblScope unit="volume">346</biblScope>
			<biblScope unit="page" from="1937" to="1947" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">A feature selection method using improved regularization discriminant analysis</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Sharma</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vision Appl</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="775" to="786" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Covariance estimation for limited training samples</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Tasjudin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">A</forename>
				<surname>Landgrebe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Geoscience and Remote Sensing Symposium (IGARSS)</title>
		<meeting>the IEEE Geoscience and Remote Sensing Symposium (IGARSS)<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="2688" to="2690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Prognostically useful gene-expression profiles in acute myeloid leukemia</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">J</forename>
				<surname>Valk</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N. Eng. J. Med</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="page" from="1617" to="1628" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">A gene-expression signature as a predictor of survival in breast cancer</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Van De Vijver</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">347</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Computational and theoretical analysis of null space and orthogonal linear discriminant analysis</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Ye</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Xiong</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1183" to="1204" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">Cv5f</forename>
				<surname>−5r / Dasym −est Loo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">/</forename>
				<surname>−est</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Cv5f</forename>
				<surname>−5r</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">Sample</forename>
				<surname>Size</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">The ratio of average compute time of CV5F-5R and leave-one-out (loo) estimators to average compute time of b e P (plug-in) and b e D (dasym-est) estimators versus sample size: (a) p ¼ 50; (b) p ¼ 150. See Supplementary Section S6 for the actual compute time in terms of seconds on a personal computer Optimum regularization parameter: gene expression data Ye Efficient model selection for regularized linear discriminant analysis</title>
		<author>
			<persName>
				<surname>Sample Size</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Fig</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 15th ACM International Conference on Information and Knowledge Management<address><addrLine>Arlington, Virginia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Classification, subtype discovery, and prediction of outcome in pediatric acute lymphoblastic leukemia by gene expression profiling</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">J</forename>
				<surname>Yeoh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Cell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="133" to="143" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">High-dimensional statistical learning: Roots, justifications, and potential machineries</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Zollanvari</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Inform</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="109" to="121" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Generalized consistent error estimator of linear discriminant analysis</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Zollanvari</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Sig. Proc</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="2804" to="2814" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>