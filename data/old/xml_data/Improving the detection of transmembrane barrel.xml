
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structural bioinformatics Improving the detection of transmembrane β-barrel chains with N-to-1 extreme learning machines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011">2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Castrense</forename>
								<surname>Savojardo</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biology</orgName>
								<orgName type="laboratory">Biocomputing Group</orgName>
								<orgName type="institution">University of Bologna CIRI-Health Science and Technology</orgName>
								<address>
									<postCode>40126</postCode>
									<settlement>Bologna</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Piero</forename>
								<surname>Fariselli</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biology</orgName>
								<orgName type="laboratory">Biocomputing Group</orgName>
								<orgName type="institution">University of Bologna CIRI-Health Science and Technology</orgName>
								<address>
									<postCode>40126</postCode>
									<settlement>Bologna</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Rita</forename>
								<surname>Casadio</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biology</orgName>
								<orgName type="laboratory">Biocomputing Group</orgName>
								<orgName type="institution">University of Bologna CIRI-Health Science and Technology</orgName>
								<address>
									<postCode>40126</postCode>
									<settlement>Bologna</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Alfonso</forename>
								<surname>Valencia</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<postCode>40127</postCode>
									<settlement>Bologna</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structural bioinformatics Improving the detection of transmembrane β-barrel chains with N-to-1 extreme learning machines</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">27</biblScope>
							<biblScope unit="issue">22</biblScope>
							<biblScope unit="page" from="3123" to="3128"/>
							<date type="published" when="2011">2011</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btr549</idno>
					<note type="submission">Received on June 3, 2011; revised on September 8, 2011; accepted on September 28, 2011</note>
					<note>[14:33 19/10/2011 Bioinformatics-btr549.tex] Page: 3123 3123–3128 Associate Editor: Contact: piero.fariselli@unibo.it</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Transmembrane β-barrels (TMBBs) are extremely important proteins that play key roles in several cell functions. They cross the lipid bilayer with β-barrel structures. TMBBs are presently found in the outer membranes of Gram-negative bacteria and of mitochondria and chloroplasts. Loop exposure outside the bacterial cell membranes makes TMBBs important targets for vaccine or drug therapies. In genomes, they are not highly represented and are difficult to identify with experimental approaches. Several computational methods have been developed to discriminate TMBBs from other types of proteins. However, the best performing approaches have a high fraction of false positive predictions. Results: In this article, we introduce a new machine learning approach for TMBB detection based on N-to-1 Extreme Learning Machines that significantly outperforms previous methods achieving a Matthews correlation coefficient of 0.82, a probability of correct prediction of 0.92 and a sensitivity of 0.73. Availability: The method and the cross-validation sets are available at the web page</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Transmembrane β-barrel (TMBB) proteins cross the lipid bilayer with a series of β-strands arranged in a cylindrical geometry and forming a structure that resembles a barrel (<ref type="bibr" target="#b30">Schulz, 2000</ref>). TMBBs can be membrane anchors or membrane-bound enzymes endowed with functions relevant to the entire cell metabolism and including active ion transport, passive nutrient intake and defense against attack proteins (<ref type="bibr" target="#b30">Schulz, 2000</ref>). While all living organisms have transmembrane proteins organized into all α-helical bundles, TMBBs are presently found in the outer membranes of Gram-negative bacteria, mitochondria and chloroplasts. TMBBs are estimated to be encoded by as many as 2–3% of the genes in Gramnegative bacteria (<ref type="bibr" target="#b10">Casadio et al., 2003;</ref><ref type="bibr" target="#b12">Freeman and Wimley, 2010;</ref><ref type="bibr" target="#b29">Wimley, 2003</ref>). However, very few TMBB structures are available at atomic resolution from Gram-negative organisms<ref type="bibr">[</ref>Several computational methods have been developed to predict TMBBs from protein sequences. Two prediction problems can be addressed: (i) the prediction of protein topology (both strand localization along the protein sequence and orientation of the loops with respect to the membrane plane); and (ii) TMBB detection in genomes. When the interest focuses on the prediction of the protein topology, it has been shown (<ref type="bibr" target="#b7">Bagos et al., 2005</ref>) that the best performing methods are based on hidden Markov models (<ref type="bibr" target="#b6">Bagos et al., 2004;</ref><ref type="bibr" target="#b9">Bigelow et al., 2004;</ref><ref type="bibr" target="#b24">Martelli et al., 2002</ref>) or Grammatical Restrained Hidden Conditional Random Fields (<ref type="bibr" target="#b11">Fariselli et al., 2009</ref>). The detection of TMMBs in a set of proteins, as large as the whole genome, is more difficult due to the cryptic nature of the TMBB structure as compared with that of the all α-membrane proteins (<ref type="bibr" target="#b28">Wimley, 2002</ref>). Computational methods for TMBB detection can identify candidate genes in order to perform experimental validations. To address this task, a wide variety of algorithms has been developed including: approaches based on sequence homology detection (<ref type="bibr" target="#b27">Remmert et al., 2009</ref>), machine learning (<ref type="bibr" target="#b9">Bigelow et al., 2004;</ref><ref type="bibr" target="#b10">Casadio et al., 2003;</ref><ref type="bibr" target="#b13">Gromiha and Suwa, 2006</ref>) and physicochemical properties of TMBBs (<ref type="bibr" target="#b12">Freeman and Wimley, 2010;</ref><ref type="bibr" target="#b28">Wimley, 2002</ref>). In this article, we tackle the problem of TMBB detection and significantly improve over existing algorithms taking advantage of previous works. In particular, we exploit the effort made by<ref type="bibr" target="#b12">Freeman and Wimley (2010)</ref>in generating a reliable dataset and defining a thorough comparison with previous methods. Furthermore, we refine the approach of Pollastri and co-workers (<ref type="bibr" target="#b25">Mooney et al., 2011</ref>) by incorporating their N-to-1 method in an Extreme Learning Machine (ELM) framework (<ref type="bibr" target="#b15">Huang et al., 2006a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">N-TO-1 ELMS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">N-to-1 neural networks</head><p>Recently, a new formulation of single hidden layer feedforward neural networks (SLFNs) aimed at encoding an entire protein sequence into a single object has been described by Pollastri and co-workers (<ref type="bibr" target="#b25">Mooney et al., 2011</ref>). The basic idea is to encode in a single hidden layer of a neural network the piecewise information defined by all the segments generated by a sliding window over the protein sequence. In a more formal way, given a protein sequence of length N and a non-linear sigmoid activation function σ, it is possible to map the entire input sequence of length N into the single hidden layer vector H by 'enrolling' an input sliding window</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.Savojardo et al.</head><p>protein residue) as:</p><formula>H h = k N j=1 σ(&lt; W h , X j &gt;)</formula><formula>(1)</formula><p>where H h is the h-th element of the hidden neuron vector H, W h is the weight vector that connects the j-th input window X j with the h-th hidden neuron and the brackets '&lt;&gt;' define the dot product of the two vectors (here we implicitly include in the weight matrix W also the bias thresholds of the hidden neurons). For sake of simplicity we set k equal to 1/N. With this choice H represents the average hidden layer of the entire sequence of length N and it becomes independent of the sequence length. For each protein of length N, the hidden layer encodes the information of N input vectors ( X j ), obtained by sliding a symmetric window of an odd length (L = 2n+1) along the protein sequence, one residue at a time. The encoding vector X j consists of 20 * L components, where 20 is the number of residue types. X j is computed starting from the position-specific score matrix (PSSM) as internally computed by PSI-BLAST (<ref type="bibr" target="#b5">Altschul et al., 1997</ref>). When the n most terminal residues (N-and C-termini) are encoded, the empty positions are set with zero values. For each protein we generate a PSSM by aligning its sequence against the Uniref90 dataset (www.uniprot.org/help/uniref). The final network output (used to assign the prediction) for a given protein is obtained by considering H as a normal hidden layer:</p><formula>O k = σ(&lt; β k , H &gt;)</formula><formula>(2)</formula><p>where β is the weight matrix that connects the hidden layer H with the neural network output vector O. Equations (1)</p><p>and</p><p>(2) allow to encode N different input windows into a single hidden layer, making it possible to treat a protein sequence as a single object (and independently of its length). However, differently from Pollastri and co-workers (<ref type="bibr" target="#b25">Mooney et al., 2011</ref>), we take advantage of an ELM approach to set the network junctions (<ref type="bibr" target="#b15">Huang et al., 2006a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">ELMs</head><p>The main idea of our approach is to couple the N-to-1 encoding with an ELM approach (<ref type="bibr" target="#b15">Huang et al., 2006a</ref>). One interesting theoretical point about neural networks with ELM is the fact that differently from many other learning methods their universal approximation capability has been proved (<ref type="bibr">Chen, 2007, 2008;</ref><ref type="bibr" target="#b16">Huang et al., 2006b</ref>). In the ELM context, the first layer of weights (W ) is set with random values. Only the second layer (β) is linearly trained. The first layer generates a non-linear 'random' encoding of the entire protein by mapping in a feature space for each sliding window. Once the N-to-1 random-enrolling is generated, the basic ELM machinery is used to train the β learnable parameters. Differently from what commonly thought, the input weights of SLFNs do not need to be learned (<ref type="bibr" target="#b14">Huang, 2003</ref>). Since in ELM the hidden layer does not need to be tuned and the hidden layer parameters can be fixed, the output weights can then be resolved using a least-square method. In practice, taking advantage of the fact that only the output layer (β) is trainable, the non-linearity of Equation (2) can be removed and β values can be obtained by the fitting the targets as:</p><formula>T = β ·H (3)</formula><p>Page: 3125 3123–3128</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improving the detection of TMBB chains</head><p>Since the set contains a significant amount of sequence similarity, we clustered chains by local similarity (computed with BLAST) and we created 10 different subsets that internally confine sequence homology. By this sequence identity between pairs of proteins belonging to two different subsets is &lt;25% (this is both for TMBB and non-TMBB proteins).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Measuring the performance</head><p>Here TP, TN, FP and FN are, respectively, the true positives, the true negatives, the false positives and the false negatives with respect to the TMBB class. Performances are evaluated adopting the following scoring indexes: @BULLET Accuracy (AC) evaluates the number of correctly predicted TMBB proteins divided by the total number of proteins:@BULLET F1is defined as the harmonic mean of PPV and SN:</p><formula>F1 = 2 × PPV × SN PPV+SN (10)</formula><p>@BULLET The Matthews correlation coefficient (MCC) is:</p><formula>MCC = (TP × TN − FP × FN) (TP+FP)×(TP+FN)×(TN+FP)×(TN+FN) (11)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model selection procedure</head><p>ELM models depend on some hyper-parameters (i.e. random initial weights, number of hidden neurons, the input window) that can influence the method performance and need to be optimized. For this and in order to avoid over-fitting, we carried out a 10-fold cross-validation procedure on the training set, based on a distinction among validation and test sets. In particular, given the 10 non-homologous subsets generated as described above from NRPDB, we trained on the 10 different learning sets as follows. Eight subsets at a time are adopted for training and the remaining two are used for validation and testing. Furthermore, in order to cope with the stochastic nature of ELMs, for eachwindow/hidden-neuron pair, we generated five random sets of weights (W ). Scoring values for model selection are obtained by averaging on the 10 different 'validation' subsets. More formally, if T i is the i-th test set (i = 0 ... 9), the corresponding validation set V i and training set L i are defined as V i = T (i+1) mod 10 and L i ={T (i+2) mod 10 ,T (i+3) mod 10 , ...T (i+9) mod 10 }, where (k)mod j is the modulo operation that computes the remainder of division of k by j. For the ensemble selection we adopted an exhaustive procedure based on MCC scores computed on the validation sets. First, we selected the N-to-1 ELM models whose MCC scored ≥0.77. Then for each combination of these models, we defined an 'ensemble' by averaging their predictions. Finally, the best performing ensemble on the validation sets was retained. The final performance was assessed on the test sets without any further hyper parameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Performance of the ELM-based predictor</head><p>The aim of this article is to generate a reliable method for finding new TMBB proteins that do not share sequence similarity with those of our training sets. In order to properly train the method, it is necessary that sequence similarity between training and testing proteins is kept at a minimum (&lt;25%). For this reason, we split NRPDB in 10 different subsets so that any pair of sequences selected from two different subsets does not have a local sequence similarity ≥25%. We performed a cross-validation procedure on the 10 different subsets distinguishing among validation and test sets as described in Section 3.3. Different ELM models, differing from each other in the number of neurons of the hidden layer and in the dimension of the sliding window, are evaluated. We report the MCC scores for each tested model on the validation subsets in<ref type="figure" target="#tab_1">Table 1</ref>. Each MCC score is obtained averaging over five different random initializations of the first layer of weights (W ). It appears that MCC values are &gt;0.70 for nearly any input window in the range of 800–1600 hidden neurons. In ELM, the first layer of weights is randomly set with values that are independent of the training examples (so that in our case the Page: 3126 3123–3128number of hidden neurons is the number of parameters to set). For sake of comparison, a classical network with an input window of seven residues (and the same encoding of ELM) supplied with only 20 neurons in the hidden layers needs to train &gt;2800 parameters. The number of training examples in our application is one order of magnitude higher than the number of optimal hidden neurons, ensuring that training can be properly accomplished. Moreover, the minimal norm associated to the ELM training (see Section 2.2) strongly reduces the over-fitting problems. In<ref type="figure" target="#tab_1">Table 1</ref>we also list (within parentheses) values of MCC standard deviations (SDs) for all the window/hidden neuron combinations. The small variations around the average obtained indicate that the method is quite robust and insensitive to the random initialization. Notably, this is true also for a single residue long sliding window, indicating that N-to-1 ELMs are able to detect significant differences between TMBBs and other proteins even encoding a single residue at a time in the hidden layer. Among the four models that obtained an average MCC score ≥0.77 in<ref type="figure" target="#tab_1">Table 1</ref>, we computed all their possible ensembles as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.Savojardo et al.</head><formula>O E = 1 N N x=1 O x (12)</formula><p>where O x is the output of the x-th selected model. The models for the ensemble were selected on the validation sets, using the criterion described above (see Section 3.3). After an exhaustive search we ended up with an ensemble of two models that on the validation sets achieved an MCC score of 0.83. The two models singled out for the ensemble have both 1600 neurons in the hidden layer and input windows of 7 and 11 residues, respectively. The selected ensemble (ELME), together with its constituent models is then evaluated on the test sets (<ref type="figure" target="#tab_2">Table 2</ref>). Although scores obtained on the test sets are slightly lower than those computed using the validation sets (compare<ref type="figure" target="#tab_1">Table 1</ref>with<ref type="figure" target="#tab_2">Table 2</ref>), ELME performances are still very high achieving an MCC score of 0.82 with SD equal to 0.02 (<ref type="figure" target="#tab_2">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with previous methods</head><p>Freeman and Wimley (2010) thoroughly compare performances of their algorithms with available methods (also based on machine learning approaches) on their newly generated dataset (NRPDB comprising 14 238 proteins, of which 48 are TMBBs). Thanks to their effort, here we can compare our results with their best methods under the same condition: we set a predictive threshold so that 46 or 37 of 48 true TMBBs are considered positive predictions. In<ref type="figure" target="#tab_3">Table 3</ref>we compare our ELME with the two best algorithms obtained by Freeman and Wimley (2010). In particular, they defineda new algorithm (FW in<ref type="figure" target="#tab_3">Table 3</ref>) including several improvements over a previously published implementation (<ref type="bibr" target="#b28">Wimley, 2002</ref>) and a filtered version based on a mean randomized score (MRS in<ref type="figure" target="#tab_3">Table 3</ref>). Scores in<ref type="figure" target="#tab_3">Table 3</ref>for FW and MRS were taken from Freeman and Wimley (2010). In that paper, FM and MRS performances were computed using some of the TMBB proteins included in the test sets to fit the main algorithm parameters (TMBB amino acid abundance values). Although NRPDB includes sequences that were used to generate abundance values for the FW algorithm, the authors explained that this was not a problem since their statistical approach was not readily subject to over-fitting due to the nature of the method and to the low number of parameters of the FW algorithm (120) (<ref type="bibr" target="#b12">Freeman and Wimley, 2010</ref>). Machine learning approaches, however, need cross-validation to prove the robustness of the learning process and for this reason we adopt a 10-fold crossvalidation procedure based on a distinction among validation and testing sets. Results reported in<ref type="figure" target="#tab_3">Table 3</ref>for ELME were obtained using the 10-fold cross-validation procedure by averaging the scores of the 10 predictions (two models with five different initial random weights) obtained for each protein on its test sets and by adopting a predictive threshold to match the true positive values of FW and MRS. ELME outperforms both methods (<ref type="figure" target="#tab_3">Table 3</ref>) when the number of true positives is the same. In either case with ELME the number of false positives is drastically reduced and the PPV and MCC values are significantly improved. The performance of ELME can be also evaluated in comparison with other available methods under the conditions set by<ref type="bibr" target="#b12">Freeman and Wimley (2010)</ref>, namely after matching the sensitivity values of the different algorithms based on machine learning (kNN,<ref type="bibr" target="#b22">Hu and Yan, 2008;</ref><ref type="bibr">TMDB, Ou et al., 2008</ref>) and pattern recognition (BOMP,<ref type="bibr" target="#b8">Berven et al., 2004</ref>) (<ref type="figure" target="#tab_4">Table 4</ref>). Also in this benchmark, ELME achieves the highest scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Genomic analysis</head><p>One of the main purposes of our N-to-1 ELM algorithm was to sort out TMBB proteins in genomic databases. We evaluated our method on the genome of Escherichia coli (K12 strain), which is one of the most comprehensively annotated genomes available. From UniProt we extracted the proteins from E.coli K12, which were not annotated as hypothetical or putative. Similarly of what has been done before by other authors, we did not consider proteins that were shorter than 60 or longer than 4000 residues (<ref type="bibr" target="#b12">Freeman and Wimley, 2010</ref>).For the legend seeWith this selection we ended up with 30 TMBBs and 2515 nonTMBBs. Since there is some degree of similarity between the E.coli proteins and those in NRPDB, we adopted the following procedure to asses the genomic predictions: @BULLET for each E.coli protein q, we run a BLAST search of q against NRPDB, @BULLET we extracted the most similar sequence p from NRPDB (the highest BLAST hit of q in NRPDB), @BULLET we selected the 1-to-N ELM models generated by the 10-fold cross-validation on NRPDB, from which p belonged to the 'test set', @BULLET we predicted q with those models (whose parameters and hyperparameters were chosen in the validation set).</p><p>With this procedure, we simulated the case of a genomic analysis on never-seen before proteins. The results on E.coli set are reported in<ref type="figure" target="#tab_5">Table 5</ref>, where it appears that the N-to-1 ELME is more efficient in analyzing the E.coli annotated proteins than in scoring over NRPDB (compare Tables 2 and 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this article, we present a new algorithm for the detection of TMBB proteins starting from their sequence. The main novelties of our work are of two kinds: methodological and applicative. First of all, here we introduce a new very powerful and general method that combines two ideas: an N-to-1 whole protein encoding in a single hidden layer (<ref type="bibr" target="#b25">Mooney et al., 2011</ref>) and an ELM approach (<ref type="bibr" target="#b14">Huang, 2003</ref>). Our new method is here applied to the detection of TMBB proteins. However, N-to-1 ELM models are very general and can be profitably applied to address other problems of computational biology. In this article, we also show that the N-to-1 ELM approach outperforms previously developed methods of TMBB detection, including the most recent one (<ref type="bibr" target="#b12">Freeman and Wimley, 2010</ref>). Differently from methods based on biochemical principles, the parameters of an N-to-1 ELM do not have an immediate physicochemical description. This can be a disadvantage when for the task at hand, a clear interpretation of the method parameter is needed. However, when the main goal is achieving the best predictive performance, a bottom up approach, such as learning rules from data mining, can be advantageous.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>about 0.1% of all the structures from Gram-negative organisms in the Protein Data Bank (PDB), http://www.pdb.org/pdb/home/ home.do]. * To whom correspondence should be addressed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FW</head><figDesc>a 46 599 13 591 2 0.26 0.96 0.96 0.96 0.07 0.13 MRS a 37 161 14 029 11 0.38 0.77 0.99 0.99 0.19 0.30 ELME b (1) 46 88 14 102 2 0.57 0.96 0.99 0.99 0.34 0.51 ELME b (2) 37 12 14 178 11 0.76 0.77 0.99 0.99 0.76 0.76 We changed the ELME predictive thresholds in order to match the same number of TPs [(1), threshold set to obtain 46 TPs; (2), threshold set to obtain 37 TPs] of FW and MRS, respectively. a Results taken from Freeman and Wimley (2010); FW, Freeman–Wimley algorithm with all modifications included (Freeman and Wimley, 2010); MRS, FW algorithm with MRS filter (Freeman and Wimley, 2010). b Results are obtained using a 10-fold cross-validation procedure. For index definition see Section 3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><figDesc>Funding: Ministero dell'Istruzione, dell'Università e della Ricerca (MIUR)-Fondo per gli Investimenti della Ricerca di Base (FIRB) grant for the Laboratorio Interazionale di BIoinformatica (LIBI) project delivered to R.C. Conflict of Interest: none declared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>X(centered into each</figDesc><table>[14:33 19/10/2011 Bioinformatics-btr549.tex] 

Page: 3124 3123–3128 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>Table 1.</figDesc><table>MCC scores on the validation sets as a function of the window 
width and of the number of hidden neurons 

Win\#H 25 
50 
100 
200 
400 
800 
1600 3200 

1 
36 (2) 53 (2) 65 (1) 71 (1) 71 (1) 73 (1) 74 (1) 64 (1) 
3 
43 (5) 42 (3) 51 (2) 58 (1) 69 (2) 78 (1) 77 (1) 58 (1) 
5 
44 (3) 45 (3) 47 (4) 55 (2) 63 (2) 73 (1) 76 (2) 54 (2) 
7 
44 (2) 47 (3) 48 (1) 55 (3) 64 (2) 75 (2) 78 (1) 54 (3) 
9 
44 (4) 50 (4) 50 (2) 55 (1) 63 (2) 70 (3) 75 (2) 55 (1) 
11 
47 (5) 51 (1) 51 (3) 55 (2) 64 (1) 71 (2) 77 (1) 55 (2) 
13 
45 (2) 52 (2) 54 (3) 56 (3) 62 (2) 70 (3) 74 (2) 56 (3) 
15 
43 (3) 54 (1) 55 (4) 59 (5) 64 (1) 69 (2) 73 (2) 59 (5) 
17 
47 (3) 58 (5) 57 (5) 58 (3) 63 (2) 69 (3) 75 (2) 58 (3) 
19 
45 (5) 56 (1) 59 (3) 60 (3) 64 (3) 68 (1) 71 (2) 59 (3) 
21 
45 (5) 56 (1) 59 (3) 60 (3) 64 (3) 68 (1) 68 (2) 59 (3) 

Win, width of the sliding input window; #H, number of hidden neurons. The numbers 
are in percentages with the SD reported within parentheses. The average value and SD 
are computed on five different random initializations. In bold–italic we list MCC scores 
≥0.77. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><figDesc>Table 2. Performance of the ensemble and its components on the test sets</figDesc><table>Method 
MCC 
SEN 
SPE 
PPV 
AC 
F1 

ELM_w7/1600 
77 (2) 64 (2) 100 (0) 92 (3) 100 (0) 75 (2) 
ELM_w11/1600 75 (3) 59 (6) 100 (0) 99 (3) 
99 (1) 73 (4) 
ELME 
82 (2) 73 (4) 
99 (1) 92 (3) 
99 (1) 81 (3) 

ELM_wx/1600, the single N-to-1 ELM model with window size x and 1600 hidden 
neurons; ELME, predictor that averages the predictions of the selected ELM_wx 
models. For index definition see Section 3.2. Numbers are in percentages and SD are 
reported within parentheses. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>Table 3. ELME comparison with the best Freeman–Wimley algorithms</figDesc><table>Method 
TP FP TN 
FN MCC SN SP AC PPV F1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><figDesc>Table 5. N-to-1 ELM performance on E.coli genome</figDesc><table>Method 
MCC 
SEN 
SPE 
PPV 
AC 
F1 

ELM w7/1600 
93 (1) 93 (2) 
99 (1) 
93 (2) 99 (1) 93 (1) 
ELM w11/1600 93 (1) 90 (2) 
99 (1) 
96 (2) 99 (1) 93 (1) 
ELME 
95 (1) 90 (2) 100 (1) 100 (1) 99 (1) 95 (1) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><figDesc>Table 2.</figDesc><table>In particular, we selected the annotated TMBB proteins and the 
negative complement set using the following queries: 

@BULLET organism:83333 AND keyword:181 AND reviewed:yes AND 
keyword:'Transmembrane beta strand' AND length:[60 TO 
4000] NOT name:Probable NOT name:Putative NOT name: 
Uncharacterized NOT annotation:(type:location lipid anchor) 

@BULLET organism:83333 AND keyword:181 AND reviewed:yes NOT 
keyword:'transmembrane beta strand' AND length:[60 TO 
4000] NOT name:Probable NOT existence:'inferred from 
homology' NOT existence:predicted NOT existence:uncertain 
NOT annotation:(type: location 'Cell outer membrane') NOT 
name:Putative 

</table></figure>

			<note place="foot">© The Author 2011. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">where T =[σ −1 (t 1 ,),σ −1 (t 2 ),... σ −1 (t N ),] T is the matrix of transformed labels of protein classification by means of the inverse of the activation function σ −1 (x) = log[(1−x)/x]. In order to prevent overflow, classification labels [1 or 0 for x in σ −1 (x)] are substituted with real values (for instance, 0.99 for 1 and 0.01 for 0). Our training algorithm can be stated as follows. Given a training set of protein sequences {P i } with their corresponding classification labels {t i }, the learning algorithm proceeds as: Step 1. Set the number of hidden neurons. Step 2. Assign randomly the input weights W. Step 3. For each protein P i generate a vector H i using the N-to-1 encoding of Equation (1) to obtain the final H matrix. Step 4. Calculate the output weights β as: β = H −1 T (4) where T =[σ −1 (t 1 ,),σ −1 (t 2 ),... σ −1 (t N ),] T is as in Equation (3) and H −1 is the Moore–Penrose generalized inverse of the H matrix (Huang, 2003). Differently from traditional learning algorithms, ELM not only tends to reach the smallest training error but also the smallest norm of output weights (Huang et al., 2006a). This implies that the H −1 choice minimizes both: MinimizeT −βH Minimizeβ (5) In analogy with Support Vector Machines, minimizing the norm of the output weight ||β|| is similar to maximizing the distance of the separating margins of the two different classes in the ELM feature space 2/||β|| (Huang et al., 2010). Another invaluable advantage of ELMs with respect to the classical gradient-based algorithms is their speed. Published papers addressed this issue although on problems different from the one we address here and reported that accuracy is somewhat similar (in some cases slightly lower) when back-propagation is compared to ELM (Alhamdoosh et al., 2010; Haung, 2006). For our application, the back-propagation version is at least 30 times slower with more hyper-parameters (learning rate, training cycles, etc.) to set (data not shown). The ELM speed during the learning phase allows to test a larger number of different models. In our current implementation, the random weights (W ) implicitly contain the threshold biases (set as uniform random values). On the contrary the weights of the second layers (β) do not contain biases, since it has been demonstrated that the hyper-plane crossing the origin is sufficient for universal function approximation (Huang and Chen, 2007, 2008; Huang et al., 2006b). Once a random set of weights is generated (W ) and the trainable parameters (β) are set according to Equation (4), the predictions of a set of unknown proteins (or of the validation/test sets) are assigned using Equations (1) and (2). 3 DATASET AND MEASURES OF PERFORMANCE 3.1 Dataset To train and test our method we rely on the dataset generated by Freeman and Wimley (2010). This dataset (NRPDB) consists of 14 238 chains derived from PDB, 48 of which are TMBBs from Gram-negative bacteria while 14 190 are non-TMBB proteins.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">1410</biblScope>
			<biblScope unit="page" from="33" to="52" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>btr549. .tex]</note>
</biblStruct>

<biblStruct   xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName>
				<surname>Page</surname>
			</persName>
		</author>
		<imprint>
			<biblScope unit="page" from="3127" to="3123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<monogr>
		<title level="m" type="main">Improving the detection of TMBB chains Table 4. ELME comparison with different approaches on NRPDB Index kNN a</title>
		<imprint>
			<biblScope unit="page">818181</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName>
				<forename type="first">Berven</forename>
				<surname>Bomp</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ELME, this work. a Values are taken from Freeman and Wimley</title>
		<editor>kNN, Hu and Yan</editor>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">10-fold cross-validation procedure. For index definition see Section 3.2 Disulfide connectivity prediction with extreme learning machines</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Alhamdoosh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the International Conference on Bioinformatics Models</title>
		<meeting>eeding of the International Conference on Bioinformatics Models<address><addrLine>Rome</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-01-26" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Gapped BLAST and PSI-BLAST: a new generation of protein database search programs</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">F</forename>
				<surname>Altschul</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="3389" to="3402" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">PRED-TMBB: a web server for predicting the topology of beta-barrel outer membrane proteins</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">G</forename>
				<surname>Bagos</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="400" to="404" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluation of methods for predicting the topology of beta-barrel outer membrane proteins and a consensus prediction method</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">G</forename>
				<surname>Bagos</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">BOMP: a program to predict integral β-barrel outer membrane proteins encoded within genomes of Gram-negative bacteria</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">S</forename>
				<surname>Berven</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="394" to="399" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting transmembrane beta-barrels in proteomes</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">R</forename>
				<surname>Bigelow</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2566" to="2577" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Fishing new proteins in the twilight zone of genomes: the test case of outer membrane proteins in Escherichia coli K12, Escherichia coli O157:H7, and other Gram-negative bacteria</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Casadio</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein. Sci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1158" to="1168" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Grammatical-restrained hidden conditional random fields for bioinformatics applications</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Fariselli</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="4" to="13" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">A highly accurate statistical approach for the prediction of transmembrane β-barrels</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">C</forename>
				<surname>Freeman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">C</forename>
				<surname>Wimley</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1965" to="1974" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Discrimination of outer membrane proteins using machine learning algorithms</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">M</forename>
				<surname>Gromiha</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Suwa</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="1031" to="1037" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning capability and storage capacity of two hidden-layer feedforward networks</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">B</forename>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Neural Networ</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="274" to="281" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Extreme learning machine: theory and applications</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">B</forename>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="489" to="501" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Universal approximation using incremental constructive feedforward networks with random hidden nodes</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">B</forename>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Neural Networ</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="879" to="892" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Convex incremental extreme learning machine</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">B</forename>
				<surname>Huang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Chen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3056" to="3062" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">1410</biblScope>
			<biblScope unit="page" from="33" to="52" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName>
				<surname>Page</surname>
			</persName>
		</author>
		<imprint>
			<biblScope unit="page" from="3128" to="3123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Savojardo</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Enhanced random search based incremental extreme learning machine</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">B</forename>
				<surname>Huang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Chen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="3460" to="3468" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">A method for discovering transmembrane β-barrel proteins in Gram-negative bacterial proteomes</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Hu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Yan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Chem</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="298" to="301" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimization method based extreme learning machine for classification</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">B</forename>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="155" to="163" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">A sequence-profile-based HMM for predicting and discriminating beta barrel membrane proteins</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">L</forename>
				<surname>Martelli</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="46" to="53" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">De novo protein subcellular localization prediction by N-to1 neural networks</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Mooney</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Intelligence Methods for Bioinformatics and Biostatistics</title>
		<editor>Lisboa,P.J. and Rizzo,R.</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">TMBETADISC-RBF: discrimination of β-barrel membrane proteins using RBF networks and PSSM profiles</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<forename type="middle">Y</forename>
				<surname>Ou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Chem</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="227" to="231" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Hhomp: prediction and classification of outer membrane proteins</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Remmert</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="446" to="451" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Toward genomic identification of β-barrel membrane proteins: composition and architecture of known structures</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">C</forename>
				<surname>Wimley</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Sci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="301" to="312" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">The versatile beta-barrel membrane protein</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">C</forename>
				<surname>Wimley</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Opin. Struct. Biol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="404" to="411" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">) β-Barrel membrane proteins</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">E</forename>
				<surname>Schulz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Opin. Struct. Biol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="443" to="447" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>