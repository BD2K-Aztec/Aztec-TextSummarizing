
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining Multiple-rule bias in the comparison of classification rules</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011">2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Mohammadmahdi</forename>
								<forename type="middle">R</forename>
								<surname>Yousefi</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<postCode>77843</postCode>
									<settlement>College Station</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Jianping</forename>
								<surname>Hua</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Computational Biology Division</orgName>
								<orgName type="institution">Translational Genomics Research Institute</orgName>
								<address>
									<postCode>85004</postCode>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Edward</forename>
								<forename type="middle">R</forename>
								<surname>Dougherty</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<postCode>77843</postCode>
									<settlement>College Station</settlement>
									<region>TX</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Computational Biology Division</orgName>
								<orgName type="institution">Translational Genomics Research Institute</orgName>
								<address>
									<postCode>85004</postCode>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Bioinformatics and Computational Biology</orgName>
								<orgName type="institution">University of Texas</orgName>
								<address>
									<addrLine>M. D. Anderson Cancer Center</addrLine>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining Multiple-rule bias in the comparison of classification rules</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">27</biblScope>
							<biblScope unit="issue">12</biblScope>
							<biblScope unit="page" from="1675" to="1683"/>
							<date type="published" when="2011">2011</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btr262</idno>
					<note type="submission">Received on February 19, 2011; revised on April 9, 2011; accepted on April 15, 2011</note>
					<note>[10:46 24/5/2011 Bioinformatics-btr262.tex] Page: 1675 1675–1683 USA Associate Editor: Martin Bishop Supplementary simulation results are also included. Contact: edward@ece.tamu.edu Supplementary Information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: There is growing discussion in the bioinformatics community concerning overoptimism of reported results. Two approaches contributing to overoptimism in classification are (i) the reporting of results on datasets for which a proposed classification rule performs well and (ii) the comparison of multiple classification rules on a single dataset that purports to show the advantage of a certain rule. Results: This article provides a careful probabilistic analysis of the second issue and the &apos;multiple-rule bias&apos;, resulting from choosing a classification rule having minimum estimated error on the dataset. It quantifies this bias corresponding to estimating the expected true error of the classification rule possessing minimum estimated error and it characterizes the bias from estimating the true comparative advantage of the chosen classification rule relative to the others by the estimated comparative advantage on the dataset. The analysis is applied to both synthetic and real data using a number of classification rules and error estimators. Availability: We have implemented in C code the synthetic data distribution model, classification rules, feature selection routines and error estimation methods. The code for multiple-rule analysis is implemented in MATLAB. The source code is available at</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Three recent articles in Bioiformatics have lamented the difficulty in establishing performance advantages for proposed classification rules (<ref type="bibr" target="#b0">Boulesteix, 2010;</ref><ref type="bibr" target="#b18">Jelizarow et al., 2010;</ref><ref type="bibr" target="#b22">Rocke et al., 2009</ref>). Two statistically grounded sources of overoptimism have been highlighted. One considers applying a classification rule to numerous datasets and then reporting only the results on the dataset for which the designed classifier possesses the lowest estimated error. The optimistic bias from this kind of dataset picking is quantitatively analyzed in<ref type="bibr" target="#b26">Yousefi et al. (2010)</ref>, where it is termed * To whom correspondence should be addressed. 'reporting bias' and where this bias is characterized as a function of the number of considered datasets. A second kind of overoptimism concerns the comparison of a collection of classification rules by applying the classification rules to a dataset and comparing them according to the estimated errors of the designed classifiers. This kind of bias, which we will call 'multiple-rule bias', has been considered in Boulesteix and Strobl (2009) by applying a battery of classification rules to colon cancer and prostate cancer datasets and then examining the effects of choosing classification rules having minimum cross-validation error estimates. Whereas the thrust of<ref type="bibr" target="#b1">Boulesteix and Strobl (2009)</ref>is to compare the sources of multiple-rule bias in classification rules, namely, gene selection, parameter selection and classifier function construction, our interest is in studying multiple-rule bias as a function of the number of rules being considered. In particular, we are interested in the joint distribution, as a function of the number of compared rules, between the minimum estimated error among a collection of classification rules and the true error for the classification rule having minimum estimated error, as well as certain moments associated with this joint distribution. Although different with regard to distributional specifics, this approach is analogous to the approach taken in<ref type="bibr" target="#b26">Yousefi et al. (2010)</ref>, where the joint distribution involved the minimum estimated error of the designed classifier over a collection of datasets and the true error of the designed classifier on the population corresponding to the dataset resulting in minimum estimated error. This is a natural way to proceed because any bias ultimately results from inaccuracy in error estimation, so that the behavior of the joint distribution of interest and its moments are consequent to the joint distribution of the error estimator and the true error. Owing to the methodology in<ref type="bibr" target="#b1">Boulesteix and Strobl (2009)</ref>, it would have been impossible for them to study this joint distribution because they never concern themselves with true errors, only crossvalidation estimates. Hence, when they compare a minimal error to a baseline error to arrive at a measure of optimistic bias, they are comparing cross-validation estimates. In characterizing multiple-rule bias, we begin with a more general framework than the one just described; rather than simply considering multiple classification rules, we consider multiple classifier rule models, so that there are not only multiple classification rules, but also multiple error estimation rules being employed. We define a classifier rule model as a pair (,), where is a classification rule, including feature selection if feature selection is employed, and is an error estimation rule (<ref type="bibr">Dougherty and BragaNeto, 2006</ref>). The scenario in the preceding paragraph results where there is only a single error estimation rule.Page: 1676 1675–1683</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M.R.Yousefi et al.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SYSTEMS AND METHODS</head><p>We consider r classification rules, 1 ,, 2 ,...,, r , and s error estimation rules, 1 ,, 2 ,...,, s , on a feature-label distribution F. These are combined to form m = rs classifier rule models:</p><formula>( 1 ,, 1 ),( 1 ,, 2 ), ..., ( 1 ,, s ),( 2 ,, 1 ),( 2 ,, 2 ), ..., ( r ,, s ).</formula><p>Given a random sample S n of size n drawn from F, the classification rules yield r designed classifiers:</p><formula>ψ i = i (S n ),i = 1</formula><p>,2,...,r. The true error of ψ i is given by ε i true = P(ψ i (X) = Y ), where (X,Y ) is a feature-label pair. For j = 1,2,...,s, j provides an error estimate, ε i,j est , for ψ i. Since the classification rules are not identical, neither are the distributions of ε 1 true ,ε 2 true ,...,ε r true nor are the distributions of ε i,1 est ,ε i,2 est ,...,ε i,s est. All true-error and estimated-error distributions are functions of the random sample S n. Since all classification rules operate on the same sample, the true errors can be highly correlated, as will be the estimated errors. Without loss of generality, we assume the classifier models are enumerated so that</p><formula>E Sn [ε 1 true ]≤E Sn [ε 2 true ]≤... ≤ E Sn [ε r true ].</formula><p>The minimum estimated error is</p><formula>ε min est = min{ε 1,1 est ,ε 1,2 est ,...,ε 1,s est ,ε 2,1 est ,...,ε r,s est }.</formula><formula>(1)</formula><p>Letting i min and j min denote the classifier number and error estimator number, respectively, for which the error estimate is minimum, we have</p><formula>ε min est = ε i min ,j min est</formula><p>. Suppose a researcher wishes to select the best performing of r classification rules on a feature-label distribution F and proceeds by taking a random sample from F, designing a classifier for each classification rule, and estimating the errors of the designed classifiers. If F is known, then the true error of each designed classifier can be evaluated, the classifier with minimum true error can be chosen, and the classification rule leading to that classifier be declared ' best'. The truly best classification rule has the minimum expected error across all samples from F, so that what is happening is that a single observation of ε i true is being used as an estimate for E Sn<ref type="bibr">[ε i true ]</ref>. On the other hand, if F is unknown as is used in practice, then the errors of the designed classifiers are estimated from sample data and the classification rule leading to the classifier, ψ i min , with minimum estimated error is chosen as 'best'. We are assuming that the researcher tries s error estimators and selects the one with lowest error estimate. In this case, a single observation of ε min est is being used to estimate E Sn<ref type="bibr">[ε i min true ]</ref>, the basic performance measure for i min. At issue is the goodness of this estimation. This involves the distribution of the deviation = ε min est −E Sn<ref type="bibr">[</ref><ref type="bibr">]</ref>, which is marginal to the joint distribution of (ε min est ,ε i min true ). A key performance measure derived from the deviation distribution is the bias of ε min est as an estimator of</p><formula>E Sn [ε i min true ], namely, Bias(m,n) = E Sn [] = E Sn ε min est −E Sn ε i min true .</formula><formula>(2)</formula><p>Estimation is optimistic if Bias(m,n) &lt; 0. This can happen even if none of the error estimation rules are optimistically biased, that is, even if E Sn</p><formula>[ε i,j est ]≥ E Sn ε i true for i = 1,2</formula><p>,...,r and j = 1,2,...,s. Indeed, even if this is so, owing to estimation-rule variance, on any given sample it may be that ε min est &lt;ε 1 true. For instance, if among 1 ,, 2 ,...,, s there is an error estimation rule, such as leave-one-out cross-validation, that is slightly (pessimistically) biased and possesses large variance, then we should expect that E Sn</p><formula>ε min est &lt; E Sn ε 1 true . In this case, E Sn ε min est &lt; E Sn ε 1 true ≤ E Sn [ε i min true ]</formula><p>and Bias(m,n) &lt; 0. In the way we have set up the general problem, not only can optimistic bias result from considering multiple estimated errors among classifiers but also from applying multiple error estimates for each classifier. From the generic arguments made thus far, we can state two properties concerning the bias. First, as the number m of classifier models grows, the minimum of Equation (1) is taken over more estimated errors, thereby increasing ε 1 true −ε min est and |Bias(m,n)|=E Sn<ref type="bibr">[</ref>. Second, as the sample size n is increased, under the typical condition that the variance of the error estimator decreases with increasing sample size, E Sn<ref type="bibr">[ε min est ]</ref>increases and |Bias(m,n)| decreases. Bias is only one factor in estimating E Sn<ref type="bibr">[</ref>error for ε min est as an estimator of E Sn<ref type="bibr">[</ref>Even should the bias be small, the estimation will not be accurate if the deviation variance is large. As discussed thus far, the classification rules are fixed; however, since our interest is in the number of classification rules (and error estimators), not any particular rule or estimator, we assume there is a total of R classification rules from which we randomly choose r. This corresponds to a situation where a researcher selects r from among a large collection (R) of potential classification rules and applies s error estimators to each selected rule. Given r, there are R r possible collections of classification rules to be combined with s error estimators to form R r possible collections of classifier rule models of size m. Hence, the number m of classifier rule models increments according to s,2s,...,Rs. We denote the collection of classifier models of size m by m. To get the distribution of errors, one needs to generate independent samples from the same feature-label distribution and apply the procedure shown in<ref type="figure" target="#fig_1">Figure 1</ref>. The previously discussed performance measures must be adjusted to take model randomization into account. Given a sample S n , for a realization of m we find an expected deviation according to Equation (2), but now we have a random process generating the realizations so we have to take the expectation over that process to obtain the expected bias,</p><formula>Bias Av (m,n) = E m E Sn [| m ] ,</formula><formula>(4)</formula><p>where the subscript indicates the average over m. A similar averaging arises with the deviation variance to yield Var Av (m,n) = E m [Var Sn<ref type="bibr">[| m ]</ref>]. The RMS now takes the form</p><formula>RMS Av (m,n) = E m E Sn [ 2 | m ] .</formula><formula>(5)</formula><p>Having discussed the performance measures relating to estimating</p><formula>E Sn [ε i min true ]</formula><p>by ε min est , we now consider the comparative advantage of the classification rule i min. Its true comparative advantage with respect to the other considered classification rules is</p><formula>A true = E Sn ε i min true − 1 r −1 i =i min E Sn ε i true .</formula><formula>(6)</formula><p>Its estimated comparative advantage is given by</p><formula>A est = ε min est −</formula><p>Page: 1677 1675–1683</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple-rule bias</head><p>be the error estimator chosen by a researcher for the sake of consistency). The expectation, E Sn<ref type="bibr">[A est ]</ref>, of this distribution gives the mean estimated comparative advantage of i min with respect to the collection. A key bias to be considered is C bias = E Sn<ref type="bibr">[</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Simulation design</head><p>We use a general model based on multivariate Gaussian distributions with a blocked covariance structure that conforms to various observations made in microarray expression-based studies (<ref type="bibr" target="#b17">Hua et al., 2009</ref>). A battery of distribution models can be constructed by changing model parameters to generate different synthetic data samples. We also consider four real datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Synthetic data</head><p>In microarray studies, assuming a blocked covariance matrix is a way of modeling groups of interacting genes where there is negligible interaction between the groups. It has been used in genomic classification to model genes collected into distinct pathways, each pathway being represented by a block (<ref type="bibr" target="#b11">Dougherty et al., 2007</ref></p><formula>{ 0 ,, 1 } instead of { gm 0 ,, gm 1 } and { hm 0 ,, hm 1 }. We assume that 0 = σ 2 0 and 1 = σ 2 1 , where σ 2 0 and σ 2 1</formula><p>can be different, and that has the following block structure:</p><formula>= ⎡ ⎢ ⎢ ⎢ ⎣ ρ 0 0 ... 0 0 ρ 0 ... 0 . . . . . . .. . . . . . . . 0 0 ... 0 ρ ⎤ ⎥ ⎥ ⎥ ⎦ ,</formula><p>where ρ is a l ×l matrix, with 1 on the diagonal and ρ off the diagonal. In the block-based covariance structure, the markers are divided into equalsize blocks of size l. Markers of different blocks are uncorrelated, while all the markers in the same block are correlated to each other with correlation coefficient ρ. A consequence of having unequal variances and subclasses in the classconditional distributions is to introduce non-linearities in the decision boundaries for the model, where less global markers and larger difference in the variances lead to a more non-linear decision boundary. Because the global markers and the heterogeneous markers possess the same structure, we can assume the same mean vectors {µ 0 ,µ 1 } for both groups,</p><formula>{µ gm 0 ,µ gm 1 } and {µ hm 0 ,µ hm 1</formula><p>}, as we did for the covariance matrices. Furthermore, we use the same structure for µ 0 and µ 1 in the form of m 0 ×(1,1,...,1) and</p><formula>m 1 ×(1,1</formula><p>,...,1), respectively, where m 0 and m 1 are scalars (<ref type="bibr" target="#b17">Hua et al., 2009;</ref><ref type="bibr" target="#b26">Yousefi et al., 2010</ref>). Similar to the global markers, there are two types of non-markers: highvariance and low-variance non-markers. The D hv features belonging to the former group are uncorrelated and their distributions are described by pN(m 0 ,σ 2 0 )+(1−p)N(m 1 ,σ 2 1 ), where m 0 , m 1 , σ 2 0 and σ 2 1 take values equal to the means and variances of the markers, respectively, and p is a random value uniformly distributed over<ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. The D lv remaining features are uncorrelated low-variance non-markers, each having a Gaussian distribution with parameters (m 0 ,σ 2 0 ). A typical microarray experiment usually contains tens of thousands of probes (genes) but a small number of sample points, typically less than 200. We choose the total number of features to be D = 20 000 and the number of sample points to be 60 and 120. Two variance settings are considered: equal variances {σ 2</p><formula>0 = 0.6 2 ,σ 2 1 = 0.6 2 } and unequal variances {σ 2 0 = 0.6 2 ,σ 2 1 = 1.2 2 }. For</formula><p>the blocked covariance matrix, we choose block size l = 5 and correlation coefficient ρ = 0.8, giving relatively tight correlation within a block, which would be expected for a pathway. We do not choose model parameters in accordance with the Bayes errors or the estimated errors; rather, we choose them in accordance with achievable true errors seen in real problems.<ref type="figure" target="#tab_1">Table 1</ref>shows the parameters of the distribution models used in this study. See<ref type="bibr" target="#b26">Yousefi et al. (2010)</ref>for more details about the choice of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Real data</head><p>This study uses four real datasets from microarray experiments consisting of more than 150 arrays: pediatric acute lymphoblastic leukemia (ALL) (<ref type="bibr" target="#b25">Yeoh et al., 2002</ref>), multiple myeloma (<ref type="bibr" target="#b27">Zhan et al., 2006</ref>), hepatocellular carcinoma (HCC) (<ref type="bibr" target="#b5">Chen et al., 2004</ref>) and drugs and toxicants response on rats dataset (<ref type="bibr" target="#b21">Natsoulis et al., 2005</ref>). We use n = 60 sample points for training. The remaining sample points are held-out for computing the true error. To the extent possible, we try to maintain the original labeling and follow the data preparation directions used in the papers reporting these datasets.<ref type="figure" target="#tab_2">Table 2</ref>shows a summary of the four real datasets. Full descriptions are presented in the Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 1678 1675–1683</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M.R.Yousefi et al.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Classification schemes</head><p>Six classification rules are considered: 3-nearest neighbors (3NN), linear discriminant analysis (LDA), diagonal LDA (DLDA), nearest-mean classifier (NMC), linear support vector machine (L-SVM) and radial basis function SVM (RBF-SVM). Two different feature selection methods are considered: t-test and t-test followed by sequential forward search (t-test + SFS). Onestage feature selection uses the t-test and five features are selected. For two-stage feature selection, the number of features is reduced to 500 in the first stage (t-test) and then to 5 by SFS. Three cross-validation error estimation methods are considered: 5-fold, 10-fold and leave-one-out (LOO). Combining six classification rules with two feature selection methods results in R = 12 classification rules from which we randomly choose r = 1,2,...,R and design the classifiers on one sample. Note that with three error estimators, there is a maximum of 36 different classifier rule models.<ref type="figure" target="#tab_3">Table 3</ref>lists the classification rules, feature selection methods and error estimation procedures utilized. To illustrate the definitions, let us suppose we are only considering two classification rules, LDA and 3NN, without feature selection and two error estimators, LOO and CV5 (5-fold cross-validation). LDA is based on the discriminant for the optimal classifier in a Gaussian model (Gaussian classconditional densities) with common covariance matrix by plugging the sample means and pooled sample covariance matrix obtained from the data into the discriminant. Assuming equally likely classes, LDA assigns x to class 1 if and only if</p><formula>(x− ¯ x 1 ) T −1 (x− ¯ x 1 ) ≤ (x− ¯ x 0 ) T −1 (x− ¯ x 0 ),</formula><formula>(8)</formula><p>where ¯ x u is the sample mean for class u, u = 0,1, and is the pooled sample covariance matrix. While derived under the Gaussian assumption with common covariance matrix, LDA can provide good results when these assumptions are mildly violated. For the 3NN rule, the designed classifier is defined to be 0 or 1 at x according to which is the majority among the labels of the 3 points closest to x. In k-fold cross-validation, the given sample S n is randomly partitioned into k folds (subsets) S (i) , for i = 1,2,...,k. Each fold is left out of the design process, a (surrogate) classifier, ψ n,i , is designed on S n −S (i) , the error of ψ n,i is estimated as the counting error ψ n,i makes on S (i) , and the crossvalidation estimate is the average error committed on all folds. If there is feature selection, then it must be redone for every fold because it is part of the classification rule. In leave-one-out cross-validation, each fold consists of a single point. Owing to computational requirements, k-fold cross-validation, k &lt; n, usually involves a random selection of partitions. In general, the bias of cross-validation is typically slightly pessimistic, provided that the number of folds is not too small. The problem with cross-validation is that it tends to be inaccurate for small samples because, for these, it has large variance (Braga<ref type="bibr" target="#b2">Neto and Dougherty, 2004</ref>) and is poorly correlated with the true error (<ref type="bibr" target="#b14">Hanczar et al., 2007</ref>). For all error estimators, there is variation resulting from the sampling process. For randomized cross-validation, a second variance contribution, referred to as 'internal variance', arises from the random selection of the partitions (see<ref type="bibr" target="#b15">Hanczar and Dougherty, 2010</ref>). The latter does not apply to LOO because only a single set of folds is possible. Relative to the definitions in the Section 2, if we let 1 be LDA, 2 be 3NN, 1 be LOO and 2 be CV5, then there are four classifier rule models:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Implementation</head><p>The raw output of the synthetic data simulation consists of the true and estimated error pairs resulting from applying the 36 different classification schemes on 10 000 independent random samples drawn from the aforementioned four different distribution models. We approximate the expected true error by taking the average of true errors of each classification rule over the samples. We generate all R r possible collections of classification rules of size r, each associated with s error estimation rules, resulting in R r collections of classifier rule models of size m. For each collection, we find the true and estimated error pairs from the raw output data. Then, for each sample, we find the classifier model (including the classification and error estimation rules) in the collection that gives the minimum estimated error. We record the estimated error, its corresponding true error and the classification and error estimation rules. Given the collection, m , we compute and A est for each sample. Then, we approximate E Sn<ref type="bibr">[</ref></p><formula>A est | m ], Var Sn [| m ], E Sn [ 2 | m ], E Sn [| m ]</formula><p>and A true | m , by taking the average over all the samples. Finally, we</p><formula>approximate E m [E Sn [| m ]], E m [Var Sn [| m ]], E m [ E Sn [ 2 | m ]], E m [E Sn [A est | m ]] and E m [A true | m ]</formula><p>by taking the average over the generated collections. Real data simulations differ somewhat from the synthetic data in the way that the true and estimated errors are computed. For the synthetic data, we generate 10 000 pairs of samples (training set of size 60 or 120 and test set of size 5000) from the assumed distribution model. But for a real dataset, we randomly pick 60 sample points for training (classifier design and error estimation). The remaining held-out sample points are used to calculate the true error. We repeat this process 10 000 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS AND DISCUSSION</head><p>The full set of results appears in the Supplementary Material. In the article, we provide representative examples for each issue. We consider two cases regarding the error estimators: multiple error estimators and a single error estimator. For multiple error estimators, s = 3, we consider all three error estimators at once, and m = 3,6,...,36. For a single error estimator, s = 1, we have three difference cases, depending on which error estimator is used, and m = 1,2,...,12 for each error estimator. For s = 3, keep in mind that the simulations are incremented in steps of three, 3,6,...,36, because each classification rule is evaluated with all three error estimators, as would be the case in practice if an investigator were to consider three error estimators. For a single error estimator, we show LOO in the article and leave the others to the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Joint distributions</head><p>For the synthetic data, the joint distributions are estimated with a bivariate Gaussian-kernel density estimation method. The first set of results (Supplementary Figures s1–s16) show joint distributions between the minimum estimated errors and their corresponding true errors, ε min est and ε i min true , for multiple and single error estimators, different sample sizes and variances. Each plot includes the regression line and a small circle showing the pair of sample means. As m increases, the distributions tend to be more circular (indicating less correlation) and also more compact (indicating smaller variance). Furthermore, as m increases, the distributions Page: 1679 1675–1683i min true with respect to the collection size m, for all classifier rule models for m = 6,21,36 (left column) and for single LOO error estimation for m = 2,7,12 (right column). The real dataset is multiple myeloma by<ref type="bibr" target="#b27">Zhan et al. (2006)</ref>. The white line shows the regression line and the circle indicates the sample mean of the joint distribution. move to the left, thereby demonstrating greater bias. Note the smaller variation for sample size 120. For the real data, the joint distributions are again estimated with a bivariate Gaussian-kernel density estimation method. Supplementary Figures s25–s40 show the joint distribution of (ε min est ,ε i min true ) for multiple and single error estimators, and for different real datasets.<ref type="figure" target="#fig_2">Figure 2</ref>shows the distributions and regression lines for the myeloma data: the left column is for multiple error estimators and shows m = 6,21,36; the right column is for the single error estimator LOO and shows m = 2,7,12. Similar to the synthetic data, as m increases, the distributions tend to be more circular, have smaller variances and move to the left. What is most striking is the absence of regression between ε min est and ε i min true. This lack of regression is consistent with what has been observed in other settings when cross-validation is used to estimate the true error (<ref type="bibr" target="#b14">Hanczar et al., 2007</ref><ref type="bibr" target="#b15">Hanczar et al., , 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple-rule bias</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Moments and comparative performance</head><p>For the synthetic data and the multiple error estimator case,<ref type="figure">Figure 3</ref>shows: (a) the expected bias, Bias Av ; (b) the expected variance,Var Av ; (c) the expected RMS, RMS Av ; and (d) the expected comparative performance bias, C bias. Note that<ref type="figure">Figure 3d</ref>does not graph m = s because the comparative advantages are not defined when r = 1. The same applies for analogous subfigures in the rest of the article. For increasing m, the bias and RMS get worse, but even with m = 3, the RMS is about 0.1 for sample size 60. For this sample size and m = 36, the comparative-performance bias has reached −0.1.<ref type="figure" target="#fig_3">Figure 4</ref>shows corresponding results for a single error estimator, LOO. These too are especially alarming for n = 60. Note that the RMS, actually, has a temporary small dip at m = 2, which is a result of steep decline in variance between m = 1 and m = 2. For the real data, Figures 5 and 6 show corresponding results to Figures 3 and 4, respectively (ignore for the moment the 'average' curves, which will soon be discussed). Note the widely different behaviors between the different datasets. Since we are using the full dataset as an empirical distribution to serve as an approximation of the underlying feature-label distribution and are sampling from the empirical distribution, the different biases result from different behaviors of the error estimators on the different distributions. In practice, given a single sample dataset, one would have no idea of what kind of biases and RMS deviations to expect. This uncertainty exemplifies the standard conundrum faced when one lacks prior information regarding the feature-label distribution. In our case, the problem is that error estimator performance is heavily dependent on the underlying feature-label distribution.</p><formula>(a) ( b) (c) ( d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 1680 1675–1683</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M.R.Yousefi et al.</head><formula>(a) ( b) (c) ( d)</formula><formula>, E m [C bias | m ]</formula><p>: resulted from the distributions of ε min est and ε i min true on the synthetic data for single LOO error estimation, with respect to the collection size m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Averaging over different populations</head><p>The problem of multiple-rule bias is exacerbated if one combines it with applying the multiple rules across multiple datasets (<ref type="bibr" target="#b26">Yousefi et al., 2010</ref>) and then minimizes over both the classifier models and datasets; however, using multiple datasets can mitigate multiple-rule bias if performances are averaged over the datasets. In this case, each dataset is a sample from a feature-label distribution F k , k = 1,2,...,K, and our concern is with average performance over the K feature-label distributions. Assuming multiple error estimators, there are m classification rules being considered over the K feature-label distributions. Our interest is now with</p><formula>ε min est (K) = min ⎧ ⎨ ⎩ 1 K K k=1 ε 1,1,k est , 1 K K k=1 ε 1,2,k est ,..., 1 K K k=1 ε 1,s,k est , 1 K K k=1 ε 2,1,k est ,..., 1 K K k=1 ε r,s,k est ⎫ ⎬ ⎭ (9)</formula><p>where ε i,j,k est is the estimated error of classifier ψ i and the error estimation rule j on the dataset from feature-label distribution F k. The bias takes the form</p><formula>B(m,n,K) = E ε min est (K) − 1 K K k=1 E ε i min ,k true (10)</formula><p>where ε i min ,k true is the true error of classifier ψ i min on the dataset from feature-label distribution F k .The 'average' curves in<ref type="figure" target="#fig_4">Figures 5</ref>and 6 illustrate the effects of averaging. They show less estimation bias, less variance, smaller RMS and less comparative-performance bias when averaging is employed, as opposed to using the datasets individually. The situation is similar, albeit a bit more complicated, than the argument in<ref type="bibr" target="#b26">Yousefi et al. (2010)</ref>for averaging results over a large number of datasets. Here, the averaging is done to mitigate multiple-rule bias. It is possible to obtain theoretical results regarding the effect of averaging on the bias of Equation (10). In particular, if all error estimators are unbiased, then we prove in the Appendix A that lim K→∞ B(m,n,K) = 0. If one looks closely at the proof, it is clear that the unbiasedness assumption can be relaxed in each of the lemmas. In the first lemma, which shows that B(m,n,K) ≤ 0, we need only assume that none of the error estimators is pessimistically biased. In the second lemma, which shows that lim K→∞ B(m,n,K) ≥ 0, unbiasedness can be relaxed to weaker conditions regarding the expectations of the terms making up the minimum in Equation (9); however, these conditions are rather arcane and do not add much practical insight. Moreover, we are interested in close-to-unbiased error estimators, specifically, the cross-validation estimators, so that we can expect |B(m,n,K)| to diminish with averaging, even if the limit of B(m,n,K) does not actually converge to 0.</p><formula>(a) ( b) (c) ( d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Concluding remarks</head><p>From a practical standpoint, the results obtained in the present paper quantitatively demonstrate the large degree of overoptimism that results from comparing classifier rule models via their performances on a small dataset owing to the inaccuracy of error estimation on Page: 1681 1675–1683</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple-rule bias</head><formula>(a) ( b) (c) ( d)</formula><formula>, E m [C bias | m ]</formula><p>: resulted from the distributions of ε min est and ε i min true on the real data for single LOO error estimation, with respect to the collection size m. small samples. As the array of simulations show, optimistic bias accrues rapidly with even a small number of models being compared. We have observed from both simulations and theoretical analysis that the problem can be mitigated by averaging performances across a family of datasets; indeed, this is the recommendation that we put forth. The downside is that averaging eliminates the possibility of comparing classification rule performances on a single population. In fact, the latter possibility has been precluded at the outset by the experimental design: too small of a sample to obtain accurate error estimates. If there is only a single small sample, then the multiplerule bias precludes any conclusions whatsoever, whereas at least if a collection of datasets are employed, then one may be able to make a conclusion relative to the collection of populations (depending on the accuracy of the error estimator). Even with averaging, we must offer a word of caution. While the proposition we have proven regarding the convergence of B(m,n,K) is promising, like most distributionfree results it leaves open the rate of convergence, which in practice determines the number of datasets one must utilize to reduce the bias to some predetermined level. This leads us to some final comments. The concerns expressed regarding the difficulty of establishing performance advantages for proposed classification rules (<ref type="bibr" target="#b0">Boulesteix, 2010;</ref><ref type="bibr" target="#b18">Jelizarow et al., 2010;</ref><ref type="bibr" target="#b22">Rocke et al., 2009</ref>) reflect fundamental epistemological issues confronting bioinformatics as it addresses the high-throughput environment with limited sample sizes and limited statistical knowledge of how to deal with this new world (<ref type="bibr" target="#b7">Dougherty and Braga-Neto, 2006;</ref><ref type="bibr" target="#b12">Dougherty, 2008;</ref><ref type="bibr" target="#b20">Mehta et al., 2004</ref>). The problem addressed in this article arises from the bias and variance, and therefore the RMS, of error estimators. Very little is known about the performance of common error estimators, in particular, cross-validation. To take a salient example: LOO. Prior to 2009, all that was known about LOO for LDA and Gaussian class-conditional distributions were asymptotic expressions for the expectation and variance of the estimator in one dimension (<ref type="bibr" target="#b6">Davison and Hall, 1992</ref>). In 2009, the distribution of the LOO estimator was discovered in this model for an arbitrary dimension m without assuming a common variance for m = 1 and assuming a common covariance matrix with m &gt; 1 (<ref type="bibr" target="#b28">Zollanvari et al., 2009</ref>). Still, none of these results treated the joint distribution of the estimated and true errors, nor, in particular, the RMS. In 2010, the joint distribution was found exactly for m = 1 without assuming a common variance and an approximation was found for m &gt; 1 assuming a common covariance matrix (<ref type="bibr" target="#b29">Zollanvari et al., 2010</ref>). Besides the joint distribution via complete enumeration (<ref type="bibr" target="#b24">Xu et al., 2006</ref>) and the correlation (Braga<ref type="bibr" target="#b4">Neto and Dougherty, 2010</ref>) for multinomial discrimination, there are no other analytic results regarding the joint behavior of LOO with the true error. This dearth of results is striking considering that LOO was first proposed in 1968 (<ref type="bibr" target="#b19">Lachenbruch and Mickey, 1968</ref>), it has been used extensively, and the variance problems of LOO have been known from at least 1978 (<ref type="bibr" target="#b13">Glick, 1978</ref>). As for more complicated cross-validation estimators that require random resampling, essentially nothing is known. If lamentations regarding the lack of performance characterization in bioinformatics are to be abated, then much greater knowledge regarding error estimation must be discovered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A</head><p>We prove that if all error estimators are unbiased, then</p><formula>lim K→∞ B(m,n,K) = 0. Lemma A.1. If all error estimators are unbiased, then B(m,n,K) ≤ 0. Proof. Define the set S n ={S 1 n ,S 2 n ,...,S K n }, where S k n , k = 1,2</formula><p>,...,K is a random sample taken from the distribution F k for k = 1,2,...,K. Also, we can rewrite Equation (9) as</p><formula>ε min est (K) = min i,j ⎧ ⎨ ⎩ 1 K K k=1 ε i,j,k est ⎫ ⎬ ⎭ , (A.1)</formula><p>where i = 1,2,...,r and j = 1,2,...,s. Owing to the unbiasedness of the error estimators,</p><formula>E S k n [ε i,j,k est ]=E S k n [ε i,k true ]</formula><p>. Referring to Equations (10) and (A.1), we have B(m,n,K)</p><formula>= E S n [ε min est (K)]− 1 K K k=1 E S k n ε i min ,k true = E S n ⎡ ⎣ min i,j ⎧ ⎨ ⎩ 1 K K k=1 ε i,j,k est ⎫ ⎬ ⎭ ⎤ ⎦ − 1 K K k=1 E S k n ε i min ,k true ≤ min i,j ⎧ ⎨ ⎩ E S n ⎡ ⎣ 1 K K k=1 ε i,j,k est ⎤ ⎦ ⎫ ⎬ ⎭ − 1 K K k=1 E S k n ε i min ,k true = min i,j ⎧ ⎨ ⎩ 1 K K k=1 E S n ε i,j,k est ⎫ ⎬ ⎭ − 1 K K k=1 E S k n ε i min ,k true = min i,j ⎧ ⎨ ⎩ 1 K K k=1 E S k n ε i,j,k est ⎫ ⎬ ⎭ − 1 K K k=1 E S k n ε i min ,k true = min i ⎧ ⎨ ⎩ 1 K K k=1 E S k n ε i,k true ⎫ ⎬ ⎭ − 1 K K k=1 E S k n ε i min ,k true ≤ 0. (A.2)</formula><p>where the relations in the third and sixth lines result from Jensen's inequality and unbiasedness of the error estimators, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma A.2. If all error estimators are unbiased, then</head><formula>lim K→∞ B(m,n,K) ≥ 0. Proof. Let A i,j = 1 K K k=1 ε i,j,k est , T i = 1 K K k=1 E S k n ε i,k true .</formula><formula>(A.3)</formula><p>Owing to the unbiasedness of the error estimators,</p><formula>E S n [A i,j ]=T i ≤ 1.</formula><p>Without loss of generality, we assume T 1 ≤ T 2 ≤ ... ≤ T r. To avoid cumbersome notation, we will further assume that T 1 &lt; T 2 (with some adaptation, the proof goes through without this assumption). Let 2δ = T 2 −T 1 and</p><formula>B δ = ⎛ ⎝ s j=1 T 1 −δ ≤ A 1,j ≤ T 1 +δ ⎞ ⎠ min i =1,j A i,j &gt; T 1 +δ .</formula><formula>(A.4) Because |ε i,j,k est |≤1, Var S n [A i,j ]≤1/K. hence, for τ&gt;0, there exists K δ,τ such that K ≥ K δ,τ implies P(B δ (K)) &gt; 1−τ. Hence, referring to Equation (10), for K ≥ K δ,τ , E S n ε min est (K) = E S n ε min est (K) | B δ P(B δ ) +E S n ε min est (K) | B c δ P(B c δ ) ≥ E S n ε min est (K) | B δ P(B δ ) = E S n min j A 1,j P(B δ ) ≥ (T 1 −δ)(1−τ).</formula><formula>(A.5)</formula><p>Page: 1683 1675–1683</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiple-rule bias</head><p>Again referring to Equation (10) and recognizing thatSince δ and τ are arbitrary positive numbers, this implies that for any η&gt;0, there exists K η such that K ≥ K η implies lim K→∞ B(m,n,K) ≥ 0, which is precisely what we want to prove. Combining Lemmas A.1 and A.2, we have proven that lim K→∞ B(m,n,K) = 0 under the assumption that all the error estimators are unbiased.</p><formula>i min = 1 in B δ , for K ≥ K δ,τ , 1 K K k=1 E S k n ε i min ,k true = 1 K K k=1 E S k n ε i min ,k true | B δ P(B δ ) +E S k n ε i min ,k true | B c δ P(B c δ ) ≤ 1 K K k=1 E S k n ε 1,k true | B δ P(B δ ) +P(B c δ ) ≤ T 1 +τ.</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>[10:46 24/5/2011 Bioinformatics-btr262.tex]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.1.</head><figDesc>Fig. 1. Multiple-rule testing procedure on a single sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.2.</head><figDesc>Fig. 2. The joint distributions between ε min est and ε</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.4.</head><figDesc>Fig. 4. (a) The expected bias, Bias Av ; (b) the expected variance, Var Av ; (c) the expected RMS, RMS Av ; and (d) the expected comparative performance bias, E m [C bias | m ]: resulted from the distributions of ε min est and ε</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.5.</head><figDesc>Fig. 5. (a) The expected bias, Bias Av ; (b) the expected variance, Var Av ; (c) the expected RMS, RMS Av ; and (d) the expected comparative performance bias, E m [C bias | m ]: resulted from the distributions of ε min est and ε i min true on the real data for all 36 classification models, with respect to the collection size m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.6.</head><figDesc>Fig. 6. (a) The expected bias, Bias Av ; (b) the expected variance, Var Av ; (c) the expected RMS, RMS Av ; and (d) the expected comparative performance bias, E m [C bias | m ]: resulted from the distributions of ε min est and ε</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>ε i min true</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>ε i min true ]−E Sn [ε min est ]</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>ε i min true ] by ε min est. Another is the variance of ε min</figDesc><table>est . In fact, one should consider the root-mean-square (RMS) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>[A true | m ] and the mean estimated comparative advantage becomes E m [E Sn [A est | m ]].</figDesc><table>A est ]−A true because it measures over 
optimism in comparative performance. In the case of randomization, the 
true comparative advantage becomes E m </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><figDesc>0 and C 1 , having D features. Furthermore, by putting c equally likely subclasses in C 1 , each having its own distribution, one can model cases like different stages or subtypes of a cancer. Each sample point in C 1 belongs to one and only one of these subclasses. Features are categorized into two major groups, markers and non-markers. Markers resemble genes causing disease or susceptibility to disease. The groups have different class-conditional distributions for the two classes. They can be further categorized into two different types: global and heterogeneous markers. Global markers are homogeneously distributed among the two classes with D gm-dimensional Gaussian distributions and parameters (µ gm 0 ,, gm 0 ) for class 0 and (µ gm 1 ,, gm 1 ) for class 1, where D gm is the total number of global markers. Heterogeneous markers are divided into c subclasses within class 1. Each subclass is associated with D hm mutually exclusive heterogeneous markers having D hm-dimensional Gaussian distributions with parameters (µ hm 1 ,, hm 1 ). The sample points not belonging to this particular subclass are considered to have D hm-dimensional Gaussian class-conditional distributions with parameters (µ hm 0 ,, hm 0 ). Assuming that global and heterogeneous markers possess identical covariance structures, we use</figDesc><table>; Shmulevich and 
Dougherty, 2007). As explained in Hua et al. (2009), although the model does 
not embrace all details of the experimental procedures, it is general enough to 
include major aspects and various complexity levels suitable for simulation 
of real-world scenarios. Sample points are taken from two equally likely 
classes, C </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><figDesc>Table 1. Distribution model parameters = 20 Heterogeneous markers D hm = 50 High-variance non-markers D hv = 2000 Low-variance non-markers D lv = 17880</figDesc><table>Parameters 
Values/description 

Mean 
m 0 = 0.23,m 1 = 0.8 (equal variance) 
m 0 = 0.11,m 1 = 0.9 (unequal variance) 
Variances 
σ 2 
0 = 0.6 2 ,σ 2 
1 = 0.6 2 (equal variance) 
σ 2 
0 = 0.6 2 ,σ 2 
1 = 1.2 2 (unequal variance) 
Block size 
l = 5 
Features 
D = 20000 
Feature block correlation 
ρ = 0.8 

Subclasses 
c = 2 
Global markers 
D gm </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><figDesc>Table 2. A summary of the real datasets used in this study</figDesc><table>Dataset 
Dataset type 
Feature|sample size 

Yeoh et al. (2002) 
Pediatric ALL 
5077|149/99 
Zhan et al. (2006) 
Multiple myeloma 
54613|156/78 
Chen et al. (2004) 
HCC 
10237|75/82 
Natsoulis et al. (2005) 
Drugs response on rats 
8491|120/61 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="true"><figDesc>Table 3. Classifier rule models considered in this study</figDesc><table>Classification rule 
Feature selection 
Error estimation 

3NN 
t-test 
5-fold cross-validation 
LDA 
t-test + SFS 
10-fold cross-validation 
DLDA 
LOO 
NMC 
L-SVM 
RBF-SVM 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><figDesc>(LDA, LOO), (LDA, CV5), (3NN, LOO), (3NN, CV5). There are two true errors, ε LDA true and ε 3NN true , four estimated errors, ε LDA,LOO est , ε LDA,CV5 est , ε 3NN,LOO est and ε 3NN,CV5 est , and ε min est is the minimum of the four estimated errors.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><figDesc>[C bias | m ]: resulted from the distributions of ε min est and ε i min</figDesc><table>Fig. 3. (a) The expected bias, Bias Av ; (b) the expected variance, Var Av ; (c) 
the expected RMS, RMS Av ; and (d) the expected comparative performance 
bias, E m true on the 
synthetic data for all 36 classification models, with respect to the collection 
size m. 

</table></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="1"> r −1  i =i min ε i,j min est , (7) where we use the error estimator associated with the pair (i min ,j min ) for which the minimum estimated error is obtained (assuming that this would</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We would like to thank the High-Performance Biocomputing Center of TGen for providing the clustered computing resources used in this study; this includes the Saguaro-2 cluster supercomputer, a collaborative effort between TGen and the ASU Fulton High Performance Computing Initiative. Funding: National Institutes of Health grant (Saguaro-2 cluster, 1S10RR025056-01, in part) and the National Science Foundation (CCF-0634794).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict</head><p>of Interest: none declared.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Over-optimism in bioinformatics research</title>
		<author>
			<persName>
				<forename type="first">A.-L</forename>
				<surname>Boulesteix</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="437" to="439" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimal classifier selection and negative bias in error rate estimation: an empirical study on high-dimensional prediction</title>
		<author>
			<persName>
				<forename type="first">A.-L</forename>
				<surname>Boulesteix</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Strobl</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med. Res. Meth</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Is cross-validation valid for small-sample microarray classification?</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<forename type="middle">M</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="374" to="380" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Exact performance of error estimators for discrete classifiers</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<forename type="middle">M</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1799" to="1814" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Exact correlation between actual and estimated errors in discrete classification</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<forename type="middle">M</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="407" to="412" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Novel endothelial cell markers in hepatocellular carcinoma</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Chen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Modern Pathol</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1198" to="1210" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">On the bias and variability of bootstrap and crossvalidation estimates of error rate in discrimination problems</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">C</forename>
				<surname>Davison</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Hall</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="279" to="284" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Epistemology of computational biology: mathematical models and experimental prediction as the basis of their validity</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">U</forename>
				<forename type="middle">M</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biol. Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="65" to="90" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="46" to="70" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName>
				<surname>Page</surname>
			</persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1682" to="1675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">R</forename>
				<surname>Yousefi</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Validation of computational methods in genomics</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Genomics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">On the epistemological crisis in genomics</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Genomics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="69" to="79" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Additive estimators for probabilities of correct classification</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Glick</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="211" to="222" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Decorrelation of the true and estimated classifier errors in high-dimensional settings</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Hanczar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Bioinform. Syst. Biol</title>
		<imprint>
			<biblScope unit="page">38473</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">On the comparison of classifiers for microarray data</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Hanczar</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="29" to="39" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Small-sample precision of ROC-related estimates</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Hanczar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="822" to="830" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Performance of feature selection methods in the classification of high-dimensional data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Hua</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="409" to="424" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Over-optimism in bioinformatics: an illustration</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Jelizarow</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Estimation of error rates in discriminant analysis</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">A</forename>
				<surname>Lachenbruch</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">R</forename>
				<surname>Mickey</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards sound epistemological foundations of statistical methods for high-dimensional biology</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Mehta</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="943" to="947" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Classification of a large microarray data set: algorithm comparison and analysis of drug signatures</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Natsoulis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="724" to="736" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Papers on normalization, variable selection, classification or clustering of microarray data</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">M</forename>
				<surname>Rocke</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="701" to="702" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<monogr>
		<title level="m" type="main">Genomic Signal Processing</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Shmulevich</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Confidence intervals for the true classification error conditioned on the estimated error</title>
		<author>
			<persName>
				<forename type="first">Q</forename>
				<surname>Xu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technol. Cancer Res. Treat</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="579" to="589" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Classification, subtype discovery, and prediction of outcome in pediatric acute lymphoblastic leukemia by gene expression profiling</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">J</forename>
				<surname>Yeoh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Cell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="133" to="143" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Reporting bias when using real data sets to analyze classification performance</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">R</forename>
				<surname>Yousefi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="68" to="76" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">The molecular classification of multiple myeloma</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Zhan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Blood</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="2020" to="2028" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">On the sampling distribution of resubstitution and leaveone-out error estimators for linear classifiers</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Zollanvari</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2705" to="2723" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Joint sampling distribution between actual and estimated classification errors for linear discriminant analysis</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Zollanvari</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="784" to="804" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>