
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining Structure-based variable selection for survival data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010">2010 . 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Vincenzo</forename>
								<surname>Lagani</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Computer Science</orgName>
								<orgName type="department" key="dep2">Foundation for Research and Technology -Hellas (FORTH)</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Ioannis</forename>
								<surname>Tsamardinos</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Crete</orgName>
								<address>
									<settlement>Heraklion</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining Structure-based variable selection for survival data</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Bioinformatics BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">16</biblScope>
							<biblScope unit="issue">26 15</biblScope>
							<biblScope unit="page" from="1887" to="1887"/>
							<date type="published" when="2010">2010 . 2010</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btq261</idno>
					<note type="submission">Received on March 10, 2010; revised on April 29, 2010; accepted on May 15, 2010</note>
					<note>Associate Editor: Martin Bishop Contact: vlagani@ics.forth.gr Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Variable selection is a typical approach used for molecular-signature and biomarker discovery; however, its application to survival data is often complicated by censored samples. We propose a new algorithm for variable selection suitable for the analysis of high-dimensional, right-censored data called Survival Max–Min Parents and Children (SMMPC). The algorithm is conceptually simple, scalable, based on the theory of Bayesian networks (BNs) and the Markov blanket and extends the corresponding algorithm (MMPC) for classification tasks. The selected variables have a structural interpretation: if T is the survival time (in general the time-to-event), SMMPC returns the variables adjacent to T in the BN representing the data distribution. The selected variables also have a causal interpretation that we discuss. Results: We conduct an extensive empirical analysis of prototypical and state-of-the-art variable selection algorithms for survival data that are applicable to high-dimensional biological data. SMMPC selects on average the smallest variable subsets (less than a dozen per dataset), while statistically significantly outperforming all of the methods in the study returning a manageable number of genes that could be inspected by a human expert. Availability: Matlab and R code are freely available from</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Survival-analysis studies the occurrence and timing of events of interest. Often in medicine and biology, the event of interest is death, hence the name survival analysis; however, the field is much broader as the event of interest could be disease relapse or any other event. A typical characteristic of survival data is that they are often right-censored, i.e. the data may record subject i as alive (not having experienced the event) until time f i (follow-up time) but may not contain the exact time t i the event actually occurred. Excluding the censored data from the analysis systematically excludes the instances with higher probability of survival, and thus severely skews the results. Thus, typical variable selection and regression methods cannot be applied to model the time-to-event T. Unfortunately, it is often the case that analysis techniques cannot * To whom correspondence should be addressed. be trivially extended for survival data. Arguably, this is one reason why only a handful of variable selection methods are available for this type of data. Variable selection's primary aim is often to improve understanding of the data-generating process. For example, a biologist may be more interested in the genes predictive of survival, than in the actual predictive model. Presumably, the variables involved in an optimally predictive, minimal, irreducible model carry unique information that provides insight into the data-generating process (we discuss this issue in more detail in the sections to follow). Given the above observations, algorithms with clear semantics for the selected variables, and optimality properties are desirable. In this article, we adapt and evaluate an existing, successful and scalable method for variable selection called the Max–Min Parents and Children algorithm (MMPC;<ref type="bibr" target="#b41">Tsamardinos et al., 2003</ref><ref type="bibr" target="#b44">Tsamardinos et al., , 2006</ref>) to survival analysis; the method is based on the theory of Bayesian networks (BN) and Markov blanket-based variable selection (<ref type="bibr" target="#b41">Tsamardinos and Aliferis, 2003</ref>) that has been recently shown to perform exceptionally well with complete (non-censored) data (<ref type="bibr" target="#b6">Aliferis et al., , 2010</ref>). The Max–Min part of the name refers to the heuristic used in the algorithm regarding the order of consideration of the variables; the Parents and Children part of the name refers the structural interpretation of the selected variables: under certain conditions, they are the parents and children (adjacent nodes) of T in the BN representing the data distribution. We call the novel algorithm Survival MMPC (SMMPC). Under certain conditions, the selected nodes consist of an optimal set for prediction and minimum in terms of size. In other words, the selected variables contain no irrelevant (noise) or superfluous (redundant) variables, facilitating the interpretation of the model. For microarray data, the selected gene expressions always provide predictive information for survival, even when any other combination of gene expressions is known. Under certain conditions, this implies that no other gene is expected to be causally mediating the dependence. The set of parents and children is also connected to the set of direct causes of T ; the causal interpretation of the returned variables is also discussed. SMMPC is evaluated against a variety of prototypical and state-of-the-art variable selection algorithms that can scale up to the dimensionality often encountered in bioinformatics: Univariate Association Filtering, Forward Stepwise Selection, Bayesian Variable Selection (BVS) based on greedy search, Lasso Cox Regression (used as selection method) and Bayesian Selection based on Markov-Chain Monte-Carlo (MCMC). The variable selection methods are coupled with several survival regression methods. The evaluation is performed on several real, publicly available, highdimensional, gene expression survival datasets. The results include</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.Lagani and I.Tsamardinos</head><p>Algorithm 1 SMMPC 1. procedure SMMPC(T , k, a) Input: T the target variable to predict, the maximum size of a conditioning set k, the threshold for rejecting independence a. Output: A subset of variables in V 2.an ordering of efficacy of both variable selection and regression methods as well as an analysis of the stability and learning curves of the methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SMMPC</head><p>We now present the SMMPC algorithm, and subsequently we discuss its theoretical properties and sufficient conditions for optimality and causal interpretation. The algorithm is essentially MMPC without the symmetry correction, as we call it, enhanced with a statistical test of conditional independence for censored data; it was first appeared in<ref type="bibr" target="#b41">Tsamardinos et al. (2003)</ref>for complete data and in more detail in<ref type="bibr" target="#b44">Tsamardinos et al. (2006)</ref>. The algorithm accepts the target variable T and two tuning parameters, k the maximum size of conditioning set and a the level for accepting dependence. We denote with V the predicting variables, T the time-to-event and C the time-to-censoring. What is observed is F = min(T ,C), the follow-up time, i.e. the time for which we have been gathering information about the patient. The censoring status = 1(T &lt; C) takes value 1 for data that are observed (F = T ) and 0 for the cases are censored (F = C). Survival data D can be modeled as a set of m subjects indexed by i each represented by the triplet v i ,f i ,δ i , where v i is the vector of n predicting variables, f i the follow-up time and δ i the censoring status. The objective of SMMPC is to find the neighbors of T in a minimal BN representing the data distribution. Under certain conditions (see Section 3), this coincides with a minimum-size set of variables needed in order to optimally predict T. A basic component of SMMPC is a test of conditional independence using censored data. Let us denote with Ind(X;T |Z), X ∈ V, Z ⊆ V the conditional independence of X with T given the variable set Z and with Dep(X;T |Z) ≡¬Ind(X;T |Z) the corresponding dependence. The null hypothesis of the conditional independence test is that Ind(X;T |Z) holds in the data distribution; the dependence is the alternative hypothesis. We denote with p XT |Z the P-value provided by the test. If p XT |Z ≤ a, where a is a user-defined threshold the null is rejected and we accept Dep(X;T |Z). Otherwise, the null hypothesis of independence Ind(X;T |Z) is accepted. The test of independence is the only place in the algorithm where the actual data are used. Intuitively, it seems that if a variable becomes independent of T given some Z it is superfluous for predicting T, i.e. given Z, X contains no additional information for T. SMMPC employs this basic premise to exclude from selection of all variables X for which it can find a Z ⊆ V \{X}, s.t., Ind(X;T |Z). The number of possible subsets for each variable is exponential and a brute-force approach is intractable. However, SMMPC employs BN theory to efficiently identify a Z for which Ind(X;T |Z) if one exist under the conditions stated in Section 3, and make the algorithm practical for even very high-dimensional datasets. In practice, only the conditioning sets with |Z|≤k are considered, because when |Z| is large, the statistical power of the tests is low. Prior experience with this type of algorithms (<ref type="bibr" target="#b6">Aliferis et al., 2010;</ref><ref type="bibr" target="#b44">Tsamardinos et al., 2006</ref>) as well as the results in this article shows that most distributions are captured by sparse networks; this in turn implies that a small value of k is sufficient to filter out most variables. The basic premise that if ∃Z, s.t., Ind(X;T |Z), then X is superfluous for predicting T is not always correct. Even if Ind(X;T |Z), it could still be the case that Dep(X;T |Z∪{W }) for some other W ∈ V and X may become necessary for optimal prediction given additional variables. This case is discussed in detail in the sections to follow, where sufficient conditions for soundness are provided. The pseudo-code of SMMPC is shown in Algorithm 1. It begins with a set of candidate variables to consider R, initially set to all possible predicting variables V. The algorithm also maintains a set of currently selected variables S initially set to the empty set. It then removes from R any variable that becomes independent of T condition on some subset Z ⊆ S. Subsequently, it heuristically selects and moves a member of R into S and continues until R =∅. As a final step SMMPC removes from S any variables X for which ∃Z ⊆ S \{X}, s.t., Ind(X;T |Z). SMMPC selects as the next variable to include in the S the one with the maximum–minimum association with T over all subsets Z of S (hence the name Max–Min). Association of T with X given Z is measured by −p XY |Z , so the smaller the P-value the larger the association. Intuitively, the heuristic selects next the variable that remains mostly associated with T despite our best efforts (i.e. after conditioning on all subsets of S) to make the variable independent of T. Recent theoretical results (<ref type="bibr" target="#b42">Tsamardinos and Brown, 2008</ref>) also discovered another interesting interpretation: the Max–Min heuristic selects next the variable that minimizes an upper bound on the false discovery rate of the output (i.e. the false discovery rate of the identification of the Parents and Children set). The Max–Min heuristic is important both for the efficiency of the algorithm and the quality of the output but it does not affect the asymptotic correctness properties. In terms of time complexity, assuming the size of the final output PC is about the maximum size of S in any iteration and some optimizations are in place (<ref type="bibr" target="#b44">Tsamardinos et al., 2006</ref>) the algorithm performs O(|V||PC| k ) tests of independence. Thus, for problems with sparse structure (few neighbors of T ), its complexity grows linearly to the number of variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Test of conditional independence</head><p>The null hypothesis Ind(X;T |Z) of the conditional independence test implies that X is not necessary for predicting T when Z (and only Z) is given; under this condition, the conditional independence test can be thought of as a procedure for selecting the best between two nested models: a model predicting T given Z∪{X} and a model with only Z as covariates. We employ and evaluate the log-likelihood ratio and the local score test (<ref type="bibr" target="#b25">Klein and Moeschberger, 2003</ref>), both on them based on the widely used Cox regression model (<ref type="bibr" target="#b13">Cox, 1972</ref>). Extensive experimentations showed that the former test is able to produce better results than the local score test by selecting almost the same number of variables in average, thus we retained only the log-likelihood ratio test for successive experiments. Details about tests implementation and comparative experiments are described in the Supplementary Section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 1889 1887–1894</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SMMPC</head><p>algorithm. SMMPC, while conceptually simple, is based on relatively wellunderstood BN and Causal BN theory that can be used to analyze its properties and semantics. An expanded and more in-depth version of this section is included in the Supplementary Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BN structural interpretation</head><p>MMPC and its extension, SMMPC, return variables with a specific structural interpretation in the BN representing the data distribution. We do not distinguish between the two versions of the algorithm when not necessary. We briefly review the related BN theory. A BN G,P is a directed acyclic graph G ==V,E and a probability distribution P, both defined on the set of random variables (nodes) V. In the remaining of this section, we assume V is the full set of variables, including T. G and P are such that the Markov condition is satisfied: every variable is independent of any subset of its non-descendant variables conditioned on its parents (<ref type="bibr" target="#b30">Pearl, 2000</ref>). A faithful BN is one where in addition, only the independencies that are entailed by the Markov condition hold in P. In other words, the independencies stem directly from the structure (graph) of the network and are not accidental properties of the parameters. Distributions P for which there exist a faithful network G,P are also called faithful. When choosing parameters randomly for a given structure, the probability of obtaining a non-faithful distribution has Lebesgue measure zero and so one expects them to be 'rare' (although this may not hold in systems obtained by Natural Selection;<ref type="bibr" target="#b11">Brown and Tsamardinos, 2008</ref>). For a given distribution P, there may be many graphs G such that the BN G,P is faithful. However, the set of neighbors of a variable T (parents and children of T ) is unique in all networks faithful to the same distribution and denoted by N (T ). Let us also define the extended neighbor set of order k, denoted with EN k (T ) as the set of variables that are dependent on T conditioned on any subset of the N (T ) of size at most k. It is easy to see that EN k+1 (T ) ⊆ EN k (T ). We will denote with EN (T ) the unrestricted cases for infinite k. In faithful distributions, it holds that N (T ) ⊆ EN k (T ) for any k. In<ref type="bibr" target="#b6">Aliferis et al. (2010)</ref>, we show the following:</p><formula>N (T ) ⊆ MMPC(T ,k,a) ⊆ EN k (T ).</formula><p>It is possible that MMPC outputs variables not in N (T ) under the assumptions, i.e. N (T ) ⊂ MMPC(T ,∞). First, we note that this type of false positive has to be a descendant of T in the network; second, they can be removed to end up with the true N (T ) by performing additional executions of MMPC and by employing what is called the 'symmetry correction'. In<ref type="bibr" target="#b6">Aliferis et al. (2010)</ref>, it was determined empirically that the overhead for the symmetry correction does not outweigh the theoretical benefits and that these cases are rare. Thus, we do not employ the symmetry correction in the algorithm and we will assume that N (T ) = EN (T ) and so MMPC(T ,∞,a) outputs N (T ) under the conditions above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimality properties and relation to the Markov blanket</head><p>In this section, we examine conditions under which the selected variable subset N (T ) is optimally predictive and minimum-in-size. An emerging and principled approach in variable selection is based on identifying the Markov blanket of the variable T to predict (hereafter, the target variable). The Markov blanket of T (denoted as MB(T )) relative to a set of measured variables V is defined as a minimal set conditioned on which all other variables in V become independent of T [this is the Markov Boundary in the terminology of<ref type="bibr" target="#b30">Pearl (2000)</ref>]: P(T |V \{T }) = P(T |MB(T )). Thus, all information for optimally predicting T is contained within the MB(T ) and, therefore, it may seem that this is a minimal set of variables required for optimal prediction. The latter statement is not true in general, however, as the learner and the performance metric used are important. For the MB(T ) to be the solution to the variable selection problem as it was defined above, two conditions are sufficient (<ref type="bibr" target="#b41">Tsamardinos and Aliferis, 2003</ref>):</p><p>(1) The learner used to construct the prediction model can correctly estimate the distribution P(T |MB(T )).The above conditions often hold or hold approximately in many typical applications. Specifically in our experiments, we use several regression methods and optimize their parameters trying to find the one that best matches the distribution in order to satisfy Condition (1). In addition, the performance metrics used<ref type="bibr">[</ref>are optimized only when full knowledge of T 's conditional distribution is optimized. Identifying the MB(T ) in the general case is a difficult problem. New theoretical results connect the MB(T ) with the structure of the BN capturing the data distribution and gave rise to time and sample-efficient algorithms for identifying the Markov blanket in faithful distributions represented by sparse networks; such algorithms include the Max–Min Markov blanket (MMMB;<ref type="bibr" target="#b41">Tsamardinos et al., 2003</ref>) and the HITON () algorithms. We now briefly present some of these results. Recall that N (T ) denotes the set of neighbors of T in any graph G faithful to the data distribution, while we use S(T ) to denote the set of all parents of common children of T not in N (T ). By definition, N (T )∩S(T ) =∅. In a faithful distribution it can been shown that MB(T ), N (T ) and S(T ) are unique. Thus, MB(T ) is not only minimal but also minimum; in addition, the Markov blanket has a graphical interpretation and specifically MB(T ) = N (T )∪S(T )</p><formula>(2)</formula><p>i.e. the Markov blanket of T is the set of parents, children and parents of common children with T. In the Section 3.1, we show that SMMPC returns N (T ) under the assumptions stated, a subset of the Markov blanket of T. Let us now turn our attention to the remaining part, the S(T ) variables. Recall, the basic premise for selecting variables using MMPC is that if a variable X becomes independent of T given some Z it is superfluous for predicting T. In fact, variables in S(T ) are exactly the variables for which this assumption fails: for an X ∈ S(T ), there exist a subset Z for which Ind(X;T |Z) (otherwise X would have an edge to or from T according to the theorem) but X is in MB(T ), and thus required for optimal prediction. There do exist techniques to identify S(T ) but would require more complicated method for tests of conditional independence with the censored-variable T as a covariate; as a first attempt at structure-based variable selection with censored data, SMMPC performs variable selection by approximating the Markov blanket of T with N (T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Causal interpretation</head><p>In recent work (<ref type="bibr" target="#b6">Aliferis et al., 2010</ref>), we have shown that in simulated studies with known causal structure, prominent-variable-selection algorithms indeed select subsets that lead to models with close-to-optimal predictive performance; unfortunately, however, the selected variables are distributed almost at random on the causal graph and so they could not be used to understand the domain. One could argue they are actually misleading. BNbased algorithms on the other hand, such as MMPC, output variables that also have a specific causal interpretation, under certain conditions discussed below. We will first assume the standard causal discovery setting of<ref type="bibr" target="#b30">Pearl (2000)</ref>. That is, we assume that there exists a Causal BN defined on the observed variables V, that faithfully captures the data distribution. In that network, an edge X → T means that X is directly causing (affecting) the survival time: if one manipulates X (e.g. increase medication), a change in the distribution of T will be observed. In this case, the N (T ) is the set of neighbors of T in the causal network (and in any other network faithful to the distribution) and so</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.Lagani and I.Tsamardinos</head><p>it is the set of direct causes and direct effects of T. Since in our setting of survival analysis all variables are measured prior to T , the output of MMPC in this case can be interpreted as the set of direct causes of survival. In addition, since T has no causal effects, S =∅ and so MB(T ) = N (T ). In summary,The assumption of the distribution being faithful to a Causal BN has at least three subtle and implicit subparts. First, the causal mechanism is assumed acyclic, thus there should be no feedback loops. Second, a causal interpretation of the Markov condition (known as the Causal Markov condition) may be violated due to measurement noise. Third, the assumption of a faithful Causal BN for the distribution implies no hidden confounders, i.e. latent variables that are causes of two modeled variables. This assumption is called Causal Sufficiency in the terminology of<ref type="bibr" target="#b37">Spirtes et al. (2000)</ref>. The implications of violations of these assumptions are discussed in detail in the Supplementary Material, as well as techniques to relax the assumptions. While possible violations of the assumptions require care in interpreting the output, we note that non-causally based algorithms fail to return causally meaningful variables even in simple simulated problems, where all of the above standard assumptions hold by design (<ref type="bibr" target="#b6">Aliferis et al., 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">REVIEW OF RELATED WORK</head><p>Survival data analysis has been an active field in statistics for decades and dozens of regression algorithms have appeared in the literature. The recent emergence of high-dimensional, biological datasets presents new challenges to all aspects of analysis (see van<ref type="bibr" target="#b46">Wieringen et al., 2009</ref>; Witten and Tibshirani, 2009 for a review of recent methods). A standard approach to deal with the high-dimensionality is to search for models that attempt to minimize both the error and the number of the coefficients of the variables, e.g. Survival Trees (<ref type="bibr" target="#b23">Hothorn et al., 2004</ref>), Ridge Cox Regression (<ref type="bibr" target="#b22">Hoerl and Kennard, 2000</ref>) and Lasso Cox Regression (<ref type="bibr" target="#b39">Tibshirani, 1997a</ref>). Another approach to address high-dimensional, survival analysis is to first reduce the dimensionality of the problem before applying regression. Dimensionality reduction methods for survival analysis tasks in bioinformatics include clustering algorithms (<ref type="bibr" target="#b19">Hastie et al., 2001</ref>), principal component analysis (<ref type="bibr" target="#b27">Li and Gui, 2004</ref>) or partial least-square procedures (<ref type="bibr" target="#b28">Nguyen and Rocke, 2002</ref>); see Nguyen and Rojo (2009) for an empirical comparison among different dimension reduction algorithms. General dimensionality reduction often transforms the data to lower dimensional spaces that are hard to interpret in terms of the original, input variables and measured quantities. Thus, dimensionality reduction is not appropriate in general when the goal of the analysis is to understand the effect of the measured quantities to the outcome variable. A restricted form of dimensionality reduction is variable selection, where the data are projected to a lower dimension space consisting of a subset of the original variables. Variable selection for regression and classification has been an intense field of study with hundreds of published algorithms, conferences and competitions (<ref type="bibr" target="#b17">Guyon et al., 2004</ref>). Unfortunately, it is often the case that these algorithms cannot trivially be adapted to censored, survival data; thus, the number of available variable selection algorithms for high-dimensional, survival data is relatively low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">COMPARATIVE EVALUATION</head><p>The comparative evaluation aims to address several questions of interest regarding regression and variable selection algorithms for high-dimensional, survival analysis tasks in bioinformatics. The primary goal of the evaluation is to validate the advantages of SMMPC over the prior state-of-theart in variable selection for high-dimensional, survival analysis tasks in bioinformatics. Each variable selection algorithm is coupled with several regression algorithms to identify the best combination. Interesting conclusions are also derived about the performance of regression algorithms and their interaction with variable selection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Variable selection algorithms</head><p>We consider general dimensionality reduction methods as out of the scope of this article, and focus on variable selection algorithms for high-dimensional survival data. We try to include as many methods as possible that have been applied to high-dimensional biological data. In addition, we found the BVS method in Faraggi and Simon (1998) interesting and intriguing; we have made simple adaptations to it so as to be applicable to high-dimensional data. A list of algorithms now follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">No Selection</head><p>We include No Selection (NS) (retain all original variables) as a baseline to compare against the performance of other methods. No parameter needs to be specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">SMMPC</head><p>This is the procedure introduced in Section 2. The parameters to set (see Algorithm 1) are k the maximum-size conditioning set allowed and a the significance level threshold for rejecting conditional independence. The parameter values have been optimized within k ∈{2,3} and a ∈{0.05,0.10,0.15}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Univariate Selection</head><p>The variables are ranked in descending order of pairwise association with the time-to-event T , and the most associated variables are selected. We used the implementation by<ref type="bibr" target="#b9">Bovelstad et al. (2007)</ref>. The selected variables are the ones with p XT |∅ ≤ a, where a is a threshold provided as a parameter. The parameter value has been optimized within a ∈{0.05,0.10,0.15}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Forward Selection</head><p>The method begins with the empty set as the selected predictors S. In each subsequent step, it adds the variable X that maximizes the association of T conditioned on (i.e. in the context of) S, i.e. minimizes p XT |S. We used the implementation by<ref type="bibr" target="#b9">Bovelstad et al. (2007)</ref>; this specific implementation of Forward Selection (FS) continues to add variables as long as |S|≤π ·m, where π is a parameter and m the number of samples. The parameter value has been optimized within π ∈{0.01,0.05,0.1} (the default value of the released code is 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5">BVS Prime</head><p>The original BVS method first appeared in<ref type="bibr" target="#b15">Faraggi and Simon (1998)</ref>; unfortunately, this method requires performing intensive matrix operations, and does not scale 'as is' to high-dimensional data. Hence, in our implementation we introduced some minimal modifications in order to make the algorithm applicable to the tasks in our study: (a) perform Univariate Selection (US) to reduce the number of variables to consider; select only the top N variables (we use N = 100 in our experiments), (b) a FS procedure substitutes the backward-selection procedure, (c) the ridge Cox regression (see Supplementary Section 3) was used instead of the Cox regression, due to its robustness to high-dimensional data; a fixed penalty term λ = 0.001 is used to fit the model. We name the resulting method BVS Prime (BVS ). The parameter σ that we optimize is the hyperparameter corresponding to the SD of the priors of the coefficients. It is optimized within the set {0.1,0.5,1} (the authors suggest values between 0.1 to 0.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.6">MCMC selection</head><p>This method is also based on Bayesian statistics and is introduced in<ref type="bibr" target="#b34">Sha et al. (2006)</ref>. It defines the prior distribution of the coefficients b as well as S, the selected variables over an Accelerated Failure Time (AFT) model. It then employs the Metropolis–Hastings method (<ref type="bibr" target="#b20">Hastings, 1970</ref>) to sample from the posterior distribution of b and S. As the authors suggest, we select the most probable a posteriori S as the output of the method. We employ the implementation provided by the authors.MCMC requires the specification of a large number of parameters; we set all but one to their default values; we specify c the expected number of selected variables to the number selected by SMMPC on the same task (see Supplementary Section 7 for further details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 1891 1887–1894</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SMMPC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.7">Lasso Selection</head><p>First proposed in Tibshirani (1997b), the Lasso algorithm adds a penalty term to the log partial likelihood of the Cox regression model: L(b)−w||b|| 1. Penalizing the L1-Norm shrinks the coefficients of redundant variables towards zero; thus, variables to be eliminated can be easily identified. The major drawback of this method is that it requires the solution of a non-derivable optimization problem, leading to elevated computational times. We used the implementation provided by<ref type="bibr" target="#b36">Sohn et al. (2009)</ref>, that is considerably faster than other freely available codes. The only parameter to be optimized is w, i.e. the weight that regulates the influence of the penalty term; we varied its values within the set {0.1,1,10} (1 was the default value of the used implementation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Regression algorithms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As demonstrated in Tsamardinos and Aliferis (2003) and Kohavi and John</head><p>(1997), the interplay between variable selection method and the learner is important to identify the smallest variable subset with optimal predictive power. Thus, we couple each variable selection method with several regression methods in order to evaluate their performance. In particular, we employ Cox regression, Ridge Cox regression (<ref type="bibr" target="#b22">Hoerl and Kennard, 2000</ref>), AFT models, Random Survival Forest (RSF; see<ref type="bibr" target="#b10">Breiman and Schapire, 2001</ref>) and Support Vector Machine Censored Regression (SVCR;<ref type="bibr" target="#b35">Shivaswamy et al., 2007</ref>), optimizing the performances of each regressor on a variety of parameters. We exclude Lasso regression (as a regressor, not as a variable selection method) as dominated by Ridge Cox Regression in this domain and task (<ref type="bibr" target="#b9">Bovelstad et al., 2007</ref>). Supplementary Section 3 reports an exhaustive description of regressors and parameters that are optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Dataset description</head><p>We identify several micro-array, gene expression, independent public datasets of survival studies with censored outcomes used in prior comparative studies (<ref type="bibr" target="#b7">Bair and Tibshirani, 2004;</ref><ref type="bibr" target="#b46">van Wieringen et al., 2009</ref>). Summary statistics and references are reported in<ref type="figure" target="#tab_1">Table 1</ref>. For all but the last dataset, we consider the data as preprocessed by the authors; we log-normalize the data of the last dataset (<ref type="bibr" target="#b8">Beer et al., 2002</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance metrics</head><p>Metrics of performance in the case of survival data is more complicated due to censorship: the error can only be computed exactly if the case is not censored. Thus, several specially designed evaluation metrics have been proposed in the literature, such as the Time-Depending area under the curve (AUC) (<ref type="bibr" target="#b21">Heagerty et al., 2000</ref>), the Weighted Classification Accuracy (<ref type="bibr" target="#b31">Ripley and Ripley, 1998</ref>), the CI (<ref type="bibr" target="#b18">Harrel, 2001</ref>) and the IBS (<ref type="bibr" target="#b16">Graf et al., 1999</ref>). We employ the latter two in this study.</p><p>The CI measures the percentage of pairs of subjects correctly ordered by the model in terms of their expected survival time. Notably, we can determine t i &gt; t j only when f i &gt; f j and δ j = 1, and similarly for the reverse relation. The pairs for which neither t i &gt; t j nor t i &lt; t j can be determined are excluded from the calculation of CI. When there are no censored data, the CI is equivalent to the Area Under the Receiving Operating Characteristic curve. Thus, a model ordering pairs at random (without use of the predicting variables) is expected to have a CI of 0.5, while perfect predictions would lead to a CI of 1. The Brier Score BS(t) measures the squared difference between the predicted survival probability and the observed outcome at time t, weighted for the loss of information due to the presence of censorship. The IBS consists in IBS = max (T ) −1 max (T ) 0 BS (t )dt. IBS measures the estimation error assigning a value of 0 when the distribution of S(T |v i ) is exactly estimated, while IBS = 0.25 is expected when predictions are random.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Parameter optimization and estimation of performance</head><p>A common procedure for parameter optimization is the use of a hold-out validation set. Each parameter combination is employed to learn a model on the training set and its performance is measured on the validation set. The best-performing parameter combination is then selected and a final model is learnt over all data (training plus validation). The best performance on the validation set is the maximum performance observed, and so it follows an extreme distribution. If a number of parameter combinations are attempted it is likely that the maximum observed performance is optimistic (the estimation is biased upwards; see Jensen and Cohen, 2000 for a discussion). Thus, a second hold-out set, a test set, is required for an unbiased estimation of performance. A drawback of the above protocol is that a portion of the data is not used for training. This is partially overcome by generalizing this procedure using cross-validation to what is known as nested N-fold crossvalidation (<ref type="bibr" target="#b6">Aliferis et al., 2010</ref>; Dudoit and van der<ref type="bibr" target="#b14">Laan, 2005;</ref><ref type="bibr" target="#b38">Statnikov et al., 2005</ref>). An outer cross-validation loop considers several test sets for estimation of performance. For each one of them, an inner cross-validation loop considers several validation sets (and corresponding training sets) to fit models, select the best parameters and fit a model on the train-validation data using the best parameters. Notice that, the test data are only used for performance estimation and never to fit a model or select parameters. We also note that parameter optimization in the inner loop depends on the metric used each time, i.e. CI or IBS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Determining statistical significance using permutation testing</head><p>We employ permutation testing as a non-parametric way to determine the statistical significance of the difference in performance between two methods. Let us define M 1ij and M 2ij , the models produced by the two methods, respectively, on dataset i when the test set is fold j. Let us call 0 1ij and 0 2ij , the set of predictions of the models on the test fold j of dataset i. We define our test statistic i to be the average difference of performance between Methods 1 and 2 over all folds j of dataset i. We define as the null hypothesis H i the hypothesis that the E( i ) = 0. Under the null hypothesis, it does not matter whether the predictions come from 0 1ij or 0 2ij. We thus create 1000 permuted sets of predictions k 1ij and k 2ij , k = 1,...,1000 produced by randomly swapping with probability 50% each pair of corresponding predictions π 1 ∈ 0 1ij and π 2 ∈ 0 2ij. The test statistic k i is calculated on the corresponding permuted prediction sets. The empirical distribution of { k i } estimates the distribution of i under the null hypothesis. The P-value of H i is estimated as the percentage of times | 0 i |≤| k i |, where 0 i is the statistic calculated on the original (nonpermuted) sets of predictions. We also define the null hypothesis that Method 1 produces more predictive models on dataset i than Method 2, denoted by H 1&lt;2:i. Its P-value is simply the one-sided tail of the distribution, i.e. the percentage of times 0 i ≤ k i. Similarly, we define the statistic as the Page: 1892 1887–1894of the size of the selected variable set, SMMPC selects on average the fewest number of variable, followed closely by LS. The range of performance between the best and the worst model (regressor) produced by each method is higher for the methods that select small variable sets (FS, LS, BVS and SMMPC). This implies that for such methods it becomes more important to identify the regressor that best matches the inductive bias of the variable selection technique.<ref type="figure" target="#tab_3">Table 3</ref>presents the P-values of the one-sided test of each method against each other method. A rough ranking of the four methods that select small variable sets, in terms of the predictive performance of the resulting models is SMMPC &gt; BVS &gt; LS &gt; FS.<ref type="figure" target="#tab_4">Table 4</ref>shows for each variable selection method the best match for regression method. In terms of CI, methods that select a high number of variables (NS and US) are better coupled with Ridge Cox Regression (recall also that AFT and Cox Regression are not applicable when the number of variables is higher than the number of non-censored samples). LS has a tendency to be better coupled with RSF, while for the remaining methods the best regressor is highly dependent on the dataset. In terms of IBS, Ridge Cox regression is the most frequently chosen method (Supplementary<ref type="figure">Table 8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.Lagani and I.Tsamardinos</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparing survival regression methods</head><p>In this section, we turn the focus on the survival regression methods. The nested cross-validation procedure is employed for parameter optimization and estimation of performance. CI performances are shown in<ref type="figure" target="#tab_5">Table 5</ref>for the variable selection method that best matches the regression method (the P-values of the one-sided test comparing the performance of each method against each other method are give in the Supplementary<ref type="figure" target="#tab_7">Table 7</ref>). When considering the CI metric, a rough ranking of the methods is Ridge &gt; SVCR &gt; {RSF, AFT} &gt; Cox. When we optimize for IBS, Ridge Cox Regression remains superior to all other methods and RSF significantly outperforms AFT models (Supplementary Tables 5 and 7). In termsThis conclusion is in agreement with prior studies pointing out the effectiveness and advantages of Ridge Cox Regression (<ref type="bibr" target="#b9">Bovelstad et al., 2007</ref>). SMMPC stems from the BN theory, causal induction theory and variable selection using the Markov blanket concept. The results show that these theories and corresponding variable selection algorithms may transfer their performance and scalability qualities to other tasks than classification. Thus, the extension of MMPC to different variable selection tasks (e.g. to time-series) seems promising.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>Concordance Index (CI) and Integrated Brier Score (IBS)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>[16:08 30/6/2010 Bioinformatics-btq261.tex] Page: 1890 1887–1894</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5.</head><figDesc>Nested cross-validated performances of regression methods matched with the best-performing variable selection method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><figDesc>Funding: STREP project 'HEARTFAID' (FP6-IST-2004-027107); European Research Consortium for Informatics and Mathematics (to V.L.). Conflict of Interest: none declared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>⊆ S,|Z|≤k, s.t., p XT |Z &gt; a then 6. R := R\{X} 7. := argmax X∈R min Z⊆S\{X},|Z|≤k (−p XT |Z ) 9. R := R\{M} 10. S := S ∪{M} 11. until R =∅ \{X},|Z|≤k, p XT |Z &gt; c 14. S := S \{X}</figDesc><table>:= V 
% Remaining to consider 

3. 

S := ∅ 
% Select so far 

4. 

repeat 

5. 

if ∃X ∈ R,Z end if 
% Max–Min Heuristic 

8. 

M 12. 
13. 

∀X,s.t.,∃Z ⊆ S 15. return S 
16. end procedure 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>Proposition 2. Assume that T is the time-to-event, thus having no causal effects in V, (a) the data distribution P is faithful to a Causal BN, i.e. there exist a faithful and Causal BN G,P, (b) the conditional tests of independence make no statistical errors at level a then SMMPC(T ,∞,a) outputs the direct causes of T (direct in the context of the remaining variables), which is also the Markov blanket of T .</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><figDesc>Table 1. Datasets used in the evaluation</figDesc><table>Name and Reference 
#Cases #Cens #Vars Event 

Vijver (van de Vijver et al., 2002) 
295 
207 
70 
metastasis 
Veer (van't Veer et al., 2002) 
78 
44 
4751 
metastasis 
Ros.2002 (Rosenwald et al., 2002) 240 
102 
7399 
survival 
Ros.2003 (Rosenwald et al., 2003) 
92 
28 
8810 
survival 
Bullinger (Bullinger et al., 2004) 
116 
49 
6283 
survival 
Beer (Beer et al., 2002) 
86 
62 
7129 
survival 

The columns are in order, literature reference, number of training cases, number of 
censored cases, number of predicting variables and the event of interest. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><figDesc>Table 2.</figDesc><table>Nested cross-validated performances of feature selection methods 

NS 
US 
LS 
FS 
BVS 
SMMPC 

CI 
Vijver 
0.715 
(0.017) 

− 

0.717 
(0.016) 

− 

0.717 
(0.036) 

−−− 

0.733 
(0.055) 
0.715 
(0.009) 
0.709 
(0.012) 

Veer 

+ 

0.676 
(0.014) 
0.690 
(0.010) 
0.686 
(0.050) 

++ 

0.648 
(0.037) 

+++ 

0.616 
(0.017) 
0.707 
(0.075) 

Ros.2002 

− 

0.628 
(0.032) 

− 

0.627 
(0.025) 

++ 

0.605 
(0.038) 

+++ 

0.586 
(0.029) 

−−− 

0.638 
(0.016) 
0.618 
(0.011) 

Ros.2003 
0.724 
(0.049) 
0.707 
(0.037) 

+++ 

0.602 
(0.414) 

++ 

0.667 
(0.049) 
0.722 
(0.064) 
0.707 
(0.058) 

Bullinger 

− 

0.639 
(0.006) 
0.633 
(0.012) 

−−− 

0.669 
(0.023) 

++ 

0.593 
(0.015) 

−− 

0.647 
(0.089) 
0.617 
(0.023) 

Beer 

− 

0.770 
(0.095) 
0.755 
(0.067) 

+ 

0.672 
(0.045) 

+ 

0.669 
(0.133) 

++ 

0.667 
(0.114) 
0.720 
(0.118) 

nVars 
Vijver 
70 
(0.000) 
44.3 
(2.50) 
9.75 
(10.5) 
20.65 
(12.8) 
10.5 
(3.15) 
6.15 
(0.250) 
Veer 
4751 
(0.000) 
1117 
(254.3) 
8.65 
(5.55) 
5.0 
(3.15) 
5.65 
(0.25) 
6.05 
(0.5) 
Ros.2002 
7399 
(0.000) 
1620 
(445.5) 
13.65 
(2.75) 
16.9 
(8.5) 
24.15 
(4.875) 
12.55 
(1) 
Ros.2003 
8810 
(0.000) 
1681 
(211.6) 
2.15 
(1.375) 
4.15 
(1.125) 
11.65 
(1.65) 
8.4 
(0.75) 
Bullinger 
6283 
(0.000) 
1230 
(139.4) 
6.15 
(14.65) 
6 
(2.75) 
16.4 
(1.375) 
8.05 
(0.375) 
Beer 
7129 
(0.000) 
1146 
(99.2) 
10.05 
(4.625) 
4.5 
(1.5) 
8.5 
(0.125) 
7.65 
(2.15) 

Mean 
CI 

− 

0.692 
(0.036) 
0.688 
(0.028) 

+++ 

0.658 
(0.101) 

+++ 

0.649 
(0.053) 

++ 

0.668 
(0.051) 
0.680 
(0.049) 

IBS 
0.165 
(0.022) 
0.165 
(0.010) 
0.167 
(0.045) 

+ 

0.174 
(0.019) 
0.170 
(0.041) 
0.169 
(0.031) 
nVars 
5740 
(0.00) 
1140.0 
(192.1) 
8.4 
(6.575) 
9.53 
(4.97) 
12.8 
(1.9) 
8.14 
(0.82) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><figDesc>Table 4.</figDesc><table>Best regression method for each combination dataset and feature 
selection algorithm (CI metric) 

Methods 
NS 
US 
SMMPC 
BVS 
LS 
FS 

Vijver 
RSF 
Ridge 
RSF 
SVCR 
SVCR 
SVCR 
Veer 
Ridge 
SVCR 
SVCR 
Cox 
RSF 
Cox 
Ros.2002 
Ridge 
Ridge 
RSF 
Cox 
RSF 
Cox 
Ros.2003 
Ridge 
Ridge 
Ridge 
AFT 
RSF 
Ridge 
Bullinger 
Ridge 
RSF 
Ridge 
SVCR 
Ridge 
RSF 
Beer 
Ridge 
Ridge 
SVCR 
Ridge 
RSF 
AFT 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><figDesc>Table</figDesc><table></table></figure>

			<note place="foot">© The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oxfordjournals.org 1887 at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="3"> THE THEORY AND INTERPRETATION OF SMMPC For a practitioner that employs variable selection for gaining insight and understanding in a domain, it is important to know in depth the theoretical properties and the semantic interpretation of the variables selected by an</note>

			<note place="foot">at :: on August 31, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">The metrics are presented for the best-performing regression method; the numbers in parentheses show the range of the metric between the best and the worst regression method tried. Symbols + , ++ and +++ : SMMPC outperforms the corresponding method at the significance levels of 0.1, 0.05 and 0.01, respectively; − , −− and −−− : SMMPC is outperformed by the corresponding method at the significance levels of 0.1, 0.05 and 0.01, respectively. average i over all datasets and the null hypothesis H 1&lt;2 that Method 1 produces models of higher performance than Method 2 on average in all the data populations in the study. The P-values of this hypothesis are estimated in a similar way. Regarding the prediction sets 1ij and 2ij , when the performance metric is the CI they consist of relative risk predictions RR(p,q) for each unordered pair p,q of patients in the test set (RR(p,q) = 1, if subject p is given a higher probability to survive longer than subject q). During permutations a relative risk for p,q as given by Method 1 may be swapped with a relative risk for p,q as given by Method 2. When the performance metric is the IBS, the prediction sets contain the patient-specific survival functions over all patients in the test set. During permutations a survival function for patient i as estimated by Method 1 may be swapped by the survival function for patient i given by Method 2. 6 RESULTS Experimentation results are now provided. Since the results in terms of CI and IBS are substantially in agreement, we report only CI performances. IBS performances as well as adjunctive results are reported in the Supplementary Section 8. 6.1 Comparing variable selection methods In this set of experiments, we compare the variable selection methods, namely NS, FS, Lasso Selection (LS), MCMC, BVS and SMMPC against each other. The MCMC procedure is so computationally costly that forced us to distinguish it from all other methods and use a simplified experimentation protocol to compare against. The details are in the Supplementary Section 7. SMMPC achieves a statistically significantly higher performance for both CI and IBS metrics, on most datasets individually as well as on all datasets collectively. Within the scope of our evaluation (particularly, the fact that we have used 50 000 iterations for the MCMC procedure), we consider the method dominated by SMMPC both in terms of computational efficiency and predictive performance of the resulting models. To compare against all other methods, the nested cross-validation procedure is employed for parameter optimization and estimation of performance. The results are shown in Table 2 for the regression method that best matches the variable selection method. SMMPC serves as the baseline to compare against and produce the P-values of the permutation tests. NS and US produce the highest performing models, even though the difference from SMMPC is not statistically significant for US and only marginally significant for NS (P = 0.052) considering the CI metric. However, this performance is achieved at the expense of employing a quite large number of variables in the models and thus, these methods are not suitable for knowledge discovery tasks. To further clarify the relation between US and SMMPC, we repeated the experiments for US restricting the method to only select at most the top f variables, where f is the number of variables selected by SMMPC on that fold. This restricted version is outperformed by SMMPC at a statistical level of 0.01 and 0.05 for CI and IBS, respectively, showing the SMMPC orders the variables better than US in terms of the cumulative predictive information they carry. We consider the remaining methods, namely FS, BVS , LS, and SMMPC suitable for knowledge discovery, as they reduce the number of variables to a human-management size of less than a dozen. Comparing these methods on individual datasets does not provide a clear picture. However, over all datasets SMMPC produces higher performing models in terms of CI than all other methods (P &lt; 0.05). With respect to IBS, the same conclusions hold against FS and BVS even though the significance levels increase. In terms</note>

			<note place="foot" n="7"> DISCUSSION AND CONCLUSION We adopt a successful variable selection method for classification for survival analysis tasks, named SMMPC. SMMPC is relatively simple, scalable to high-dimensional data and has a structural BN interpretation: it identifies the neighbors of T (the timeto-event) in the network representing the data distribution. In an extensive comparative evaluation over several datasets and regression methods, SMMPC is shown to statistically significantly outperform Bayesian Selection based on MCMC techniques, Bayesian Selection based on a greedy-search modified for highdimensional data, Lasso Cox Regression (treated as a variable selection procedure), FS and US restricted to return as many variables as SMMPC. SMMPC selects fewer than a dozen variables per dataset making analysis of the models amenable to human experts. Relative to the other methods tried, SMMPC is &apos;stable&apos; in terms of the returned variables (Supplementary Section 5). Finally, SMMPC shows a sharp learning curve in terms of available sample size (Supplementary Section 6). Further analyzing the results, shows that the performance of the regression methods is ordered as Ridge Cox Regression &gt; SVCR &gt; {RSFs and AFT} &gt; Cox Regression.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">Ros</forename>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">Ros</forename>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<monogr>
		<title level="m" type="main">The numbers in parentheses show the range of the metric between the best and the worst feature selection method tried. Ridge, Ridge regression; SVCR, Support Vector Censored Regression; Cox, Cox regression. of CI, regressors that are only able to handle a relatively low number of variables, such as AFT and Cox are best-matched with SMMPC and BVS. The rest are best-matched with US and BVS</title>
		<imprint/>
	</monogr>
	<note>In. term of IBS, there is no obvious pattern. see. Supplementary Table 9</note>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="issue">30</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>btq261. .tex]</note>
</biblStruct>

<biblStruct   xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName>
				<surname>Page</surname>
			</persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1894" to="1887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">HITON, a novel Markov blanket algorithm for optimal variable selection</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Lagani</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Tsamardinos</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">References</forename>
				<surname>Aliferis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">F</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Poceedings of the American Medical Informatics Association</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Local causal and Markov blanket induction algorithms for causal discovery and feature selection for classification part i: algorithms and empirical evaluation</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">F</forename>
				<surname>Aliferis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="171" to="234" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised methods to predict patient survival from gene expression data</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Bair</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Biol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="511" to="522" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Gene-expression profiles predict survival of patients with lung adenocarcinoma</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">G</forename>
				<surname>Beer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="816" to="824" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting survival from microarray data a comparative study</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Bovelstad</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2080" to="2087" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Breiman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Schapire</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<monogr>
		<title level="m" type="main">Markov blanket-based variable selection in feature space</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">E</forename>
				<surname>Brown</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Tsamardinos</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Use of gene-expression profiling to identify prognostic subclasses in adult acute myeloid leukemia</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Bullinger</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="page" from="1605" to="1616" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Regression models and life-tables</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">R</forename>
				<surname>Cox</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="187" to="220" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Asymptotics of cross-validated risk estimation in estimator selection and performance assessment</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Dudoit</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">J</forename>
				<surname>Van Der Laan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Methodol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="131" to="154" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Bayesian variable selection method for censored survival data</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Faraggi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Simon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1475" to="1485" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Assessment and comparison of prognostic classification schemes for survival data</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Graf</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Med</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="2529" to="2545" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Result analysis of the NIPS 2003 feature selection challenge</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Guyon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 17</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<monogr>
		<title level="m" type="main">Regression Modeling Strategies, With Applications to Linear Models, Logistic Regression, and Survival Analysis</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">E</forename>
				<surname>Harrel</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Supervised harvesting of expression trees</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hastie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3" to="4" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Monte carlo sampling methods using Markov chains and their applications</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">K</forename>
				<surname>Hastings</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Time-dependent ROC curves for censored survival data and a diagnostic marker</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">J</forename>
				<surname>Heagerty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="337" to="334" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Ridge Regression: biased estimation for nonorthogonal problems</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">E</forename>
				<surname>Hoerl</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">W</forename>
				<surname>Kennard</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="80" to="86" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Bagging survival trees</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hothorn</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Med</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="77" to="91" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Multiple comparisons in induction algorithms</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Jensen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Cohen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="309" to="338" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<monogr>
		<title level="m" type="main">Survival Analysis: Techniques for Censored and Truncated Data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">P</forename>
				<surname>Klein</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">L</forename>
				<surname>Moeschberger</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Wrappers for feature subset selection</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Kohavi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">H</forename>
				<surname>John</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="273" to="324" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Partial Cox regression analysis for high-dimensional microarray gene expression data</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Gui</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="208" to="215" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Partial least squares proportional hazard regression for application to DNA microarray survival data</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">V</forename>
				<surname>Nguyen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">M</forename>
				<surname>Rocke</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1625" to="1632" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Dimension reduction of microarray data in the presence of a censored survival response: a simulation study</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">S</forename>
				<surname>Nguyen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Rojo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Appl. Genet. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Article. 4</note>
</biblStruct>

<biblStruct   xml:id="b30">
	<monogr>
		<title level="m" type="main">Causality, Models, Reasoning, and Inference</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Pearl</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural networks as statistical methods in survival analysis</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">D</forename>
				<surname>Ripley</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">M</forename>
				<surname>Ripley</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks: Prospects for Medicine. Landes Biosciences Publishers</title>
		<meeting><address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="237" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">The use of molecular profiling to predict survival after chemotherapy for diffuse large B-cell lymphoma</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Rosenwald</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">346</biblScope>
			<biblScope unit="page" from="1937" to="1947" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">The proliferation gene expression signature is a quantitative integrator of oncogenic events that predicts survival in mantle cell lymphoma</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Rosenwald</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Cell</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="185" to="197" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Bayesian variable selection for the analysis of microarray data with censored outcomes</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Sha</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2262" to="2268" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">A support vector approach to censored targets</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">K</forename>
				<surname>Shivaswamy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM &apos;07: Proceedings of the 2007 Seventh IEEE International Conference on Data Mining</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="655" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<analytic>
		<title level="a" type="main">Gradient lasso for Cox proportional hazards model</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Sohn</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1775" to="1781" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b37">
	<monogr>
		<title level="m" type="main">Causation, Prediction, and Search</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Spirtes</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd. edn</note>
</biblStruct>

<biblStruct   xml:id="b38">
	<analytic>
		<title level="a" type="main">GEMS: a system for automated cancer diagnosis and biomarker discovery from microarray gene expression data</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Statnikov</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Med. Inform</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="491" to="503" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b39">
	<analytic>
		<title level="a" type="main">The lasso method for variable selection in the Cox model</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Med</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="385" to="395" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b40">
	<analytic>
		<title level="a" type="main">The lasso method for variable selection in the Cox model</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Med</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="385" to="395" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b41">
	<analytic>
		<title level="a" type="main">Towards principled feature selection: relevancy, filters and wrappers</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Tsamardinos</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">F</forename>
				<surname>Aliferis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Ninth International Workshop on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="2003" />
			<publisher>Morgan Kaufmann Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b42">
	<analytic>
		<title level="a" type="main">Bounding the false discovery rate in local bayesian network learning</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Tsamardinos</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">E</forename>
				<surname>Brown</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;08: Proceedings of the 23rd National Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>Menlo Park, California</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1100" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b43">
	<analytic>
		<title level="a" type="main">Time and sample efficient discovery of Markov blankets and direct causal relations</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Tsamardinos</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="673" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b44">
	<analytic>
		<title level="a" type="main">The Max–Min Hill-Climbing Bayesian network structure learning algorithm Gene expression profiling predicts clinical outcome of breast cancer</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Tsamardinos</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn. Nature</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">415</biblScope>
			<biblScope unit="page" from="31" to="78" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b45">
	<analytic>
		<title level="a" type="main">A gene-expression signature as a predictor of survival in breast cancer</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Van De Vijver</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">347</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b46">
	<analytic>
		<title level="a" type="main">Survival prediction using gene expression data: a review and comparison</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">N</forename>
				<surname>Van Wieringen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Stat. Data Anal</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1590" to="1603" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b47">
	<analytic>
		<title level="a" type="main">Survival analysis with high-dimensional covariates</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">M</forename>
				<surname>Witten</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Methods Med. Res</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="29" to="51" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>