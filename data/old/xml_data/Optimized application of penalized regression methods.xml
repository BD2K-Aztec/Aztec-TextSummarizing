
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimized application of penalized regression methods to diverse genomic data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011">2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Levi</forename>
								<surname>Waldron</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biostatistics</orgName>
								<orgName type="institution">Harvard School of Public Health</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">PMH/UHN</orgName>
								<orgName type="institution" key="instit1">Ontario Cancer Institute</orgName>
								<orgName type="institution" key="instit2">Campbell Family Institute for Cancer Research</orgName>
								<address>
									<settlement>Toronto</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Melania</forename>
								<surname>Pintilie</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Biostatistics</orgName>
								<orgName type="institution" key="instit1">Ontario Cancer Institute</orgName>
								<orgName type="institution" key="instit2">University Health Network</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Ming-Sound</forename>
								<surname>Tsao</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department">PMH/UHN</orgName>
								<orgName type="institution" key="instit1">Ontario Cancer Institute</orgName>
								<orgName type="institution" key="instit2">Campbell Family Institute for Cancer Research</orgName>
								<address>
									<settlement>Toronto</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Frances</forename>
								<forename type="middle">A</forename>
								<surname>Shepherd</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department">PMH/UHN</orgName>
								<orgName type="institution" key="instit1">Ontario Cancer Institute</orgName>
								<orgName type="institution" key="instit2">Campbell Family Institute for Cancer Research</orgName>
								<address>
									<settlement>Toronto</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Curtis</forename>
								<surname>Huttenhower</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biostatistics</orgName>
								<orgName type="institution">Harvard School of Public Health</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Igor</forename>
								<surname>Jurisica</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="department">PMH/UHN</orgName>
								<orgName type="institution" key="instit1">Ontario Cancer Institute</orgName>
								<orgName type="institution" key="instit2">Campbell Family Institute for Cancer Research</orgName>
								<address>
									<settlement>Toronto</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Janet</forename>
								<surname>Kelso</surname>
							</persName>
						</author>
						<title level="a" type="main">Optimized application of penalized regression methods to diverse genomic data</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">27</biblScope>
							<biblScope unit="issue">24</biblScope>
							<biblScope unit="page" from="3399" to="3406"/>
							<date type="published" when="2011">2011</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btr591</idno>
					<note type="submission">Received on May 3, 2011; revised on October 12, 2011; accepted on October 17, 2011</note>
					<note>[10:14 21/11/2011 Bioinformatics-btr591.tex] Page: 3399 3399–3406 Gene expression Associate Editor: Supplementary Information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Penalized regression methods have been adopted widely for high-dimensional feature selection and prediction in many bioinformatic and biostatistical contexts. While their theoretical properties are well-understood, specific methodology for their optimal application to genomic data has not been determined. Results: Through simulation of contrasting scenarios of correlated high-dimensional survival data, we compared the LASSO, Ridge and Elastic Net penalties for prediction and variable selection. We found that a 2D tuning of the Elastic Net penalties was necessary to avoid mimicking the performance of LASSO or Ridge regression. Furthermore, we found that in a simulated scenario favoring the LASSO penalty, a univariate pre-filter made the Elastic Net behave more like Ridge regression, which was detrimental to prediction performance. We demonstrate the real-life application of these methods to predicting the survival of cancer patients from microarray data, and to classification of obese and lean individuals from metagenomic data. Based on these results, we provide an optimized set of guidelines for the application of penalized regression for reproducible class comparison and prediction with genomic data. Availability and Implementation: A parallelized implementation of the methods presented for regression and for simulation of synthetic data is provided as the pensim R package, available at http://cran.r-project.org/web/packages/pensim/index.html.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Multivariate regression is a flexible machine learning method, suited to prediction of discrete, continuous and censored time-toevent (survival) outcomes from arbitrary combinations of predictor variable classes. In genomic settings, collinear predictors typically greatly outnumber available samples (p &gt; n), a now-classic example being the prediction of cancer patient survival from tumor gene * To whom correspondence should be addressed. expression data (<ref type="bibr" target="#b0">Beer et al., 2002;</ref><ref type="bibr" target="#b24">Shedden et al., 2008;</ref><ref type="bibr" target="#b30">Sørlie et al., 2001;</ref><ref type="bibr" target="#b36">van de Vijver et al., 2002;</ref><ref type="bibr" target="#b40">Wigle et al., 2002</ref>). In this setting, ordinary regression is subject to overfitting and instability of coefficients (<ref type="bibr" target="#b13">Harrell et al., 1996</ref>), and stepwise variable selection methods do not scale well (<ref type="bibr" target="#b41">Yuan and Lin, 2006</ref>). Regression has been successfully adapted to high-dimensional situations by penalization methods (review by<ref type="bibr" target="#b14">Hesterberg, 2008</ref>), and penalized regression has been shown to outperform univariate and other multivariate regression methods in multiple genomic datasets (<ref type="bibr">Bøvelstad et al., 2007</ref>). Two penalization methods, and a hybrid of these, are most commonly used. Ridge regression (<ref type="bibr" target="#b15">Hoerl and Kennard, 1970</ref>) uses a penalty on the L 2 norm of the coefficients, which introduces bias in the prediction error in exchange for reduced variance. However, ridge regression keeps all variables in the model and thus cannot produce a parsimonious model from many variables. LASSO regression (<ref type="bibr" target="#b32">Tibshirani, 1996;</ref><ref type="bibr" target="#b33">1997)</ref>penalizes the L 1 norm, which tends to reduce many coefficients to exactly zero and thus performs variable selection in addition to prediction. However, the LASSO has been noted to be inferior to Ridge regression for prediction in lower dimensional situations, and tends to select only one of a group of collinear variables, which may not always be desirable (<ref type="bibr" target="#b42">Zou and Hastie, 2005</ref>).<ref type="bibr" target="#b42">Zou and Hastie (2005)</ref>thus proposed the Elastic Net, penalizing both the L 1 and L 2 norms with individual tuning parameters, as a way to achieve the best of both LASSO and Ridge. These three variants of penalized regression—LASSO, Ridge and Elastic Net—have since been applied to a variety of phenotype prediction tasks using genomic data (for example,<ref type="bibr" target="#b23">Sharma et al., 2008;</ref><ref type="bibr" target="#b24">Shedden et al., 2008</ref>). Several previous simulation studies have investigated properties of the Elastic Net (<ref type="bibr" target="#b42">Zou and Hastie, 2005</ref>), the LASSO and Ridge regression (<ref type="bibr" target="#b2">Bøvelstad et al., 2007;</ref><ref type="bibr" target="#b10">Gui and Li, 2005;</ref><ref type="bibr" target="#b41">Yuan and Lin, 2006</ref>), but have not compared all these methods with alternative strategies for their application. We present a comprehensive assessment and optimization of these methods, using two contrasting configurations of simulated genomic data and two genome-scale experimental datasets. Comparative studies of this nature provide the most realistic and unbiased assessments of available machine learning methods, issues which have been identified as critical to researchers' selection of appropriate methodology (<ref type="bibr" target="#b1">Boulesteix, 2006;</ref><ref type="bibr" target="#b17">Jelizarow et al., 2010</ref>). We introduce a 2D optimization of the Elastic Net penalty parameters, and show that it or a comparable procedure is necessary</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L.Waldron et al.</head><p>to distinguish the Elastic Net from LASSO and Ridge regression. In particular, we show that successive 1D tuning of the Elastic Net restricts the search for tuning parameters sufficiently that it can yield inferior prediction to a single-penalty counterpart. A univariate prefilter is commonly used to reduce dimensionality and computation time, but we demonstrate a simulated situation in which the pre-filter can reduce predictive performance of the Elastic Net by reducing the importance of the L 1 penalty relative to the L 2 penalty. We applied these optimized regression procedures in two very differing genomic settings: predicting survival of cancer patients from microarray data, and classification of lean and obese individuals from metagenomic sequence data. We use a crossvalidation strategy for assessment of model prediction (<ref type="bibr" target="#b20">Molinaro et al., 2005;</ref><ref type="bibr" target="#b26">Simon et al., 2011</ref>), with an additional inner level of cross-validation for model tuning in training data (<ref type="bibr">Goeman, 2011</ref>). Both examples proved favorable to the L 2 penalty, and models trained by Ridge regression and Elastic Net showed independent predictive ability, whereas models trained by the LASSO did not. We emphasize evidence of overfitting in both simulated and real experimental data, and summarize methods for realistic assessment of prediction accuracy with limited sample size. Based on results from this study and best practices for high-dimensional model validation, we conclude with an end-to-end methodology for effective application of penalized regression to diverse genomic data for prediction and variable selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head><p>We first consider the use of penalized regression to select features and predict outcome in simulated high-dimensional data, focusing specifically on the Cox proportional hazards model for potentially censored survival data (<ref type="bibr" target="#b7">Cox, 1972</ref>). We further apply the guidelines for penalized regression developed from these synthetic data to real expression data and to penalized logistic regression for metagenomic data from the gut microbiomes and obesity status of subjects in the MetaHIT study (<ref type="bibr" target="#b21">Qin et al., 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.0.1">Creation of synthetic data</head><p>Five hundred simulations were generated as summarized in<ref type="figure" target="#tab_1">Table 1</ref>, in one configuration favoring the LASSO penalty and one configuration favoring the Ridge penalty. Simulated variables were standard normal distributed, with covariance matrix specified by the withingroup correlations in<ref type="figure" target="#tab_1">Table 1</ref>and between-group covariance of zero. 'True' hazards were generated from a weighted sum of the predictor variables according to the proportional hazards assumption:</p><formula>h j = h 0 exp ⎛ ⎝ i=p i=1 β i X ij ⎞ ⎠ (1)</formula><p>Where h j is the hazard for each patient j (j = 1,2, ... ,n), h o is an arbitrary baseline hazard (set to a constant 0.2), i is the feature index, each X ij is the simulated expression value of feature i in patient j, and β i are the associations of each predictor with outcome, given in<ref type="figure" target="#tab_1">Table 1</ref>. Survival times for 100 patients were then sampled from a random exponential distribution with decay rate h j and censored on a uniform U(2,10) distribution as, for example, in Gui and Li (2005). Under these conditions, ∼34% of events were censored. Results without censoring for 500:40 and 2000:40 noise:associated variables, and for 150 patients with a 500:40 variable ratio with censoring, in the LASSO-favoring scenario, are shown in the Supplemental Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.0.2">Methods for penalized regression</head><p>Penalized regression was performed using the pensim R package, described in the Implementation, and the penalized R package (Goeman, 2010; Version 0.9-33). Five different penalization schemes were considered: LASSO regression (Tibshirani,In the Elastic Net (<ref type="bibr" target="#b42">Zou and Hastie, 2005</ref>), the usual partial log-likelihood is penalized by the L 1 and L 2 norms of the regression coefficients with weights λ 1 and λ 2 , respectively, i.e.:</p><formula>l(β) penalized = l(β)−λ 1 i=p i=1 |β i |−λ 2 i=p i=1 (β i ) 2 (2)</formula><p>where λ 1 and λ 2 are tuned by maximizing l(β) (<ref type="bibr" target="#b10">Gui and Li, 2005</ref>; Verweij and Van<ref type="bibr" target="#b38">Houwelingen, 1993;</ref><ref type="bibr" target="#b39">1994)</ref>, and l(β) is the cross-validated partial log-likelihood. LASSO and Ridge regression are described by Equation</p><p>(2)</p><p>with λ 1 or λ 2 non-zero, respectively. The λ 1 +λ 2 Elastic Net involves 2D optimization of the penalties. Reasonable initial guesses for the penalties were determined by computing 10-fold cross-validated partial log-likelihood (CVL) of Elastic Net models on a regular λ 1 /λ 2 grid. The 2D optimization of CVL as a function of λ 1 and λ 2 was then performed by maximizing CVL, as visualized in the contour plots in<ref type="figure" target="#fig_0">Figure 1</ref>. To avoid spurious results due to local maxima, initial values of λ 1 /λ 2 were selected randomly from the five best positions determined from the scan of the regular λ 1 /λ 2 grid. 2D tuning of the penalties was performed by the quasi-Newton method described by<ref type="bibr" target="#b5">Byrd et al. (1995)</ref>, as implemented by the optim function in the R stats package (R Development Core Team, 2010). The penalty parameters were tuned 50 times using different folding of the data for calculating CVL, and the penalty parameters with maximum CVL were selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.0.3">Assessment of regression in simulated data</head><p>Methods for regression and selection from multiple starts were assessed in terms of precision of variable selection and in terms of prediction of future events. Precision and prediction are presented as ranks within each dataset. Prediction was assessed in an independently generated test dataset simulated with the same properties as the training data, but with 200 samples. Three methods for assessing prediction accuracy were considered: (i) area under the survival ROC curve at the median survival time, using the survivalROC R package (Version 1.0.0); (ii) area under the Prediction Error Curve, using the PEC R package (Version 1.1.1); and (iii) Spearman's correlation between known true hazards and the hazards estimated by the penalized Cox regression model.These methods all generated similar rankings of the methods (Supplementary<ref type="figure">Fig. S2</ref>); however, we present area under the survival ROC curve for direct comparability to real datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 3401 3399–3406</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Penalized regression methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Application to genomic datasets</head><p>Methods for model selection and validation in the two genomic applications are summarized in<ref type="figure" target="#fig_0">Figure 1</ref>, and detailed in the Supplementary Material. In each case, a context-appropriate filter against low-variance features was applied. LASSO and Elastic Net without univariate pre-filter were performed for each dataset. Prediction accuracy was assessed by 10-fold crossvalidation, with model tuning by a cross-validation nested within the training samples. Variable scalings were determined from the training samples, and applied to the test samples, as implemented in the opt.nested.crossval function of the pensim R package.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>To establish optimized guidelines for reproducibly linking genomic features (such as gene expression) to outcome using penalized regression (<ref type="figure" target="#fig_0">Fig. 1</ref>), we compared the LASSO, Ridge regression and Elastic Net with three strategies for 2D tuning of its penalty parameters. We additionally varied the stringencies of an optional univariate pre-filter and evaluated two methods for selecting from repeated tunings, using simulated high-dimensional datasets with several configurations of signal-to-noise ratio and censoring of survival outcomes. Results of these simulations, along with current best practices for high-dimensional model assessment, were used to guide analyses of two publicly available experimental datasets: a microarray experiment investigating survival of cancer patients with lung adenocarcinomas (<ref type="bibr" target="#b0">Beer et al., 2002</ref>) and metagenomic data from the MetaHIT consortium (<ref type="bibr" target="#b21">Qin et al., 2010</ref>) describing the composition of gene function in the gut microbiotas of lean and obese individuals. Based on the results of these simulations and subsequent comparable analyses of real experimental data, we provide a set of guidelines for penalized regression analysis of diverse high-dimensional data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Establishing optimal penalized regression guidelines using simulated genomic data</head><p>Using simulated data (see Section 2,<ref type="figure" target="#tab_1">Table 1</ref>), we compared LASSO, Ridge and Elastic Net regression for variable selection and for prediction accuracy in independent test sets. The LASSO performed variable selection as expected, by preferentially selecting variables associated with outcome over noise variables, and by preferring the strongest individually representative feature from within each correlated block, identifying strongly associated features more frequently than weakly associated features (Supplementary<ref type="figure" target="#fig_0">Fig. S1</ref>). We found that simultaneous tuning of the Elastic Net parameters was necessary to distinguish it from LASSO and Ridge regression, and assessed the value of repeating tuning of the penalty parameters on different groupings of the data for cross-validation. Finally, we examined the effects of several stringencies of pre-filter for univariate association with outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Selection from multiple starts</head><p>Repeated tuning of the penalty parameters on randomized partitions of the samples for cross-validation produced small improvements in prediction accuracy, at least for the LASSO (Supplementary<ref type="figure" target="#tab_1">Table S1</ref>). This benefit was realized by selecting the model with maximum CVL, but not when selecting the median penalty. The benefit of multiple tunings was reduced with increasing stringencies of the univariate pre-filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Effect of a univariate pre-filter</head><p>The use of a pre-filter for univariate association with response is a standard approach in multivariate analysis (<ref type="bibr" target="#b16">Hosmer and Lemeshow, 1999</ref>), chapter 5;<ref type="bibr">Simon et al., 2003, chapter 8</ref>). It is commonly used as a first step in<ref type="bibr">[</ref>high-dimensional model development (<ref type="bibr" target="#b11">Guyon et al., 2010</ref>) or even for development of the model itself (<ref type="bibr" target="#b0">Beer et al., 2002;</ref><ref type="bibr" target="#b2">Bøvelstad et al., 2007;</ref><ref type="bibr" target="#b6">Chen et al., 2007</ref>). The use of univariate selection for model development is developed formally for Cox regression by<ref type="bibr" target="#b34">Tibshirani (2009)</ref>, and implemented by the R package uniCox. Unsupervised dimension reduction can be used in addition to or instead of univariate screening, as described for example by Harrell (2001, Section 4.7). We computed a nominal P-value for each predictor in the simulated training set by logrank test, and considered several stringencies of pre-filter: P &lt; 0.1, P &lt; 0.3, P &lt; 0.5 and no pre-filter. Normally, the correct stringency of pre-screening is unknown, and Fan and Lv (2008, R package SIS, Version 0.6) suggest a nonparametric method to screen variables with weak association to outcome while ensuring maintenance of the important variables. In the situation of many weakly correlated variables of equal association with outcome, the pre-filter had little effect on prediction accuracy. However, with few true positives (a situation favoring the λ 1 penalty), Ridge regression improved greatly from the univariate pre-filter, but LASSO regression was barely affected. For the λ 1 +λ 2 Elastic Net, the univariate pre-filter reduced the influence of the λ 1 penalty relative to λ 2 , causing a loss of prediction accuracy in the LASSO-favoring scenario (<ref type="figure" target="#fig_1">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Choice of regression method</head><p>We evaluated three methods of tuning the Elastic Net. Tuning one parameter followed by the other (λ 1 −λ 2 and λ 2 −λ 1 ) caused the Elastic net to mimic LASSO and Ridge regression, respectively, in terms of the penalty parameters selected (<ref type="figure">Fig. 2</ref>) as well as prediction accuracy and variable selection (<ref type="figure" target="#fig_1">Fig. 3</ref>). All regression methods outperformed the null model in most cases, but in 1000 datasets simulated from the same population, each method performed near-best in at least one instance and near-worst in at least one, particularly with respect to prediction. Large overall performance differences existed between simulated datasets, but as relative performance of available methods for a particular dataset is typically of interest, we ranked the competing methods within each dataset and aggregated ranks across datasets to compare methods. The relative prediction performance of LASSO and Ridge regression depends on the sparsity of true positive variables, as the LASSO penalty selects no more variables than there are samples, and tends to select a single representative from a group of correlated variables (<ref type="bibr" target="#b42">Zou and Hastie, 2005</ref>). We simulated situations in which the LASSO and Ridge penalties each produced superior prediction, and showed that in each case, prediction performance of the Elastic Net was comparable to the better of the two, provided that a true 2D tuning was used (λ 1 +λ 2 method) and no univariate pre-filter was applied. Sequential tuning of the Elastic Net produced behavior dominated by the first variable tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 3403 3399–3406</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Penalized regression methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Predicting survival of cancer patients from microarray data</head><p>The<ref type="bibr" target="#b0">Beer et al. (2002)</ref>lung adenocarcinoma study was a seminal demonstration of the application of microarrays to predict overall survival of cancer patients based on tumor gene expression profiles. Subsequently, it has become a classic dataset for new analyses and method development (see for example<ref type="bibr" target="#b6">Chen et al., 2007;</ref><ref type="bibr" target="#b19">Michiels et al., 2005;</ref><ref type="bibr" target="#b31">Subramanian et al., 2005</ref>). We applied a standard non-specific filter against unexpressed genes, without reference to patient outcome. Prediction accuracy was assessed by 10-fold cross-validation, with scaling of features and all steps of model training performed in training data only. Prediction models were trained and risk scores calculated for each fold of the cross-validation, and collected together, using the opt.nested.crossval function of the pensim R package. In separate instances, we performed model training by LASSO, Ridge and Elastic Net with the λ 1 +λ 2 tuning method, in each case with no univariate pre-filter and selecting the penalties with highest cross-validated partial log likelihood in 50 starts. This prediction problem favored the L 2 penalty, as independently predictive models were obtained by Ridge regression (P = 0.003) and Elastic Net (P = 0.002), but not by LASSO (P = 0.72, all tests of significance by Likelihood Ratio test). The models produced by Ridge regression and Elastic Net were very similar: the Elastic Net set only 43 of1310 variables exactly to zero, and the continuous risk predictions of the two models were highly correlated (Pearson's correlation = 0.99, df = 84, P &lt; 2e-16). The assumption of sample independence is violated in cross-validated predictions, so following<ref type="bibr" target="#b26">Simon et al. (2011)</ref>, we also assessed significance for the Elastic Net model by permuting the outcome labels 500 times. For each permutation, we repeated the entire procedure of generating cross-validated risk predictions for all samples. The Likelihood Ratio test statistic did not exceed the observed value in 500 permutations (P &lt; 0.002). Superior performance of L 2-penalized regression, with associated complex prediction models, may not be uncommon in gene expression prediction problems, as<ref type="bibr">Bøvelstad et al. (2007)</ref>also found Ridge regression to produce superior survival prediction compared with the LASSO in three other gene expression datasets. We applied the same methods using all samples to determine coefficients for use in independent datasets. When this model was used to make predictions for these same samples (resubstitution), unrealistically accurate performance was observed due to overfitting (<ref type="figure" target="#fig_2">Fig. 4</ref>). This highlights the importance of validation in samples not used for any part of model training (see Section 4, Guideline 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Classification of obesity from metagenomics data</head><p>Metagenomic data, consisting of short DNA sequences sampled from uncultured microbial communities, are biologically very different from microarray gene expression data (<ref type="bibr" target="#b21">Qin et al., 2010</ref>) and represent a new biological application of high-dimensional regression. In a second example, we analyzed data from the combined genomes of microbes in stool samples from the MetaHIT Danish cohort of 85 lean and obese individuals. Whereas gene expression data represent expression of a single gene transcript, these sequence data were summarized by assigning open reading frames from the MetaHIT assembly to functionally characterized orthologous gene families (see Section 2) and calculating the relative abundances of 665 gene functions. We used the same three regression methods and the cross-validation procedure described in the first example, assessing model prediction by area under the ROC curve. This dataset also favored the L 2 penalty, with Ridge regression (AUC = 0.61, P = 0.04) and Elastic Net (AUC = 0.59, P = 0.08) outperforming the LASSO, which converged on the null Page: 3404 3399–3406<ref type="figure">Fig. 5</ref>. Application to high-dimensional metagenomic data. ROC curves for classification of obese (n = 37) and non-obese (n = 48) individuals using metagenomic data describing the gut microbiota (<ref type="bibr" target="#b21">Qin et al., 2010</ref>), trained by Elastic Net. High-dimensional features are no longer gene expression but the relative abundance of specific microbial pathways in the stool microbiome. Overfitting to the training set is again observed in resubstitution predictions, but cross-validation shows marginal evidence of independent predictive ability (AUC = 0.59, P = 0.08). model in all folds of cross-validation. The permutation test for Elastic Net, with area under the ROC curve as the test statistic, produced a similar result (P = 0.03). As with the first example, resubstitution predictions showed evidence of overfitting to the training set (<ref type="figure">Fig. 5</ref>, AUC = 0.85, P = 2×10 −9 , results shown for Elastic Net). Elastic Net and Ridge regression models were again similar, with the Elastic Net setting only 139 of 532 coefficients exactly to zero, with similar continuous predictions (Pearson's correlation = 0.91, df = 83, P &lt; 2e-16). In these models, protein localization, secretion and signaling were associated with leanness, and biosynthetic proteins determining cell wall phospholipid composition were positively associated with obesity. These are potentially indicative of the Gram-positive/negative shift, previously associated with obesity due to changes in the relative abundances of the Bacteroidetes and Firmicutes phyla (<ref type="bibr" target="#b18">Ley et al., 2006;</ref><ref type="bibr" target="#b35">Turnbaugh et al., 2006</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L.Waldron et al.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GUIDELINES FOR THE APPLICATION OF PENALIZED REGRESSION TO DIVERSE GENOMIC DATA</head><p>As discussed above, the performance of different regression methods and training procedures varied widely in our simulated data, with each individual method performing both best and worst in individual instances of 500 simulations from the same population. This emphasizes the random variation in model development that can be expected, and the care that must be taken to avoid reporting on results solely due to overfitting. We summarize previously established and presently determined steps for development and assessment of predictive models from high-dimensional genomic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Page: 3405 3399–3406</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Penalized regression methods</head><p>Other machine learning algorithms may be considered (for example,<ref type="bibr" target="#b3">Breiman, 2001</ref>), as well as boosting to improve on a set of weak learners (<ref type="bibr">Bühlmann, 2007</ref>). 9. Final model selection: if evidence of an independently predictive model is found in the cross-validation procedure, the same methods are used on all samples to select a model for studying selected variables, and for prediction on new, independent datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this study, we outline and implement guidelines to optimize the performance of penalized regression, as assessed by parameter tuning, variable selection and prediction in independent highdimensional genomic data. We simulated two contrasting scenarios that favored the LASSO and Ridge penalties with high-dimensional collinear predictors and survival outcome. We found that a simultaneous tuning of the Elastic Net penalties, which was previously unavailable, was required to differentiate it from LASSO and Ridge regression. In the LASSO-favoring simulation scenario, application of a permissive univariate pre-filter reduced performance of the Elastic Net, by reducing the influence of the λ 1 penalty, but in other situations was neutral. Selecting the penalty associated with highest cross-validated partial log-likelihood from repeated tunings produced better predictions in independent data for nearly all methods. Based on findings from the simulations, we demonstrated the application of penalized regression in two very different genomic contexts, differing in biological data type (gene expression and metagenomic structure) and in response variable type (censored survival time and binary obesity). In both scenarios, Ridge regression and Elastic Net produced similar models, which outperformed the LASSO. We observed overfitting in both synthetic and real data, highlighting the importance of proper model validation with independent data. We summarize the methods employed here in a set of specific guidelines to guide the development of reproducibly predictive models from genomic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">IMPLEMENTATION</head><p>To perform these studies, we introduced a 2D tuning of the Elastic Net and simplified the processes of repeated tuning of the penalty parameters and of cross-validated estimation of model accuracy. We parallelized the tasks of repeated tuning of the LASSO, Ridge and Elastic Net penalties, and of cross-validation for assessment of prediction accuracy. We also developed a function for generating synthetic high-dimensional data with time-to-event or binary outcome, and blocks of predictor variables defined by collinearity and association with outcome, with options for introducing labeling errors and for censoring of survival times. These functions are available in the pensim R package from the CRAN repository. It utilizes the penalized R package (Goeman, 2010; Version 0.9–33) for regression, the snow R package Version 0.3-7 for parallelization of the multiple starts, the MASS R package (<ref type="bibr" target="#b37">Venables and Ripley, 2002</ref>) for correlated random number generation and the rlecuyer R package Version 0.3-1 for parallelization of random number generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. (A) Methodology for model selection and validation of highdimensional data. Objectives include both feature selection and outcome prediction, e.g. for patient survival given tumor gene expression data. A nearly unbiased assessment of prediction accuracy for small samples sizes is obtained by repeating all steps of model selection in each iteration of the cross-validation. Variable selection and model conditioning are achieved within the training sets by an optional, permissive univariate pre-filter followed by repeated cross-validation for parameter tuning. These steps are detailed in Section 4. (B) Over-fitting occurs in spite of tuning the models by cross-validation, as evidenced by reduced prediction accuracy in simulated test sets compared to resubstitution of training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.3.</head><figDesc>Fig. 3. Ranking of methods for prediction accuracy in two scenarios, simulated to favor (A) the LASSO penalty and (B) the Ridge penalty. In both scenarios and with all levels of pre-filtering, sequential tuning of the Elastic Net is dominated by the first penalty tuned (λ 1 −λ 2 is similar to LASSO, and λ 1 −λ 2 is similar to Ridge). Only with 2D tuning (λ 1 +λ 2 ) does the Elastic Net perform comparably in both scenarios to the better single-penalty method. The pre-filter has little effect on prediction in most cases, except in the LASSO-favoring where it improves prediction by Ridge regression, and worsens prediction by λ 1 +λ 2 Elastic Net by decreasing the relative importance of the λ 1 penalty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.4.</head><figDesc>Fig. 4. Model selection guidelines allow reproducible outcome prediction from tumor gene expression data. Kaplan–Meier plots for cross-validated risk prediction for lung adenocarcinoma patients from Beer et al., using the Elastic Net. A naive model is overfit to training data, as evidenced by reduced prediction accuracy in cross-validation compared with resubstitution in the training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><figDesc>Funding: National Science Foundation grant (NSF DBI-1053486 to C.H.). Canada Foundation for Innovation (CFI #12301 and CFI #203383 to I.J.) and Ontario Research Fund (GL2-01-030 to I.J.) supporting computational analysis, partially; Canada Research Chair Program (to I.J. in part); Ontario Ministry of Health and Long Term Care, in part. The views expressed do not necessarily reflect those of the OMOHLTC. Conflict of Interest: none declared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>Table 1. 1 and λ 2 penalties: @BULLET optimization of λ 1 with λ 2 set to zero, followed by optimization of λ 2 (λ 1 −λ 2 method), @BULLET optimization of λ 2 with λ 1 set to zero, followed by λ 1 (λ 2 −λ 1 method) and @BULLET a 2D optimization of λ 1 and λ 2 simultaneously (λ 1 +λ 2 method).</figDesc><table>Simulated genomic predictor variables associated with a survival 
outcome 

Variable 
Within-group 
correlation 

Association (β i ) 
No. of variables 

+λ 1 
+λ 2 
+λ 1 
+λ 2 
+λ 1 
+λ 2 

A 
0.8 
0.2 
0.5 
0.2 
10 
100 
B 
0 
0.2 
0.5 
0.2 
10 
100 
C 
0.8 
0.2 
0.3 
0.2 
10 
170 
D 
0 
0 
0.3 
0 
10 
170 
E 
0.8 
– 
0 
– 
50 
0 
F 
0 
– 
0 
– 
450 
0 

Simulated variables were standard normally distributed with population mean of within-
group pair-wise Pearson's correlations indicated, and zero correlation between groups. 
Association indicates the coefficient of the variables when creating the risk for each 
sample according to Equation (1). +λ 1 and +λ 2 refer to the LASSO and Ridge-favoring 
scenarios, respectively. 

1996), Ridge regression (Hoerl and Kennard, 1970) and the Elastic Net with 
three methods of tuning the λ </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>dominated by the first penalty tuned, as evidenced by the similarity of values of that penalty to the Ridge or LASSO penalty shown in the adjacent histogram, and smaller values of the second penalty tuned compared with the other single-penalty regression or sequential-tuning regression. Assessment of model prediction and precision of variable selection are correspondingly similar for these methods (Fig. 3 and Supplementary Fig. S3). Note the different y-axis scale for λ 2 −λ 1 Elastic Net and Ridge regression in the LASSO-favoring scenario. Application of a univariate pre-filter reduced the relative influence of the λ 1 penalty, particularly in the LASSO-favoring scenario (for example, 9 versus 10). Univariate pre-filtering (P &lt; 0.1) reduced the tuned values of all penalty parameters and, in particular, reduced the influence of λ 1 relative to λ 2 in the λ 1 +λ 2 Elastic Net (panels 9 versus 10 and 19 versus 20). These results show that sequential tuning of the λ 1 penalty Elastic Net (λ 2 −λ 1 and λ 1 −λ 2 methods) is not adequate to enjoy any benefit over LASSO and Ridge regression, and that even in a problem where the λ 1 penalty is preferred, application of a univariate pre-filter causes the λ 2 penalty to dominate the λ 1 +λ 2 Elastic Net.</figDesc><table>10:14 21/11/2011 Bioinformatics-btr591.tex] 

Page: 3402 3399–3406 

L.Waldron et al. 

Fig. 2. Optimized values of the Elastic Net tuning parameters in simulated scenarios favoring the LASSO and the Ridge penalties, with comparison to LASSO 
and Ridge regression. Selected values of the Elastic Net tuning parameters depend on the nature of the problem at hand (left half versus right half), whether 
pre-filtering precedes the tuning (inner left versus right), and the tuning strategy (represented in each row). In both scenarios, with and without pre-filtering, 
sequential tuning of the Elastic Net penalties (λ 1 −λ 2 and λ 2 −λ 1 methods) was </table></figure>

			<note place="foot" n="3400"> at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="3402"> at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="1">. Data quality control and normalization: in addition to normalization and quality control appropriate for the data at hand, features should be transformed to the same scale, as the impact of penalization on coefficients is scale-dependent (Tibshirani, 1997). 2,8. Assess prediction accuracy in independent test data or by cross-validation for small sample sizes: overfitting in training data was evident in simulations and both experimental datasets analyzed, despite model tuning by optimizing cross-validated partial log-likelihood. Overfitting of highdimensional data is a well-known issue (Simon et al., 2011), that may be inevitable in high-dimensional model training. It is thus necessary to assess model prediction accuracy in samples not used in any way for model training. For small sample sizes, split training/test evaluation produces downward bias and instability in estimated prediction accuracy, and cross-validation or 0.632 bootstrap resampling is preferable (Molinaro et al., 2005). 3. Non-specific feature filter: features with consistently low values or variance are likely to be affected primarily by noise, and should be removed without reference to the response variable prior to scaling this noise to the same variability as true signals in the next step. The specifics of this filter depend on knowledge of the data at hand, e.g. removing genes never significantly above background in a microarray dataset or metagenomic taxa never above a relative abundance cutoff. 4. Scaling of predictor variables: the effect of penalization depends on the magnitude of coefficients and therefore on the scale of the coefficients. Therefore, predictor variables should be on a comparable scale or scaled if they are not. Scaling is done in training data only, and the transformations determined in training data are then applied to the test data. 5. Optional application of a univariate pre-filter: univariate prefilters have commonly been used for feature selection and to reduce computation time for model training. In an L 1favoring simulated scenario, a pre-filter reduced the relative importance of the L 1 penalty in the Elastic Net, which worsened predictions, but it improved the otherwise inferior predictions of Ridge regression. It had little influence in an L 2-favoring simulated scenario. We therefore recommend caution when applying a univariate pre-filter with the Elastic Net. 6-7. Model selection in training samples: properties of a given dataset may favor either Ridge-or LASSO-penalized regression. The Elastic Net can provide the advantages of both penalties and perform no worse than the better of its single-penalty counterparts, provided an adequate 2D tuning strategy is used. In simulated high-dimensional data with few true predictors and many noise variables, LASSO provided the best prediction, with short computation time. In a contrasting scenario with many weakly correlated variables with equal predictive value, Ridge regression performed superior to the LASSO. In both instances, the Elastic Net performed comparably to the better of the single-penalty methods, provided that (i) a 2D tuning strategy was used (λ 1 +λ 2 method) and no univariate pre-filter was applied. This flexibility of the Elastic Net comes at the cost of substantially greater computation time. Repeated tuning the penalty parameter(s) with different foldings of the samples for 10-fold cross-validation, and selecting the model associated with highest CVL, produced some improvement in prediction accuracy. 3404 at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Gene-expression profiles predict survival of patients with lung adenocarcinoma</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">G</forename>
				<surname>Beer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="816" to="824" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Reader&apos;s reaction to &quot;Dimension reduction for classification with gene expression microarray data</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">L</forename>
				<surname>Boulesteix</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Appl. Genet. Mol. Biol</title>
		<editor>Dai et al</editor>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting survival from microarray data-a comparative study</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">M</forename>
				<surname>Bøvelstad</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2080" to="2087" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Random Forests</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Breiman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Boosting algorithms: regularization, prediction and model fitting</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Bühlmann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Sci</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="477" to="505" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">A limited memory algorithm for bound constrained optimization</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">H</forename>
				<surname>Byrd</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1190" to="1208" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">A five-gene signature and clinical outcome in non–small-cell lung cancer</title>
		<author>
			<persName>
				<forename type="first">H.-Y</forename>
				<surname>Chen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="page" from="11" to="20" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Regression models and life-tables</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">R</forename>
				<surname>Cox</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="187" to="220" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Sure independence screening for ultrahigh dimensional feature space</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Fan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Lv</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="849" to="911" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">L1 penalized estimation in the Cox proportional hazards model</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">J</forename>
				<surname>Goeman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometr. J. Biometri. Zeitsch</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="70" to="84" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Penalized Cox regression analysis in the high-dimensional and low-sample size settings, with applications to microarray gene expression data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Gui</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Li</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="3001" to="3008" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Model selection: beyond the Bayesian/frequentist divide</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Guyon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="61" to="87" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<monogr>
		<title level="m" type="main">Regression modeling strategies: with applications to linear models, logistic regression, and survival analysis</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">E</forename>
				<surname>Harrell</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Multivariable prognostic models: issues in developing models, evaluating assumptions and adequacy, and measuring and reducing errors</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">E</forename>
				<surname>Harrell</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Jr</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Med</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="361" to="387" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Least angle and 1 penalized regression: a review</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hesterberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Surv</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="61" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Ridge regression: biased estimation for nonorthogonal problems</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">E</forename>
				<surname>Hoerl</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">W</forename>
				<surname>Kennard</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<monogr>
		<title level="m" type="main">Applied survival analysis: regression modeling of time to event data</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">W</forename>
				<surname>Hosmer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Lemeshow</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>J. Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Over-optimism in bioinformatics: an illustration</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Jelizarow</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Microbial ecology: human gut microbes associated with obesity</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">E</forename>
				<surname>Ley</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">444</biblScope>
			<biblScope unit="page" from="1022" to="1023" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Prediction of cancer outcome with microarrays: a multiple random validation strategy</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Michiels</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet</title>
		<imprint>
			<biblScope unit="volume">365</biblScope>
			<biblScope unit="page" from="488" to="492" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Prediction error estimation: a comparison of resampling methods</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">M</forename>
				<surname>Molinaro</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="3301" to="3307" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">A human gut microbial gene catalogue established by metagenomic sequencing</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Qin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">464</biblScope>
			<biblScope unit="page" from="59" to="65" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">R: A Language and Environment for Statistical Computing</title>
	</analytic>
	<monogr>
		<title level="j">Development Core Team</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Systemic inflammatory response predicts prognosis in patients with advanced-stage colorectal cancer</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Sharma</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Colorectal Cancer</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="331" to="337" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Gene expression-based survival prediction in lung adenocarcinoma: a multi-site, blinded validation study</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Shedden</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="822" to="827" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<monogr>
		<title level="m" type="main">Design and analysis of DNA microarray investigations</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">M</forename>
				<surname>Simon</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Using cross-validation to evaluate predictive accuracy of survival risk classifiers based on high-dimensional data</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">M</forename>
				<surname>Simon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brief. Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="203" to="214" />
			<date type="published" when="2011-08-30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">bioinformatics.oxfordjournals.org/ Downloaded from Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="14" to="21" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>btr591. .tex</note>
</biblStruct>

<biblStruct   xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName>
				<surname>Page</surname>
			</persName>
		</author>
		<imprint>
			<biblScope unit="page" from="3399" to="3406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Waldron</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Gene expression patterns of breast carcinomas distinguish tumor subclasses with clinical implications</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Sørlie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA, 98</title>
		<meeting>. Natl Acad. Sci. USA, 98</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="10869" to="10874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Gene set enrichment analysis: A knowledge-based approach for interpreting genome-wide expression profiles</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Subramanian</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="15545" to="15550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the Lasso</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">The lasso method for variable selection in the Cox model</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Med</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="385" to="395" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Univariate shrinkage in the Cox model for high dimensional data</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">J</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Appl. Genet. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="21" to="21" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">An obesity-associated gut microbiome with increased capacity for energy harvest</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">J</forename>
				<surname>Turnbaugh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">444</biblScope>
			<biblScope unit="page" from="1027" to="1131" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<analytic>
		<title level="a" type="main">A gene-expression signature as a predictor of survival in breast cancer</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">J</forename>
				<surname>Van De Vijver</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">347</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b37">
	<monogr>
		<title level="m" type="main">Modern Applied Statistics with S</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">N</forename>
				<surname>Venables</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">D</forename>
				<surname>Ripley</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-validation in survival analysis</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">J</forename>
				<surname>Verweij</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">C</forename>
				<surname>Van Houwelingen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Med</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2305" to="2314" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b39">
	<analytic>
		<title level="a" type="main">Penalized likelihood in Cox regression</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">J</forename>
				<surname>Verweij</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">C</forename>
				<surname>Van Houwelingen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Med</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2427" to="2436" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b40">
	<analytic>
		<title level="a" type="main">Molecular profiling of non-small cell lung cancer and correlation with disease-free survival</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">A</forename>
				<surname>Wigle</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Res</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="3005" to="3008" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b41">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Yuan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Lin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b42">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Zou</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hastie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="301" to="301" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>