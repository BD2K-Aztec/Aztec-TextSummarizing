
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">QVZ: lossy compression of quality values</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Greg</forename>
								<surname>Malysa</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Mikel</forename>
								<surname>Hernaez</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Idoia</forename>
								<surname>Ochoa</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Milind</forename>
								<surname>Rao</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Karthik</forename>
								<surname>Ganesan</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Tsachy</forename>
								<surname>Weissman</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">QVZ: lossy compression of quality values</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btv330</idno>
					<note type="submission">Received on November 19, 2014; revised on April 7, 2015; accepted on April 9, 2015</note>
					<note>Sequence analysis *To whom correspondence should be addressed. Associate Editor: Inanc Birol Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Recent advancements in sequencing technology have led to a drastic reduction in the cost of sequencing a genome. This has generated an unprecedented amount of genomic data that must be stored, processed and transmitted. To facilitate this effort, we propose a new lossy com-pressor for the quality values presented in genomic data files (e.g. FASTQ and SAM files), which comprise roughly half of the storage space (in the uncompressed domain). Lossy compression allows for compression of data beyond its lossless limit. Results: The proposed algorithm QVZ exhibits better rate-distortion performance than the previously proposed algorithms, for several distortion metrics and for the lossless case. Moreover, it allows the user to define any quasi-convex distortion function to be minimized, a feature not supported by the previous algorithms. Finally, we show that QVZ-compressed data exhibit better performance in the genotyping than data compressed with previously proposed algorithms, in the sense that for a similar rate, a genotyping closer to that achieved with the original quality values is obtained. Availability and implementation: QVZ is written in C and can be downloaded from https://github. com/mikelhernaez/qvz.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There has been a recent explosion of interest in genome sequencing, driven by advancements in the sequencing technology. Although early sequencing technologies required years to capture a 3 billion nucleotide genome (<ref type="bibr" target="#b19">Schatz and Langmead, 2013</ref>), genomes as large as 22 billion nucleotides are now being sequenced within days (<ref type="bibr" target="#b22">Zimin et al., 2014</ref>) using next-generation sequencing technologies (<ref type="bibr" target="#b16">Metzker, 2010</ref>). Further, the cost of sequencing a human-length genome has dropped from billions of dollars to merely $4000 (http://systems.illumina.com/systems/hiseq-x-sequencing-system.ilmn) within the past 15 years (<ref type="bibr" target="#b7">Hayden, 2014</ref>). These developments in efficiency and affordability have allowed many to envision wholegenome sequencing as an invaluable tool to be used in both personalized medical care and public health (<ref type="bibr" target="#b0">Berg et al., 2011</ref>). In anticipation of the storage challenges that increasingly large and ubiquitous genomic datasets could present, compression of the raw data generated by sequencing machines has become an important topic. The output data of the sequencing machines is generally stored in the widely accepted FASTQ format (<ref type="bibr" target="#b16">Metzker, 2010</ref>). A FASTQ file dedicates four lines to each fragment of a genome (a 'read') analyzed by the sequencing machine. The first line contains a header with some identifying information, the second lists the nucleotides in the read, the third is similar to the first one and the fourth lists a 'quality value' (also referred to as quality score) for each nucleotide. The quality values are generally stored using the Phred score, which corresponds to the particular number Q ¼ À10log 10 P, where P is an estimate (calculated by the base calling software running on the sequencing machine) of the probability that the corresponding nucleotide in the read is in error. These scores are commonly represented in the FASTQ file with an ASCII alphabet ½33 : 73 or ½64 : 104, where the value correspondsto Q þ 33 or Q þ 64, respectively. In addition, the information contained in the FASTQ files is also found in the SAM files (<ref type="bibr" target="#b13">Li et al., 2009</ref>), which store the information pertaining to the alignment of the reads to a reference. Quality values, which comprise more than half of the compressed data, have proven to be more difficult to compress than the reads (<ref type="bibr" target="#b1">Bonfield and Mahoney, 2013</ref>). Thus, generating better compression tools for quality values is crucial for reducing the storage required for large files. Unlike nucleotide information, the quality values generated by sequencing machines tend to exhibit predictable behavior within each read. Strong correlations exist between adjacent quality values as well as the trend that quality values degrade drastically as a read progresses (<ref type="bibr" target="#b9">Kozanitis et al., 2011</ref>). There is also evidence that quality values are corrupted by some amount of noise introduced during sequencing (<ref type="bibr" target="#b1">Bonfield and Mahoney, 2013</ref>). These features are well explained by imperfections in the base-calling algorithms, which estimate the probability that the corresponding nucleotide in the read is in error (<ref type="bibr" target="#b3">Das and Vikalo, 2012</ref>). Further, applications which operate on reads (referred to as 'downstream applications') often make use of the quality values in a heuristic manner. This is particularly true for sequence alignment algorithms (<ref type="bibr" target="#b10">Langmead et al., 2009;</ref><ref type="bibr" target="#b12">Li and Durbin, 2009</ref>) and single-nucleotide polymorphism (SNP) calling (<ref type="bibr" target="#b4">DePristo et al., 2011;</ref><ref type="bibr" target="#b11">Li, 2011</ref>), the latter having been shown to be resilient to changes in the quality values (in the sense that, in general, little is compromised in performance when quality values are modified (<ref type="bibr" target="#b17">Ochoa et al., 2013;</ref><ref type="bibr" target="#b21">Yu et al., 2014</ref>) (http://www.illumina.com/documents/products/ whitepapers/whitepaper_datacompression.pdf). Based on these observations, lossy (as opposed to lossless) compression of quality values emerges as a natural candidate for significantly reducing storage requirements while maintaining adequate performance of downstream applications. While rate-distortion theory provides a framework to evaluate lossy compression algorithms, the criterion under which the goodness of the reconstruction should be assessed is a crucial question. It makes sense to pick a distortion measure by examining how different distortion measures affect the performance of downstream applications, but the abundance of applications and variations in how quality values are used makes this choice too dependent on the specifics of the applications considered. These trade-offs suggest that an ideal lossy compressor for quality values should not only provide the best possible compression and accommodate downstream applications, but it should provide flexibility to allow a user to pick a desired distortion measure and/or rate. In this work, we present such a scheme which we call QVZ ('Quality Values Zip'), which achieves significantly better ratedistortion performance than any of the existing algorithms. Specifically, the proposed algorithm obtains up to four times better compression than previously proposed algorithms for the same average distortion. In addition, QVZ achieves lossless compression. Moreover, we analyze the effect of QVZ on the genotyping and show that better results are obtained than with the previously proposed algorithms. Finally, we present some preliminary results that suggest that lossy compression could potentially improve the genotyping with respect to the uncompressed data. This may be due to the inherently noisy nature of the quality values, in ways that will be thoroughly investigated in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Survey of lossy compressors for quality values</head><p>Lossy compression for quality values has recently started to be explored. Slimgene (<ref type="bibr" target="#b9">Kozanitis et al., 2011</ref>) fits fixed-order Markov encodings for the differences between adjacent quality values and compresses the prediction using a Huffman code (ignoring whether or not there are prediction errors). Q-Scores Archiver (<ref type="bibr" target="#b20">Wan et al., 2012</ref>) quantizes quality values via several steps of transformations and then compresses the lossy data using an entropy encoder. Fastqz (<ref type="bibr" target="#b1">Bonfield and Mahoney, 2013</ref>) uses a fixed-length code, which represents quality values above 30 using a specific byte pattern and quantizes all lower quality values to 2. Scalce (<ref type="bibr" target="#b6">Hach et al., 2012</ref>) first calculates the frequencies of different quality values in a subset of the reads of a FASTQ file. Then the quality values which achieve local maxima in frequency are determined. Anytime these local maximum values appear in the FASTQ file, the neighboring values are shifted to within a small offset of the local maximum, thereby reducing the variance in quality values. The result is compressed using an arithmetic encoder. QualComp (<ref type="bibr" target="#b17">Ochoa et al., 2013</ref>) applied rate-distortion theory as a framework for designing a lossy compression algorithm when mean-squared error (MSE) is the distortion measure. Quality value data are first clustered using a k-means algorithm and then an optimization problem is solved to minimize MSE of the compressed output with respect to a rate constraint. BEETL (<ref type="bibr" target="#b8">Janin et al., 2013</ref>) first applies the Burrows–Wheeler Transform to reads and uses the same transformation on the quality values. Then, the nucleotide suffixes generated by the Burrows–Wheeler Transform are scanned. Groups of suffixes which start with the same k bases while also sharing a prefix of at least k bases are found. All the quality values for the group are converted to a mean quality value, taken within the group or across all the groups. RQS (<ref type="bibr" target="#b21">Yu et al., 2014</ref>) first generates off-line a dictionary of commonly occurring k-mers throughout a population-sized read dataset of the species under consideration. It then computes the divergence of the k-mers within each read to the dictionary and uses that information to decide whether to preserve or discard the corresponding quality values. PBlock (<ref type="bibr">Cá novas et al., 2014</ref>) allows the user to determine a threshold for the maximum per-symbol distortion. The first quality value in the file is chosen as the first 'representative'. Quality values are then quantized symbol-by-symbol to the representative if the resulting distortion would fall within the threshold. If the threshold is exceeded, the new quality value takes the place of the representative and the process continues. The algorithm keeps track of the representatives and run-lengths, which are compressed losslessly at the end. RBlock (<ref type="bibr">Cá novas et al., 2014</ref>) uses the same process, but the threshold instead sets the maximum allowable ratio of any quality value to its representative as well as the maximum value of the reciprocal of this ratio. (<ref type="bibr">Cá novas et al., 2014</ref>) also compared the performance of existing lossy compression schemes for different distortion measures. Finally, Illumina proposed a new binning scheme for reducing the size of the quality values. This binning scheme has been implemented in the state-of-the-art compression tools CRAM (<ref type="bibr" target="#b5">Fritz et al., 2011</ref>) and DSRC2 (<ref type="bibr" target="#b18">Roguski and Deorowicz, 2014</ref>). To our knowledge, and based on the results of Cá novas et al.</p><p>(2014), RBlock, PBlock and QualComp provide the best ratedistortion performance among existing lossy compression algorithms for quality values that do not use any extra information. For this reason, in Section 3 we use RBlock, PBlock and QualComp as a representation of the existing state-of-the-art when comparing with QVZ, together with CRAM and DSRC2, which apply Illumina's binning scheme. For completeness, we also compare the lossless performance of QVZ with that of CRAM, DSRC2 (in their lossless mode) and gzip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QVZ: lossy compression of quality values</head><p>As described previously, we seek to compress the quality scores presented in the genomic data. Let N be the number of quality score sequences to be compressed. The proposed algorithm assumes that all the quality score sequences are of the same length L (for trimmed or hard-clipped reads, please refer to the Supplementary Data). Each sequence consists of ASCII characters representing the scores, belonging to an alphabet X, for example X ¼ ½33 : 73. These quality score sequences are extracted from the genomic file (e.g. FASTQ and SAM files) prior to compression. We model the quality score sequence X ¼ ½X 1 ; X 2 ;. .. ; X L  by a</p><p>Markov chain of order one: we assume the probability that X i takes a particular value depends on previous values only through the value of X iÀ1. We further assume that the quality score sequences are independent and identically distributed (i.i.d.). We use a Markov model based on the observation that quality scores are highly correlated with their neighbors within a single sequence, and we refrain from using a higher order Markov model to avoid the increased overhead and complexity this would produce within our algorithm. The Markov model is defined by its transition probabilities PðX i jX iÀ1 Þ, for i 2 1; 2;. .. ; L, where PðX 1 jX 0 Þ ¼ PðX 1 Þ. QVZ finds these probabilities empirically from the entire dataset to be compressed and uses them to design a codebook. The codebook is a set of quantizers indexed by position and previously quantized value (the context). These quantizers are constructed using a variant of the Lloyd–Max algorithm (<ref type="bibr" target="#b14">Lloyd, 1982</ref>). After quantization, a lossless, adaptive arithmetic encoder is applied to achieve entropy-rate compression. In summary, the steps taken by QVZ are as follows:</p><p>1. Compute the empirical transition probabilities of a Markov-1 Model from the data. 2. Construct a codebook (Section 2.2) using the Lloyd–Max algorithm (Section 2.1). 3. Quantize the input using the codebook and run the arithmetic encoder over the result (Section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Lloyd–Max quantizer</head><p>Given a random variable X governed by the probability mass function PðÁÞ over the alphabet X of size K, let D 2 R KÂK be a distortion matrix where each entry D x;y ¼ dðx; yÞ is the penalty for reconstructing symbol x as y. We further define Y to be the alphabet of the quantized values of size M K. Thus, a Lloyd–Max quantizer, denoted hereafter as LMðÁÞ, is a mapping X ! Y that minimizes an expected distortion. Specifically, the Lloyd–Max quantizer seeks to find a collection of boundary points b k 2 X and reconstruction points y k 2 Y, where k 2 f1; 2;. .. ; Mg, such that the quantized value of symbol x 2 X is given by the reconstruction point of the region to which it belongs (<ref type="figure" target="#fig_1">Fig. 1</ref>). For region k, any x 2 fb kÀ1 ;. .. ; b k À 1g is mapped to y k , with b 0 being the lowest score in the quality alphabet and b M the highest score plus one. Thus, the Lloyd–Max quantizer aims to minimize the expected distortion by solving</p><formula>fb k ; y k g M k¼1 ¼ argmin b k ;y k X M j¼1 X bjÀ1 x¼bjÀ1</formula><p>PðxÞdðx; y j Þ:</p><formula>(1)</formula><p>To approximately solve Equation (1), which is an integer programming problem, we employ an algorithm which is initialized with uniformly spaced boundary values and reconstruction points taken at the midpoint of these bins. For an arbitrary D and PðÁÞ, this problem requires an exhaustive search. We assume that the distortion measure d(x, y) is quasi-convex over y with a minimum at y ¼ x, i:e: when x y 1 y 2 or y 2 y 1 x; dðx; y 1 Þ dðx; y 2 Þ. If the distortion measure is quasi-convex, an exchange argument suffices to show the optimality of contiguous quantization bins and a reconstruction point within the bin. The following steps are iterated until convergence: 1. Solving for y k : We first minimize Equation (1) partially over the reconstruction points given boundary values. The reconstruction points are obtained as,</p><formula>y k ¼ argmin y¼fb kÀ1 ;...;b k À1g X b k À1 x¼b kÀ1</formula><p>PðxÞdðx; yÞ; 8 k ¼ 1; 2;. .. ; M:</p><formula>(2)</formula><p>2. Solving for b k : This step minimizes Equation (1) partially over the boundary values given the reconstruction points. b k could range from fy k þ 1;. .. ; y kþ1 g and is chosen as the largest point where the distortion measure to the previous reconstruction value y k is lesser than the distortion measure to the next reconstruction value y kþ1 , i.e.Note that this algorithm, which is a variant of the Lloyd–Max quantizer, converges in at most K steps. Given a distortion matrix D, the defined Lloyd–Max quantizer depends on the number of regions M and the input probability mass function PðÁÞ. Therefore, we denote the Lloyd–Max quantizer with M regions as LM P M ðÁÞ and the quantized value of a symbol x 2 X as LM P M ðxÞ. An ideal lossless compressor applied to the quantized values can achieve a rate equal to the entropy of LM P M ðXÞ, which we denote by HðLM P M ðXÞÞ. For a fixed probability mass function PðÁÞ, the only varying parameter is the number of regions M. Since M needs to be an integer, not all rates are achievable. Because we are interested in achieving an arbitrary rate R, we define an extended version of the LM quantizer, denoted as LME. The extended quantizer consists of two LM quantizers with the numbers of regions given by q and q þ 1, each of them used with probability 1 À r and r, respectively (where 0 r 1). Specifically, q is given by the maximum number of regions such that HðLM P q ðXÞÞ &lt; R (which implies HðLM P qþ1 ðXÞÞ &gt; R). Then, the probability r is chosen such that the average entropy (and hence the rate) is equal to R, the desired rate. More formally,</p><formula>LME P R ðxÞ ¼ ( LM P q ðxÞ; w:p: 1 À r;</formula><p>LM P qþ1 ðxÞ; w:p: r; q ¼ max fx 2 f1;. .. ; Kg :</p><formula>HðLM P x ðXÞÞ Rg r ¼ R À HðLM P q ðXÞÞ HðLM P qþ1 ðXÞÞ À HðLM P q ðXÞÞ : (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Codebook generation</head><p>Because we assume the data follows a Markov-1 model, for a given position i 2 f1;. .. ; Lg we design as many quantizers Q i q as there were unique possible quantized values q in the previous context i – 1. This collection of quantizers forms the codebook for QVZ. For an unquantized quality score X i , we denote the quantized version as Q i , so Q ¼ ½Q 1 ; Q 2 ;. .. ; Q L  is the random vector representing a quantized sequence. The quantizers are defined as</p><formula>Q 1 ¼ LME PðX1Þ aHðX1Þ (5) Q i q ¼ LME PðXijQiÀ1¼qÞ aHðXijQiÀ1¼qÞ ; for i ¼ 2;. .. ; L (6)</formula><p>where a 2 ½0; 1 is the desired compression factor. a ¼ 0 corresponds to 0 rate encoding, a ¼ 1 to lossless compression and any value in between scales the input file size by that amount. Note that the entropies can be directly computed from the corresponding empirical probabilities. Next we show how the probabilities needed for the LMEs are computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Computation</head><p>of the probability P To compute the quantizers defined above, we require PðX iþ1 jQ i Þ, which must be computed from the empirical statistics PðX iþ1 jX i Þ found earlier. The first step is to calculate PðQ i jX i Þ recursively and then to apply Bayes rule and the Markov Chain property to find the desired probability:</p><formula>PðQ i jX i Þ ¼ X QiÀ1 PðQ i ; Q iÀ1 jX i Þ ¼ X QiÀ1 PðQ i jX i ; Q iÀ1 Þ X XiÀ1 PðQ iÀ1 ; X iÀ1 jX i Þ ¼ X QiÀ1 PðQ i jX i ; Q iÀ1 Þ X XiÀ1 PðQ iÀ1 jX iÀ1 ; X i ÞPðX iÀ1 jX i Þ ¼ X QiÀ1 PðQ i jX i ; Q iÀ1 Þ X XiÀ1 PðQ iÀ1 jX iÀ1 ÞPðX iÀ1 jX i Þ (7)</formula><p>Equation (7) follows from the fact that</p><formula>Q iÀ1 $ X iÀ1 $ X i form a Markov chain. Additionally, PðQ i jX i ; Q iÀ1 ¼ qÞ ¼ PðQ i q ðX i Þ ¼ Q i Þ,</formula><p>which is the probability that a specific quantizer produces Q i given previous context q. This can be found directly from r<ref type="bibr">[defined in Eq. (4)]</ref>and the possible values for q. We now proceed to compute the required conditional probability as</p><formula>PðX iþ1 jQ i Þ ¼ X Xi PðX i jQ i ÞPðX iþ1 jX i ; Q i Þ ¼ X Xi PðX i jQ i ÞPðX iþ1 jX i Þ (8) ¼ 1 PðQ i Þ X Xi PðQ i jX i ÞPðX i ; X iþ1 Þ; (9)</formula><p>where Equation (8) follows from the same Markov chain as earlier. Terms in Equation (9) are: (i) PðX i ; X iþ1 Þ: joint pmf computed empirically from the data, (ii) PðQ i jX i Þ: computed in Equation (7) and</p><p>(iii) PðQ i Þ: normalizing constant given by</p><formula>PðQ i ¼ qÞ ¼ X Xi PðQ i ¼ qjX i ÞPðX i Þ:</formula><p>The steps necessary to compute the codebook are summarized in Algorithm 1. Note that supportðXÞ denotes the support of the random variable X or the set of values that X takes with non-zero probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Generate codebook</head><p>Input: Transition probabilities PðX i jX iÀ1 Þ, compression factor a Output: Codebook: collection of quantizers fQ l q g P PðX 1 Þ Compute and store Q 1 based on P using Equation (5) for all columns i ¼ 2 to L do Compute PðQ iÀ1 jX iÀ1 ¼ xÞ 8x 2 supportðX iÀ1 Þ Compute PðX i jQ iÀ1 Þ 8q 2 supportðQ iÀ1 Þ for all q 2 supportðQ iÀ1 Þ do P PðX i jQ iÀ1 ¼ qÞ Compute and store Q i q based on P using Equation (6) end for end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Encoding</head><p>The encoding process is summarized in Algorithm 2. First, we generate the codebook and quantizers. For each read, we quantize all scores sequentially, with each value forming the left context for the next value. As they are quantized, scores are passed to an adaptive arithmetic encoder, which uses a separate model for each position and context. For a detailed explanation of the arithmetic encoder, we refer the reader to the Supplementary Data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2. Encoding of quality scores</head><p>Input: Set of N reads fX j g N j¼1</p><p>Output: Set of quantizers fQ l q g (codebook) and compressed representation of reads Compute empirical statistics of input reads Compute codebook fQ l q g according to Algorithm 1 for all j ¼ 1 to N do ½X 1 ;. .. ; X L  X j Q 1 Q 1 ðX 1 Þ for all i ¼ 2 to L do Q i Q i QiÀ1 ðX i Þ end for Pass ½Q 1 ;. .. ; Q L  to arithmetic encoder end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Clustering</head><p>The performance of the compression algorithm depends on the conditional entropy of each quality score given its predecessor. Earlier we assumed that the data were all i.i.d., but it is more effective to allow each read to be independently selected from one of several distributions. If we first cluster the reads into C clusters, then the variability within each cluster may be smaller. In turn, the conditional entropy would decrease and fewer bits would be required to encode X i at a given distortion level, assuming that an individual codebook is available unique to each cluster. Thus, QVZ has the option of clustering the data prior to compression. Specifically, it uses the K-means algorithm (<ref type="bibr" target="#b15">MacQueen et al., 1967</ref>), initialized using C quality value sequences chosen at random from the data. It assigns each sequence to a cluster by means of Euclidean distance. Then, the centroid of each cluster is computed as the mean vector of the sequences assigned to it. Because of the lack of convergence guarantees, we have incorporated a stop criterion that avoids further iterations once the centroids of the clusters have moved QVZ: lossy compression of quality valuesless than U units (in Euclidean distance). The parameter U is set to 4 by default, but it can be modified by the user. Finally, storing which cluster each read belongs to incurs a rate penalty of at most log 2 ðCÞ= L bits per symbol, which allows QVZ to reconstruct the series of reads in the same order as they were in the uncompressed input file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and discussion</head><p>To assess the performance of the proposed algorithm QVZ, we compare it with the state of the art lossy compression algorithms PBlock, RBlock (<ref type="bibr">Cá novas et al., 2014</ref>) and QualComp (<ref type="bibr" target="#b17">Ochoa et al., 2013</ref>). We also consider CRAM (<ref type="bibr" target="#b5">Fritz et al., 2011</ref>), DSRC2 (<ref type="bibr" target="#b18">Roguski and Deorowicz, 2014</ref>) and gzip. In this assessment, we focus on two aspects that we believe are important: the ratedistortion curve and the behavior in genotyping. The rate-distortion curve provides a framework for comparison that is independent of the downstream applications, which vary significantly in their use of quality scores. It also gives a measure of fidelity for each of the algorithms: how similar are the reconstructed quality scores to the original values? On the other hand, examining the behavior in genotyping aims to provide a comparison on how the different lossy compressors affect the downstream applications, which are widely used in practice. Specifically, we focus on SNP calling, because analyzing the effects of lossy compression on this application is of significant importance in practice. The dataset used for our analysis is the NA12878.HiSeq. WGS.bwa.cleaned.recal.hg19.20.bam, which corresponds to the chromosome 20 of a Homo sapiens individual. We downloaded it from the GATK bundle (http://tiny.cc/3i49tx). This dataset pertains to one of the most studied human individuals in the literature (<ref type="bibr" target="#b4">DePristo et al., 2011;</ref><ref type="bibr" target="#b23">Zook et al., 2014</ref>), making it a suitable baseline for comparison. We generated the SAM file from the BAM file and then extracted the quality score sequences from it. The dataset contains 51, 585, 658 sequences, each of length 101. We consider four more datasets for our study, namely, the chromosome 20 of the H.sapiens dataset SRR622461, the whole genome of a Saccharomyces cerevisiae (SRR1179906) and two ChIP-Seq datasets from a Mus musculus (SRR32209) and a Drosophila melanogaster (ERR011354). Because of space constraints, their analyses are presented in the Supplementary Data. The machine used to perform the experiments has the following specifications: 39 GB RAM, Intel Core i7-930 CPU at 2.80 GHz x 8 and Ubuntu 12.04 LTS. The next two subsections report on the results of our study as they pertain to rate-distortion and genotyping, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Rate-Distortion analysis</head><p>First, we describe the options used to run each algorithm. QVZ was run with the default parameters, multiple rates and different number of clusters. PBlock and RBlock (http://tiny.cc/kg49tx) were run with different values of p and r, respectively, and with m ¼ 1 (the default value). QualComp (http://tiny.cc/9b49tx) was run with three clusters and multiple rates, and CRAM and DSRC2 were run with the lossy mode that implements Illumina's proposed binning scheme. Finally, we also run each of the mentioned algorithms in the lossless mode, except QualComp, since it does not support lossless compression. We refer the reader to the Supplementary Data for more details. QVZ can minimize any quasi-convex distortion, if the corresponding matrix is provided, or any of the following three built-in distortion metrics: (i) the average MSE, where dðx; yÞ ¼ jx À yj 2 ;</p><p>(ii) the average L1 distortion, where dðx; yÞ ¼ jx À yj and (iii) the average Lorentzian distortion, where dðx; yÞ ¼ log 2 ð1 þ jx À yjÞ. Hereafter, we refer to each of them as QVZ-M, QVZ-A and QVZ-L, respectively. QVZ can also perform clustering prior to compression—similar to QualComp– using a user-specified number of clusters, so we ran it with 1, 3 and 5 clusters for each distortion metric to examine the effects of clustering on the rate-distortion curve. Assuming N reads of length L each, the distortion D used to compare the different algorithms is computed as</p><formula>D ¼ 1 NL X N k¼1 X L i¼1 dðx i ðkÞ; y i ðkÞÞ; (10)</formula><p>where x i ðkÞ denotes the quality score value of read k at position i, y i ðkÞ the corresponding reconstructed value (after lossy compression) and dðÁ; ÁÞ the distortion metric under consideration. Since QVZ can select to optimize for MSE, L1 or Lorentzian distortions, we provide results for all three. Any other distortion metric can be used for comparison, but we limit our attention to these three due to space constraints and refer the reader to the Supplementary Data for results on other distortion metrics. As a measure of rate, we use the final size of the quality score sequences after compression. The results are presented in<ref type="figure" target="#fig_2">Figure 2</ref>. As can be seen in<ref type="figure" target="#fig_2">Figure 2</ref>, QVZ outperforms the previously proposed algorithms for all three choices of distortion metric. Furthermore, although QualComp reconstructs the quality score sequences in a different order, QVZ maintains the original order. This is achieved by storing the cluster to which each quality scoresequence belongs, in contrast to QualComp which produces one file per cluster. Note that storing this information for C clusters would incur a cost of approximately Nlog 2 C bits, assuming uniform distribution of sequences across the clusters, which is not included for QualComp in<ref type="figure" target="#fig_2">Figure 2</ref>. The lossy modes of CRAM and DSRC2 can each achieve only one rate-distortion point, and both are outperformed by QVZ. We further observe that although QualComp outperforms RBlock and PBlock for low rates (in all three distortions), the latter two achieve a smaller distortion for higher rates. QVZ, however, outperforms all previously proposed algorithms in both low and high rates. QVZ's advantage becomes especially apparent for distortions other than MSE. It is also significant that QVZ achieves a zero distortion at a rate at which the other lossy algorithms exhibit positive distortion. In other words, QVZ achieves lossless compression faster than QualComp, RBlock or PBlock. In fact, due to its design, QualComp cannot achieve lossless compression, even for very high rates. Moreover, QVZ also outperforms the lossless compressors CRAM and gzip and achieves similar performance to that of DSRC2 (<ref type="figure" target="#tab_1">Table 1</ref>). Finally, we observe that applying clustering prior to compression in QVZ is especially beneficial at low rates. For higher rates, the performance of 1, 3 and 5 clusters is almost identical. Therefore we recommend using multiple clusters at low rates for better distortion and 1 cluster at high rates for faster compression. The results obtained from this analysis are in line with the ones presented in the Supplementary Data for the other studied datasets. QVZ compares favorably with the other schemes insofar as running times are concerned. For example, it requires approximately 13 min to compress the analyzed dataset with one cluster and 12 min to decompress it. If three clusters are used instead, the compression time increases to 18 min. QualComp, on the other hand, takes more than 1 h to cluster the data (if more than one cluster is used): around 90 min to compute the necessary statistics and 20 min to finally compress the quality scores. The decompression is done in 15 min. DSRC2 requires 20 min to compress and decompress, whereas CRAM employs 14 min to compress and 4 min to decompress. Finally, both Pblock and Rblock take around 4 min to compress and decompress, being the algorithms with the least running times among those that we analyzed. The running times of gzip to compress and decompress are 7 and 30 min, respectively. In terms of memory usage, QVZ uses 5.7 GB to compress the analyzed dataset and less than 1 MB to decompress, whereas QualComp employs less than 1 MB for both operations. Pblock and Rblock have more memory usage than QualComp, but this is still below 40 MB to compress and decompress. DSRC2 uses 3 GB to compress and 5 GB to decompress, whereas CRAM employs 2 GB to compress and 3 GB to decompress. Finally, gzip uses less than 1 MB for both operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Genotyping analysis</head><p>To perform the genotyping analysis, and following a similar analysis to the one presented in<ref type="bibr">Cá novas et al. (2014)</ref>, we compare the SNP calling of the original SAM file with that obtained when the quality values are replaced with the reconstructed quality values. Note that we replace the quality scores directly in the SAM file: we do not regenerate the SAM file by running an alignment program. The reason is that similar to SNP calling, the alignment program uses quality values (in general) to generate the alignment, and thus by running both it will become impossible to separate the effect that quality scores have on SNP calling from alignment. Note that if the alignment program does not use the quality values [e.g. BWA (<ref type="bibr" target="#b12">Li and Durbin, 2009)]</ref>, modifying them in the original SAM file is equivalent to re-running the alignment program. We use the programs provided by the HTS library (http://www. htslib.org) to perform SNP calling, with the parameters and commands suggested by the SNP calling workflow therein (exact commands can be found in the Supplementary Data). Any SNP calling program could have been used for this purpose.<ref type="figure" target="#fig_3">Figure 3</ref>shows the number of false negatives (FN) versus the number of false positives (FP) with respect to the uncompressed version. The point (0, 0) corresponds to lossless compression. We chose to show the performance of QVZ-M with three clusters for the sake of clarity, although similar performance was obtained for the other configurations of QVZ (see the Supplementary Data). As shown previously in the rate-distortion analysis, QVZ achieves lossless compression and thus same genotyping as the uncompressed version, with a file size of only 1626 MB, while RBlock needs 3229 MB. On the other hand, QualComp behaves similarly to QVZ, although its files are generally larger for the same genotyping results. Moreover, QualComp cannot achieve the same genotyping as the uncompressed version as it cannot generate a lossless file. When comparing with Illumina's binning, we observe that QVZ achieves a similar point in the genotyping with 491 MB, whereas DSRC2 and CRAM need 646 MB and 980 MB, respectively. The differences in convergence to the lossless genotyping between QVZ (and QualComp) and both PBlock and RBlock for this dataset are very interesting. While the variant calling of PBlock-and RBlock-reconstructed data does not generate many FP, it misses several SNPs (i.e., it generates more FN) before achieving perfect genotyping. On the other hand, using QVZ-and QualComp-reconstructed data seems to result in more calls with higher compression ratios. This behavior makes the number of FP increase as the file size decreases, while the number of true positives remains almost constant for different sizes. Even with a high compression ratio (small size), the observed number of true positives is nearly identical to the uncompressed version. Similar results are observed in the extra analyses provided in the Supplementary Data.This observation provided the motivation for our next experiment. Specifically, we wanted to explore whether among the FP called with QVZ (especially for low rates), there were actually true positives that were missed with the original SAM file. This situation is conceivable, as the quality scores are inherently noisy, so their lossy compression may serve to denoise them as well, thereby boosting the inferential power of the downstream applications. To verify if this was the case, we compared the SNP calling generated with the modified SAM files with what we refer to as the 'ground truth'. In particular, the 'ground truth' corresponds to the SNPs called over the same individual after following the Best Practices workflow for SNP calling provided by the Broad Institute. The corresponding VCF file containing the SNPs can be found in the Broad Institute Resource Bundle. Note that the SAM file used for this purpose has been previously pre-processed according to the Best Practices provided by the Broad Institute, thus removing most of the FP introduced by duplicates and bad alignments around indels.<ref type="figure" target="#fig_4">Figure 4</ref>shows the difference between the number of TPs and FPs called with the uncompressed version and the different lossy versions with respect to the 'ground truth'. A similar convergence to the lossless case can be seen, just as before when comparing to the unmodified SAM file. In the case of PBlock and RBlock, no new TPs are found. This seems to be a consequence of the fact that fewer SNPs are called than with the uncompressed version. Moreover, fewer FP are also called than with the uncompressed version, as shown in the lower-left quadrant of the figure. We also observe that with QVZ, fewer FPs and more TPs are obtained than those obtained with the Illumina's binning, while achieving more compression. The upper-left quadrant deserves special attention. It contains those cases where not only are more true positives achieved, but there are also fewer FN. This means that in these cases the genotyping improves over the uncompressed version. It is intriguing to observe that all the files above 700 MB generated with our proposed algorithm QVZ are in this quadrant. A similar behavior is also observed when the 'ground truth' is chosen as the one provided by the NIST Proposed Standard, as shown in the Supplementary Data. This is a very interesting finding, as it seems to suggest that the proposed lossy compressor can potentially be used not only as a means to reduce the storage requirements but also for improving the downstream analysis performed on the data. These preliminary findings are admittedly anecdotal. However, they provide a glance of the potential of applying lossy compression for genotype improvement. Further analysis in this direction is left for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we have presented QVZ, a new lossy compression algorithm for quality scores in genomic data. The proposed algorithm can work for several distortion metrics, including any quasi-convex distortion metric provided by the user, a feature not supported by the previously proposed algorithms. Moreover, it exhibits better rate-distortion performance. Unlike some of the previously proposed algorithms, QVZ also allows for lossless compression and a seamless transition from lossy to the lossless with increasing rate. Moreover, we have shown that in comparison to previously proposed lossy algorithms, using QVZcompressed data achieves genotyping performance closer to that obtained with uncompressed quality values, for similar compression rates. Finally, we have obtained some preliminary and promising results which suggest that lossy compression could be beneficial not only for storage and transmission but also for boosting performance in downstream applications. The extent of this phenomenon, the relation between the distortion criterion, the compression rate, the characteristics of the noise in the quality values and the resulting performance boosts are due further investigation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.1.</head><figDesc>Fig. 1. Example of the boundary points and reconstruction points found by a Lloyd–Max quantizer, for M ¼ 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.2.</head><figDesc>Fig. 2. Rate-distortion curves of PBlock, RBlock, QualComp and QVZ, for MSE, L1 and Lorentzian distortions. In QVZ, c1, c3 and c5 denote 1, 3 and 5 clusters, respectively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.3.</head><figDesc>Fig. 3. SNP calling results of the original SAM file (NA12878), denoted as uncompressed, and those generated with the different lossy compression algorithms. Note that the y-axis refers to the FNs times minus one</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.4.</head><figDesc>Fig. 4. SNP calling results of the original SAM file (NA12878), denoted as uncompressed, and those generated with the different lossy compression algorithms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>V C The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 3122 Bioinformatics, 31(19), 2015, 3122–3129 doi: 10.1093/bioinformatics/btv330 Advance Access Publication Date: 28 May 2015 Original Paper</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>Table 1. Lossless results of the different algorithms for the NA12878 dataset QVZ (3 clusters) PBlock RBlock DSRC2 CRAM gzip</figDesc><table>Size (MB) 1632 
3229 
1625 
2000 
1999 

</table></figure>

			<note place="foot">at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">G.Malysa et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Golan Yona for providing initial motivation for this work and the anonymous reviewers for helpful suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This work was partially supported by a Stanford Graduate Fellowships Program in Science and Engineering, a fellowship from the Basque Government, a grant from the Center for Science of Information (CSoI) and the 1157849-1-QAZCC NSF grant. Conflict of Interest: none declared.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Deploying whole genome sequencing in clinical practice and public health: meeting the challenge one bin at a time</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Berg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genet. Med</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="499" to="504" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Compression of FASTQ and SAM format sequencing data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">K</forename>
				<surname>Bonfield</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">V</forename>
				<surname>Mahoney</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">59190</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Lossy compression of quality scores in genomic data</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Cá Novas</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2130" to="2136" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Onlinecall: fast online parameter estimation and base calling for illumina&apos;s next-generation sequencing</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Das</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Vikalo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1677" to="1683" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">A framework for variation discovery and genotyping using next-generation DNA sequencing data</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Depristo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="491" to="498" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient storage of high throughput DNA sequencing data using reference-based compression</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">H</forename>
				<surname>Fritz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">-Y</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="734" to="740" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Scalce: boosting sequence compression algorithms using locally consistent encoding</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Hach</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3051" to="3057" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Technology: the $1 000 genome</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">C</forename>
				<surname>Hayden</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">507</biblScope>
			<biblScope unit="page" from="294" to="295" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive reference-free compression of sequence quality scores</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Janin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="24" to="30" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Compressing genomic sequence fragments using slimgene</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Kozanitis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="401" to="413" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Ultrafast and memory-efficient alignment of short DNA sequences to the human genome</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Langmead</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">A statistical framework for SNP calling, mutation discovery, association mapping and population genetical parameter estimation from sequencing data</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Li</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2987" to="2993" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast and accurate short read alignment with Burrows-Wheeler transform</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Durbin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1754" to="1760" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">The Sequence Alignment/Map format and SAMtools</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Li</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2078" to="2079" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Lloyd</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Macqueen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Fifth Berkeley Symposium on Mathematical Statistics and Probability<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequencing technologies the next generation</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">L</forename>
				<surname>Metzker</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Genet</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="31" to="46" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Qualcomp: a new lossy compressor for quality scores based on rate distortion theory</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Ochoa</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">187</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">DSRC 2—industry-oriented compression of FASTQ files</title>
		<author>
			<persName>
				<forename type="first">Ł</forename>
				<surname>Roguski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Deorowicz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2213" to="2215" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">The DNA data deluge</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">C</forename>
				<surname>Schatz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Langmead</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spectr</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="28" to="33" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Transformations for the compression of FASTQ quality scores of next-generation sequencing data</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Wan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="628" to="635" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Traversing the k-mer landscape of NGS read datasets for quality score sparsification</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<forename type="middle">W</forename>
				<surname>Yu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: Research in Computational Molecular Biology</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="385" to="399" />
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequencing and assembly of the 22-gb loblolly pine genome</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Zimin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetics</title>
		<imprint>
			<biblScope unit="volume">196</biblScope>
			<biblScope unit="page" from="875" to="890" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Integrating human sequence data sets provides a resource of benchmark SNP and indel genotype calls</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">M</forename>
				<surname>Zook</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="246" to="251" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<monogr>
		<title level="m" type="main">QVZ: lossy compression of quality values</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>