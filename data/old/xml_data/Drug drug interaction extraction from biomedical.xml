
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining Drug drug interaction extraction from biomedical literature using syntax convolutional neural network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Zhehuan</forename>
								<surname>Zhao</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116024</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Zhihao</forename>
								<surname>Yang</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116024</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Ling</forename>
								<surname>Luo</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116024</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Hongfei</forename>
								<surname>Lin</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116024</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Jian</forename>
								<surname>Wang</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116024</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining Drug drug interaction extraction from biomedical literature using syntax convolutional neural network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btw486</idno>
					<note type="submission">Received on November 25, 2015; revised on June 26, 2016; accepted on July 18, 2016</note>
					<note>*To whom correspondence should be addressed. Associate Editor: Alfonso Valencia Contact: yangzh@dlut.edu.cn Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Detecting drug-drug interaction (DDI) has become a vital part of public health safety. Therefore, using text mining techniques to extract DDIs from biomedical literature has received great attentions. However, this research is still at an early stage and its performance has much room to improve. Results: In this article, we present a syntax convolutional neural network (SCNN) based DDI extraction method. In this method, a novel word embedding, syntax word embedding, is proposed to employ the syntactic information of a sentence. Then the position and part of speech features are introduced to extend the embedding of each word. Later, auto-encoder is introduced to encode the traditional bag-of-words feature (sparse 0–1 vector) as the dense real value vector. Finally, a combination of embedding-based convolutional features and traditional features are fed to the softmax classifier to extract DDIs from biomedical literature. Experimental results on the DDIExtraction 2013 corpus show that SCNN obtains a better performance (an F-score of 0.686) than other state-of-the-art methods. Availability and Implementation: The source code is available for academic use at http://202.118. 75.18:8080/DDI/SCNN-DDI.zip.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Adverse drug reactions (ADRs) lead to 300 000 deaths per year in the USA and Europe (<ref type="bibr" target="#b8">Businaro, 2013</ref>). Drug-drug interaction (DDI), which is broadly described as a change in the effect of one drug by the presence of another drug (<ref type="bibr" target="#b1">Baxter and Preston, 2010</ref>), is an important subset of ADRs. Landau reported that about 2.2 million people in USA, aged 57–85, faced potentially dangerous drug combinations (<ref type="bibr" target="#b24">Landau, 2009</ref>). As a result, detecting DDIs has become a vital part of public health safety. With rich DDIs knowledge, the patients can be prevented from taking two or more drugs simultaneously which will cause harmful interactions. Currently, some drug related databases, like DrugBank (<ref type="bibr" target="#b21">Knox et al., 2011</ref>) and Stockley's Drug Interactions (<ref type="bibr" target="#b1">Baxter and Preston, 2010</ref>), have been created to help detect DDIs. However, since the volume of biomedical literature is growing rapidly, a large number of valuable DDIs remain hidden in the unstructured biomedical texts. Thus, the automatic extraction of DDIs information from biomedical literature has become an important research area. In recent years, DDIExtraction 2011 (<ref type="bibr" target="#b29">Segura Bedmar et al., 2011</ref>) and 2013 challenges (<ref type="bibr" target="#b30">Segura Bedmar et al., 2013</ref>) have been held successfully. DDIExtraction 2011 only focuses on the identification of all possible DDIs while the 2013 challenge requires, in addition to DDI detection, the classification of each DDI, i.e. the DDIs need to be classified into four predefined DDI types: ADVICE, EFFECT, INT and MECHANISM (<ref type="bibr" target="#b15">Herrero-Zazo et al., 2013</ref>). ADVICE is assigned when a recommendation or advice regarding concomitant use of two drugs involved is described; EFFECT is assigned when the effect of the DDI is described; INT is assigned when a DDI appears in the text without any additional information provided; MECHANISM is assigned when a DDI is described by its pharmacokinetic mechanism. Existing DDI extraction methods can be roughly divided into two categories: the one-stage and two-stage methods. The one-stage method accomplishes DDI detection and classification simultaneously by training a multiclass SVM. It directly classifies each candidate instance into one of the five DDI types (ADVICE, EFFECT, INT, MECHANISM and NEGATIVE for the negative instances). The two-stage method divides the learning problem into two stages: first, all the DDIs are detected and second, the detected DDIs are classified into one of the four specific DDI types (ADVICE, EFFECT, INT and MECHANISM). In DDIExtraction 2013 challenge, FBK-irst team used a twostage method (<ref type="bibr" target="#b8">Chowdhury and Lavelli, 2013</ref>). It employs a hybrid kernel to detect DDIs and, then, assign them to one of the four DDI types by training four separate models (one-against-all), respectively. The method achieves an F-score of 0.651, which ranks top in the challenge. Later,<ref type="bibr" target="#b20">Kim et al. (2015)</ref>also proposed a two-stage method based on linear SVM using rich features, and obtained a higher F-score of 0.670 on the same corpus. The method uses the word feature, word pair feature, parse tree feature and noun phraseconstrained coordination feature. In addition, its one-against-one multiclass classification strategy also contributes greatly to its performance while the other methods choose one-against-all strategy. An example of the one-stage method is the one presented by UTurku team (<ref type="bibr" target="#b4">Bjö rne et al., 2013</ref>) in DDIExtraction 2013 challenge. It accomplishes DDI detection and classification tasks simultaneously by training a multiclass SVM (<ref type="bibr" target="#b10">Crammer and Singer, 2002</ref>) and classifies each candidate instance into one of the five DDI types (ADVICE, EFFECT, INT, MECHANISM and NEGATIVE). The method achieves the third best performance (an F-score of 0.594) in DDIExtraction 2013 challenge. Although many methods have been proposed, DDI extraction research is still at an early stage and its performance has much room to improve. For example, in DDIExtraction 2013 challenge, the best performance achieved is 0.651 in F-score (<ref type="bibr" target="#b8">Chowdhury and Lavelli, 2013</ref>). In addition, the state-of-the-art DDI extraction methods are all feature engineering based ones, i.e. they need to design effective features elaborately using various NLP tools and knowledge resources, which is still a labor-intensive and skill-dependent task. Therefore, the performance of these methods is heavily dependent on the choice of features. Finally, since DDIExtraction 2013 challenge is a multiclass classification problem, a multiclass classifier is needed. However, most methods with top performance train several binary SVMs to solve the multiclass classification problem (<ref type="bibr" target="#b8">Chowdhury and Lavelli, 2013;</ref><ref type="bibr" target="#b20">Kim et al., 2015</ref>). In fact, K or K (K-1)/2 (here K is the number of target classes) binary classifiers are needed when oneagainst-all or one-against-one strategies are chosen, respectively (<ref type="bibr" target="#b17">Hsu and Lin, 2002</ref>). Although such methods can achieve better performances, they increase the complexity of the DDI extraction. To solve these problems, we propose a syntax convolutional neural network (SCNN) based DDI extraction method. The method uses a novel word embedding (word embedding is a parameterized function that maps words to high-dimensional vectors and was first introduced by<ref type="bibr" target="#b2">Bengio et al. (2003)</ref>to fight the curse of dimensionality in the process of learning language model using neural network), the syntax word embedding, to introduce the syntactic information of a sentence which has proven to be helpful in boosting the performance of relation extraction (<ref type="bibr" target="#b7">Bunescu and Mooney, 2005</ref>). Then the syntax word embedding is extended by the position feature (<ref type="bibr" target="#b37">Zeng et al., 2014</ref>) and the part of speech (POS) feature to introduce the position and POS information. The latter is first introduced in our method to capture the POS information of each word. Later, auto-encoder (<ref type="bibr" target="#b16">Hinton and Zemel, 1994</ref>) is introduced to transfer the traditional sparse 0–1 features to the dense real value features before they are combined with the embedding-based convolutional features. Finally, the combined features are passed to the softmax (<ref type="bibr" target="#b37">Zeng et al., 2014</ref>) to learn the DDI classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and methods</head><p>In this section, we first introduce a SCNN-based one-stage method (SCNN 1 ) which directly classifies each candidate instance into one of the five DDI types (ADVICE, EFFECT, INT, MECHANISM and NEGATIVE). Then a SCNN-based two-stage method (SCNN 2 ) is presented since the performances of the two-stage methods are usually better than those of the one-stage ones. Our one-stage method SCNN 1 contains six processing steps as shown in<ref type="figure" target="#fig_0">Figure 1</ref>: 1. Negative instance filtering step that rebalances the datasets' class distribution by removing possible negative instances. 2. Preprocessing step that constructs an easily understood corpus for classifiers. 3. Learning word embedding step that generates the syntax word embedding using Enju parser (<ref type="bibr" target="#b28">Miyao and Tsujii, 2008</ref>) and word2vec (<ref type="bibr" target="#b27">Mikolov et al., 2013</ref>). 4. Feature extraction step that extracts the convolutional and traditional features. 5. Training the classifier 5 step that learns a five-class classifier based on the extracted features. 6. DDI detection and classification step that classifies each instance of the test set into one of the five DDI types using the convolutional neural network (CNN) model.The details are described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Negative instance filtering</head><p>Classification of the data with imbalanced class distribution has encountered a significant drawback of the performance attainable by most standard classifier learning algorithms which assume a relatively balanced class distribution and equal misclassification costs (<ref type="bibr" target="#b32">Sun et al., 2009</ref>). DDIExtraction 2013 challenge also suffers from the imbalanced class distribution problem (e.g. the ratio of the positive instances to the negative instances in the training set is 1:5.91). To alleviate this problem, we construct a less imbalanced corpus by removing possible negative examples with the following two rules: Rule 1: The instances in which two candidate drugs refer to the same drug are removed as any drug is unlikely to interact with itself (<ref type="bibr" target="#b8">Chowdhury and Lavelli, 2013</ref>). More specifically, the following two cases are considered: (i) two drugs have the same name; (ii) one drug is the abbreviation of the other drug (the method of finding the abbreviation is described in the Supplementary Material: the method of finding the abbreviation). Two examples are given as follows.</p><p>Same name: Interactions for Vitamin B2 (Riboflavin drug1 ): Alcohol impairs the intestinal absorption of riboflavin drug2</p><p>Abbreviation: Methyldopa does not interfere with measurement of vanillylmandelic_acid drug1 (vanillylmandelic (VMA) drug2 ), a test for pheochromocytoma, by those methods which convert VMA to vanillin</p><p>Rule 2: The instances in which two candidate drugs are in coordinate relations are filtered out since they are prone to false positives (<ref type="bibr" target="#b31">Segura Bedmar et al., 2014</ref>). For example, the following instance will be removed with rule 2. Methscopolamine may interact with antidepressants (tricyclic type), monoamine oxidase (MAO) inhibitors (e.g. phenelzine drug1 , linezolid drug2 , tranylcypromine, isocarboxazid, selegiline, furazolidone).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Preprocessing</head><p>Appropriate preprocessing can boost the final performance significantly. In our method, two preprocessing operations are conducted, i.e. tokenization and transforming the numbers to two uniform forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Tokenization</head><p>In our method, tokenization process is performed since it is one of the standard preprocessing steps. We employ the tokenization tool developed specifically for biomedical literature by Jiang and Zhai (2007). Besides the normal tokenization strategies, the tool uses the rules to handle the complicated biomedical entities (e.g. Macrophage inflammatory protein (MIP)-1alpha, 1, 2, 3, 4-TeCDD, 2', 5'-linked 3'-deoxyribonucleotides).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Transforming</head><p>the numbers to two uniform forms Numbers (integers and decimals) occur frequently in the DDI corpus. For example, in the sentence 'The serum concentration of phenytoin drug1 increased dramatically from 16.6 to 49.1 microg/mL when fluvoxamine was coadministered, although the daily dosage of phenytoin drug2 and other drugs had not changed', there are two decimals ('16.6' and '49.1'). Transforming the two decimals to a uniform form ('float') won't change the DDI's semantic expression. Therefore, the sentence becomes 'The serum concentration of phenytoin drug1 increased dramatically from float to float microg/mL when fluvoxamine was coadministered, although the daily dosage of phenytoin drug2 and other drugs had not changed'. Then we train a word embedding on the processed sentences with word2vec (<ref type="bibr" target="#b27">Mikolov et al., 2013</ref>) (a widely used tool for learning word embedding) and it will generate an embedding for 'float' instead of for '16.6' and '49.1'. Since word2vec trains a sentence based on sliding window mechanism, the 'float' will be trained twice while '16.6' and '49.1' are trained once only. As more training times will generate more accurate embedding, replacing all the integers and decimals with 'num' and 'float', respectively, will provide more powerful embedding for 'num' and 'float'. In addition, it will significantly reduce the size of the vocabulary and make the embedding more compact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Syntax word embedding</head><p>The syntactic information plays a key role in the sentence level relation classification problems (<ref type="bibr" target="#b7">Bunescu and Mooney, 2005;</ref><ref type="bibr" target="#b12">Fundel et al., 2007</ref>). Therefore, it is also employed by deep learning methods to solve the relation classification problems.<ref type="bibr" target="#b35">Xu et al. (2015)</ref>proposed a CNN-based relation classification model that utilizes the shortest dependency paths information. It takes the word sequence on the shortest path order as the input instead of the original sentence order.<ref type="bibr" target="#b36">Yan et al. (2015)</ref>solved the relation classification problem with long short term memory networks along the shortest dependency path, whose information is used to generate the new ordered input sequence. However, these methods use the syntactic information to generate the new ordered input sequence instead of training the word embedding. A word embedding is a parameterized function that maps words to high-dimensional vectors. Word embedding was first introduced by<ref type="bibr" target="#b2">Bengio et al. (2003)</ref>to fight the curse of dimensionality in the process of learning language model using neural network. Since then, various word embeddings were proposed for learning language models (<ref type="bibr" target="#b9">Collobert et al., 2011;</ref><ref type="bibr" target="#b18">Huang et al., 2012;</ref><ref type="bibr" target="#b27">Mikolov et al., 2013</ref>). In addition, word embeddings were also widely used in various NLP tasks.<ref type="bibr" target="#b9">Collobert et al. (2011)</ref>achieved the state-of-the-art performance on POS tagging, chunking, Named Entity Recognition and Semantic Role Labeling (SRL) using CNN with the word embedding as the input.<ref type="bibr" target="#b37">Zeng et al. (2014)</ref>proposed a word embedding based convolutional neural network to solve the relation classification problem. However, the word embeddings mentioned above are all based on the linear contexts (i.e. the surrounding words in linear order of a sentence). They ignore the syntactic information that plays a key role in the sentence level classification problems like DDI extraction. Levy and Goldberg,<ref type="bibr">(2014)</ref>proposed the dependency-based syntactic contexts to learn the word embedding.<ref type="bibr">Hashimoto et al. (2014)</ref>learned the word embedding using predicate-argument structure contexts and used it to measure semantic similarity between short phrases. In their methods, the syntactic information is introduced by constructing the syntactic contexts instead of the normal linear contexts (i.e. the surrounding words in linear order of a sentence) for the training process. Compared with the latter, the syntactic context yields more inclusive and more focused embeddings (<ref type="bibr" target="#b26">Levy and Goldberg, 2014</ref>). In our method, we propose a novel word embedding that contains the syntactic information, syntax word embedding, by changing the input of the word2vec tool (<ref type="bibr" target="#b27">Mikolov et al., 2013</ref>), i.e. the word sequences on the shortest path in the predicate-argument structure instead of the original linear order word sequences are inputted into the word2vec. Take the sentence 'Trimethoprim may inhibit the hepatic metabolism of phenytoin' as an example. We first use Enju parser (<ref type="bibr" target="#b28">Miyao and Tsujii, 2008</ref>) to parse the sentence and generate the predicate-argument structure of the sentence as shown in<ref type="figure" target="#fig_3">Figure 2</ref>. Then the word sequence, 'Trimethoprim inhibit metabolism of phenytoin', is obtained by combining the words on the shortest path connecting the first and last words. It retains the backbone of the Drug drug interaction extraction from biomedical literaturesentence which is vital for representing a sentence's syntactic structure while filtering out the less important adjunct words (e.g. 'may', 'the' and 'hepatic'). In this way, the syntax word sequence becomes more concise. Then these shortest path order sequences are inputted into word2vec to generate the syntax word embedding. We use E word 2 R mÂn to represent the syntax word embedding, where m is the size of the vocabulary and n is the dimension size of the syntax word embedding. In contrast to previous syntactic context word embeddings (<ref type="bibr">Hashimoto et al., 2014;</ref><ref type="bibr" target="#b26">Levy and Goldberg, 2014</ref>), our embedding is learned only based on the concise syntax word sequence, which represents a sentence's syntactic structure while discards the less important words. It is simple but proven to be effective for DDI extraction by our experiments.The method uses both the embedding-based convolutional features (dense real value vectors) and the traditional bag-ofwords features (sparse 0–1 vectors). Then auto-encoder is utilized to narrow down their difference. Thus, we can integrate two kinds of features more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Feature set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Convolutional features</head><p>2.4.1.1 Word representation. Collobert et al.'s architecture enables each word's feature vector to be extended to any discrete features which are helpful to the task of interest.<ref type="bibr" target="#b37">Zeng et al. (2014)</ref>extended the position feature to specify two target nouns in a relation extraction problem. In our method, as shown in<ref type="figure" target="#fig_4">Figure 3</ref>, a word is represented with the syntax word embedding, position feature and POS feature.Hyaluronan_lyase drug1 had a limited effect and collagenase drug2 was ineffective. The relative distances of the word 'effect' to drug1 (Hyaluronan lyase) and drug2 (collagenase) are À4 and 2, respectively. Then the relative distances are mapped to a ten bit binary vector, where the first bit stands for the sign and the remaining bits for the distance. The position feature (E p 2 R 19Â10 ) is shown in Supplementary Table S4 due to space limitation. As can be seen from the table, more attention is paid to the words near the two drugs, especially the ten surrounding words. On the other hand, the words whose relative distances exceed thirty are all treated the same. It is consistent with the intuition that the closer words contain more information than the more distant words do for the current word.POS feature. In our method, besides the position feature, the POS feature is also proposed to extend the syntax word embedding. Intuitively, the POS information of the words will be helpful to DDI extraction. Take the following instance as an example. Fluvoxamine drug1<ref type="bibr">[inhibits]</ref>the CYP2C9 catalyzed biotransformation of tolbutamide drug2. Drug1 (Fluvoxamine) is more likely to interact with drug2 (tolbutamide) since there is a verb 'inhibits' (annotated with a POS tag 'vb') between them. Therefore, the POS feature is informative for DDI extraction and is introduced in our method. The similar POSs generated by Enju parser are assigned to the same group and all 37 POSs (as shown in Supplementary<ref type="figure">Table S5</ref>) are divided into eight groups. Then each group is mapped to an eight bit binary vector using POS feature. In this way, the dimension of the POS feature is reduced. The POS feature is represented with the traditional bag-of-words feature (sparse 0–1 vector). Intuitively, concatenating the embedding based features (dense real value vectors) and sparse 0–1 vectors directly is not elegant enough. Therefore, we introduce the autoencoder (<ref type="bibr" target="#b16">Hinton and Zemel, 1994</ref>) to solve this problem as discussed in the following section. 2.4.1.2 Auto-encoder. Auto-encoder is a non-linear unsupervised learning model that is widely used as one of the building blocks in the greed layer-wise training process of deep learning (<ref type="bibr" target="#b3">Bengio et al., 2013</ref>). It is a neural network (NN) trained to compute a representation of the input from which it can be reconstructed with as much accuracy as possible (<ref type="bibr" target="#b16">Hinton and Zemel, 1994</ref>). Coding and decoding processes of the input are obtained from Equations (1) and (2), respectively. h j ðxÞ ¼ f ða j ðxÞÞ where a j ðxÞ ¼ b j</p><formula>þ X i W ji x i (1)</formula><formula>x Ã k ¼ gða k Þ where a k ¼ c k þ X j W Ã kj h j ðxÞ (2)</formula><p>where x is the input, h(x) is the hidden representation that we need and x* is the reconstruction result. We utilize tanh and sigmoid as the activation functions for f() and g(), respectively, as the input values range from 0 to 1 and the hidden representation values should be real numbers which can be negatives. To avoid the network learning a trivial identity function, we set W T ¼ W Ã as did in the work of<ref type="bibr" target="#b25">Larochelle et al. (2009)</ref>. As the goal is minimizing the reconstruction error, the loss function of auto-encoder can be defined as Equation (3):</p><formula>Lðx Ã ; xÞ ¼ X n ðx ðnÞ Ã À x ðnÞ Þ 2 þ b 2 W 2 (3)</formula><p>Then, back-propagation algorithm is used to learn the autoencoder model with the training data which is represented with our POS feature vector. Using the learned auto-encoder model, we encode the sparse POS feature vector as the dense real value feature vector (E pos 2 R 8Â8 ). Finally, given a sentence (X 1 ,X 2 ,.. .,X i ,.. .,X t ) of length t, the ith word X i will be represented as</p><formula>V i ¼ ½E word i ; E p iÀdrug1 ; E p iÀdrug2 ; E pos i  2 R n0 ,</formula><p>where each element of V i represents the syntax word embedding, position feature for drug1, position feature for drug2, and POS feature of the word X i , respectively, and n 0 ¼ n þ 10 þ 10 þ 8 where n is the dimension size of the syntax word embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1.3">Window</head><p>approach. After representing each word with a n 0-dimensional vector, we use the window approach to capture each word's context information. Given a word, we will combine a fixed size (win) of its surrounding words' vectors. And a new vector WinV i ¼ [V i-win/2 ,.. .,V i-1 ,V i ,V i þ 1 ,.. .,V i þ win/2 ]2R n1 is obtained with the window approach, where i is the word's index in a sentence and n 1 ¼ win Â n 0. Take the following sentence as an example. Hyaluronan_lyase<ref type="bibr">[1]</ref>had<ref type="bibr">[2]</ref>a<ref type="bibr">[3]</ref>limited<ref type="bibr">[4]</ref>effect<ref type="bibr">[5]</ref>and<ref type="bibr">[6]</ref>collagenase<ref type="bibr">[7]</ref>was<ref type="bibr">[8]</ref>ineffective<ref type="bibr">[9]</ref>After the window process, the word 'limited', whose index in the sentence is 4, will be represented as</p><formula>WinV 4 ¼ [V 3 ,V 4 ,V 5 ]</formula><p>where win ¼ 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1.4">Convolution. DDI extraction</head><p>task is a sentence level relation classification problem which predicts relation types for each sentence that is marked with two target nouns. Therefore, it is necessary to utilize all the local features of the sentence (<ref type="bibr" target="#b37">Zeng et al., 2014</ref>). In our method the convolutional approach (<ref type="bibr" target="#b9">Collobert et al., 2011;</ref><ref type="bibr" target="#b37">Zeng et al., 2014</ref>) as expressed by Equation (4) is employed to merge all the local information obtained with the window approach.</p><formula>ConS ¼ WinS Á M (4)</formula><p>where WinS ¼2 R tÂn1 , t is the number of words in a sentence and M n1 Â n2 is the transformation matrix that is the same across all local features of t in the sentence. ConS 2 R tÂn2 is the transforming result of WinS using M, where n 2 is a hyperparameter. Then the max pooling operation is conducted over times on ConS. Suppose ConS(. ,i) is the ith column of ConS and MaxS i is the maximum value of ConS(. ,i). These MaxS i are found out over time on ConS with Equation (5) and are regarded as the most useful features in each dimension of features.</p><formula>MaxS i ¼ maxðConSðÁ; iÞÞ; 1 &lt; i &lt; n 2 (5)</formula><p>Finally, the hyperbolic tanh activation function (Equation 6) is used to learn the complicated non-linear features.</p><formula>MaxF ¼ tanhðMaxSÞ (6)</formula><p>where MaxF 2 R n2. We can find that the varying length sentences will be represented by the same dimensional (n 2 ) feature vectors after the convolution process. Thus standard NN can be applied easily to the max feature vectors (MaxF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1.5">Convolutional</head><p>feature. After the preceding process, max feature vector (MaxF) is obtained and then a feed forward layer is stacked over MaxF, using Equation (7), to learn the higher level convolutional features.</p><formula>ConvF ¼ tanhðMaxF Á W conv Þ (7)</formula><p>where W conv 2 R n2Ân3 , n 3 is the dimension of the convolutional feature vector (ConvF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Traditional features</head><p>Some traditional features, such as the context of two target entities and information on the shortest path connecting them, can</p><p>Drug drug interaction extraction from biomedical literatureprovide important cues for predicting two entities' relationship in a sentence. Therefore, in our method, the context and shortest path features are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2.1">Context</head><p>features. Our context features include two drug names, their predicate words, their surrounding words, and their surrounding words' predicate words. One word's predicate word is found in the predicate-argument structure of a sentence. As shown in<ref type="figure" target="#fig_3">Figure 2</ref>, the predicate of Trimethoprim is inhibit. Thus, the contexts of both linear and syntactic structure are considered. Finally, all these words' syntax word embeddings are concatenated to generate the context feature vectors (ContF 2 R n4 where n 4 is the dimension of the context feature vector).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2.2">The</head><p>shortest path features. Our shortest path features include the words, the dependency types and biomedical semantic types of the words on the shortest path connecting two drug names. As shown in<ref type="figure" target="#fig_3">Figure 2</ref>, the dependency types on the shortest path connecting Trimethoprim and phenytoin are verb_arg and prep_arg. And the biomedical semantic types of the words are generated by MetaMap (<ref type="bibr" target="#b0">Aronson, 2001</ref>), which is a tool that maps biomedical texts to corresponding concepts of UMLS Metathesaurus (<ref type="bibr" target="#b5">Bodenreider, 2004</ref>). MetaMap can generate 104 kinds of semantic types, including ftcn (functional concept), chem (chemical), anim (Animal), etc. Like the POS feature, the shortest path information is represented with traditional bag-of-words feature (sparse 0–1 vector). Therefore, in the similar way as discussed in Section 2.4.1.2, the auto-encoder is utilized to encode it as the dense real value feature vector (ShortF 2 R n5 where n 5 is the dimension of the encoded shortest path feature vector) so that it can be concatenated with the embedding based features (dense real value vectors) more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2.3">Traditional feature generation. Then</head><p>the context feature (ContF) and encoded shortest path feature (ShortF) are concatenated to generate the context and shortest path features (CSF ¼ ½ContF; ShortF 2 R n6 , where n 6 ¼ n 4 þ n 5 ). Similar to the convolutional features, a feed forward layer is stacked over CSF, using Equation (8), to learn the higher level traditional feature (TradF)</p><formula>TradF ¼ tanhðCSF Á W trad Þ (8)</formula><p>where W trad 2 R n6Ân7 , n 7 is the size of the traditional feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Classifier training</head><p>The convolutional features and the traditional features are concatenated into one feature vector, OutF¼ ½ConvF; TradF 2 R n8 , where n 8 ¼ n 3 þ n 7. Then the OutF is fed into the output layer: out ¼ OutF Á W out</p><formula>(9)</formula><p>W out 2 R n8Ân9 , n 9 is the size of the output layer that is equal to the number of the DDI types in DDI classification problem. The output can be represented as out ¼ [out 1 ,out 2 ,.. .,out i ,.. .,out n9 ], where out i is interpreted as the confidence score of the corresponding DDI type i. The parameters of the model can be stated as a quad h ¼ (M, Wconv, W trad , W out ). The probability value of each DDI type is obtained through the following softmax operation over all DDI types using Equation(10):</p><formula>pðijx; hÞ ¼ e outi X n9 j¼1 e outj (10)</formula><p>Then the log likelihood of the parameters is calculated using Equation (11) when all training instances (T ¼ {(x (i) ,y (i) )}) are given:As in the work of<ref type="bibr" target="#b37">Zeng et al. (2014)</ref>, the stochastic gradient descent technique is used in our method to maximize the log likelihood.</p><formula>JðhÞ ¼ X i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Two-stage method</head><p>As shown in<ref type="figure" target="#fig_0">Figure 1</ref>of Supplementary Materials, our SCNN-based two-stage method (SCNN 2 ) includes the following processing steps: 1. Negative instance filtering step that rebalances the datasets' class distribution by removing possible negative instances. 2. Preprocessing step that constructs an easily understood corpus for classifiers. 3. Learning word embedding step that generates the syntax word embedding using Enju parser and word2vec. 4. Feature extraction step that extracts the convolutional and traditional features. 5. Training the classifier 2 step that trains a binary classifier based on the extracted features. 6. DDI detection step that detects the DDIs from the test set using the classifier 2. 7. Training the classifier 4 step that trains a four-class classifier based on the extracted features. 8. DDI classification step that classifies the extracted DDIs in step 6 into four specific DDI types using the classifier 4. Like other two-stage methods, our two-stage method divides the DDI extraction task into DDI detection stage and DDI classification stage. The only difference between our one-stage and two-stage methods is that the latter needs training two SCNN classifiers (the output layer sizes are two and four, respectively) while the former only needs training one SCNN classifier with the output layer size five.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental results and discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental settings</head><p>Our SCNN model is coded with Python and trained using Numbapro (http://docs.continuum.io/numbapro/index-archived), a Python compiler from Continuum Analytics (https://www.con tinuum.io/), which can compile Python code for execution on CUDA-capable GPUs or multicore Central Processing Units. With a Graphics Processing Unit (GPU) of Nvidia Tesla k20, it takes only a few hours to train our model. However, the syntax word embedding learning will take almost one month and most of the time is spent on parsing the massive amount of texts with Enju parser to generate the syntactic information. Since the larger corpus will generate the better embedding (<ref type="bibr" target="#b23">Lai et al., 2015</ref>), besides the original DDI corpus, a total of 4 653 097 Medline abstracts were downloaded from PubMed website (http://www.ncbi.nlm. nih.gov/pubmed/) to learn the syntax word embedding with a query string 'drug'. Our method was evaluated on the DDIExtraction 2013 corpus which consists of 1017 texts (784 DrugBank texts and 233 MedLine abstracts) and was manually annotated with a total of 18 491 pharmacological substances (drug names) and 5021 DDIs (<ref type="bibr" target="#b15">Herrero-Zazo et al., 2013;</ref><ref type="bibr" target="#b30">Segura Bedmar et al., 2013</ref>). The corpus contains four different DDI types: ADVICE, EFFECT, INT and MECHANISM. As mentioned in Section 2.1, we use the negative instance filtering to construct a more balanced corpus. Then the ratios of the positive instances to the negative instances increase from 1:5.9 and 1:4.9 to 1:2.3 and 1:2.2 for the training and test sets, respectively. In addition, as in Chowdhury and Lavelli's (2013) work, any positive instance removed from the test set is automatically considered a false negative during the calculation of F-score.<ref type="figure" target="#tab_1">Table 1</ref>shows the statistics of the DDI corpus before and after the negative instance filtering process. The existing DDI extraction methods use the balanced F-score measure for quantifying the performance (<ref type="bibr" target="#b30">Segura Bedmar et al., 2013</ref>). This metric is defined as F-score ¼ (2PR)/(P þ R), where P denotes the precision and R denotes the recall. To compare with these methods, we also use F-score to evaluate the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance comparison with other methods</head><p>The performance comparison between our method and others is shown in<ref type="figure" target="#tab_2">Table 2</ref>. As can be seen from it, the two-stage methods usually outperform the one-stage methods. For example, the best two results in DDIExtraction 2013 challenge (FBK-irst (<ref type="bibr" target="#b8">Chowdhury and Lavelli, 2013</ref>) and WBI (<ref type="bibr" target="#b33">Thomas et al., 2013</ref>)) are both achieved with the two-stage methods. Later,<ref type="bibr" target="#b20">Kim et al. (2015)</ref>also employed a two-stage method to achieve an even better performance. This may clash with the intuition that the two-stage method cannot outperform the one-stage one since it has a drawback that the errors in the DDI detection stage will propagate to the DDI classification stage. The reasons are as follows. First, dividing the DDI extraction problem into two stages decreases the imbalance degree of the corpus, which is denoted by the ratio of the sample size of the small class to that of the prevalent class (<ref type="bibr" target="#b32">Sun et al., 2009</ref>). The number of instances in the original training set for five DDI types (ADVICE, EFFECT, INT, MECHANISM and NEGATIVE) are 826, 1687, 188, 1319 and 23 772, respectively. For a one-stage method, the ratio of the small type (INT) to the prevalent type (NEGATIVE) is 1:126. However, for a two-stage method, in DDI detection, the ratio of the small type (the sum of ADVICE, EFFECT, INT and MECHANISM) to the prevalent type (NEGATIVE) is 1:5.9 and, in DDI classification, the ratio of the small type (INT) to the prevalent type (EFFECT) is 1:9, where the imbalance degree decreases significantly compared with the one-stage method. Since most classification methods, including neural networks, suffer from the class imbalanced corpus problem (<ref type="bibr" target="#b32">Sun et al., 2009</ref>), it is reasonable that the two-stage methods outperform the one-stage methods by alleviating the problem. Another possible reason why the two-stage methods outperform the one-stage methods is that, intuitively, the difficulty degree of a classification problem will rise along with the increase of the target class number. For the one-stage methods, DDI Extraction 2013 challenge task is a five-class classification problem since it needs to classify each candidate instance into one of the five DDI types (ADVICE, EFFECT, INT, MECHANISM and NEGATIVE). But for the two-stage methods, the task is divided into two classification problems, i.e. the DDI detection and DDI classification problems. The former predicts whether an instance is a DDI or not (a binary classification problem), and then the latter classifies each DDI candidate into one of the four DDI types (ADVICE, EFFECT, INT and MECHANISM) (a four-class classification problem). It can be seen as decomposing a complicated problem into two simpler ones. Since the classifiers usually can solve a simple problem better than a complicated one, the two-stage methods usually outperform the onestage ones. In DDIExtraction 2013 challenge, the teams UTurku (<ref type="bibr" target="#b4">Bjö rne et al., 2013</ref>) and NIL_UCM (<ref type="bibr" target="#b6">Bokharaeian and Dıaz, 2013</ref>) used the one-stage methods. As shown in<ref type="figure" target="#tab_2">Table 2</ref>, our one-stage method (SCNN 1 ) achieves an F-score of 0.670 which is much better than those of UTurku and NIL_UCM (0.594 and 0.517 in F-score, respectively). The improvements have been proven to be significant using MCNemar's test (<ref type="bibr" target="#b11">Dietterich, 1998</ref>) at the 0.05 level. The performance is even the same with the known best result achieved with a two-stage method (<ref type="bibr" target="#b20">Kim et al., 2015</ref>). The reason is that CNN is a powerful multiclass classifier (<ref type="bibr" target="#b22">Krizhevsky et al., 2012</ref>). With the introduction of the syntax word embedding extended by the position and POS features and the utilization of auto-encoder for transferring the sparse traditional feature vector to the dense real value vector, the performance is further improved. To compare with the existing two-stage methods, we propose a two-stage one (SCNN 2 ) as well. In this method, we use the SCNNbased method for the DDI detection by setting n 9 (the size of the output layer, refer to Equation 9) to 2. Then another SCNN model (n 9 is set to 4) is applied to classify the detected DDIs into four specific DDI types, i.e. ADVICE, EFFECT, INT and MECHANISM. Compared with the two top-performing two-stage methods (<ref type="bibr" target="#b20">Kim et al., (2015)</ref>and FBK-irst), our method achieves almost equal or lower F-scores in DDI detection (0.772 Versus 0.775 and 0.800). The reason is that, as may happen when a complicated model is used to learn an easy problem (<ref type="bibr" target="#b34">Wan et al., 2013</ref>), SCNN may overfit the DDI detection, an easy binary classification problem. However, in the DDI classification, its performance exceeds those of other two methods (F-scores of 0.686 Versus 0.670 and 0.651). The improvement of SCNN 2 over FBK-irst has been proven to be significant using MCNemar's test at the 0.05 level. However, the significant test between SCNN 2 and<ref type="bibr" target="#b20">Kim et al.'s (2015)</ref>method cannot be performed since their detailed classification results are not available. This verifies that SCNN is a large capacity model which is more powerful for a complicated problem (e.g. four-class or five-class DDI classification problems). In addition, SCNN is based on CNN, a multiclass classifier inherently (<ref type="bibr" target="#b22">Krizhevsky et al., 2012</ref>). It can solve the multiclass classification problem better than the methods combining multiple SVMs do since SVM is originally designed for the binary classification problem. For example, compared with the best method in DDIExtraction 2013 challenge (FBK-irst, which trains four binary SVMs in the classification stage), the performance of SCNN 2 is much better (0.686 Versus 0.651 in F-score) though its DDI detection performance is inferior to that of the latter (0.772 Versus 0.800 in F-score). In the meantime, as shown in Supplementary Table S7, SCNN can obtain the state-of-the-art performance with fewer classifiers.Notes. Ratio denotes the ratio of the positives to the negatives in the corpus. OriginalTrainingSet and OriginalTestSet denote the original training and test sets, respectively. NewTrainingSet and NewTestSet denote the new training and test sets obtained after possible negatives are removed, respectively. It should be noted that 22 interactions in the training set whose corresponding sentences can't be parsed correctly by the Enju parser are removed and, therefore, the number of positives (4020) in OriginalTrainingSet is slightly different with that (4042) of the DDIExtraction 2013 corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Drug drug interaction extraction from biomedical literature</head><p>For the DDI classification,<ref type="bibr" target="#b20">Kim et al. (2015)</ref>and FBK-irst team learned the multiclass classifiers by combining several binary SVMs. They used six (n 9 Â (n 9 À1)/2, n 9 is four) and four (n 9 ) binary SVMs, respectively, as they chose one-against-one and one-against-all strategies (<ref type="bibr" target="#b17">Hsu and Lin, 2002</ref>). SCNN 2 uses only one classifier, but its performance is better than those of other two methods. This advantage will be demonstrated more fully when there are more target classes. Along with the increase of the target class number, for the other two methods, the number of binary classifiers will increase rapidly while this won't happen to SCNN. In fact, there are many multiclass classification problems with large target classes. For example, SemEval-2010 Task 8 (<ref type="bibr" target="#b14">Hendrickx et al., 2009</ref>) is a relation classification problem of nineteen target classes. Large Scale Visual Recognition Challenge (<ref type="bibr" target="#b22">Krizhevsky et al., 2012</ref>) is an image classification problem of one thousand target classes. It would be a disaster to train a one-thousand-class classifier with one-against-one strategy, as it needs 499 500 binary SVMs (1000 Â (1000À1)/2). Furthermore, the DDIExtraction 2013 corpus consists of two parts, i.e. documents from the DrugBank database and MedLine abstracts. The performance comparisons of various methods on DrugBank and MedLine test sets are made in Supplementary Materials due to space limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The effect of the strategies and features on performance</head><p>In addition, to evaluate the effectiveness of the strategies and features of our method, the corresponding experiments are conducted with SCNN 2 : we remove a feature or a strategy each time and then calculate the F-score and the corresponding decrease compared with the one before it is removed as shown in<ref type="figure" target="#tab_3">Table 3</ref>. Negative instance filtering: after the negative instance filtering strategy is removed, the F-score decreases by 4.1%. By further analysis, we found that alleviating the imbalanced class distribution problem contributes to an F-score improvement of 1.4% and removing the false positives from the final result that otherwise will be generated by the classification model contributes the rest 2.7%. In fact, some of our negative instance filtering rules are first introduced in our method. For example, the instances in which two candidate drugs are in coordinate relations are filtered out since they are prone to false positives. These rules will remove numerous possible negatives from both the training and test sets. On the one hand, the reduction of numerous negatives can improve the classification performance of our SCNN model by constructing a more balanced training set. But on the other hand, it leads to the loss of much information about the filtered negatives. The final performance depends on the trade-off between the above two factors. Therefore, our pre-filtering rules may be not necessarily effective for any classification model as it is for ours, which has been verified by the experimental results on the UTurku system (http://jbjorne. github.io/TEES/, the only DDI classification system we could successfully download, install and run). For UTurku, when our prefiltering rules are only applied on the test set, the F-score is improved only a little (0.7 point in F-score from 0.593 to 0.6). The reason is that, trained with numerous negatives in the original training set, UTurku has achieved enough ability to recognize the negatives in the test set and, therefore, the pre-filtering rules cannot boost the performance significantly any more. When the rules are also applied on the training set, the F-score of UTurku even drops 2.6 points (from 0.6 to 0.574). The cause behind is that, for UTurku, the loss of much information about filtered negatives due to the pre-filtering overwhelms the performance improvement brought with a more balanced training set, leading to its worse final performance. Therefore, the final effect of our negative instance filtering strategy on performance is closely related to the classification model and it cannot be regarded as a simple rule necessarily effective for any classification model by accurately filtering the negatives from the test set. Experimental results show that it is suitable for our SCNN model and significantly improves the performance. In some sense, it can be regarded as one part of our SCNN approach. Syntax: replacing syntax word embedding with normal word embedding decreases the F-score by 3.6%. The syntax word sequence is concise since it only retains the backbone of the sentenceNotes. D denotes the corresponding F-score decrease percentage when a strategy or feature is removed. by filtering out the adjunct words. Thus the syntax word embedding is learned based only on the core words, which are vital for representing a sentence's syntactic structure. It is the reason that the syntax word embedding works better than the common one does. POS, shortest path and their encodings: removing the POS and shortest path features lead to the decreases of F-score by 2.4% and 6.0%, respectively. The reason is that the shortest path feature contains much more information than the POS feature since their feature dimensions are 1705 and 8, respectively. In addition, removing the encoding mechanism for the shortest path feature and POS feature leads to the decreases of F-scores by 4.8% and 1.6%, respectively. This shows that encoding the sparse POS and shortest path feature vectors (0–1 vectors) as the dense real value feature vectors can boost the DDI classification performance. Position: the position feature is indispensable to the relation classification problem as the F-score decreases by 2.9% when it is removed. Without the position feature, the two target nouns' information, which is essential for the relation classification problem, will be missed. Word embedding and context: removing the word embedding from the convolutional feature set (only the position and POS features are left) lowers the performance significantly (8.2% in F-score) while, when the context feature is removed from the traditional feature set, the F-score drops dramatically by 5.9%. This shows that the word information plays a key role in our feature sets. Convolutionlayer: a CNN is a normal NN with additional convolutional layer. Therefore, we tested the influence of removing the convolutional layer on performance. However, different from CNN that allows the inputs with different sizes, NN requires the fixed input size (the number of words). Therefore, we used the following two methods to solve this problem: Convolutionlayer1, extending each sentence by a padding word to the size of the maximal sentence length (74 words in our corpus), where the padding word's embedding is a zero-vector; Convolutionlayer2, extracting two target drugs and their 10 surrounding words from a sentence. Both mechanisms lead to much worse performances: the F-scores drop by 9.4% and 7.5%, respectively. It shows that the convolution process can integrate all words' information more effectively than the method of simply concatenating them does. More detailed discussions are provided in Supplementary Material: The effect of the strategies and feature on performance. In addition, the combination of the convolutional and traditional feature sets is explored in Supplementary Material: Combinations of the convolutional and traditional features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this article, we present a SCNN based DDI extraction approach. In this approach, a novel word embedding (syntax word embedding) is proposed to exploit the syntactic information of a sentence. Then the syntax word embedding is extended by the position and POS features to introduce the position and POS information. In addition, autoencoder is employed to transfer sparse bag-of-words feature vectors to dense real value feature vectors before they are combined with the convolutional features. Finally, their combination is passed to a softmax to learn the DDI classifier. Experimental results on the DDIExtraction 2013 corpus show that our method achieves an F-score of 0.686 which is superior to those of the state-of-the-art methods. The main contributions of our work can be summarized as follows: (i) Utilizing concise syntax word sequence to learn the syntax word embedding. (ii) Applying POS feature to extend the syntax word embedding. (iii) Using auto-encoder to transfer the sparse bagof-words features to the dense real value feature vectors. In addition, SCNN is designed for the multiclass problem, and, with the fewer classifiers, it outperforms other methods that implement the multiclass classifiers by combining several binary SVMs. However, the performance of SCNN on DDI detection is not as satisfactory as on DDI multiclass classification since, as a large capacity model, it fits a complicated problem well, but may over-fit an easy problem (e.g. the two-class DDI detection problem). This is a problem to be further studied. In addition, considering the limited generalization ability of the rule-based negative instance filtering method, in the future, we will try to improve the SCNN method to make it be less influenced by the unbalanced class distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. The processing flow of our one-stage method SCNN 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>Collobert et al. (2011) proposed a CNN-based architecture to solve sentence level tasks like SRL. Then Zeng et al. (2014) applied this architecture to relation classification problems. In this article, we also propose a CNN-based method to extract DDIs from biomedical literature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><figDesc>Syntax word embedding. As described in Section 2.3, the shortest path order sequences are inputted into word2vec to generate the syntax word embedding. Position feature. DDI extraction task considers the relationship of two candidate drugs in a sentence. But the syntax word embedding itself cannot capture the position information of two drugs which is important for a relation extraction problem. Therefore, the position feature presented by Zeng et al. (2014) is also used in our method. The position feature consists of two relative distances, [d 1 , d 2 ], where d 1 and d 2 represent relative distances of the current word to drug1 and drug2, respectively. Take the following sentence as an example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.2.</head><figDesc>Fig. 2. The predicate-argument structure of the example sentence. The nodes and edges on shortest path connecting the first and last words in the predicate-argument structure are shown in bold and the rest in dotted line. The rectangular nodes represent words. The ellipse vertices represent specific relationships between the predicates and their arguments</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.3.</head><figDesc>Fig. 3. Convolutional feature extraction. X i (i ¼ 0, 1,.. ., t) represents ith word in a t length sentence</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><figDesc>This work was supported by the grants from the Natural Science Foundation of China (No. 61070098, 61272373, 61340020, 61572102 and 61572098), Trans-Century Training Program Foundation for the Talents by the Ministry of Education of China (NCET-13-0084), the Fundamental Research Funds for the Central Universities (No. DUT13JB09 and DUT14YQ213) and the Major State Research Development Program of China (No. 2016YFC0901902). Conflict of Interest: none declared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>logðpðy ðiÞ jx ðiÞ ; hÞÞ (11)</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 1. The statistics of the DDI corpus</figDesc><table>Corpus 
Positives 
Negatives 
Total 
Ratio 

OriginalTraining Set 
4020 
23 772 
27 792 
1:5.9 
NewTrainingSet 
3840 
8989 
12 829 
1:2.3 
OriginalTestSet 
979 
4782 
5761 
1:4.9 
NewTestSet 
971 
2084 
3055 
1:2.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><figDesc>Table 2. Performance comparison on DDIExtraction 2013 test set</figDesc><table>Method 
Classification 
Detection 

P 
R 
F -score 

P 
R 
F -score 

One-stage 
SCNN 1 
0.691 
0.651 
0.670 
7.6% 
0.747 
0.768 
0.757 
UTurku (Bjö rne et al., 2013A) 
0.732 
0.499 
0.594 
0.858 
0.585 
0.696 
NIL_UCM (Bokharaeian and Dıaz, 2013) 
0.535 
0.501 
0.517 
0.608 
0.569 
0.588 
Two-stage 
SCNN 2 
0.725 
0.651 
0.686 
1.6% 
0.775 
0.769 
0.772 
Kim et al., (2015) 
– 
– 
0.670 
– 
– 
0.775 
FBK-irst (Chowdhury and Lavelli, 2013) 
0.646 
0.656 
0.651 
0.794 
0.806 
0.800 
WBI (Thomas et al., 2013) 
0.642 
0.579 
0.609 
0.801 
0.722 
0.759 

Notes. SCNN 1 denotes our SCNN-based one-stage method and SCNN 2 denotes our SCNN-based two-stage method. D denotes the performance improvement 
of SCNN 1 over UTurku, and SCNN 2 over that of Kim et al. (2015). The boldfaced numerals are the highest values in the corresponding column. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><figDesc>Table 3. The effect of the strategies and features on performance</figDesc><table>Strategy or feature removed 
P 
R 
F -score 
D 

None 
0.725 
0.651 
0.686 
– 
Negative instance filtering 
0.685 
0.610 
0.645 
À4.1% 
Syntax 
0.711 
0.599 
0.650 
À3.6% 
POS 
0.707 
0.623 
0.662 
À2.4% 
POS Encoding 
0.690 
0.652 
0.670 
À1.6% 
Shortest Path 
0.671 
0.586 
0.626 
À6.0% 
Shortest Path Encoding 
0.661 
0.616 
0.638 
À4.8% 
Position 
0.680 
0.636 
0.657 
À2.9% 
Word Embedding 
0.639 
0.572 
0.604 
À8.2% 
Context 
0.657 
0.599 
0.627 
À5.9% 
ConvolutionLayer1 
0.611 
0.576 
0.592 
À9.4% 
ConvolutionLayer2 
0.577 
0.648 
0.611 
À7.5% 

</table></figure>

			<note place="foot">Z.Zhao et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">R</forename>
				<surname>Aronson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AMIA Symposium, American Medical Informatics Association</title>
		<meeting>the AMIA Symposium, American Medical Informatics Association<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<monogr>
		<title level="m" type="main">Stockley&apos;s Drug Interactions</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Baxter</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">L</forename>
				<surname>Preston</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Pharmaceutical Press</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Bengio</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learning Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Representation learning: a review and new perspectives</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Bengio</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<monogr>
		<title level="m" type="main">UTurku: drug named entity recognition and drug-drug interaction extraction using SVM classification and domain knowledge. The North American Chapter</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Bjö Rne</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page">651</biblScope>
			<pubPlace>Atlanta, GA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">The Unified Medical Language System (UMLS): integrating biomedical terminology</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Bodenreider</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="270" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<monogr>
		<title level="m" type="main">NIL UCM: Extracting Drug-Drug Interactions from Text Through Combination of Sequence and Tree Kernels. The North American Chapter</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Bokharaeian</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Dıaz</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page">644</biblScope>
			<pubPlace>Atlanta, GA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">C</forename>
				<surname>Bunescu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">J</forename>
				<surname>Mooney</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, B.C., Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Why we need an efficient and careful pharmacovigilance FBK-irst: A multi-phase kernel based approach for drug-drug interaction detection and classification that exploits linguistic information. The North American Chapter</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Businaro</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">F M</forename>
				<surname>Chowdhury</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Lavelli</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Pharmacovigilance</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">53</biblScope>
			<date type="published" when="2013" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Collobert</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learning Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">On the algorithmic implementation of multiclass kernel-based vector machines</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Crammer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Singer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learning Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="265" to="292" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Approximate statistical tests for comparing supervised classification learning algorithms</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">G</forename>
				<surname>Dietterich</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1895" to="1923" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">RelEx-Relation extraction using dependency parse trees</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Fundel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="365" to="371" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<monogr>
		<title level="m" type="main">Jointly learning word representations and composition functions using predicate-argument structures</title>
		<author>
			<persName>
				<forename type="first">K</forename>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1544" to="1555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Hendrickx</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">The DDI corpus: an annotated corpus with pharmacological substances and drug–drug interactions</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Herrero-Zazo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Inform</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="914" to="920" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Autoencoders, minimum description length, and Helmholtz free energy</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">E</forename>
				<surname>Hinton</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">S</forename>
				<surname>Zemel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Processing Syst</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="3" to="10" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">A comparison of methods for multiclass support vector machines</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">W</forename>
				<surname>Hsu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">J</forename>
				<surname>Lin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="415" to="425" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">H</forename>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">An empirical study of tokenization strategies for biomedical information retrieval</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Jiang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Zhai</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Retrieval</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="341" to="363" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Extracting drug–drug interactions from literature using a rich feature-based linear kernel approach</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Kim</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Inform</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="23" to="30" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">DrugBank 3.0: a comprehensive resource for &apos;omics&apos; research on drugs</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Knox</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1035" to="1041" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Krizhevsky</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Processing Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<monogr>
		<title level="m" type="main">How to generate a good word embedding? Intelligent Systems IEEE</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Lai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Liu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>He</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Zhao</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Jackson&apos;s Death Raises Questions About Drug Interactions</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Landau</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CNN</title>
		<imprint>
			<date type="published" when="2009-06-26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring strategies for training deep neural networks</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Larochelle</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learning Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Levy</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Goldberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dependency-based word embeddings. ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Mikolov</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HLT-NAACL</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="746" to="751" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Feature forest models for probabilistic HPSG parsing</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Miyao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Tsujii</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="35" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">The 1st DDIExtraction-2011 challenge task: Extraction of Drug-Drug Interactions from biomedical texts</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Segura Bedmar</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Martinez</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Cisneros</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Challenge Task on DrugDrug Interaction Extraction, CEUR-WS</title>
		<meeting>the 1st Challenge Task on DrugDrug Interaction Extraction, CEUR-WS<address><addrLine>Huelva, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Semeval-2013 task 9: extraction of drug-drug interactions from biomedical texts The North American Chapter</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Segura Bedmar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Workshop On Semantic Evaluation</title>
		<meeting>the 7th International Workshop On Semantic Evaluation<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="341" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Lessons learnt from the DDIExtraction-2013 shared task</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Segura Bedmar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Inform</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="152" to="164" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Classification of imbalanced data: a review</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Sun</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Pattern Recognit. Artificial Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="687" to="719" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">WBI-DDI: drug-drug interaction extraction using majority voting The North American Chapter</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Thomas</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Joint Conference On Lexical and Computational Semantics</title>
		<meeting>the Second Joint Conference On Lexical and Computational Semantics<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="628" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect The North American Chapter</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Wan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference On Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference On Machine Learning (ICML-13)<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantic relation classification via convolutional neural networks with simple negative sampling</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Xu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing, SIGDAT</title>
		<meeting>Empirical Methods in Natural Language Processing, SIGDAT<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="536" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<analytic>
		<title level="a" type="main">Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Path</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Yan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing. SIGDAT</title>
		<meeting>Empirical Methods in Natural Language Processing. SIGDAT<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b37">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Zeng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING, the Association for Computational Linguistics</title>
		<meeting>COLING, the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>