
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Faster and more accurate graphical model identification of tandem mass spectra using trellises</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Shengjie</forename>
								<surname>Wang</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">John</forename>
								<forename type="middle">T</forename>
								<surname>Halloran</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName>
								<forename type="first">Jeff</forename>
								<forename type="middle">A</forename>
								<surname>Bilmes</surname>
							</persName>
							<email>bilmes@uw.edu or william-noble@uw.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">William</forename>
								<forename type="middle">S</forename>
								<surname>Noble</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Genome Sciences</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Faster and more accurate graphical model identification of tandem mass spectra using trellises</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btw269</idno>
					<note>*To whom correspondence should be addressed. Contact: Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Tandem mass spectrometry (MS/MS) is the dominant high throughput technology for identifying and quantifying proteins in complex biological samples. Analysis of the tens of thousands of fragmentation spectra produced by an MS/MS experiment begins by assigning to each observed spectrum the peptide that is hypothesized to be responsible for generating the spectrum. This assignment is typically done by searching each spectrum against a database of peptides. To our knowledge , all existing MS/MS search engines compute scores individually between a given observed spectrum and each possible candidate peptide from the database. In this work, we use a trellis, a data structure capable of jointly representing a large set of candidate peptides, to avoid redundantly recomputing common sub-computations among different candidates. We show how trellises may be used to significantly speed up existing scoring algorithms, and we theoretically quantify the expected speedup afforded by trellises. Furthermore, we demonstrate that compact trellis representations of whole sets of peptides enables efficient discriminative learning of a dynamic Bayesian network for spectrum identification, leading to greatly improved spectrum identification accuracy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A critical problem in medicine and biology is accurately identifying the proteins in a complex mixture, such as a drop of blood. Solutions to this problem have many important applications, including the early detection of diseases and congenital defects (<ref type="bibr" target="#b23">Walters et al., 1996</ref>). The most widely used high throughput technology to identify proteins in complex mixtures is tandem mass spectrometry (MS/MS), whose output is a collection of tens of thousands of fragmentation spectra, each of which ideally corresponds to a single generating peptide. The core problem in the interpretation of MS/MS data involves identifying the peptide responsible for generating each observed spectrum, which we call the spectrum identification problem. The most accurate methods to solve the spectrum identification problem employ a database of peptides (reviewed in<ref type="bibr" target="#b18">Nesvizhskii, 2010</ref>). Given an observed spectrum, peptides in the database are scored, and the top scoring peptide is assigned to the spectrum. The pair consisting of an observed spectrum and a peptide sequence is referred to as a peptide-spectrum match (PSM).</p><p>In this work, we show how trellises may be used to make this database search significantly more efficient and accurate. A trellis is a data structure capable of representing an exponential size collection of strings in polynomial space. Trellises have been used to speed up inference in hidden Markov models (<ref type="bibr" target="#b8">Huang and Soong, 1991;</ref><ref type="bibr" target="#b13">Jelinek, 1997;</ref><ref type="bibr" target="#b26">Young et al., 1997</ref>), dynamic Bayesian networks (DBNs) and dynamic graphical models (DGMs;<ref type="bibr" target="#b14">Ji et al., 2006</ref>). In the context of MS/MS, we use a trellis to compactly represent the collection of candidate peptides associated with an observed fragmentation spectrum, i.e. peptides whose masses are close to the observed peptide mass associated with the spectrum. Using the trellis allows for the sharing of computation across candidate peptides. We describe how to apply trellises to any scoring function expressible as a DGM. This includes linear scoring functions such as the SEQUEST XCorr (<ref type="bibr" target="#b7">Eng et al., 1994</ref>), the score functions employed by X!Tandem (<ref type="bibr" target="#b3">Craig and Beavis, 2004</ref>), Morpheus (<ref type="bibr" target="#b25">Wenger and Coon, 2013</ref>), MS-GFÃ¾ (<ref type="bibr" target="#b16">Kim and Pevzner, 2014</ref>) and OMSSA (<ref type="bibr" target="#b9">Geer et al., 2004</ref>), as well non-linear methods such as our recently proposed DBN for Rapid Identification of Peptides (DRIP;<ref type="bibr" target="#b10">Halloran et al., 2014</ref>). For common MS/MS application settings, we prove and quantify the extent to which, in expectation, determining the top scoring data instance in a trellis is substantially more efficient than scoring each data instance individually. We then demonstrate empirically that trellises provide a significant reduction in the computational costs of the XCorr and DRIP scoring functions, ranging from 12-to 16-fold speedups in the low-resolution and high-resolution datasets examined here. Next, we show that trellises may be used to discriminatively train DBNs for MS/MS, leading to significantly improved peptide identification accuracy. In particular, we modify the DRIP model, which was originally trained via maximum likelihood estimation, to instead employ discriminative training via maximum mutual information (MMI) estimation (<ref type="bibr" target="#b21">Povey, 2003</ref>). Maximum likelihood estimation maximizes the log-likelihood given a set of high-confidence PSMs, whereas MMI estimation maximizes the conditional log-likelihood between the highconfidence PSMs and a large 'background' collection of alternative PSMs. Thus, MMI estimation encourages learned parameters which both explain the high-confidence labels well and discriminate against the background labels. However, MMI estimation is costly because it requires computing denominator quantities over whole sets of peptides before being able to take a single gradient step in the parameter space; this is infeasible if considering each peptide in the set individually. We demonstrate that using trellises renders the discriminative training procedure feasible. Furthermore, we present empirical evidence that this discriminative approach significantly improves identification accuracy relative to the generatively trained DRIP model (Section 7.2), resulting in 11.2% and 6.2% more correct identifications at a stringent false discovery rate (FDR) of 0.5% for two datasets. The general trellis-based discriminative training procedure used herein is applicable to any DGM for any class of problem, but we are unaware of any previous work that does this. The article is organized as follows. In Section 2, we formally define the spectrum identification problem and introduce the two database search scoring functions, XCorr and DRIP. We then define trellises in Section 3, which we utilize to compress the theoretical spectra of candidate peptides to be scored during database search. We show how graphical models may be used to efficiently traverse elements in a trellis (Section 3.3), enabling easy combination with any scoring function represented as a graphical model. Thus, in Section 4, we show how the vastly different scoring functions, XCorr and DRIP, may be represented as graphical models. The combination of trellises with these graphical model scoring functions (Section 5) allows for significantly faster database search (Section 7.1). The highly compressed trellises allow feasible discriminative training for DRIP (Section 6), providing significantly more accurate peptide identification accuracy (Section 7.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Database search in tandem mass spectrometry</head><p>We describe the spectrum identification problem as follows. Given an observed spectrum x (check<ref type="figure">Table 1</ref>for notations in the paper) with precursor m/z m x and precursor charge c x , and given a database D, we wish to find the peptide y 2 D responsible for generating x. Using the precursor m/z and charge, we constrain the set of peptides to be scored by setting a mass tolerance threshold, w, such that we score the set of candidate peptides DÃ°m x ;c x ;D;wÃ Â¼ y : y 2 D; jmÃ°yÃ=c x Ã m x j w f g , where mÃ°yÃ denotes the mass of peptide y. Denoting an arbitrary scoring function as f Ã°y;xÃ, the spectrum identification problem, for a given x, involves finding: y Ã 2 argmax y2DÃ°m x ;c x ;D;wÃ f Ã°y; xÃ:</p><formula>(1)</formula><p>We study two very different database search algorithms: SEQUEST (<ref type="bibr" target="#b7">Eng et al., 1994</ref>) and DRIP (<ref type="bibr" target="#b10">Halloran et al., 2014</ref>). SEQUEST begins by quantizing and preprocessing the observed spectrum x into a vector b x. Given a candidate peptide y, a theoretical spectrum b y (typically, a sparse vector) is constructed from y with length equal to that of b x. This yields the XCorr score function:</p><formula>XCorrÃ°x; yÃ Â¼ b y T b x Ã 1 151 X 75 sÂ¼Ã75 b x s ! Â¼ b y T b x 0 ; (2)</formula><p>where b x s denotes the vector shifted by s m/z units. XCorr is thus a foreground minus a background inner product, and is hence linear. Our recently proposed DBN-based scoring function, DRIP (<ref type="bibr" target="#b10">Halloran et al., 2014</ref>), constructs a potentially non-linear alignment between the theoretical and observed spectra. A peptide's theoretical spectrum is given, and hidden variables are used to represent two MS/MS alignment phenomena: insertions, which are observed peaks that do not match a theoretical peak, and deletions, which are theoretical peaks that do not match an observed peak. The most probable alignment, i.e. sequence of insertions and deletions, is calculated via the max-product (or the Viterbi) algorithm (<ref type="bibr" target="#b1">Bilmes, 2010</ref>), and peptides are scored using the log-likelihood of the most probable alignment. A key advantage of DRIP over most other scoring functions is that the alignment costs are automatically deduced using a machine learning method such as maximum likelihood estimation or (in the present paper) conditional likelihood.Note: x and its variations denote the observed spectrum for different methods/contexts. y and its variations denote the theoretical spectrum for different methods/contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trellises i323</head><p>For these two scoring functions, we define their corresponding DGMs in Section 4. We note that the DGM used to model XCorr may be used to model any linear MS/MS scoring function, including functions employed by many commonly used search algorithms (e.g. X!Tandem, the base scores for MS-GFÃ¾ and OMSSA, and Morpheus). By combining these DGMs with trellises, we efficiently model the scoring of all candidate peptides in Equation (1) within a single data structure (Section 3), affording efficient discriminative training for MS/MS (Section 6) and significantly faster database search (Section 7.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Trellises</head><p>Trellises are powerful structures capable of representing an exponentially large set of sequential data hypotheses compactly and efficiently, often using only linear space. For example, natural language dictionaries can be stored in trellises for more efficient querying; in speech recognition, trellises constructed out of the top phoneme (or word, or sentence) hypotheses (e.g. N-best lists) can be used to rescore and select the best hypothesis much more efficiently than a simple linear max computation over all N scores (<ref type="bibr" target="#b6">Dyer et al., 2008;</ref><ref type="bibr" target="#b8">Huang and Soong, 1991;</ref><ref type="bibr" target="#b13">Jelinek, 1997;</ref><ref type="bibr" target="#b26">Young et al., 1997</ref>). In this section, we first formally define a trellis, then show how to efficiently construct a trellis given a large (exponential) set of strings, and lastly show how to apply the constructed trellises to MS/MS scoring. A trellis over an alphabet R is a directed graph G Â¼ Ã°Â¥; L; n s ; n t Ã, where Â¥ is the node set, L is the link set, and n s ; n t 2 Â¥ denote the source and target nodes, respectively. To avoid confusion, we use the terms 'vertex' and 'edge' for graphical model graphs (Section 3.3), and we use 'node' and 'link' for trellis graphs. Every link l 2 L is a tuple Ã°n 1 ; n 2 ; aÃ°lÃÃ, where n 1 , n 2 are the from-node, and to-node respectively, and aÃ°lÃ 2 R is the alphabet element encoded in l. Each path in the trellis from n s to n t represents a sequence. Thus, letting PÃ°G; n s ; n t Ã denote the set of paths from n s to n t , every p Â¼ l 1 ; l 2 ;. .. ; l jpj , p 2 PÃ°G; n s ; n t Ã represents a sequence of characters, or a string, over alphabet R: aÃ°l 1 Ã; aÃ°l 2 Ã;. .. ; aÃ°l jpj Ã. Also, let PÃ°G; l 1 ; l 2 Ã, where l 1 ; l 2 2 L, denote the set of paths starting with l 1 and ending with l 2. For a set of strings Y Â¼ f y 1 ; y 2 ;. .. ; y m g, the trellis representation of the strings GÃ° Y Ã can be extremely compact because the elements of Y might be highly redundant. For example, if y i and y j share some substring in common, then we can merge the common parts into a sequence of common links.<ref type="figure" target="#fig_0">Figure 1A</ref>shows a trellis over four character strings, with the shared substrings collapsed into common trellis links to reduce redundancy. The common links of trellises not only reduce memory requirements when representing a set of strings but can also speed up computations over the encoded string set, sometimes quite significantly.For this article, we focus on the task of DGM inference with the Viterbi algorithm. Trellises allow us to reduce the state space of the Viterbi algorithm and to apply smart pruning strategies more effectively, achieving orders of magnitude reductions in computation time (Section 7.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Trellis construction</head><p>Constructing the optimal trellis from the input set of strings Y over alphabet R is a difficult problem. The objective of the 'optimal' trellis is task dependent. For example, for natural language dictionary queries to be computationally optimal, the trellis should have a minimal number of nodes. For data compression, trellises are often stored as a node list and a link list, where each entry of the link list records the starting/ending node and possibly some additional features. The optimal trellis should, thus, be minimal in overall size, so both the nodes and links matter. Moreover, some tasks do not require the trellis to be an 'exact' representation of the input strings. For a set of strings Y , let GÃ° Y Ã be a trellis representation, and for a trellis G, let TÃ°GÃ be the set of strings represented by the trellis. We define an exact trellis to be one where precisely TÃ°GÃ° Y ÃÃ Â¼ Y. For our task of speeding up DGM training/inference for MS/MS database search, our objective is to construct a trellis GÃ° Y Ã Â¼ Ã°Â¥; L; n s ; n t Ã that is exact but where jLj is minimized. Constructing the optimal trellis is a hard problem, as we can think of a trellis as a non-deterministic finite automaton (NFA;<ref type="bibr" target="#b12">Hopcroft et al., 2001</ref>), and it has been proven (<ref type="bibr" target="#b22">Schnitger and Gramlich, 2005</ref>) that NFA minimization in terms of the number of states/transitions (trellis nodes/links) is NP-hard to approximate within a constant factor. We have hence developed a heuristic algorithm (<ref type="figure" target="#fig_0">Fig. 1C</ref>) that is similar to the determinizeâminimize procedure of<ref type="bibr" target="#b24">Watson (1993)</ref>but that is specialized to sets of MS/MS theoretical spectra. The resulting trellis GÃ° Y Ã is the minimum state deterministic finite automaton (DFA;<ref type="bibr" target="#b12">Hopcroft et al., 2001</ref>) of the given language of peptides Y. An example run of this algorithm for the strings 'ac', 'ad', 'bc' and 'bd' is depicted in steps (1)â(5) of<ref type="figure" target="#fig_0">Figure 1D</ref>. The for loop, which merges prefixes of input strings, constructs a DFA out of Y. Minimization on the constructed DFA can be thought of as a process that merges nodes which share the same suffixes. Both merging prefixes and suffixes reduce the number of links in the trellis, making the algorithm a powerful heuristic in practice. The complexity of the algorithm is bounded by the DFA minimization step. With Hopcroft's algorithm (<ref type="bibr" target="#b11">Hopcroft, 1971</ref>), the running time is OÃ°jRjj Y jl max logÃ°j Y jl max ÃÃ, where l max Â¼ max y2 Y j yj.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Trellises for MS/MS scoring functions</head><p>To speed up the scoring of a set of candidate peptides, we construct a trellis, G p , consisting of the set of theoretical peaks from the candidate peptides to be scored (i.e. peptides whose masses lie within the specified precursor mass tolerance window). The alphabet R p for G p contains all possible peak m/z values (in Thomsons) discretized according to the resolution of the dataset (e.g. for low-resolution fragment ion spectra, the values are rounded to the nearest integers).<ref type="figure" target="#fig_0">Figure 1B</ref>gives an example of a trellis constructed over the theoretical spectra of three peptides. For each observed spectrum, a trellis is thus constructed containing the theoretical peaks of all peptide candidates within the corresponding mass window. Each trellis then becomes a 'database' to search for the best peptide candidate match. All trellises (one for each 1 Th mass bin) are pre-computed and stored during a database indexing step prior to search. The complexity of construction is bounded by the DFA minimization step as stated above (OÃ°jRjj Y jl max logÃ°j Y jl max ÃÃ), and for trellises of theoretical peptide peaks, jRj is the number of distinct peak m/z values and is a constant based on the resolution of the data, j Y j is the number of theoretical peptides in the querying mass window, and l max is the number of theoretical peaks of the longest peptide candidate in the mass window. We next show how MS/MS trellis traversal can be expressed and implemented with DGMs (Section 3.3). This enables the combined use of trellises with both linear and non-linear MS/MS scoring functions expressed by DGMs (Section 4), thereby allowing significantly faster database search (Section 5.1) and efficient generative and discriminative training for improved identification accuracy (Section 7.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Traversing trellises using DGMs</head><p>A graphical model compactly represents the factorization properties of a family of probability distributions defined over a set of random variables. In a graphical model's graph, vertices represent random variables and edges denote allowable direct interaction between variables. A Bayesian network is one type of graphical model that uses directed acyclic graphs. DGMs are defined over temporal sequences where each element in the sequence (called a frame) is represented by a repeated set of vertices and edges. DGMs provide a great deal of modeling power and flexibility, offering a calculus with which to construct widely varying and potentially very complex models to reason about the underlying data while providing strategies to maintain tractable inference. DBNs are DGMs where the graphs are directed and acyclic. As in<ref type="bibr" target="#b14">Ji et al. (2006)</ref>, we can use DGMs to traverse over a trellis. At frame t, we use three vertices to access the trellis: a trellis-node vertex V t , a trellis-link vertex L t , and a transition vertex D t. Intuitively, V t corresponds to a node in the trellis, L t corresponds to a link in the trellis and D t controls the traversal of the trellis. V tÃ1 , V t , and D t determine the set of possible values for L t , with each value of L t corresponding to one character in the encoded strings. Our trellises can be represented as a DGM structure (<ref type="figure" target="#fig_1">Fig. 2</ref>). Values V t Â¼ n i and D tÃ¾1 Â¼ d (d ! 0) determine the allowable set of trellis nodes V tÃ¾1 2 fn j 2 Nj9p 2 PÃ°G; n i ; n j Ã; jpj Â¼ dg (V tÃ¾1 has 0 probability for values not in the allowable set, and has the same probability for all values in the allowable set). Also, values V t Â¼ n i , V tÃ¾1 Â¼ n j , and D tÃ¾1 Â¼ d together determine the allowable set of links L tÃ¾1 2 fl 2 Lj9p 2 PÃ°G; n i ; n j Ã; jpj Â¼ d; pÂ½d Ã 1Â Â¼ lg. Thus, L tÃ¾1 is a random variable corresponding to all links that go into n j and can be reached from n i with a path of length d. If d Â¼ 0 (i.e. ato the set of links being traversed, which contains the data to access. The value of L t is decided based on the previous node VtÃ1, the current node V t , and the transition D t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trellises i325</head><p>zero length path connecting two nodes), then the algorithm stays at the current node and the link stays put as well, i.e. L tÃ¾1 Â¼ L t. In the simplest case, D t is binary (so D t 2 f0; 1g), so that if D tÃ¾1 Â¼ 0, the algorithm stays at the current node and link, and if D tÃ¾1 Â¼ 1; L tÃ¾1 may be the outgoing link incident to n i , and V tÃ¾1 the set of corresponding destination trellis-nodes for those outgoing links. Taking<ref type="figure" target="#fig_0">Figure 1A</ref>as an example, suppose V t Â¼ fn 0 ; n 1 g and D tÃ¾1 Â¼ 1, then V tÃ¾1 Â¼ fn t g and L tÃ¾1 Â¼ f ' ttle', 'food', 'fu'g. The 'FIRST_NODE' vertex is observed to be the node value n s and is used only for initializing the time-dependent structure. The complexity for constructing the conditional probability table (CPT) to store PrÃ°L t jV t ; V tÃ¾1 ; D tÃ¾1 Ã, which is required for traversing the trellis as described above, is jfÃ°l 1 ; l 2 Ã : l 1 ; l 2 2 L; 9 p 2 PÃ°G; l 1 ; l 2 Ã; jpj d max gj, where d max is the largest value D t may take (i.e. the maximum number of deletions). The CPT is likely quite sparse because many paths do not exist. The value of d max can vary based on the underlying DGM. If only link-by-link traversal is desired, then d max Â¼ 1. Setting d max Â¼ 1 encodes all subsequences of the data instance in the trellis. The CPT can be constructed online to save memory. Our implementation in the Graphical Model Toolkit (GMTK) (<ref type="bibr" target="#b2">Bilmes and Zweig, 2002</ref>) efficiently supports sparse trellis CPTs. A trellis DGM representation is applicable to any DGM that accesses data in a sequential manner. Rather than accessing data in the traditional way, the trellis representation, moreover, offers two major benefits over accessing sets of sequences in the traditional way, namely: (i) various pruning and approximate inference strategies can be applied locally that speed up the underlying DGM significantly, since pruning causes many trellis paths to be removed simultaneously; (ii) compressed trellis representation makes practical certain expensive learning methods that requires access to the entire set of data, such as discriminative training. Below, we show how we take advantage of both of these benefits in our trellis/DGMbased peptide-spectrum score functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Graphical model MS/MS scoring functions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Linear scoring via graphical models</head><p>Many MS/MS scoring functions, including XCorr, X!Tandem, the base scores for MS-GFÃ¾ and OMSSA, and Morpheus (<ref type="bibr" target="#b25">Wenger and Coon, 2013</ref>), are linear in the theoretical and observed spectra (i.e. they constitute a dot product between two preprocessed vectors corresponding to the theoretical and the observed spectrum). Using virtual evidence (<ref type="bibr" target="#b0">Bilmes, 2004;</ref><ref type="bibr" target="#b20">Pearl, 1988</ref>), where the conditional distributions of observed child variables may be unnormalized nonnegative scores, we may easily represent any MS/MS linear scoring function in a graphical model as the log-likelihood of a mixture-like model. This can then be combined with the aforementioned trellis constructs. In this section, we first describe linear score functions and then, in Section 4.2, show how a non-linear score function is also compatible with trellises. Let z t be an observed child variable that is always observed to be a fixed and known value (e.g. unity). Such a child is known as 'virtual evidence' since it may be used to impart a soft version of evidence into a model as follows: given a distribution PrÃ°y t Ã over a random variable y t , the construct PrÃ° z t jy t ÃPrÃ°y t Ã is a function of only y t but is a generalization of evidence for y t. For example, if PrÃ° z t jy t Ã Â¼ dÃ°y t Â¼ y t Ã, where d is a Kronecker delta, then this would be the same as y t being observed at value y t. If, on the other hand, Pr Ã° z t jy t Ã is a non-negative vector (indexed by y t ) of real values, this imparts virtual evidence for different values of y t. Another construct we utilize (<ref type="bibr" target="#b2">Bilmes and Zweig, 2002</ref>) is that of 'switching parents' and 'switching weights'. Let a t be what is known as a switching parent, and consider a conditional distribution PrÃ° z t jy t ; a t Ã. When a t is a switching parent, the current value of a t determines the subset of other parents of z t that are active. For example, a t Â¼ 3 might say that z t is no longer dependent on y t. The construct of switching weights, moreover, allows a t to determine an exponential weight of the distribution. That is, PrÃ° z t jy t ; a t Ã / Ã°PrÃ° z t jy t ÃÃ wa t , where PrÃ° z t jy t Ã is some locally normalized y t-dependent distribution on z t , and w at is some non-negative a t-dependent constant weight. More details of these constructs are given in Bilmes (2004). Virtual evidence, along with switching weights, allows us to express any linear (i.e. dot-product) MS/MS score within a DGM in a way that is ideally suited to DGM-expressed trellises. Given a peptide y, recall that its binned theoretical spectrum, b y, is a length-B sparse vector that corresponds to the positions, along the binned m/z axis, where there are peaks in a theoretically derived spectrum. Let y 8 be an increasing-order sorted vector of indices from 0 to B Ã 1, that is y 8 Â¼ Ã°0; 1;. .. ; B Ã 1Ã. Also, recall that b x 0 is the length-B processed observed spectrum that is utilized in a dot-product scoring function hb x 0 ; b yi (for example, Equation (2) in the case of XCorr). Most MS/MS linear scoring functions use different weights depending on the type of theoretical peak. For instance, in XCorr, band y-ions are each assigned weight 50, and the neutral losses of ammonia, water and carbon monoxide are each assigned weight 10. These weights are the unique values in the vector b y and are then multiplied by the corresponding processed observed spectrum b x 0 , as expressed by the dot-product hb x 0 ; b yi. Let a Â¼ Ã°a 0 ; a 1 ;. .. ; a BÃ1 Ã be a length-B vector of peak type indices, so there are B peak types indices, one for each peak. Each a t is integer valued and takes on values a t 2 f0; 1;. .. ; kg for k peak types. Since b y is sparse, some of the positions along the binned m/z are empty, and we encode the empty condition of position t as a t Â¼ 0. Moreover, let w Â¼ Ã°w 0 ; w 1 ;. .. ; w k Ã be a fixed vector of peak type non-negative weights, one weight for each of the k possible peak types, and where w 0 Â¼ 0. That is, the value of wÃ°a t Ã depends on the ion type of the theoretical t-th theoretical peak (i.e. whether it is a b-/y-ion or a neutral loss), or if the peak is non-present. Therefore, Ã°b yÃ°y</p><formula>Ã Â¼ Y BÃ1 tÂ¼0 pÃ°u t ÃpÃ°a t ÃpÃ° z t jy 8 t ; a t Ã (3) / Y BÃ1 tÂ¼0 pÃ°y 8 t ÃpÃ°a t ÃÃ°PrÃ° z t jy 8 t ÃÃ wÃ°at Ã (4)</formula><p>When it is the case that the current theoretical peptide is observed, then both y 0:BÃ1 and a 0:BÃ1 are also observed, and if at these observed values we set pÃ°y</p><formula>8 t Ã Â¼ pÃ°a t Ã Â¼ 1</formula><p>, we, therefore, get that logPrÃ° z 0:BÃ1 ; y 8 0:BÃ1 ; a 0:BÃ1 Ã Â¼ hb x 0 ; b yi Ã¾ const. All of the above can be expressed with a graphical model of length T Â¼ B (<ref type="figure">Fig. 3A</ref>). If a sequencer over a trellis determines the vector variables y 8 0:TÃ1 and a 0:TÃ1 (which are no longer observed), then the specifics of the dotproduct be directly controlled and used via a trellis, and all of the aforementioned benefits of trellises become applicable (see<ref type="figure">Fig. 4B</ref>and details in Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Non-linear scoring via graphical models</head><p>A linear model is not the only one that can be used with trellises with a DGM. DRIP is a model that (potentially non-linearly) aligns an unquantized observed spectrum with a peptide's theoretical spectrum. In DRIP, the theoretical spectrum is represented by hidden variables, as are constructs corresponding to insertions (spurious observed peaks) and deletions (missing theoretical peaks). An instantiation of the random variables in DRIP thus correspond to an alignment between the theoretical and observed spectra, where an alignment corresponds to the sequence of theoretical peaks used to score observed peaks as well as the sequences of insertions and deletions. During exact probabilistic inference, all possible alignments between the theoretical and observed spectra are considered, and the most probable alignment is used to score a peptide. We next describe particulars of how DRIP aligns spectra, and in Section 5 we show how it naturally combines with trellises. The DRIP DGM template is displayed in<ref type="figure">Figure 3B</ref>, where the middle frame (the chunk) is dynamically expanded to fit the length of the current observed spectrum; that is, like in<ref type="figure">Figure 3A</ref>, DRIP considers the observed spectrum as the temporal sequence being modeled. Let ~ n x be the number of frames and t be an arbitrary frame number. Each frame of the model corresponds to a single observed peak, with observed variables o mz t and o in t corresponding to the t-th m/z value and intensity, respectively. Parent to these variables, the Bernoulli random variable i t denotes whether an observed peak is an insertion (in which case, we score these observations using a constant penalty) or not (in which case, we score these observations using a Gaussian centered along the m/z accessâa Gaussian centered near the current frame's theoretical peak will of course score much higher than a Gaussian centered farther away). To score a sequence of observed peaks using a set of theoretical peaks (which corresponds to a particular theoretical spectrum), a sequencer, expressed using a set of hidden random variables, probabilistically traverses through the spectrum from left to right, and this corresponds to the blue shaded portion of<ref type="figure">Figure 3B</ref>. Let v y be a vector containing a theoretical peptide's fragment ions, sorted in increasing order. We index into elements of this vector via the random variable K t , which denotes the element index of v y in a particular frame. The non-negative, discrete random variable d t indicates the number of theoretical peaks we skip when advancing in the model by one frame; hence, PrÃ°K tÃ¾1 Â¼ ijK t Â¼ jÃ Â¼ 1 fiÂ¼jÃ¾dt g. Hence, the number of deletions that occur after frame t is d t Ã 1. The observed variable ~ n y , d t 's parent, ensures that d t does not increment past the number of remaining theoretical peaks in the current theoretical peptide. We note that despite the use of probability in DRIP to traverse through a theoretical peptide's spectrum to achieve an alignment with an observed spectrum, only one theoretical peptide is considered at a time to control the sequencer. In the next section, we describe trellises which mitigate this limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>Connecting trellises with graphical model MS/MS scoring functions<ref type="figure">Figure 4A</ref>shows the DGM for DRIP that uses trellises. The vertex d t , which controls the number of deletions, behaves similarly to the transition random variable D t in the trellis representation in DGMs, and we feed the value of d t into D t (green cone in<ref type="figure">Fig. 4A</ref>). As d t ranges from 0 to the maximum length of the candidate peptides, all subsequences of a peptide are encoded in the trellis. The value of L t , which contains the set of m/z values of b/y-ions stored as trellis links given values of V t and D t , is fed into the 'THEO_PEAK' vertex v y Ã°K t Ã (pink cone in<ref type="figure">Fig. 4A</ref>) for peptide scoring (Section 4). In general, the trellis variant of DRIP scores all the candidate peptides all together unlike standard DRIP, which scores candidate peptides separately and one by one. Thus, the trellis acts like a database for querying theoretical peaks, and it does not affect the mechanism of the underlying DGM but it does allow joint decoding and reuse of common computational patterns. A B<ref type="figure">Fig. 4. (</ref>A) The DRIP trellis model. The trellis DBN (Trellis Part) is attached to the DRIP DBN (DRIP Part) by taking the input from d t from DRIP (green cone), which controls the traversal of theoretical peaks, and outputting L t for DRIP to score (pink cone), which is the m/z values of theoretical peaks. The DRIP DBN structure remains unchanged (the part with green background is unchanged from<ref type="figure">Fig. 3B</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trellises i327</head><p>Similar to DRIP, candidate theoretical peaks for XCorr can be represented as a trellis and accessed via the node, transition, link and index variables, as in the 'Trellis Part' of<ref type="figure">Figure 4B</ref>(the transition is always 1 for XCorr). However, unlike DRIP, XCorr requires that a theoretical peak be weighted differently depending on the corresponding ion type, i.e. whether it is a b/y-ion or neutral loss. To traverse two sets of sequences simultaneously within the same trellisâthe sequence of theoretical peaks as well as the sequence of theoretical peak ion typesâan extra bit is appended to each theoretical peak in the trellis. This bit denotes the value of the ion type variable, which acts as a switching parent and changes the weight of the theoretical peak (Section 4). This mechanism may be easily extended to include any variable number of ion types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Speeding up graphical model inference with trellises</head><p>jointly modeling multiple sequences (in our case, theoretical spectra) in a single trellis, compared to performing approximate inference with beam pruning independently on each sequence. This pruning results in a substantial computational reduction while ensuring that we have not pruned the most probable hypothesis. Define a hypothesis h Â¼ h 1 ;. .. ; h t Â¼ h 1:t to be a sequence of instantiated random variables for frames 1. .. ; t in a DGM. For frame t 0 , let h Ã t0 be the most probable hypothesis we are trying to infer, m be the number sequences represented in the trellis, n t 0 be the number of hypotheses with higher probability than h Ã t0 , and r t 0 be the total number hypotheses in the trellis without pruning at frame t 0 .</p><p>Theorem 1: Suppose m and n t 0 are large, r t 0 ) n t 0 , and the n t 0 hypotheses with higher probability than h Ã t0 are uniformly distributed from the m sequences in the trellis. In expectation, performing inference on each sequence independently requires XÃ°m ffiffiffiffiffiffiffiffiffiffiffi ffi n t 0 =m p Ã more beam width to ensure h Ã is not pruned compared to inference in a trellis.</p><p>The proof of Theorem 1 is provided in Section 1 of the Supplementary Appendix. Though a uniform distribution is assumed over the hypotheses, such a distribution is biased in practice for many applications, and the trellis may be even more efficient, because we may more aggressively prune using the k-beam strategy while still preserving the top hypothesis. This theoretically quantifies the expected speedup using trellises to score MS/MS candidate peptides as opposed to scoring candidate peptides individually (in Section 7.1, we demonstrate this speedup empirically).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Training DRIP with trellises</head><p>In<ref type="bibr" target="#b10">Halloran et al. (2014)</ref>, generatively training the DRIP model's Gaussian parameters was shown to significantly increase performance. Here we extend this framework to discriminative training, using trellises to make such training tractable. For the overall training procedure, assume that we have a collection, C, of N i.i.d. pairs Ã°x i ; y i Ã, where x i is an observed spectrum and y i the corresponding peptide we have strong evidence to believe generated x i. Let h be the set of parameters for which we would like to learn (in our case, DRIP's Gaussian parameters). For generative training, we then wish</p><formula>to find h Ã Â¼ argmax h P N iÂ¼1 PrÃ°x i jy i ; hÃ, i.</formula><p>e. we wish to maximize DRIP's likelihood with respect to the parameters to be learned, achieved via the expectationâmaximization algorithm (<ref type="bibr" target="#b4">Dempster et al., 1977</ref>). A much more difficult training paradigm is that of discriminative training, where we not only wish to maximize the likelihood of a set of parameters, but would also like to simultaneously minimize a parameterized distribution defined over a set of alternative hypotheses. In our case, this alternative set consists of all candidate peptides within the precursor mass tolerance not equal to y i , i.e. all incorrect explanations of x i. More formally, our discriminative training criterion is that of MMI estimation (<ref type="bibr" target="#b21">Povey, 2003</ref>). Defining the set of candidate peptides for x i within precursor mass tolerance w as C i Â¼ DÃ°m x ; c x ; D; wÃ and the set of all training spectra and highconfidence PSMs as X and Y, respectively, the MMI objective function we maximize with respect to h is</p><formula>~ I h Ã°X; YÃ Â¼ 1 N X N iÂ¼1 log PrÃ°x i jy i ; hÃ X x2C i PrÃ°x i ; xjhÃ Â¼ 1 N X N iÂ¼1 Ã°logPrÃ°x i jy i ; hÃ Ã log X y2C i PrÃ°x i ; yjhÃÃ;</formula><formula>(5)</formula><p>i328 S.<ref type="bibr">Wang et al.</ref>where we call M n Ã°x i ; y i ; hÃ Â¼ logPrÃ°x i jy i ; hÃ the numerator model and M d Ã°y i ; hÃ Â¼ log P y2C i PrÃ°x i ; yjhÃ the denominator model. Note that the numerator model is our objective function for generative training. Intuitively, Equation (5) is maximized by learning parameters which increase the numerator model (what is done in generative training) and/or decrease the denominator model. Thus, while generative training only learns parameters based on the high-confidence PSMs in the numerator, discriminative training learns parameters which also discourage the ensemble of PSMs in the denominator model (which are incorrect matches). We solve maximize Equation (5) with respect to h using stochastic gradient ascent. In stochastic gradient ascent, we calculate the gradient of the objective function with regards to a single training instance,</p><formula>r h ~ I h Ã°x i ; y i Ã Â¼ r h M n Ã°x i ; y i ; hÃ Ã r h M d Ã°x i ; hÃ,</formula><p>where the gradients of M n and M d are vectors typically referred to as Fisher scores. We update the parameters h using the previous parameters plus a damped version of the objective function's gradient, iterating this process until convergence. In practice, we begin the algorithm by initializing h 0 to a good initial value, i.e. the output of generative training, and the learning rate g j is updated with g jÃ¾1 Â¼ Ã° ffi ffi j p Ã Ã1. Intuitively, the gradients move in the direction maximizing the difference between the numerator and denominator models, encouraging improvement for the numerator while discriminating against the incorrect labels in the denominator. Discriminative training is computationally expensive. The denominator model requires calculating the gradients of all candidate peptides C i , which can be infeasible for many tasks. A further challenge in DRIP's case is that it is difficult to constrain the model to consider valid peptides only, because the distance between subsequent theoretical peaks can take on any value. Trellises address both of these challenges. The denominator model of the discriminative training for DRIP is exactly the same as the DRIP trellis model (Section 5). Furthermore, the trellis of all possible labels can be very compact; together with different strategies to speed up graphical models with trellises discussed in the previous session, discriminative training with trellises is highly efficient. In practice, we use a trellis constructed from a decoy set, which contains permutations of the target peptides as the denominator. As the denominator set grows larger, trellises make the computational cost grow sub-linearly so that discriminative training with trellises is efficient. Our experimental results show that discriminative training positively influences performance (Section 7.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>A significant challenge in evaluating the quality of a spectrum identification algorithm is the absence of a 'ground truth' dataset where the generating peptide is known for each observed spectrum. We, therefore, follow the standard approach of using decoy peptides (which in our case correspond to randomly shuffled versions of each real target peptide) to estimate the number of incorrect identifications in a given set of identified spectra. In this work, targets and decoys are scored separately and used to estimate the number of identified matches at a particular FDR, i.e. the fraction of spectra improperly identified at a given significance threshold. We estimate FDR using the target-only variant of target-decoy competition, TTDC (<ref type="bibr" target="#b15">Keich et al., 2015</ref>). Because the FDR is not monotonic with respect to the underlying score, we instead use the q-value, defined to be the minimum FDR threshold at which a given score is deemed to be significant. Since datasets containing more than 10% incorrect identifications are generally not practically useful, we only plot qvalues in the range Â½0; 0:1Â.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Faster graphical model identification of MS/MS spectra using trellises</head><p>In Section 5.1.1, the expected performance of Viterbi decoding with beam pruning using trellises was proven to be significantly more efficient than considering each sequence independently. We first verified that DRIP's performance, in terms of the number of spectra identified at a fixed FDR threshold, is equivalent with and without use of the trellis (Supplementary Appendix<ref type="figure" target="#fig_0">Fig. 1</ref>). We then investigate the improved inference speed using trellises. To do so, we randomly select and search 200 spectra each from three datasets (described in Supplementary Appendix Section 2): yeast and worm, both acquired using low-resolution precursor scans (63 Th tolerance) and low-resolution fragment ions; and Plasmodium, acquired using a high-resolution precursor scan (650 ppm tolerance) and high-resolution fragment ions. Further details regarding these datasets and search settings may be found in Section 2 of Supplementary Appendix. For all methods, we use the same graphical model inference engine, GMTK (<ref type="bibr" target="#b2">Bilmes and Zweig, 2002</ref>). Experiments were carried out on a 3.40 GHz CPU with 16G memory. For each experiment, the lowest CPU time out of three runs is recorded, and we report the relative CPU time of methods using trellises to those without. For the XCorr mixture model (Section 4), a fixed k-beam for all frames was used. Per spectrum, the trellis-inferred top PSM scores were exactly the same as computing each XCorr individually and determining the top PSM. We test DRIP trellis with two beam pruning strategies (some discussion of beam pruning with trellises can be found in Section 5.1), and we compare the results against DRIP using the beam pruning settings of<ref type="bibr" target="#b10">Halloran et al. (2014)</ref>. The trellis base pruning uses k-beams that are dynamic across time frames, with wider beams for the early part and narrower beams later on. The trellis speed pruning also uses a dynamic k-beam but with a narrower beam, followed by another pruning strategy that removes all hypotheses whose score falls below some fraction of the currently top scoring hypothesis while building up the inference structures. Timing tests show that DRIP runs 7â15 times faster using trellises versus without trellises (<ref type="figure">Fig. 5A</ref>). The absolute timing numbers of all the searches implemented using the graphical model engine are relatively high. XCorr (without a trellis) takes $2 s per spectrum to search the low-resolution datasets (yeast and worm) and $0.5 s per spectrum to search the highresolution Plasmodium dataset. Without a trellis, the more complex DRIP model takes $10 s per spectrum searching the low-resolution datasets and $2 s searching the high-resolution dataset. Constant factor improvements for these models may be accomplished by optimized C Ã¾Ã¾ implementations. For instance, a highly optimized implementation of XCorr (<ref type="bibr" target="#b17">McIlwain et al., 2014</ref>) takes 0.024 s per spectrum searching the high-resolution Plasmodium dataset (minimum time over three runs) with the same compute environment and search settings used in our timing tests. Thus, we expect that an optimized trellis XCorr implementation would search the same spectra (under the same settings and environment) in roughly 0.0024 s per spectrum. We note that trellises can speed up many scoring algorithms simply by changing the preprocessing of data; the trellis approach is agnostic to the underlying scoring algorithm and, therefore, compatible with any method that makes the underlying scoring Trellises i329 algorithm more efficient. We also note that our use of a prototyping language (GMTK) to express the above methods, while slower than highly optimized and specialized C implementation, allows the exploration of vastly different models (linear XCorr and non-linear DRIP) without needing to re-implement each model from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">More accurate graphical model identification of MS/ MS spectra using trellises</head><p>As detailed in Section 6, we use a set of high confidence, charge Ã¾2 PSMs and their corresponding peptide database to discriminatively train the DRIP model, using trellises in the denominator model. We compare the discriminatively trained DRIP model to generatively trained and hand-tuned DRIP models, as well as the methods described in Section 2. The discriminatively trained DRIP model identifies more spectra at a 1% FDR threshold relative to the generatively trained and hand-tuned models (<ref type="figure">Fig. 5</ref>). Note that the discriminatively trained model employs the trellis base pruning strategy; thus, the model yields an increase in accuracy as well as an approximately 7-fold speedup. Comparisons against other search engines (MS-GFÃ¾, XCorr, XCorr P-value and X!Tandem) may be found in Supplementary Appendix Section 3. We further note that, while MS/ MS scoring methods that afford efficient parameter learning are scarce, any DBN may be discriminatively trained using trellises in the same fashion as DRIP. Because several widely used MS/MS algorithms may be expressed as DBNs (shown in Section 5), they may also adapt this overall training procedure to their benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>We have proven that, for many practical settings, the expected runtime when using a trellis with beam pruning for Viterbi decoding is significantly faster than considering sequences independently. We have also empirically shown that trellises may be used to significantly improve the speed and accuracy of peptide identification in tandem mass spectrometry. Using the DRIP model and a DBN implementation of XCorr, we have shown how to apply trellises to dramatically speed up inference (6-to 15-fold), both for lowresolution and high-resolution precursor mass spectra. We further note that the algorithmic speedup afforded by using trellises may, in future work, be combined with previous work on improving XCorr runtime, which focused on constant-factor improvements (<ref type="bibr" target="#b5">Diament and Noble, 2011</ref>). We also note that trellises constructed from peptides with variable modifications can be potentially more efficient as variable modifications produce a lot redundancy among peptides. Our MS/MS trellises support traversing all subsequences of a data instance, allowing 'jumps' over whole subsequences, a novel feature in contrast to traditional trellises (such as those used for speech recognition), which only allow data instances to be sequentially traversed. As in DRIP, where jumps correspond to deletions, this feature may be used to model noise or missing data. Furthermore, the jump feature enriches the hypothesis space representation, allowing more sophisticated models to be expressed and evaluated efficiently. With this feature and the ability to compactly represent entire sets of peptides, we have extended DRIP's learning framework to discriminative training, significantly improving its performance relative to previous training strategies. In future work, we plan to further explore using trellises for improved MS/MS identification. Using trellises to efficiently evaluate and score peptides beyond those in the given database, we will investigate ways to take thresholds with respect to DBN-based scoring functions to compute P-values, similar to the P-value calculations done via dynamic programming by MS-GFÃ¾ and XCorr P-value. By improving score calibration, we expect this approach to greatly improve DRIP's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This work was supported by National Institutes of Health awards R01GM096306 and P41GM103533.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. (A) A trellis encoding 'seattle', 'seafood', 'kungfu' and 'tofu'. ns ; n0; n1; and n t are trellis nodes, and every arrow corresponds to a trellis link, e.g. Ã°ns ; n0;'sea'Ã. (B) An example of a simple trellis for MS/MS scoring functions, consisting of the theoretical peaks (discretized b/y-ions) for three peptides: 'ELAK', 'EALK' and 'EAKK'. Every edge corresponds to the m/z value of a fragment ion rounded to the nearest integer. Three colored paths from source node n s to sink node n t correspond respectively to three peptides. The observed spectrum is charge Ã¾2 with low-resolution fragment ions. (C) Trellis construction algorithm that takes as input a set of strings Y and the corresponding alphabet R and returns a trellis representation of Y. (D) Sample trellis construction for strings: 'ac', 'ad', 'bc' and 'bd'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.2.</head><figDesc>Fig. 2. DBN for traversing a trellis. L t corresponds to the set of links being traversed, which contains the data to access. The value of L t is decided based on the previous node VtÃ1, the current node V t , and the transition D t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>BÃ1</head><figDesc>ÃÃ Â¼ Ã°wÃ°a 0 Ã; wÃ°a 1 Ã;. .. ; wÃ°a BÃ1 ÃÃ. Now for t Â¼ 1;. .. ; B, define a virtual evidence factor so that logPrÃ° z t j y 8 t Ã Â¼ b x 0 Ã°y 8 t Ã. Then with virtual evidence, and switching weights, we produce a probability model as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><figDesc>Fig. 4. (A) The DRIP trellis model. The trellis DBN (Trellis Part) is attached to the DRIP DBN (DRIP Part) by taking the input from d t from DRIP (green cone), which controls the traversal of theoretical peaks, and outputting L t for DRIP to score (pink cone), which is the m/z values of theoretical peaks. The DRIP DBN structure remains unchanged (the part with green background is unchanged from Fig. 3B). (B) The graphical model representation of linear MS/MS scoring functions incorporated with trellis structure. Trellis Part (pink background) is attached to the linear MS/MS function graphical model (green background), which remains unchanged.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><figDesc>Table 1. Notation used in this article</figDesc><table>Symbol 
Function 

x 
observed spectrum of length T x , x t is a (m/z, intensity) pair 
t 
index along m/z axis for DGM expansion, t Â¼ 0; . . . ; T Ã 1 
T 
DGM frame unrolling amount and number of peaks in 
observed spectrum 
m; m x 
precursor mass, precursor mass of spectrum x 
c; c x 
precursor charge, precursor charge of spectrum x 
B 
number of bins of m/z axis quantization (i.e. typically 2000 
for low-resolution data) 
b 
x 
binned and processed observed spectrum of length B 
b 
x 0 
difference observed spectrum (used in XCorr). 
y 
a peptide 
M y 
length of y 
M 
number of theoretical peaks in some peptide 
b 
y 
binned sparse theoretical vector of length B (dot product 
with binned observed spectrum: hb x 0 ; b 
yi) 

y 
length M y sequence of increasing incremental m/z values of 
fragment ions of y (used for trellis) 

Y 
set of strings 
y 2 
Y to be compressed into a trellis. 
y 

vector of peaks indices of y of length T 
y 


t1:t2 

subsequence of 8y from position t 1 to t 2 
a 
vector of peaks types of y of length T 
n i ; n s ; n t trellis nodes 
l 
trellis links 
D 
trellis DGM transition variable 
N 
trellis DGM node variable 
L 
trellis DGM link variable 

z t 
observed child variable for use in a DGM (i.e. 
z t Â¼ 1 always) 

</table></figure>

			<note place="foot">V C The Author 2016. Published by Oxford University Press. i322 This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com Bioinformatics, 32, 2016, i322âi331 doi: 10.1093/bioinformatics/btw269 ISMB 2016 at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">S.Wang et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">Utilizing trellises within DGMs means the state space for accessing all the data instances is much smaller compared with accessing each instance separately. Intuitively, consider a &apos;simple trellis&apos; that contains j Y j disjoint paths from n s to n t , where each path corresponds to one data instance (e.g. the &apos;simple trellis&apos; in Fig. 1D). The state space for the simple trellis is no different than accessing each data instance separately. By constructing the compact trellis, redundant structures in the simple trellis get merged into shared links so that the state space is greatly reduced [step (5) in Fig. 1D]. Depending on the data, the state space reduction can be quite significant. In general, as datasets get larger, we expect more shared structures since there is more tendency for redundancy. Hence, the size of a trellis will grow sublinearly with the size of the input data. For the task of peptide identification in mass spectrometry, there are often thousands of candidate peptides within a certain mass window for one spectrum having the potential of being compressed. Moreover, trellises can be even more effective when post-translational modifications or sequence variants are considered (which otherwise greatly increase the number of separate peptide candidates). Along with trellises, approximate inference algorithms are effective at significantly reducing the state space of DGMs, decreasing runtime but without keeping the most probable sequence from being inferred. One such method, ideally suited for trellis inference, is k-beam pruning (histogram pruning in Ney et al. (1994), a heuristic where only the k most probable hypotheses (or states) at each time frame are retained and all other hypotheses are pruned away (i.e. no longer modeled). While k-beam pruning may be used in DGMs without trellises, utilizing k-beam pruning with trellises is significantly more effective since we are pruning hypotheses from the joint representation shared by all sequences. For example, the hypotheses of the original DRIP model only consists of a single peptide&apos;s alignments between an observed spectrum so that, when using beam pruning, poor alignments are pruned away early on. With trellises, beam pruning induces a competition among all peptide hypotheses, where peptide candidates which align poorly with the observed spectrum are pruned away early, so we end up scoring only a subset of the candidates. Note that, while we use k-beam pruning in this work, other beam pruning methods or forms of approximate inference may also be sped up using trellises, because trellises only alter the representation of the underlying sequential hypothesis space. 5.1.1 Optimal pruning bounds for Viterbi decoding in trellises Here we prove under generally applicable assumptions that, in expectation, a small beam width may be used with impunity when</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B C</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<monogr>
		<title level="m" type="main">On virtual evidence and soft evidence in Bayesian networks</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Bilmes</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic graphical models</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Bilmes</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Proc. Mag</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="29" to="42" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">The Graphical Models Toolkit: an open source software system for speech and time-series processing</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Bilmes</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Zweig</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech, and Signal Processing<address><addrLine>Orlando, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Tandem: matching proteins with tandem mass spectra</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Craig</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">C</forename>
				<surname>Beavis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1466" to="1467" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">P</forename>
				<surname>Dempster</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Faster sequest searching for peptide identification from tandem mass spectra</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Diament</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">S</forename>
				<surname>Noble</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Proteome Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="3871" to="3879" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<monogr>
		<title level="m" type="main">Generalizing word lattice translation</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Dyer</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">An approach to correlate tandem mass spectral data of peptides with amino acid sequences in a protein database</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">K</forename>
				<surname>Eng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Soc. Mass Spectrom</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="976" to="989" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">A tree-trellis based fast search for finding the n-best sentence hypotheses in continuous speech recognition</title>
		<author>
			<persName>
				<forename type="first">E.-F</forename>
				<surname>Huang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">K</forename>
				<surname>Soong</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting><address><addrLine>Toronto, Canada, IEEE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="705" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Open mass spectrometry search algorithm</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">Y</forename>
				<surname>Geer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Proteome Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="958" to="964" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning peptide-spectrum alignment models for tandem mass spectrometry</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">T</forename>
				<surname>Halloran</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<monogr>
		<title level="m" type="main">An n log n algorithm for minimizing states in a finite automaton</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Hopcroft</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Introduction to automata theory, languages, and computation</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">E</forename>
				<surname>Hopcroft</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGACT News</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="60" to="65" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<monogr>
		<title level="m" type="main">Statistical Methods for Speech Recognition</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Jelinek</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, US</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Graphical model representations of word lattices</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Ji</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ ACL Workshop on Spoken Language Technology (SLT), Palm Beach</title>
		<meeting><address><addrLine>Aruba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved false discovery rate estimation procedure for shotgun proteomics</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Keich</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Proteome Res</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="3148" to="3161" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Ms-gfÃ¾ makes progress towards a universal database search tool for proteomics</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Kim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">A</forename>
				<surname>Pevzner</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">5277</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Crux: rapid open source protein tandem mass spectrometry analysis</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Mcilwain</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Proteome Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="4488" to="4491" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey of computational methods and error rate estimation procedures for peptide and protein identification in shotgun proteomics</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">I</forename>
				<surname>Nesvizhskii</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Proteom</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="2092" to="2123" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Improvments in beam search</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Ney</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings the International Conference on Spoken Language Processing</title>
		<meeting>the International Conference on Spoken Language Processing<address><addrLine>Yokohama, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Pearl</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<monogr>
		<title level="m" type="main">Discriminative training for large vocabulary speech recognition</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Povey</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Minimizing NFA&apos;s and regular expressions</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Schnitger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Gramlich</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd International Symposium on Theoretical Aspects of Computer Science</title>
		<meeting><address><addrLine>Stuttgart, Germany, Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="399" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Bone marrow transplantation for sickle cell disease. New Engl</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">C</forename>
				<surname>Walters</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med</title>
		<imprint>
			<biblScope unit="volume">335</biblScope>
			<biblScope unit="page" from="369" to="376" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">A taxonomy of finite automata minimization algorithms</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">W</forename>
				<surname>Watson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci. Note</title>
		<imprint>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">A proteomics search algorithm specifically designed for high-resolution tandem mass spectra</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">D</forename>
				<surname>Wenger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">J</forename>
				<surname>Coon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Proteome Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1377" to="1386" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<monogr>
		<title level="m" type="main">The HTK Book, 2.1 edn. Cambridge</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Young</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>