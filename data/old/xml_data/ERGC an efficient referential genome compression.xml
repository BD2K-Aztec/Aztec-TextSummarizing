
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ERGC: an efficient referential genome compression algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Subrata</forename>
								<surname>Saha</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Connecticut</orgName>
								<address>
									<postCode>06269</postCode>
									<settlement>Storrs</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Sanguthevar</forename>
								<surname>Rajasekaran</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Connecticut</orgName>
								<address>
									<postCode>06269</postCode>
									<settlement>Storrs</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ERGC: an efficient referential genome compression algorithm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btv399</idno>
					<note type="submission">Received on February 11, 2015; revised on June 12, 2015; accepted on June 17, 2015</note>
					<note>Sequence analysis *To whom correspondence should be addressed. Associate Editor: John Hancock Contact: rajasek@engr.uconn.edu</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Genome sequencing has become faster and more affordable. Consequently, the number of available complete genomic sequences is increasing rapidly. As a result, the cost to store, process, analyze and transmit the data is becoming a bottleneck for research and future medical applications. So, the need for devising efficient data compression and data reduction techniques for biological sequencing data is growing by the day. Although there exists a number of standard data compression algorithms, they are not efficient in compressing biological data. These generic algorithms do not exploit some inherent properties of the sequencing data while compressing. To exploit statistical and information-theoretic properties of genomic sequences, we need specialized compression algorithms. Five different next-generation sequencing data compression problems have been identified and studied in the literature. We propose a novel algorithm for one of these problems known as reference-based genome compression. Results: We have done extensive experiments using five real sequencing datasets. The results on real genomes show that our proposed algorithm is indeed competitive and performs better than the best known algorithms for this problem. It achieves compression ratios that are better than those of the currently best performing algorithms. The time to compress and decompress the whole genome is also very promising. Availability and implementation: The implementations are freely available for non-commercial purposes. They can be downloaded from</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Next-generation sequencing (NGS) technologies are producing millions to billions of short reads from DNA molecules simultaneously in a single run within a very short time period, leading to a sharp decline in whole genome sequencing costs. As a result, we are observing an explosion of genomic data from various species. Storing these data is an important task that the biologists have to perform on a daily basis. To save space, compression could play an important role. Also, when the size of the data transmitted through the Internet increases, the transmission cost and congestion in the network also increase proportionally. Here again compression could help. Although we can compress the sequencing data through standard general purpose algorithms, these algorithms may not compress the biological sequences effectively, since they do not exploit inherent properties of the biological data. Genomic sequences often contain repetitive elements, e.g. microsatellite sequences. The input sequences might exhibit high levels of similarity. An example will be multiple genome sequences from the same species. Additionally, the statistical and information-theoretic properties of genomic sequences can potentially be exploited. General purpose algorithms do not exploit these properties. In this article, we offer a novel algorithm to compress genomic sequences effectively and efficiently. Our algorithm achieves compression ratios that are better than the currently best performing algorithms in this domain. By compression ratio, we mean the ratio of the uncompressed data size to the compressed data size.The following five versions of the compression problem have been identified in the literature: (i) genome compression with a reference: here we are given many (hopefully very similar) genomic sequences. The goal is to compress all the sequences using one of them as the reference. The idea is to utilize the fact that the sequences are very similar. For every sequence other than the reference, we only have to store the difference between the reference and the sequence itself; (ii) reference-free genome compression: this is the same as Problem 1, except that there is no reference sequence. Each sequence has to be compressed independently; (iii) reference-free reads compression: it deals with compressing biological reads where there is no clear choice for a reference; (iv) reference-based reads compression: in this technique, complete reads data need not be stored but only the variations with respect to a reference genome are stored; and (v) metadata and quality scores compression: in this problem, we are required to compress quality sequences associated with the reads and metadata such as read name, platform and project identifiers. In this article, we focus on Problem 1. We present an algorithm called ERGC (Efficient Referential Genome Compressor) based on a reference genome. It employs a divide and conquer strategy. At first it divides both the target and reference sequences into some parts of equal size and finds one-to-one maps of similar regions from each part. It then outputs identical maps along with dissimilar regions of the target sequence. The rest of this article is organized as follows: Section 2 has a literature survey. Section 3 describes the proposed algorithm and analyses its time complexity. Our experimental platform is explained in Section 4. This section also contains the experimental results and discussions. Section 5 concludes the article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A survey of compression algorithms</head><p>We now survey some of the algorithms that have been proposed in the literature to solve Problem 1. In referential genome compression, the goal is to compress a large set S of similar sequences. The core idea of reference-based compression can be described as follows. We first choose the reference sequence R. Then we compress every other sequence s 2 S by comparing it with R. The target (i.e. the current sequence to be compressed) is first aligned to the reference. Then mismatches between the target and the reference are identified and encoded. Each record of a mismatch may consist of the position with respect to the reference, the type (e.g. insertion, deletion or substitution) of mismatch and the value.<ref type="bibr">Brandon et al. (2009)</ref>have used various coders like Golomb (<ref type="bibr" target="#b4">Golomb, 1966</ref>), Elias (<ref type="bibr" target="#b10">Peter et al., 1975</ref>) and Huffman (<ref type="bibr" target="#b5">Huffman, 1952</ref>) to encode the mismatches. Wang and Zhang (2011) have presented a de novo compression program, GRS, which obtains variant information by using a modified Unix diff program. The algorithm GReEn (<ref type="bibr" target="#b11">Pinho et al., 2012</ref>) employs a probabilistic copy model that calculates target base probabilities based on the reference. Given the base probabilities as input, an arithmetic coder was then used to encode the target. Recently, another algorithm, namely, iDoComp (<ref type="bibr" target="#b8">Ochoa et al., 2014</ref>) has been proposed which outperforms some of the previously best known algorithms like GRS, GReEn and GDC. GDC (<ref type="bibr" target="#b1">Deorowicz and Grabowski, 2011</ref>) is an LZ77-style compression scheme for relative compression of multiple genomes of the same species. In contrast to the algorithms mentioned above,<ref type="bibr" target="#b0">Christley et al. (2009)</ref>have proposed the DNAzip algorithm that exploits the human population variation database, where a variant can be a single-nucleotide polymorphism (SNP) or an indel (an insertion or a deletion of multiple bases). Some other notable algorithms that employ VCF (Variant Call Format) files to compress genomes have been given by<ref type="bibr" target="#b2">Deorowicz et al. (2013) and</ref><ref type="bibr" target="#b9">Pavlichin et al. (2013).</ref>These algorithms have been used in the 1000 Genomes project to encode SNPs and other structural genetic variants. Next we explain elaborately some of the best known algorithms in this domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">GRS</head><p>GRS is a reference-based genome compression tool exclusively dependent on the Unix program diff as mentioned above. Specifically, the primary step of GRS is to find longest common subsequences in two input strings. The auxiliary Unix program diff is employed to calculate a similarity measure between a target genomic sequence and a reference genomic sequence. If the similarity score exceeds a predefined threshold, the difference between the target and reference genomic sequences is encoded using Huffman encoding. If the similarity score is below the threshold, the target and reference sequences are split into smaller blocks and the computation is restarted on each pair of the blocks. It is to be noted that GRS does not require any additional information about the sequences, e.g. a reference SNPs map. If there exists an excessive difference between the target and reference sequences, GRS will not be able to compress the target sequence effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GDC</head><p>GDC is a LZ77-style (<ref type="bibr" target="#b14">Ziv and Lempel, 1977</ref>) compression algorithm closely related to RLZopt (<ref type="bibr" target="#b12">Shanika et al., 2011</ref>) where GDC performs a non-greedy parsing of the target into the reference by hashing. On the contrary, RLZopt uses a suffix array. The main difference between GDC and the other reference-based compression tools is that it can choose a suitable reference (or, more than one reference) sequence among the set of genomic sequences from the same species using a heuristic and use it to compress the rest. It also introduces a clever trick to encode approximate matches. The algorithm slightly alters the original Lempel-Ziv parsing scheme by considering trade-offs between the length of matches and distance between matches. Compression is performed on input blocks with shared Huffman codes by enabling random access of the reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">GReEn</head><p>GReEn is also a reference-based genome compression tool. It encodes the target sequence using an arithmetic encoder. At first it generates statistics using the reference sequence and then performs the compression of the target by employing arithmetic coding. Arithmetic encoder uses the previously computed statistics to encode the target. From experimental results, it is evident that GReEn outperforms both GRS and the non-optimized RLZ. Similar to the non-referential compression scheme XM (<ref type="bibr">Cao et al., 2007</ref>), GReEn introduces a copy expert model. This model tries to find identical k-mers between the target and reference sequences. Raw characters in the form of arbitrary ASCII characters are encoded with arithmetic encoding. However, there is a special case where target and reference sequences have equal length. Although not justified, GReEn assumes that the sequences are already aligned and can be distinguished by SNPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">iDoComp</head><p>The basic functioning of iDoComp can be summarized in three main steps: (i) mapping generation: in this stage, the target genome is expressed in terms of the reference genome. It uses suffix arrays to parse the target into the reference; (ii) post-processing: the postprocessing looks for consecutive matches that can be merged together and converted into an approximate match and (iii) entropy encoding: entropy encoder compresses the mapping and generates the compressed file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Our algorithm</head><p>We have developed a reference-based genome compression algorithm called ERGC. It performs better than the best known algorithms existing in the current literature. Our algorithm runs in stages. Each stage is independent of the previous stage(s). In this setting, it can be readily transferred from in-core to out-of-core model and single-core to multi-core environment. We will discuss these enhancements later in this section. Details of our algorithm follow. Assume that R is the reference sequence and T is a target sequence to be compressed. At first ERGC divides the entire reference and target genomes into parts of equal sizes and processes each pair of parts sequentially. If the parts in R and T are r 1 ; r 2 ;. .. ; r q and t 1 ; t 2 ;. .. ; t q , respectively, then r 1 and t 1 are processed first, r 2 and t 2 are processed next and so on. Let ðr 0 ; t 0 Þ be the pair processed at some point in the algorithm (where r 0 comes from the reference genomic sequence R and t 0 comes from the target genomic sequence T). To find the similarities between r 0 and t 0 , we need to align t 0 onto r 0. Similar regions between two sequences can be found globally aligning t 0 onto r 0 using Needleman–Wunsch algorithm as the sequences in the query set are similar and of roughly equal size. Since the time complexity of the global alignment algorithm is quadratic and thus based on dynamic programming, it is a very time and space intensive procedure especially when the length of the sequences is very large. In this context, we have devised our own greedy alignment algorithm to find similar regions between two sequences with high confidence (it is applicable when the sequences of interest are highly similar, e.g. two genomic sequences of the same species). Now we describe our greedy algorithm next. Our greedy alignment algorithm is based on hashing. At first, the algorithm generates all the k-mers from r 0 and hashes them into a hash table H (for some suitable values of k). It then generates k-mers from t 0 one at a time and hashes them to H until one of these k-mers collides with an entry in H. If none of the k–mers collides with an entry in H, the algorithm generates another set of k 0-mers (where k 0 &lt; k) from r 0 and hashes them into a hash table H 0. In a similar way, it then generates k 0-mers from t 0 one at a time and hashes them to H 0 until one of these k 0-mers collides with an entry in H 0. We employ a predefined set of values for k and try these values one at a time until a collision happens. The reason for taking a range of k–mer values is that the occurrences of substitutions, insertions and/ or deletions can be more frequent in some parts of r 0 and t 0 than in the others. A range of values for k ensures that for at least one value a collision will occur. If there is no such collision, it is not possible to align t 0 onto r 0. If none of the values of k from this set results in a collision, then the algorithm extends the length of r 0 on both sides and a similar scheme is followed as described above. If all of the above mentioned techniques fail, then t 0 is saved as a raw sequence. Otherwise we align r 0 and t 0 with the k-mer that causes a collision as the anchor and extend the alignment beyond that position until there is a mismatch between r 0 and t 0. We record the matching length and the starting position of this stretch of matching in the reference genome. At this point, we delete the matching sequences from r 0 and t 0 and align the rest using our greedy algorithm as described above until the length of r 0 or t 0 becomes zero or there can not be any further alignment possible. This is how the algorithm proceeds iteratively. Next we describe how ERGC takes care of unmatched sequences. As there can be substitutions, insertions and/or deletions in the reference and target genomes, some portions of the genomes between two alignments will not be matched perfectly. In this case, we align those sequences using edit scripting. If the edit distance is large enough to exceed the cost to store the unmatched sequence of the target genome, we discard the edit script and store the raw sequence. The information generated to compress the target sequence is stored in an ASCII formatted file. After having processed all the parts of r 0 and the corresponding parts in t 0 , we compress the starting positions and matching length using delta encoding. The resulting file is further compressed by using PPMd lossless data compression algorithm. It is a variant of prediction by partial matching algorithm and an adaptive statistical data compression technique based on context modeling and prediction. For more details, the reader is referred to<ref type="bibr" target="#b7">Moffat et al. (1990)</ref>. Some recent implementations of PPMd are effective in compressing text files containing natural language text. The seven-zip open-source compression utility provides several compression options including the PPMD algorithm. Details of the algorithm are shown as Algorithm 1. Values of parameters such as set K, q and s have been optimized to get the best results. In our experiments, we have used default values of K and q where s is chosen dynamically. The set K contains two fixed values, i.e. K ¼ f21; 9g. At first ERGC tries to align using 21-mers (i.e. k ¼ 21). If it fails, k ¼ 9 is picked to align the parts. The value of q is chosen in such a way that each part is composed of 20 000 nucleotides approximately. If the unaligned substrings from the reference and target are approximately equal and the memory needed to store the cost of edit distance information exceeds the memory needed to store the raw sequence, ERGC discards the edit distance information and stores the raw sequence as an ASCII formatted text file. Again, to speed up the proposed algorithm we have used several techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: ERGC</head><p>Input: Reference sequence R, target sequence T, a threshold s, a set K of values for k. The set K contains two fixed values K ¼ f21; 9g. s is the memory needed to store the raw sequence of interest Output: Compressed sequence T C begin 1 Divide R and T into q equal parts. Let these be</p><formula>r 1 ; r 2 ;. .. ; r q and t 1 ; t 2 ;. . .</formula><p>; t q , respectively; 2 for i :¼ 1 to q do 3 Hash the k-mers (for a suitable value of k from K) of r i into a hash table H; 4 Generate one k-mer at a time from t i and hash it into H; 5 If there is no collision try different values of k from K and repeat lines 3 and 4; 6 If all the different k-mers have been tried with no collision, extend the length of r i and go to line 3; 7 When a collision occurs in H, align r i and t i with this common kmer as the anchor; 8 Extend the alignment beyond the common k-mer until there is a mismatch; 9 Record the matching length and the starting position of this match in the reference genome R; 10 Compute the edit distance between unmatched subsequences (one each from r i and t i ); 11 If the edit distance d i is !s, store the raw (unmatched) subsequence of t i ; 12 Otherwise store the edit script information; 13 Compress the stored information using delta encoding; 14 Encode the stored information using PPMD encoder; 15 Return the compressed sequence T C</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">An illustrative example</head><p>Let us illustrate our algorithm with a suitable example in brief. Please see<ref type="figure" target="#fig_0">Figure 1</ref>for visual details. Suppose ðr 0 ; t 0 Þ is the pair processed at some point in the algorithm. As described above, r 0 comes from the reference genomic sequence R, and t 0 comes from the target genomic sequence T. Let a 1 a 2. .. a jr 0 j and b 1 b 2. .. b jt 0 j be nucleotide positions in r 0 and t 0 , respectively. At first, we hash the k-mers (for a suitable value of k from K) of r 0 into a hash table H. In this procedure, similar k-mers fall into the same bucket in the hash table H. We record the position of occurrence of each k-mer. Next we generate one k-mer at a time from t 0 and hash it into H. Let a specific k 0-mer starting at position b p in t 0 collide with an entry of H. As a number of identical k 0-mers can be found across the genomic sequence, the bucket can have multiple positions of identical k 0-mers. Next we retrieve the k 0-mer from the bucket which has the least position among all the identical k 0-mers in the same bucket. Let the position be a i in r 0. The proposed method then aligns t 0 onto r 0 using b p and a i and extend the alignment beyond the common k 0-mers until a mismatch is found. While extending, the first mismatch occurs in a jþ1 and b qþ1 of r 0 and t 0 , respectively. So, b p. .. b q can be represented with respect to r 0 by recording the position of a i and the length of b p. .. b q. The same procedure is repeated again. The unaligned substring b qþ1. .. b rÀ1 is aligned with a jþ1. .. a kÀ1 by considering some heuristics. At this point, three cases are possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Case 1: mutations in r 0 and t 0</head><p>In this case, the unaligned substring of t 0 is aligned with r 0 employing edit distance calculations. In our example, this case arises while we attempt to align unmatched substring b qþ1. .. b rÀ1 onto a jþ1. .. a kÀ1. If the lengths of b qþ1. .. b rÀ1 and a jþ1. .. a kÀ1 are approximately equal and the memory needed to store the edit distance information between them is less than for the raw sequence b qþ1. .. b rÀ1 , we store b qþ1. .. b rÀ1 with respect to a jþ1. .. a kÀ1 by recording the starting position of a jþ1 and edit distance information. Otherwise we store the raw sequence b qþ1. .. b rÀ1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Case 2: insertions in r 0</head><p>In this case, nucleotides are inserted in positions a lþ1. .. a mÀ1 of r 0. We align b sþ1 onto a m and extend it until we find any mismatch using a similar procedure and record the positions of a m and jb sþ1 .. . b t j as described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Case 3: insertions in t 0</head><p>Case 3 arises when nucleotides are inserted in positions b tþ1. .. b uÀ1 of t 0. In this special case, we store the raw sequence b tþ1. .. b uÀ1 and the starting position of b tþ1 as there is no other choice left.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Time complexity analysis</head><p>Consider a pair of parts r and t (where r comes from the reference and t comes from the target). Let jrj ¼ jtj ¼ '. We can generate k-mers from r and hash them in Oð'kÞ time. The same amount of time is spent, in the worst case, to generate and hash the k-mers of t. The number of different k-values that we try is a small constant and hence the total time spent in all the hashing that we employ is Oð'kÞ. If a collision occurs, then the alignment we perform is greedy and takes only Oð'Þ time. After the alignment recording the difference and subsequent encoding also takes linear (in ') time. If no collision occurs for any of the k-values tried, t is stored as such and hence the time is linear in '. Put together, the run time for processing r and t is Oð'kÞ. Extending this analysis to the entire target sequence, we infer that the run time to compress any target sequence T of length n is O(nk) where k is the largest value used in hashing. It is easy to see that our algorithm can be implemented in a single pass through the data and thus can be employed in an out-of-core setting by using an appropriate value for the length of r and t. This can be ensured by choosing the length of r and t to be HðMÞ where M is the size of the core memory. The performance of any outof-core algorithm is measured in terms of the number of I/O operations performed. A single pass through the data refers to the number of I/O operations needed to bring each data item exactly once into the core memory. As a result, ERGC is optimal in terms of out-of-core computing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental environment</head><p>We have compared our algorithm with the best known algorithms existing in the referential genome compression domain. In this section, we summarize the results. All the experiments were done on an Intel Westmere compute node with 12 Intel Xeon X5650 Westmere cores and 48 GB of RAM. The operating system running was Red Hat Enterprise Linux Server release 5.7 (Tikanga). ERGC</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.. bq</head><p>is aligned onto a i. .. a j using greedy alignment algorithm, we only need to store the position of a i and the length of the matching alignment, i.e. ja i. .. a j j. The next alignment begins at a k and b r from r 0 and t 0 , respectively. The unmatched sequences in between are a jþ1. .. a kÀ1 and b qþ1. .. b rÀ1. b qþ1. .. b rÀ1 can be saved as a raw sequence or using edit scripting. In a similar fashion, the sequence information of br. .. bs ; bsþ1. .. bt and bu. .. bv are stored using a k. .. a l ; am. .. an and anþ1. .. ao , respectively. Since btþ1 .. . buÀ1 could not be aligned, it is stored as a raw sequence compression and decompression algorithms are written in standard Java programming language. Java source code is compiled and run by Java Virtual Machine (JVM) 1.6.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets</head><p>To measure the effectiveness of our proposed algorithm, we have done a number of experiment using real datasets. We have used hg18 release from the UCSC Genome Browser, the Korean genomes KOREF 20090131 (KOR131 for short) and KOREF 20090224 (KOR224 for short) (<ref type="bibr">Ahn et al., 2009</ref>) and the genome of a Han Chinese known as YH (<ref type="bibr" target="#b6">Levy et al., 2008</ref>). Since to compress a genomic sequence we need a reference genome, we have randomly chosen five pairs of target-reference sequences from the above benchmark datasets. We have taken chromosomes 1–22, X and Y chromosomes (i.e. a total of 24 chromosomes) for comparison purposes. Please see<ref type="figure" target="#tab_1">Table 1</ref>for details about the datasets we have used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>Next we present details on the performance evaluation of our proposed algorithm ERGC with respect to both compression and running time. We have compared ERGC with two of the three best performing algorithms namely GDC and iDoComp using several standard benchmark datasets. GReEn is another state-of-the-art algorithm existing in the literature. But we could not compare it with our algorithm, as the site containing the code was down at the time of experiments. GDC, GReEn and iDoComp are highly specialized algorithms designed to compress genomic sequences with the help of a reference genome. These are the best performing algorithms in this area as of now. Given a reference sequence, our algorithm compresses the target sequence by exploiting the reference. So, it needs the reference sequence at the time of decompression also. We use the target and reference pairs of sequences illustrated in<ref type="figure" target="#tab_1">Table 1</ref>sion ratios. But it may not be possible to find variation files for every species and these algorithms will not work without variation files. Our algorithm does not employ variation files and so it can compress any genomic sequence given a reference. As a result, we feel that algorithms that employ variation files form a separate class of algorithms and are not comparable to our algorithm. Again our proposed algorithm is devised in such a way that it is able to work with any alphabet used in the genomic sequence. Every other algorithm works only with valid alphabets intended for genomic sequence e.g. P ¼fA; a; C; c; G; g; T; t; N; ng. The characters most commonly seen in sequences are in P but there are several other valid characters that are used in clones to indicate ambiguity about the identity of certain bases in the sequence. It is not uncommon to see these 'wobble' codes at polymorphic positions in DNA sequences. It also differentiates between lower-case and upper-case letters. GDC, GReEn and iDoComp can differentiate between upper-case and lower-case letters specified in P but previous algorithms like GRS or RLZ-opt only work with A, C, G, T and N in the alphabet. iDoComp replaces the character in the genomic sequence that does not belong to P with N.</p><p>Specifically, ERGC will compress the target genome file regardless of the alphabets used and decompress the compressed file which is exactly identical to the target file. This is the case for GDC and iDoComp also but GReEn does not include the metadata information and output the sequence as a single line instead of multiple lines. Effectiveness of various algorithms including ERGC is measured using several performance metrics such as compression size, compression time, decompression time, etc. Gain measures the percentage improvement over the compression achieved by ERGC with respect to GDC and iDoComp. Comparison results are shown in<ref type="figure" target="#tab_2">Table 2</ref>. Clearly, our proposed algorithm is competitive and performs better than all the best known algorithms. In Tables 3 and 4, we show a comparison between compressed size (from different algorithms) and the actual size of individual chromosomes for some datasets. Memory consumption is also very low in our algorithm as it only processes one and only one part from the target and reference sequences at any time. Please note that we did not report the performance evaluation of GDC for every dataset, as it ran at least 1 h but did not able to compress a single chromosome for some datasets. As stated above, ERGC differentiates upper-case and lowercase characters. It compresses target file containing the genomic sequence to be compressed and metadata if any with the help of a reference. The decompression procedure produces exactly the same file as the input. It does not depend on the alphabets and is universal in this sense. Consider dataset D 1 where the target and reference sequences/chromosomes are from YH and hg18, respectively (<ref type="figure" target="#tab_1">Table 1</ref>). In this setting, GDC runs indefinitely. iDoComp<ref type="figure" target="#tab_2">Table 2</ref>for details). Now consider dataset D 2 where the target and reference sequences are from YH and KO224, respectively. The compressions achieved by GDC and iDoComp are roughly equal, whereas ERGC is about 3Â better than them. GDC's compression time is longer than both of iDoComp and ERGC, but it decompresses the sequences very quickly. ERGC's compression is approximately 2:5Â and 7:5Â faster than iDoComp and GDC, respectively. Next consider D 5. GDC runs indefinitely for this dataset. The percentage improvement ERGC achieves with respect to iDoComp is 90.73%. Specifically, ERGC takes 11 Â fewer disk space compared to iDoComp for this particular dataset. ERGC is also faster than iDoComp in terms of both compression (2Â) and decompression (6Â) times.<ref type="figure" target="#fig_3">Figure 2</ref>shows a comparative study of different algorithms including ERGC with respect to compression ratio, compression and decompression time. In brief, the minimum and maximum improvements observed from datasets D 1 À D 5 were 27.97% and 90.73% with respect to iDoComp, respectively. The minimum and maximum improvements over GDC observed were 57.9% and 75.24%, respectively. ERGC compresses at least 2:12Â and at most 5:21Â faster than iDoComp. Although it is better than iDoComp and GDC in compression time for every dataset, it is slower than GDC with respect to decompression for datasets D 2 À D 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Data compression is a very important problem in biology especially for NGS data. Five different NGS data compression problems have been identified and studied. In this article, we have presented a novel algorithm for one of these problems, namely, reference-based genome compression to effectively and efficiently compress genomic sequences. From the experimental results, it is evident that our algorithm indeed achieves compression ratios that are better than those of the currently best known algorithms. The compression time is also better than that of state-of-the-art algorithms in this domain. Although GDC is better than ERGC in terms of decompression time, the time ERGC takes to decompress the genomic sequences is also very promising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This research has been supported in part by the NIH grant R01-LM010101 and the NSF grant 1447711.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.1.</head><figDesc>Fig. 1. An illustrative example of ERGC. Here ðr 0 ; t 0 Þ is the pair processed at some point in the algorithm. As bp. .. bq is aligned onto a i. .. a j using greedy alignment algorithm, we only need to store the position of a i and the length of the matching alignment, i.e. ja i. .. a j j. The next alignment begins at a k and b r from r 0 and t 0 , respectively. The unmatched sequences in between are a jþ1. .. a kÀ1 and b qþ1. .. b rÀ1. b qþ1. .. b rÀ1 can be saved as a raw sequence or using edit scripting. In a similar fashion, the sequence information of br. .. bs ; bsþ1. .. bt and bu. .. bv are stored using a k. .. a l ; am. .. an and anþ1. .. ao , respectively. Since btþ1 ... buÀ1 could not be aligned, it is stored as a raw sequence</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>to assess the effectiveness of the algorithm. Some notable algorithms such as Pavlichin et al. (2013), Deorowicz et al. (2013) and Christley et al. (2009) exploit SNPs/indels variation files and achieve high compres</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><figDesc>Conflict of Interest: none declared. References Ahn,S.-M. et al. (2009) The first Korean genome sequence and analysis: full genome sequencing for a socio-ethnic group. Genome Res., 19, 1622–1629. Brandon,M.C. et al. (2009) Data structures and compression algorithms for genomic sequence data. Bioinformatics, 25, 1731–1738. Cao,M.D. et al. (2007) A simple statistical algorithm for biological sequence compression. In: Proceedings of the 2007 IEEE Data Compression Conference (DCC 07), IEEE, pp. 43–52.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.2.</head><figDesc>Fig. 2. Comparative statistics of compression ratios, compression and decompression times observed in our experiments. Compression ratio is the ratio of the uncompressed data size to the compressed data size. Compression and decompression times shown are in seconds</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>V C The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 3468 Bioinformatics, 31(21), 2015, 3468–3475 doi: 10.1093/bioinformatics/btv399 Advance Access Publication Date: 2 July 2015 Original Paper</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>Table 1.</figDesc><table>Genomic sequence datasets used for the referential com-
pression evaluation 

Dataset Species 
Chr. Sequence 
Taken from 

D 1 
Homo sapiens 24 
Target: YH 
yh.genomics.org.cn 
Reference: hg18 
ncbi.nlm.nih.gov 
D 2 
Homo sapiens 24 
Target: YH 
yh.genomics.org.cn 
Reference: KO224 koreangenome.org 
D 3 
Homo sapiens 24 
Target: YH 
yh.genomics.org.cn 
Reference: KO131 koreangenome.org 
D 4 
Homo sapiens 24 
Target: KO224 
koreangenome.org 
Reference: KO131 koreangenome.org 
D 5 
Homo sapiens 24 
Target: hg18 
ncbi.nlm.nih.gov 
Reference: KO131 koreangenome.org 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 2. Performance evaluation of different algorithms using various metrics</figDesc><table>GDC 
iDoComp 
ERGC 
Gain 

Dataset 
A.size 
R.size 
C.time 
D.time 
R.size 
C.time 
D.time 
R.size 
C.time 
D.time 
GDC 
iDoComp 

D 1 
2987 
NA 
NA 
NA 
65 708.47 
3157.00 
812.02 
7704.75 
606.12 
129.69 
NA 
88.27% 
D 2 
2987 
31 832.76 
5980.40 
40.70 
29 723.53 
2192.00 
268.97 
9016.41 
840.56 
124.15 
71.68% 
69.67% 
D 3 
2987 
37 154.19 
6785.40 
40.93 
33 131.26 
1857.00 
276.47 
9200.24 
875.30 
118.13 
75.24% 
72.23% 
D 4 
2938 
11 851.95 
4829.10 
50.09 
7043.82 
2534.00 
137.45 
5073.44 
624.48 
225.22 
57.19% 
27.97% 
D 5 
2996 
NA 
NA 
NA 
209 380.79 
3040.00 
953.06 
19 396.40 
988.12 
148.32 
NA 
90.73% </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><figDesc>Table 3. Chromosome-wise performance evaluation of different algorithms using various metrics on dataset D 2</figDesc><table>GDC 
iDoComp 
ERGC 

Chromosome 
A.size 
R.size 
C.time 
D.time 
R.size 
C.time 
D.time 
R.size 
C.time 
D.time 

C 1 
251 370 554 
2 391 974 
720.8 
3.19 
2 366 447 
200.00 
23.09 
700 785 
57.81 
8.78 
C 2 
247 000 341 
2 345 540 
755.90 
3.29 
1 822 538 
175.00 
14.62 
685 102 
49.02 
8.51 
C 3 
202 826 864 
1 266 255 
368.20 
2.72 
1 124 827 
164.00 
11.11 
612 271 
38.30 
8.18 
C 4 
194 460 954 
1 485 973 
281.50 
2.66 
1 305 098 
140.00 
11.12 
626 786 
44.44 
7.03 
C 5 
183 872 170 
1 575 619 
353.60 
2.45 
1 364 733 
117.00 
10.56 
545 591 
36.56 
7.41 
C 6 
173 748 332 
1 260 446 
327.00 
2.33 
1 087 335 
117.00 
9.76 
555 871 
38.97 
6.82 
C 7 
161 468 454 
1 893 681 
413.40 
2.14 
1 495 373 
108.00 
10.27 
512 645 
30.61 
6.79 
C 8 
148 712 746 
1 224 754 
198.50 
1.91 
1 056 722 
96.00 
8.93 
468 036 
30.25 
5.82 
C 9 
142 611 146 
2 258 374 
232.40 
1.76 
2 606 197 
82.00 
19.51 
421 259 
28.04 
5.66 
C 10 
137 630 990 
1 427 667 
304.40 
1.83 
1 295 147 
106.00 
9.18 
413 975 
28.16 
5.66 
C 11 
136 693 265 
1 127 511 
218.40 
1.79 
935 087 
96.00 
7.84 
418 994 
28.18 
5.91 
C 12 
134 555 367 
901 759 
223.20 
1.79 
760 267 
87.00 
6.99 
406 557 
27.36 
5.74 
C 13 
116 045 370 
682 679 
63.30 
1.40 
858 431 
86.00 
13.92 
302 716 
22.28 
4.61 
C 14 
108 141 402 
808 036 
79.10 
1.29 
931 770 
70.00 
13.78 
293 895 
19.69 
4.30 
C 15 
102 011 238 
1 130 386 
96.00 
1.17 
1 275 730 
61.00 
14.75 
251 420 
18.92 
4.31 
C 16 
90 307 716 
1 260 031 
175.20 
1.13 
1 157 411 
60.00 
10.08 
284 209 
17.95 
3.58 
C 17 
80 087 662 
870 827 
286.20 
1.10 
677 073 
51.00 
4.58 
245 816 
16.21 
3.43 
C 18 
77 385 780 
560 056 
55.00 
1.03 
447 788 
47.00 
4.27 
236 401 
16.15 
3.15 
C 19 
64 875 186 
546 966 
128.30 
0.84 
577 941 
39.00 
7.00 
276 688 
12.87 
2.66 
C 20 
63 476 571 
414 322 
50.20 
0.84 
384 499 
52.00 
4.38 
199 830 
12.92 
2.44 
C 21 
47 726 736 
318 750 
12.60 
0.50 
445 831 
46.00 
8.44 
141 432 
13.73 
2.10 
C 22 
50 519 630 
573 318 
28.00 
0.56 
685 738 
36.00 
9.88 
149 413 
9.13 
2.10 
C X 
157 495 656 
3 628 432 
584.60 
2.35 
2 824 069 
123.00 
12.76 
351 252 
107.16 
6.85 
C Y 
58 735 843 
2 643 399 
24.60 
0.63 
2 950 851 
33.00 
22.15 
131 863 
135.85 
2.32 </table></figure>

			<note place="foot">at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">S.Saha and S.Rajasekaran at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">Best values are shown in bold face. A.size and R.size refer to actual size in MB and reduced size in KB, respectively. C.time and D.time refer to the compression time and decompression time in seconds, respectively.</note>
		</body>
		<back>
			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Human genomes as email attachments</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Christley</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="274" to="275" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust relative compression of genomes with random access</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Deorowicz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Grabowski</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2979" to="2986" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Genome compression: a novel approach for large collections</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Deorowicz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient storage of high throughput DNA sequencing data using reference-based compression</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">H</forename>
				<surname>Fritz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">-Y</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="734" to="740" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Run-length encodings</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">W</forename>
				<surname>Golomb</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="399" to="401" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">A method for the construction of minimum-redundancy codes</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Huffman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Institute of Radio Engineers</title>
		<meeting>the Institute of Radio Engineers</meeting>
		<imprint>
			<date type="published" when="1952" />
			<biblScope unit="page" from="1098" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">The diploid genome sequence of an Asian individual</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Levy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">456</biblScope>
			<biblScope unit="page" from="60" to="66" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Implementing the PPM data compression scheme</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Moffat</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1917" to="1921" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">iDoComp: a compression scheme for assembled genomes</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Ochoa</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="626" to="633" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">The human genome contracts again</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Pavlichin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2199" to="2202" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Universal codeword sets and representations of the integers</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Peter</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="194" to="203" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">GReEn: a tool for efficient compression of genome resequencing data</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Pinho</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimized relative lempel-ziv compression of genomes</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Shanika</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th Australasian Computer Science Conference</title>
		<editor>Reynolds,M.</editor>
		<meeting><address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">A novel compression tool for efficient storage of genome resequencing data</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">45</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">A universal algorithm for sequential data compression</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Ziv</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Lempel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="337" to="343" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>