
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining Cross-validation under separate sampling: strong bias and how to correct it</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014">2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Ulisses</forename>
								<forename type="middle">M</forename>
								<surname>Braga-Neto</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Bioinformatics and Genomic Systems Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Amin</forename>
								<surname>Zollanvari</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<addrLine>College Station</addrLine>
									<postCode>77843</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Edward</forename>
								<forename type="middle">R</forename>
								<surname>Dougherty</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Bioinformatics and Genomic Systems Engineering</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining Cross-validation under separate sampling: strong bias and how to correct it</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">30</biblScope>
							<biblScope unit="issue">23</biblScope>
							<biblScope unit="page" from="3349" to="3355"/>
							<date type="published" when="2014">2014</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btu527</idno>
					<note type="submission">Received on December 30, 2013; revised on July 23, 2014; accepted on July 30, 2014</note>
					<note>BIOINFORMATICS ORIGINAL PAPER Associate Editor: Jonathan Wren with the Supplementary Materials, is available at: http://gsp.tamu. edu/Publications/supplementary/zollanvari13/. Contact: ulisses@ece.tamu.edu Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: It is commonly assumed in pattern recognition that cross-validation error estimation is &apos;almost unbiased&apos; as long as the number of folds is not too small. While this is true for random sampling, it is not true with separate sampling, where the populations are independently sampled, which is a common situation in bioinformatics. Results: We demonstrate, via analytical and numerical methods, that classical cross-validation can have strong bias under separate sampling , depending on the difference between the sampling ratios and the true population probabilities. We propose a new separate-sampling cross-validation error estimator, and prove that it satisfies an &apos;almost unbiased&apos; theorem similar to that of random-sampling cross-validation. We present two case studies with previously published data, which show that the results can change drastically if the correct form of cross-validation is used. Availability and implementation: The source code in C+ +, along</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The most important property of a classifier is its error rate (probability of misclassification) because the error rate quantifies the predictive capacity of the classifier. If the feature-label distribution is known, then the true error can be found exactly; however, in practice, the feature-label distribution is unknown and the error must be estimated. If the sample is small, then the estimation must be computed using the same data as that used for training the classifier. Perhaps the most commonly used training data-based classification error estimator is cross-validation. It has a long history going back to 1968 (<ref type="bibr" target="#b13">Lachenbruch and Mickey, 1968</ref>). In its most basic form, the k-fold cross-validation error estimate, ^ " cvðkÞ n , for a sample of size n (it is assumed that k divides n) is computed by selecting randomly a partition of the sample into k data 'folds' (subsets), for each fold applying the classification rule on the data not in the fold, computing the error rate of the designed classifier on the left-out fold, and then averaging the resulting k error rates. When k = n, one gets the leave-one-out estimator, ^ " l n. Cross-validation's salient good property is that, under random sampling, it can be proved (see<ref type="bibr" target="#b4">Devroye et al., 1996</ref>) that it is 'almost unbiased', in the sense that</p><formula>E½^ " cvðkÞ n =E½" nÀn=k ; ð1Þ</formula><p>where " n is the true error (probability of misclassification) of a classifier designed on a sample of size n. Hence, the bias is not too great as long as n/k is small. For leave-one-out, E½^ " l n =E½" nÀ1 , and the estimator is essentially unbiased. The salient point motivating the present article is that (1) depends on the sampling being random, and that when sampling is not random, there can be severe bias. The importance of bias for an arbitrary error estimator ^ " n can also be gleaned from its role in the estimator root-mean-square error:</p><formula>R M S ½^ " n =E½ð^ " n À " n Þ 2  1=2 = ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi Bias½^ " n  2 +Var dev ½^ " n  q ,</formula><p>where Bias½^ " n =E½^ " n À " n  and Var dev ½^ " n =Var½^ " n À " n  (Braga<ref type="bibr" target="#b2">Neto and Dougherty, 2004</ref>). As mentioned previously, for classical cross-validation under random sampling, it follows from</p><p>(1) that, if n/k is small, then Bias½^ " n  % 0, in which case RMS½^ " n  % Var 1=2 dev ½^ " n . While the variance of CV is known to be large in small-sample cases (Braga<ref type="bibr" target="#b2">Neto and Dougherty, 2004;</ref><ref type="bibr" target="#b9">Glick, 1973</ref>), it will typically reduce to zero as n ! 1 (<ref type="bibr" target="#b4">Devroye et al., 1996</ref>). However, the bias introduced by application of the classical CV estimator under nonrandom sampling will generally not approach zero as n ! 1. The result is an inconsistent estimator, which is imprecise under arbitrarily large sample sizes. Under random sampling, an independent and identically distributed (i.i.d.) sample S is drawn from the mixture of the populations Å 0 and Å 1. This means that if a sample of size n is drawn for binary classification, then the numbers of sample points n 0 and n 1 drawn from the populations Å 0 and Å 1 , respectively, are random variables n 0 $Binomialðn; cÞ and n 1 $ Binomialðn; 1 À cÞ, where c=PðY=0Þ is the a priori probability that the label Y is zero, i.e. the sample point comes from population Å 0. This random-sampling assumption is so pervasive that it is usually assumed without mention and in books is often stated at the outset and then forgotten. For instance,<ref type="bibr" target="#b6">Duda et al. (2000)</ref>state, 'In typical supervised pattern classification problems, the estimation of the prior probabilities presents no serious difficulties'. They are referring to the fact that the prior probability c=Pr ðY=0Þ can be consistently estimated by the sampling ratio, ^Numbers: n0 n ! c in probability. However, suppose the sampling is not random, in the sense that the ratios r= n0 n and 1 À r= n1 n are chosen before the sampling procedure. In this separate-sampling case, S=S 0 [ S 1 , where the sample points in S 0 and S 1 are selected randomly from Å 0 and Å 1 , but given n, the individual class counts n 0 and n 1 are not determined by the sampling procedure. With separate sampling, we have no sensible estimate of c. Recognition of this particular problem of estimating the prior probability when sampling is separate and its effect on linear discriminant analysis (LDA) goes back to 1951 (<ref type="bibr" target="#b1">Anderson, 1951</ref>). Often, one says that for separate sampling the ratios r= n0 n and 1 À r= n1 n are chosen 'prior to' the sampling procedure. But there is in fact no temporal meaning to this. For instance, one could simply separately randomly sample Å 0 and Å 1 with n 0 and n 1 being randomly selected by a process independent of the sampling procedure, and the sampling would still be separate. The point is that r cannot be reasonably used as an estimate of c.<ref type="figure" target="#fig_1">Figure 1</ref>(taken from Esfahani and Dougherty, 2014) illustrates the effects of separate sampling on the expected true classifier error for two classification rules and multivariate Gaussian distributions of equal and unequal covariance structures and dimensionality d = 3. For a given sample size n, sampling ratio r, and classification rule, the expected true error rate E½" n jr is plotted for different class prior probabilities c, for LDA and a non-linear radial basis function support vector machine (RBF-SVM). For each r and n, n 0 is determined as n 0 =dnre. We observe that the expected error is close to minimal when r = c and that it can greatly increase when r 6 ¼ c. This kind of poor performance for separate sampling ratios not close to c is commonplace (<ref type="bibr" target="#b7">Esfahani and Dougherty, 2014</ref>). In this article, we investigate the effect of separate sampling on cross-validation error estimation. We will see that for a separatesampling ratio r not close to c there can be large bias, optimistic or pessimistic. A serious consequence of this behavior can be ascertained by looking at<ref type="figure" target="#fig_1">Figure 1</ref>. Whereas the expected true error of the designed classifier grows large when r greatly deviates from c, a large optimistic cross-validation bias when r is far from c can obscure the large error and leave one with the illusion of good performance—and this illusion is not mitigated by large samples! To overcome the bias problem for classical crossvalidation with separate sampling, we introduce a new crossvalidation estimator designed for separate sampling and prove that it satisfies a bias property analogous to (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SYSTEMS AND METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Discriminant analysis</head><p>We treat classification via discriminants to facilitate demarcation of the individual contributions of the class-conditional distributions to the error analysis. A sample-based discriminant is defined as a (measurable) function W n : S ° &lt;, where from the definition, we see that we actually have a family of discriminants indexed by n. A discriminant W n defines a classification rule via É n ðSÞðXÞ= 1; W n ðS; XÞ 0 0; otherwise ;</p><formula>( ð2Þ</formula><p>where X comes from either Å 0 or Å 1. Because any classification rule É n can be expressed as a discriminant via W n ðS; XÞ=I ÉnðSÞðXÞ=0 À I ÉnðSÞðXÞ=1 , where I A is the indicator function, discriminant analysis is completely general. We assume a common sense property of discriminants, that the order of the sample points within a sample does not matter. With separate sampling, there are two separate samples S 0 =fX 1 ;. .. ; X n0 g and S 1 =fX n0+1 ;. .. ; X n0+n1 g from populations Å 0 and Å 1 , respectively. To demarcate the separatesampling case from the random-sampling case, we will write the discriminant and corresponding classifier by W n0;n1 ðS 0 ; S 1 ; XÞ and É n0;n1 ðS 0 ; S 1 ; XÞ, respectively, with the latter defined in the same manner as (2) with W n0;n1 ðS 0 ; S 1 ; XÞ replacing W n ðS; XÞ: The true classification error with random sampling is given by</p><formula>" n =c" 0 n +ð1 À cÞ" 1 n ; ð3Þ</formula><p>where</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Classical cross-validation error estimation</head><p>For U &amp; f1;. .. ; ng, let S ðUÞ denote the sample S with the points indexed by U deleted, and define W ðUÞ n ðS; XÞ=W nÀm ðS ðUÞ ; XÞ ; ð7Þ where jUj=m is the size of U. Now let k divide n and consider a (random) partition fU i ; i=1;. .. ; kg of f1;. .. ; ng. Then the classical k-fold cross-validation estimator is given by</p><formula>^ " cvðkÞ n = 1 n X k i=1 X q2Ui ðI W ðU i Þ n ðS;XqÞ 0 I Yq=0 +I W ðU i Þ n ðS;XqÞ40 I Yq=1 Þ : ð8Þ</formula><p>If k = n, this reduces to the leave-one-out estimator</p><formula>^ " l n = 1 n X n i=1 ðI W ðiÞ n ðS;XiÞ 0 I Yi=0 +I W ðiÞ n ðS;XiÞ40 I Yi=1 Þ; ð9Þ</formula><p>where we have omitted the braces around the singleton index set {i}. Using the classical definition of cross-validation, (1) does not hold with separate sampling, in general. To demonstrate this, let N 0 = X n i=1 I Yi=0 be the (random) number of points from population Å 0 in the sample S; the expected cross-validation error rate under separate sampling is E½^ " cvðkÞ n jN 0 =n 0 . For simplicity, we consider leave-one-out cross-validation. From (9),</p><formula>E½^ " l n jN 0 =n 0 = n 0 n PðW ð1Þ n ðS; X 1 Þ 0jY 1 =0; N 0 =n 0 Þ + n 1 n PðW ð1Þ n ðS; X 1 Þ40jY 1 =1; N 0 =n 0 Þ = n 0 n PðW n0À1;n1 ðS ð1Þ 0 ; S 1 ; XÞ 0jX 2 Å 0 Þ + n 1 n PðW n0;n1À1 ðS 0 ; S ðn0+1Þ 1 ; XÞ40jX 2 Å 1 Þ = n 0 n E½" 0 n0À1;n1  + n 1 n E½" 1 n0;n1À1  : ð10Þ</formula><p>On the other hand, it follows from</p><p>(3) that E½" nÀ1 jN 0 =n 0 =c E½" 0 nÀ1 jN 0 =n 0  +ð1 À cÞE½" 1 nÀ1 jN 0 =n 0  =c E½" 0 n0;n1À1 +ð1 À cÞE½" 1 n0;n1À1  : ð11Þ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cross-validation for separate sampling</head><p>To adapt cross-validation to separate sampling, let U &amp; f1;. .. ; n 0 g, let V &amp; fn 0 +1;. .. ; n 0 +n 1 g, let S ðUÞ 0 and S ðVÞ 1 denote the samples S 0 and S 1 , with the points indexed by U and V deleted, respectively, and defineIf c is known (or known to a high degree of accuracy), then one can use it in (14). If c is unknown, then there is no proper cross-validation estimator of the overall error rate. If k 0 =n 0 and k 1 =n 1 , then the ðk 0 ; k 1 Þ-fold cross-validation estimators defined previously reduce to separate-sampling leaveone-out estimators:Therefore, from</p><formula>= 1 n 0 k 1 X k0 i=1 X k1 j=1 X r2Ui I W ðU i ;V j Þ n 0 ;n 1 ðS0;S1;XrÞ 0 ; ^ " cvðk0;k1Þ;1 n0;n1 = 1 n 1 k 0 X k0 i=1 X k1 j=1 X r2Vj I W ðU i ;V j Þ n 0 ;n 1 ðS0;S1;XrÞ40 : ð13Þ</formula><formula>^ " l;0 n0;n1 = 1 n 0 n 1 X n0 i=1 X n1 j=1 I W</formula><p>(13),In the case of the separate-sampling leave-one-out estimator defined in (16), the preceding theorem reduces to E½^ " l n0;n1 =E½" n0À1;n1À1  : ð19Þ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Simulation study with synthetic and real data</head><p>We have performed a set of experiments using both synthetic models and real data to examine the behavior of classical and separate-sampling cross-validation under separate sampling. Throughout we use 5-fold cross-validation. We consider four well-known classification rules: LDA, Quadratic Discriminant Analysis (QDA), Linear Support Vector Machine (L-SVM) and RBF-SVM (see the Supplementary Material for definitions of these classification rules). To generate synthetic data, we use a model with class-conditional 3-dimensional Gaussian distributions, Nðl y ; S y Þ, y = 0, 1, where l 0 =½0; 0;. .. ; 0; 0; l 1 =½0; 0;. .. ; 0;  and S y has 2 on the diagonal and y off the diagonal. The pair ð 0 ; 1 Þ can take on the values (0.8, 0.8) or (0.8, 0.4). We set so that the Mahalanobis distance between the classes for equal covariance matrices and the Bhattacharyya distance between the classes for unequal covariance matrices is 3. We consider n = 80 and n = 1000, so that we can compare small-sample and largesample results. We consider four public microarray real datasets: pediatric acute lymphoblastic leukemia (ALL;<ref type="bibr" target="#b16">Yeoh et al., 2002</ref>), acute myeloid leukemia (AML;<ref type="bibr" target="#b14">Valk et al., 2004</ref>), multiple myeloma (<ref type="bibr" target="#b17">Zhan et al., 2006</ref>) and breast cancer (<ref type="bibr" target="#b3">Desmedt et al., 2007</ref>).<ref type="figure" target="#tab_1">Table 1</ref>provides a summary of these real datasets, including the total number of features and sample size. For a detailed description of the data preparation, the readers are referred to the Supplementary Materials. The experiments on real data are essentially similar to those on synthetic data except that in real data experiments we use t-test feature selection to reduce the dimensionality to d = 3. In real data experiments, we consider only n = 80, which allows sufficient data for holdout error estimation. All experiments are performed for a range of r= n0 n 2 ½0:15; 0:85. We fix n and determine n 0 according to n 0 =dnre. At each iteration, S 0 and S 1 are randomly picked from either a synthetic model or real data to train the classifier and compute the two cross-validation estimates. Finding the bias requires knowing the true error, which is estimated on 5000 independent sample points from the synthetic distributions, or held out points in the case of real data; however, owing to separate sampling the ordinary holdout method cannot be applied, and we use separate-sampling holdout as explained by Esfahani and Dougherty (2014). We consider c=0:001; 0:1; 0:3; 0:4; 0:5; 0:6; 0:7; 0:9; 0:999. For each classification rule, we repeat the process of obtaining the true error and its estimates 4000 times for each value of r and c to obtain a distribution of estimates and true errors from which to compute the bias. In<ref type="figure">Figure 2</ref>, we provide the results for the synthetic data with unequal covariance matrices [ð 0 ; 1 Þ=ð0:8; 0:4Þ] for n = 80, 1000 and for two of the real datasets (<ref type="bibr" target="#b3">Desmedt et al., 2007;</ref><ref type="bibr" target="#b14">Valk et al., 2004</ref>). The complete set of results is given in the Supplementary Material. In the figure, from left to right, the columns correspond to LDA, QDA, L-SVM and RBF-SVM, respectively. The top two rows of the figure correspond to the real data from<ref type="bibr" target="#b3">Desmedt et al. (2007) and</ref><ref type="bibr" target="#b14">Valk et al. (2004)</ref>, and the third and fourth rows correspond to the synthetic data with n = 80 and n = 1000. The x-axis corresponds to the sampling ratio r, the y-axis gives the bias, the solid lines are for the proposed separate-sampling cross-validation, the dashed lines are for classical cross-validation, and the colors code the value of c. The trends are consistent across all experiments (including those in the Supplementary Material): (i) for classical crossvalidation with c near 0.5, there is significant optimistic bias for large jr – cj; (ii) for classical cross-validation with small or large c, there is optimistic bias for large jr – cj and pessimistic bias for small jr – cj as long as jr À cj is not very close to 0; (iii) for separatesampling cross-validation, estimation is slightly optimistic and almost unbiased across the range of jr – cj. Combined with the results of Esfahani and Dougherty (2014), the bias behavior of classical cross-validation is especially harmful for large jr – cj because it masks the increase in classifier error that occurs for large jr – cj, as shown in<ref type="figure" target="#fig_1">Figure 1</ref>. Furthermore, although the deviation variance of classical cross-validation can be mitigated by large samples, the bias issue generally remains just as bad for large samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Two case studies</head><p>To further illustrate the effects of separate sampling on classical cross-validation bias, we consider two published studies. The first (<ref type="bibr" target="#b0">Ambroise and McLachlan, 2002</ref>) uses a colon microarray dataset containing gene-expression measurements taken from 2000 genes for 62 tissue specimens, 40 tumorous tissues (class 0) and 22 normal tissues (class 1). Using the SVM-RFE classification rule (<ref type="bibr" target="#b10">Guyon et al., 2002</ref>), the authors split the data into a training and a test set, each including 31 specimens, by sampling without replacement, such that the training data contain 20 tumorous and 11 normal specimens. They compare the 10-fold cross-validation error using (8) to the standard holdout estimate obtained by counting the errors on the test set. But the standard holdout estimate is unbiased under random sampling, not separate sampling. For the latter, holdout estimation must take into account the value of c to be unbiased (<ref type="bibr" target="#b7">Esfahani and Dougherty, 2014</ref>). Assuming the classifier is applied to the US population, based on the incidence rate of colorectal cancer among the US population, which is 40/100 000 (<ref type="bibr" target="#b11">Haggar and Boushey, 2009</ref>), c=40=100 000. The black solid and dotted curves in<ref type="figure" target="#fig_5">Figure 3</ref>error, while the proposed cross-validation scheme is almost unbiased. In the second case study, we use the Parkinson's dataset used by<ref type="bibr" target="#b12">Kaya et al. (2011)</ref>. This dataset contains 22 biomedical voice features and 195 measurements in which 48 belong to individuals with Parkinson (class 0) and 147 measurements are taken from healthy individuals (class 1), so that r=0:246. The authors use this dataset to construct classifiers for diagnosis of Parkinson's disease based on distorted voice features. Four classifiers are constructed: naive Bayes (NB;<ref type="bibr" target="#b8">Friedman et al., 1997</ref>), C4.5 (<ref type="bibr" target="#b5">Dietterich, 2000</ref>), kNN (k = 5) (<ref type="bibr" target="#b4">Devroye et al., 1996</ref>) and RBF-SVM. Although<ref type="bibr" target="#b12">Kaya et al. (2011)</ref>have reported the estimated classical cross-validation error on a single sample of the data, we repeat the sampling procedure 200 times to get an estimate of the expected cross-validation error using both the classical (8) and the corrected cross-validation scheme (14). We assume the prior probability c of Parkinson's disease is determined by the incidence rate of Parkinson's disease across the USA, which is 13.4/100 000 (Van Den<ref type="bibr" target="#b15">Eeden et al., 2003</ref>). In<ref type="figure" target="#fig_6">Figure 4</ref>, the white bars are the expected classical cross-validation error rates; the shaded bars are the estimated error rates using the separate-sampling cross-validation scheme. The bars show the averaged estimated error obtained on 200 samplings of the data. The behavior observed in<ref type="figure">Figure 2</ref>makes it plausible that the error estimates for classical cross-validation will exceed those of separate-sampling cross-validation, which is nearly unbiased. This is true in all cases except for NB. However, if we look carefully at<ref type="figure">Figure 2</ref>, we see that the point at which the bias becomes optimistic (for increasing r) can be well left of 0.5. This point is affected by the covariance structure and the classification rule. In this case, for NB, it is to the left of 0.246.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.1.</head><figDesc>Fig. 1. Expected true error, E½" n jr as a function of r for LDA (left column) and an RBF-SVM classifier (right column) using synthetic data. Top row: n=n 0 +n 1 =80 and equal covariance matrices; bottom row: n=n 0 +n 1 =80 and unequal covariance matrices</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>W</head><figDesc>ðU;VÞ n0;n1 ðS 0 ; S 1 ; XÞ=W n0Àm;n1Àl ðS ðUÞ ; S ðVÞ ; XÞ : ð12Þ where jUj = m and jV j = l are the sizes of U and V, respectively. Now let k 0 divide n 0 and k 1 divide n 1 , and consider (random) partitions fU i ; i=1;. .. ; k 0 g of f1;. .. ; n 0 g and fV i ; i=1;. .. ; k 1 g of fn 0 +1;. .. ; n 0 +n 1 g. Separate-sampling ðk 0 ; k 1 Þ-fold cross-validation estimators are defined by ^ " cvðk0;k1Þ;0 n0;n1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Again</head><figDesc>, in the absence of knowledge of c, no proper estimator of the overall error rate is possible. We now show that a version of (1) holds for the separatesampling cross-validation estimator. THEOREM. The cross-validation estimator in (14) satisfies; n 0 +n 1 g, with jUj = m and jV j = l, then W ðU;VÞ n0;n1 ðS 0 ; S 1 ; X i Þ $ W n0Àm;n1Àl ðS 0 ; S 1 ; XÞjX 2 Å 0 ; i 2 U; W ðU;VÞ n0;n1 ðS 0 ; S 1 ; X i Þ $ W n0Àm;n1Àl ðS 0 ; S 1 ; XÞjX 2 Å 1 ; i 2 V:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><figDesc>resemble the curves plotted in Figure 1 of Ambroise and McLachlan (2002). The gray solid and dashed curves are obtained by considering the cross-validation scheme (14) and computing the true error from (3). The error bars refer to the 95% confidence interval. All curves show the averaged error and estimated error obtained on 200 random splits of the data as mentioned above. These curves show that regardless of the number of genes considered in the classifier, using the classical cross-validation (8) induces $13% optimistic bias with respect to the true</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.3.</head><figDesc>Fig. 3. The black solid and dotted curves resemble the expected true error and cross-validation error rates reported in Figure 1 of Ambroise and McLachlan (2002). The gray solid and dotted curves are the expected true error and estimated error by using cross-validation scheme (14) when the prior probability of colon cancer is set to be the incidence rate across the USA. All curves show the averaged error and estimated error obtained on 200 random splits of the data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig.4.</head><figDesc>Fig. 4. The white bars are the expected classical cross-validation error rates on the Parkinson's dataset used by Kaya et al., (2011) for four classifiers. The shaded bars are the estimated error rates by using crossvalidation scheme (14) when the prior probability of Parkinson's disease is set to be the incidence rate across the USA. The bars show the averaged estimated error obtained on 200 samplings of the data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><figDesc>Table 1. Microarray studies used in this study</figDesc><table>Dataset 
Description 
Features 
n 0 /n 1 

(Desmedt et al., 2007) 
Breast cancer 
22 215 
98/77 
(Yeoh et al., 2002) 
Pediatric ALL 
5077 
149/99 
(Valk et al., 2004) 
AML 
22 215 
116/157 
(Zhan et al., 2006) 
Multiple myeloma 
54 613 
156/78 </table></figure>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">U.M.Braga-Neto et al. at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">Cross-validation under separate sampling at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="4"> CONCLUDING REMARKS We show in this article that classical cross-validation may display substantial bias when it is applied in the separate sampling scenario, which is common in biomedical studies. If one wishes to use cross-validation with separate sampling, then one should use the separate-sampling version of cross-validation, which is proposed here, or else, significant bias may result. This means that one must know the prior probability c (at least a good approximation of it). A similar requirement was made by Esfahani and Dougherty (2014) to ensure proper performance of the classification rule. Using a sampling ratio significantly different from c will result in poor classifier design and, often, optimistic bias to obscure the poor design. As concluded by Esfahani and Dougherty (2014), given the ubiquity of separate sampling in biomedicine, although it would incur some cost, it would behoove the medical community to gather population statistics so that accurate estimates of prior class probabilities would be available. In the absence of such statistics, separate sampling should not be used. Funding: This work was supported by NSF award CCF-0845407 (Braga-Neto) and NIH grant 2R25CA090301 (Nutrition, Biostatistics and Bioinformatics) from the National Cancer Institute. Conflict of interest: none declared.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Selection bias in gene extraction on the basis of microarray gene-expression data</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Ambroise</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">J</forename>
				<surname>Mclachlan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="6562" to="6566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Classification by multivariate analysis</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Anderson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="31" to="50" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Is cross-validation valid for microarray classification?</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="374" to="380" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Strong time dependence of the 76-gene prognostic signature for node-negative breast cancer patients in the transbig multicenter independent validation series</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Desmedt</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Cancer Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="3207" to="3214" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<monogr>
		<title level="m" type="main">A Probabilistic Theory of Pattern Recognition</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Devroye</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">An experimental comparison of three methods for constructing ensembles of decision trees: bagging, boosting, and randomization</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">G</forename>
				<surname>Dietterich</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="139" to="157" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<monogr>
		<title level="m" type="main">Pattern Classification</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">O</forename>
				<surname>Duda</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Effect of separate sampling on classification accuracy</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">S</forename>
				<surname>Esfahani</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="242" to="250" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Bayesian network classifiers</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Friedman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="131" to="163" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Sample-based multinomial classification</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Glick</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="241" to="256" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Gene selection for cancer classification using support vector machines in machine learning</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Guyon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="389" to="422" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Colorectal cancer epidemiology: incidence, mortality, survival, and risk factors</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">A</forename>
				<surname>Haggar</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">P</forename>
				<surname>Boushey</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Colon. Rectal. Surg</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="191" to="197" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Effect of discretization method on the diagnosis of parkinsons disease</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Kaya</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Innov. Comput. Inf</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4669" to="4678" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimation of error rates in discriminant analysis</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">A</forename>
				<surname>Lachenbruch</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">R</forename>
				<surname>Mickey</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Prognostically useful gene-expression profiles in acute myeloid leukemi</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">J</forename>
				<surname>Valk</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="page" from="1617" to="1628" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Incidence of parkinson&apos;s disease: variation by age, gender, and race/ethnicity</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">K</forename>
				<surname>Van Den Eeden</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Epidemiol</title>
		<imprint>
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="page" from="1015" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Classification, subtype discovery, and prediction of outcome in pediatric acute lymphoblastic leukemia by gene expression profiling</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">J</forename>
				<surname>Yeoh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Cell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="133" to="143" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">The molecular classification of multiple myeloma</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Zhan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Blood</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="2020" to="2028" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<monogr>
		<title level="m" type="main">Cross-validation under separate sampling</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>