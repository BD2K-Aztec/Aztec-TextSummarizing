
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Genetics and population analysis Reliable ABC model choice via random forests</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Pierre</forename>
								<surname>Pudlo</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Montpellier</orgName>
								<orgName type="institution" key="instit2">IMAG</orgName>
								<address>
									<settlement>Montpellier</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institut de Biologie Computationnelle (IBC)</orgName>
								<address>
									<settlement>Montpellier</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Jean-Michel</forename>
								<surname>Marin</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Montpellier</orgName>
								<orgName type="institution" key="instit2">IMAG</orgName>
								<address>
									<settlement>Montpellier</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institut de Biologie Computationnelle (IBC)</orgName>
								<address>
									<settlement>Montpellier</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Arnaud</forename>
								<surname>Estoup</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Institut de Biologie Computationnelle (IBC)</orgName>
								<address>
									<settlement>Montpellier</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">CBGP</orgName>
								<orgName type="institution">INRA</orgName>
								<address>
									<settlement>Montpellier</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Jean-Marie</forename>
								<surname>Cornuet</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="laboratory">CBGP</orgName>
								<orgName type="institution">INRA</orgName>
								<address>
									<settlement>Montpellier</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Mathieu</forename>
								<surname>Gautier</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Institut de Biologie Computationnelle (IBC)</orgName>
								<address>
									<settlement>Montpellier</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">CBGP</orgName>
								<orgName type="institution">INRA</orgName>
								<address>
									<settlement>Montpellier</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Christian</forename>
								<forename type="middle">P</forename>
								<surname>Robert</surname>
							</persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Université Paris Dauphine</orgName>
								<orgName type="institution" key="instit2">CEREMADE</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<settlement>Coventry</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Genetics and population analysis Reliable ABC model choice via random forests</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btv684</idno>
					<note type="submission">Received on May 29, 2015; revised on September 2, 2015; accepted on September 30, 2015</note>
					<note>*To whom correspondence should be addressed. † The authors wish it to be known that, in their opinion, the first two authors should be regarded as Joint First Authors. Associate Editor: Inanc Birol Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Approximate Bayesian computation (ABC) methods provide an elaborate approach to Bayesian inference on complex models, including model choice. Both theoretical arguments and simulation experiments indicate, however, that model posterior probabilities may be poorly evaluated by standard ABC techniques. Results: We propose a novel approach based on a machine learning tool named random forests (RF) to conduct selection among the highly complex models covered by ABC algorithms. We thus modify the way Bayesian model selection is both understood and operated, in that we rephrase the inferential goal as a classification problem, first predicting the model that best fits the data with RF and postponing the approximation of the posterior probability of the selected model for a second stage also relying on RF. Compared with earlier implementations of ABC model choice, the ABC RF approach offers several potential improvements: (i) it often has a larger discriminative power among the competing models, (ii) it is more robust against the number and choice of statistics summarizing the data, (iii) the computing effort is drastically reduced (with a gain in computation efficiency of at least 50) and (iv) it includes an approximation of the posterior probability of the selected model. The call to RF will undoubtedly extend the range of size of datasets and complexity of models that ABC can handle. We illustrate the power of this novel methodology by analyzing controlled experiments as well as genuine population genetics datasets. Availability and implementation: The proposed methodology is implemented in the R package abcrf available on the CRAN.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Approximate Bayesian computation (ABC) represents an elaborate statistical approach to model-based inference in a Bayesian setting in which model likelihoods are difficult to calculate (due to the complexity of the models considered). Since its introduction in population genetics (<ref type="bibr" target="#b5">Beaumont et al., 2002;</ref><ref type="bibr" target="#b29">Pritchard et al., 1999;</ref><ref type="bibr" target="#b35">Tavaré et al., 1997</ref>), the method has found an ever increasing range of applications covering diverse types of complex models in various scientific fields (see, e.g.<ref type="bibr" target="#b1">Arenas et al., 2015;</ref><ref type="bibr" target="#b3">Beaumont, 2008</ref><ref type="bibr" target="#b4">Beaumont, , 2010</ref><ref type="bibr" target="#b12">Chan et al., 2014;</ref><ref type="bibr" target="#b17">Csillèry et al., 2010;</ref><ref type="bibr" target="#b37">Theunert et al., 2012;</ref><ref type="bibr" target="#b38">Toni et al., 2009</ref>). The principle of ABC is to conduct Bayesian inference on a dataset through comparisons with numerous simulated datasets. However, it suffers from two major difficulties. First, to ensure reliability of the method, the number of simulations is large; hence, it proves difficult to apply ABC for large datasets (e.g. in population genomics where tens tohundred thousand markers are commonly genotyped). Second, calibration has always been a critical step in ABC implementation (<ref type="bibr" target="#b10">Blum et al., 2013;</ref><ref type="bibr" target="#b26">Marin et al., 2012</ref>). More specifically, the major feature in this calibration process involves selecting a vector of summary statistics that quantifies the difference between the observed data and the simulated data. The construction of this vector is therefore paramount and examples abound about poor performances of ABC model choice algorithms related with specific choices of those statistics (<ref type="bibr" target="#b19">Didelot et al., 2011;</ref><ref type="bibr" target="#b27">Marin et al., 2014;</ref><ref type="bibr" target="#b31">Robert et al., 2011</ref>), even though there also are instances of successful implementations. We advocate a drastic modification in the way ABC model selection is conducted: we propose both to step away from selecting the most probable model from estimated posterior probabilities and to reconsider the very problem of constructing efficient summary statistics. First, given an arbitrary pool of available statistics, we now completely bypass selecting among those. This new perspective directly proceeds from machine learning methodology. Second, we postpone the approximation of model posterior probabilities to a second stage, as we deem the standard numerical ABC approximations of such probabilities fundamentally untrustworthy. We instead advocate selecting the posterior most probable model by constructing a (machine learning) classifier from simulations from the prior predictive distribution (or other distributions in more advanced versions of ABC), known as the ABC reference table. The statistical technique of random forests (RF) (<ref type="bibr" target="#b11">Breiman, 2001</ref>) represents a trustworthy machine learning tool well adapted to complex settings as is typical for ABC treatments. Once the classifier is constructed and applied to the actual data, an approximation of the posterior probability of the resulting model can be produced through a secondary RF that regresses the selection error over the available summary statistics. We show here how RF improves upon existing classification methods in significantly reducing both the classification error and the computational expense. After presenting theoretical arguments, we illustrate the power of the ABC-RF methodology by analyzing controlled experiments as well as genuine population genetics datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and methods</head><p>Bayesian model choice (<ref type="bibr" target="#b6">Berger, 1985;</ref><ref type="bibr" target="#b30">Robert, 2001</ref>) compares the fit of M models to an observed dataset x 0. It relies on a hierarchical modelling, setting first prior probabilities pðmÞ on model indices m 2 f1;. .. ; Mg and then prior distributions pðhjmÞ on the parameter h of each model, characterized by a likelihood function f ðxjm; hÞ. Inferences and decisions are based on the posterior probabilities of each model pðmjx 0 Þ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">ABC algorithms for model choice</head><p>While we cannot cover in much detail the principles of ABC, let us recall here that ABC was introduced in<ref type="bibr" target="#b35">Tavaré et al. (1997) and</ref><ref type="bibr" target="#b29">Pritchard et al. (1999)</ref>for solving intractable likelihood issues in population genetics. The reader is referred to, e.g.<ref type="bibr" target="#b3">Beaumont (2008</ref><ref type="bibr" target="#b4">Beaumont ( , 2010</ref>),<ref type="bibr" target="#b38">Toni et al. (2009</ref><ref type="bibr" target="#b17">), Csillèry et al. (2010</ref><ref type="bibr" target="#b26">) and Marin et al. (2012</ref>for thorough reviews on this approximation method. The fundamental principle at work in ABC is that the value of the intractable likelihood function f ðx 0 jhÞ at the observed data x 0 and for a current parameter h can be evaluated by the proximity between x 0 and pseudo-data xðhÞ simulated from f ðxjhÞ. In discrete settings, the indicator IðxðhÞ ¼ x 0 Þ is an unbiased estimator of f ðx 0 jhÞ (<ref type="bibr" target="#b32">Rubin, 1984</ref>). For realistic settings, the equality constraint is replaced with a tolerance region IðdðxðhÞ; x 0 Þ Þ, where dðx 0 ; xÞ is a measure of divergence between the two vectors and &gt; 0 is a tolerance value. The implementation of this principle is straightforward: the ABC algorithm produces a large number of pairs ðh; xÞ from the prior predictive, a collection called the reference table, and extracts from the table the pairs ðh; xÞ for which dðxðhÞ; x 0 Þ. To approximate posterior probabilities of competing models, ABC methods (<ref type="bibr" target="#b23">Grelaud et al., 2009</ref>) compare observed data with a massive collection of pseudo-data, generated from the prior predictive distribution in the most standard versions of ABC; the comparison proceeds via a normalized Euclidean distance on a vector of statistics SðxÞ computed for both observed and simulated data. Standard ABC estimates posterior probabilities pðmjx 0 Þ at stage (B) of Algorithm 1 below as the frequencies of those models within the k nearest-to-x 0 simulations, proximity being defined by the distance between Sðx 0 Þ and the simulated SðxÞ's. Selecting a model means choosing the model with the highest frequency in the sample of size k produced by ABC, such frequencies being approximations to posterior probabilities of models. We stress that this solution means resorting to a k-nearest neighbor (k-nn) estimate of those probabilities, for a set of simulations drawn at stage (A), whose records constitute the so-called reference(A) Generate a reference table including N ref simulations ðm; SðxÞÞ from pðmÞpðhjmÞf ðxjm; hÞ (B) Learn from this set to infer about m at s 0 ¼ Sðx 0 Þ Selecting a set of summary statistics S(x) that are informative for model choice is an important issue. The ABC approximation to the posterior probabilities pðmjx 0 Þ will eventually produce a right ordering of the fit of competing models to the observed data and thus select the right model for a specific class of statistics on large datasets (<ref type="bibr" target="#b27">Marin et al., 2014</ref>). This most recent theoretical ABC model choice results indeed shows that some statistics produce nonsensical decisions and that there exist sufficient conditions for statistics to produce consistent model prediction, albeit at the cost of an information loss due to summaries that may be substantial. The toy example comparing MA(1) and MA(2) models in Supplementary Informations and<ref type="figure" target="#fig_4">Figure 1</ref>clearly exhibits this potential loss in using only the first two autocorrelations as summary statistics.<ref type="bibr" target="#b2">Barnes et al. (2012)</ref>developed an interesting methodology to select the summary statistics but with the requirement to aggregate estimation and model pseudo-sufficient statistics for all models. This induces a deeply inefficient dimension inflation and can be very time consuming. It may seem tempting to collect the largest possible number of summary statistics to capture more information from the data. This brings pðmjSðx 0 ÞÞ closer to pðmjx 0 Þ but increases the dimension of SðxÞ. ABC algorithms, like k-nn and other local methods suffer from the curse of dimensionality [see e.g. Section 2.5 in<ref type="bibr" target="#b24">Hastie et al. (2009)</ref>] so that the estimate of pðmjSðx 0 ÞÞ based on the simulations is poor when the dimension of SðxÞ is too large. Selecting summary statistics correctly and sparsely is therefore paramount, as shown by the literature in the recent years.<ref type="bibr">[See Blum et al. (2013)</ref>surveying ABC parameter estimation.] For ABC model choice, two main projection techniques have been considered so far.<ref type="bibr">First, Prangle et al. (2014)</ref>show that the Bayes factor itself is an acceptable summary (of dimension one) when comparing two models, but its practical evaluation via a pilot ABC simulation induces a poor approximation of model evidences (<ref type="bibr" target="#b19">Didelot et al., 2011;</ref><ref type="bibr" target="#b31">Robert et al., 2011</ref>). The recourse to a regression layer like linear discriminant analysis (LDA,<ref type="bibr" target="#b20">Estoup et al., 2012</ref>) is discussed below and in Supplementary Section S1. Other projection techniques have been proposed in the context of parameter estimation: see, e.g. Fearnhead and Prangle (2012);<ref type="bibr" target="#b0">Aeschbacher et al. (2012)</ref>. Given the fundamental difficulty in producing reliable tools for model choice based on summary statistics (<ref type="bibr" target="#b31">Robert et al., 2011</ref>), we now propose to switch to a different approach based on an adapted classification method. We recall in the next section the most important features of the RF algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">RF methodology</head><p>The classification and regression trees (CART) algorithm at the core of the RF scheme produces a binary tree that sets allocation rules for entries as labels of the internal nodes and classification or predictions of Y as values of the tips (terminal nodes). At a given internal node, the binary rule compares a selected covariate X j with a bound t, with a left-hand branch rising from that vertex defined by X j &lt; t. Predicting the value of Y given the covariate X implies following a path from the tree root that is driven by applying these binary rules. The outcome of the prediction is the value found at the final leaf reached at the end of the path: majority rule for classification and average for regression. To find the best split and the best variable at each node of the tree, we minimize a criterium: for classification, the Gini index and, for regression, the L 2-loss. In the randomized version of the CART algorithm (see Supplementary Algorithm S1), only a random subset of covariates of size n try is considered at each node of the tree. The RF algorithm (<ref type="bibr" target="#b11">Breiman, 2001</ref>) consists in bagging (which stands for bootstrap aggregating) randomized CART. It produces N tree randomized CART trained on samples or sub-samples of size N boot produced by bootstrapping the original training database. Each tree provides a classification or a regression rule that returns a class or a prediction. Then, for classification we use the majority vote across all trees in the forest, and, for regression, the response values are averaged. Three tuning parameters need be calibrated: the number N tree of trees in the forest, the number n try of covariates that are sampled at a given node of the randomized CART and the size N boot of the bootstrap sub-sample. This point will be discussed in Section 3.4. For classification, a very useful indicator is the out-of-bag error (<ref type="bibr">Hastie et al., 2009, Chapter 15</ref>). Without any recourse to a test set, it gives some idea on how good is your RF classifier. For each element of the training set, we can define the out-of-bag classifier: the aggregation of votes over the trees not constructed using this element. The out-of-bag error is the error rate of the out-of-bag classifier on the training set. The out-of-bag error estimate is as accurate as using a test set of the same size as the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">ABC model choice via RF</head><p>The above-mentioned difficulties in ABC model choice drives us to a paradigm shift in the practice of model choice, namely to rely on a classification algorithm for model selection, rather than a poorly estimated vector of pðmjSðx 0 ÞÞ probabilities. As shown in the example described in Section 3.1, the standard ABC approximations to posterior probabilities can significantly differ from the true pðmjx 0 Þ. Indeed, our version of stage (B) in Algorithm 1 relies on a RF classifier whose goal is to predict the suited model ^ mðsÞ at each possible value s of the summary statistics SðxÞ. The RF is trained on the simulations produced by stage (A) of Algorithm 1, which constitute the reference table. Once the model is selected as m ? , we opt to approximate pðm Ã jSðx 0 ÞÞ by another RF, obtained from regressing the probability of error on the (same) covariates, as explained below. A practical way to evaluate the performance of an ABC model choice algorithm (test a given set of summary statistics and a given classifier) is to check whether it provides a better answer than others. The aim is to come near the so-called Bayesian classifier, which, for the observed x, selects the model having the largest posterior probability pðmjxÞ. It is well known that the Bayesian classifier minimizes the 0–1 integrated loss or error (<ref type="bibr" target="#b18">Devroye et al., 1996</ref>). In the ABC framework, we call the integrated loss (or risk) the prior error rate, since it provides an indication of the global quality of a given classifier ^ m on the entire space weighted by the prior. This rate is the expected value of the misclassification error over the hierarchical priorIt can be evaluated from simulations ðh; m; SðyÞÞ drawn as in stage (A) of Algorithm 1, independently of the reference table (<ref type="bibr" target="#b34">Stoehr et al., 2015</ref>), or with the out-of-bag error in RF that, as explained above, requires no further simulation. Both classifiers and sets of summary statistics can be compared via this error scale: the pair that minimizes the prior error rate achieves the best approximation of the ideal Bayesian classifier. In that sense, it stands closest to the decision we would take were we able to compute the true pðmjxÞ. We seek a classifier in stage (B) of Algorithm 1 that can handle an arbitrary number of statistics and extract the maximal information from the reference table obtained at stage (A). As introduced above, RF classifiers (<ref type="bibr" target="#b11">Breiman, 2001</ref>) are perfectly suited for that purpose. The way we build both a RF classifier given a collection of statistical models and an associated RF regression function for predicting the allocation error is to start from a simulated ABC reference table made of a set of simulation records made of model indices and summary statistics for the associated simulated data. This table then serves as training database for a RF that forecasts model index based on the summary statistics. The resulting algorithm, presented in Algorithm 2 and called ABC-RF, is implemented in the R package abcrf associated with this article.The justification for choosing RF to conduct an ABC model selection is that, both formally (<ref type="bibr" target="#b8">Biau, 2012;</ref><ref type="bibr" target="#b33">Scornet et al., 2015</ref>) and experimentally (<ref type="bibr">Hastie et al., 2009, Chapter 5</ref>), RF classification was shown to be mostly insensitive both to strong correlations Reliable ABC model choicebetween predictors (here the summary statistics) and to the presence of noisy variables, even in relatively large numbers, a characteristic that k-nn classifiers lack. This type of robustness justifies adopting a RF strategy to learn from an ABC reference table for Bayesian model selection. Within an arbitrary (and arbitrarily large) collection of summary statistics, some may exhibit strong correlations and others may be uninformative about the model index, with no terminal consequences on the RF performances. For model selection, RF thus competes with both local classifiers commonly implemented within ABC: It provides a more non-parametric modelling than local logistic regression (<ref type="bibr" target="#b3">Beaumont, 2008</ref>), which is implemented in the DIYABC software (<ref type="bibr" target="#b16">Cornuet et al., 2014</ref>) but is extremely costly—see the method of<ref type="bibr" target="#b20">Estoup et al. (2012)</ref>to reduce the dimension using linear discriminant projection before resorting to local logistic regression. This software also includes a standard k-nn selection procedure [i.e. the socalled direct approach in<ref type="bibr" target="#b14">Cornuet et al. (2008)</ref>] which suffers from the curse of dimensionality and thus forces selection among statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Approximating the posterior probability of the selected model</head><p>The outcome of RF computation applied to a given target dataset is a classification vote for each model which represents the number of times a model is selected in a forest of n trees. The model with the highest classification vote corresponds to the model best suited to the target dataset. It is worth stressing here that there is no direct connection between the frequencies of the model allocations of the data among the tree classifiers (i.e. the classification vote) and the posterior probabilities of the competing models. Machine learning classifiers hence miss a distinct advantage of posterior probabilities, namely that the latter evaluate a confidence degree in the selected model. An alternative to those probabilities is the prior error rate. Aside from its use to select the best classifier and set of summary statistics, this indicator remains, however, poorly relevant since the only point of importance in the data space is the observed dataset Sðx 0 Þ. A first step addressing this issue is to obtain error rates conditional on the data as in<ref type="bibr" target="#b34">Stoehr et al. (2015)</ref>. However, the statistical methodology considered therein suffers from the curse of dimensionality and we here consider a different approach to precisely estimate this error. We recall (<ref type="bibr" target="#b30">Robert, 2001</ref>) that the posterior probability of a model is the natural Bayesian uncertainty quantification since it is the complement of the posterior error associated with the loss Ið ^ mðSðx 0 ÞÞ 6 ¼ mÞ. While the proposal of<ref type="bibr" target="#b34">Stoehr et al. (2015)</ref>for estimating the conditional error rate induced a classifier given S ¼ Sðx 0 Þ Pð ^ mðSðYÞÞ 6 ¼ mjSðYÞ ¼ Sðx 0 ÞÞ ;</p><formula>(1)</formula><p>involves non-parametric kernel regression, we suggest to rely instead on a RF regression to undertake this estimation. The curse of dimensionality is then felt much less acutely, given that RF can accommodate large dimensional summary statistics. Furthermore, the inclusion of many summary statistics does not induce a reduced efficiency in the RF predictors, while practically compensating for insufficiency. Before describing in more details the implementation of this concept, we stress that the perspective of<ref type="bibr" target="#b34">Stoehr et al. (2015)</ref>leads to effective estimates of the posterior probability that the selected model is the true model, thus providing us with a non-parametric estimation of this quantity. Indeed, the posterior expectation (1) satisfies</p><formula>E½Ið ^ mðs 0 Þ 6 ¼ mÞjSðx 0 Þ ¼ X k i¼1 E½Ið ^ mðSðx 0 ÞÞ 6 ¼ m ¼ iÞjSðx 0 Þ ¼ X k i¼1 P½m ¼ iÞjSðx 0 Þ Â Ið ^ mðSðx 0 ÞÞ 6 ¼ iÞ ¼ P½m 6 ¼ ^ mðSðx 0 ÞÞjSðx 0 Þ ¼ 1 À P½m ¼ ^ mðSðx 0 ÞÞjSðx 0 Þ :</formula><p>It therefore provides the complement of the posterior probability that the true model is the selected model. To produce our estimate of the posterior probability P½m ¼ ^ mðSðx 0 ÞÞjSðx 0 Þ, we proceed as follows:</p><p>1. We compute the value of Ið ^ mðsÞ 6 ¼ mÞ for the trained RF ^ m and for all terms in the ABC reference table; to avoid overfitting, we use the out-of-bag classifiers; 2. We train a RF regression estimating the variate Ið ^ mðsÞ 6 ¼ mÞ as a function of the same set of summary statistics, based on the same reference table. This second RF can be represented as a function. ðsÞ that constitutes a machine learning estimate of P½m 6 ¼ ^ mðsÞjs; 3. We apply this RF function to the actual observations summarized as Sðx 0 Þ and return 1 À .ðSðx 0 ÞÞ as our estimate of</p><formula>P½m ¼ ^ mðSðx 0 ÞÞjSðx 0 Þ.</formula><p>This corresponds to the representation of Algorithm 3 which is implemented in the R package abcrf associated with this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results: illustrations of the ABC-RF methodology</head><p>To illustrate the power of the ABC-RF methodology, we now report several controlled experiments as well as two genuine population genetic examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Insights from controlled experiments</head><p>The Supplementary Information details controlled experiments on a toy problem, comparing MA(1) and MA(2) time-series models, and two controlled synthetic examples from population genetics, based on single-nucleotide polymorphism (SNP) and microsatellite data. The toy example is particularly revealing with regard to the discrepancy between the posterior probability of a model and the version conditioning on the summary statistics Sðx 0 Þ.<ref type="figure" target="#fig_4">Figure 1</ref>shows how far from the diagonal are realizations of the pairs ðpðmjx 0 Þ; pðmjSðx 0 ÞÞÞ, even though the autocorrelation statistic is quite informative (<ref type="bibr" target="#b26">Marin et al., 2012</ref>). Note in particular the vertical accumulation of points near Pðm ¼ 2jx 0 Þ ¼ 1. Supplementary<ref type="figure" target="#tab_1">Table  S1</ref>demonstrates the further gap in predictive power for the full Bayes solution with a true error rate of 12% versus the best solution (RF) based on the summaries barely achieving a 16% error rate.</p><p>For both controlled genetics experiments in the Supplementary Information, the computation of the true posterior probabilities of the three models is impossible. The predictive performances of the competing classifiers can nonetheless be compared on a test sample. Results, summarized in Supplementary Tables S2 and S3 in the Supplementary Information, legitimize the use of RF, as this method achieves the most efficient classification in all genetic experiments. Note that that the prior error rate of any classifier is always bounded from below by the error rate associated with the (ideal) Bayesian classifier. Therefore, a mere gain of a few percents may well constitute an important improvement when the prior error rate is low. As an aside, we also stress that, since the prior error rate is an expectation over the entire sampling space, the reported gain may exhibit much better performances over some areas of this space. Supplementary<ref type="figure" target="#fig_5">Figure S2</ref>displays differences between the true posterior probability of the model selected by Algorithm 2 and its approximation with Algorithm 3. Moreover, we found that the values of the votes provided by Algorithm 2 is only useful to assess the model that best fits the data but that any conclusion regarding level of confidence necessitates the computation of the posterior probability of the selected model provided by Algorithm 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Microsatellite dataset: retracing the invasion routes of the Harlequin ladybird</head><p>The original challenge was to conduct inference about the introduction pathway of the invasive Harlequin ladybird (<ref type="bibr" target="#b25">Lombaert et al. (2011)]</ref>. We now compare our results from the ABC-RF algorithm with other classification methods for three sizes of the reference table and with the original solutions by<ref type="bibr" target="#b25">Lombaert et al. (2011) and</ref><ref type="bibr" target="#b20">Estoup et al. (2012)</ref>. We included all summary statistics computed by the DIYABC software for microsatellite markers (<ref type="bibr" target="#b16">Cornuet et al., 2014</ref>), namely 130 statistics, complemented by the nine LDA axes as additional summary statistics (see Supplementary Section S4). In this example, discriminating among models based on the observation of summary statistics is difficult. The overlapping groups of Supplementary<ref type="figure">Figure S8</ref>reflect that difficulty, the source of which is the relatively low information carried by the 18 autosomal microsatellite loci considered here. Prior error rates of learning methods on the whole reference table are given in<ref type="figure" target="#tab_1">Table 1</ref>. As expected in such a high dimension settings (<ref type="bibr" target="#b24">Hastie et al., 2009</ref>, Section 2.5), k-nn classifiers behind the standard ABC methods are all defeated by RF for the three sizes of the reference table, even when k-nn is trained on the much smaller set of covariates composed of the nine LDA axes. The classifier and set of summary statistics showing the lowest prior error rate is RF trained on the 130 summaries and the nine LDA axes. Supplementary<ref type="figure">Figure S9</ref>shows that RFs are able to automatically determine the (most) relevant statistics for model comparison, including in particular some crude estimates of admixture rate defined in<ref type="bibr" target="#b13">Choisy et al. (2004)</ref>, some of them not selected by the experts in<ref type="bibr" target="#b25">Lombaert et al. (2011)</ref>. We stress here that the level of information of the summary statistics displayed in Supplementary<ref type="figure">Figure  S9</ref>is relevant for model choice but not for parameter estimation issues. In other words, the set of best summaries found with ABCRF should not be considered as an optimal set for further parameter estimations under a given model with standard ABC techniques (<ref type="bibr" target="#b5">Beaumont et al., 2002</ref>). The evolutionary scenario selected by our RF strategy agrees with the earlier conclusion of<ref type="bibr" target="#b25">Lombaert et al. (2011)</ref>, based on approximations of posterior probabilities with local logistic regression solely on the LDA axes, i.e. the same scenario displays the highest ABC posterior probability and the largest number of selection among the decisions taken by the aggregated trees of RF. Using Algorithm 3, we got an estimate of the posterior probability of the selected scenario equal to 0.4624. This estimate is significantly lower than the one of about 0.6 given in<ref type="bibr" target="#b25">Lombaert et al. (2011)</ref>based on a local logistic regression method. This new value is more credible because it is based on all the summary statistics and, on a method adapted to such an high dimensional context and less sensitive to calibration issues. Moreover, this small posterior probability corresponds better to the intuition of the experimenters and indicates that new experiments are necessary to give a more reliable answer (e.g. the genotyping of a larger number of loci).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SNP dataset: inference about human population history</head><p>Because the ABC-RF algorithm performs well with a substantially lower number of simulations compared to standard ABC methods, it is expected to be of particular interest for the statistical processing of massive SNP datasets, whose production is on the increase in the field of population genetics. We analyze here a dataset including 50 000 SNP markers genotyped in four Human populations (<ref type="bibr">The 1000</ref><ref type="bibr">Genomes Project Consortium, 2012</ref>). The four populations include Yoruba (Africa), Han (East Asia), British (Europe) and American individuals of African ancestry, respectively. Our intention is not to bring new insights into Human population history, which has been and is still studied in greater details in research using genetic data but to illustrate the potential of ABC-RF in this context. We compared six scenarios (i.e. models) of evolution of the four Human populations which differ from each other by one ancient and one recent historical events: (i) a single out-of-AfricaReliable ABC model choicecolonization event giving an ancestral out-of-Africa population which secondarily split into one European and one East Asian population lineages, versus two independent out-of-Africa colonization events, one giving the European lineage and the other one giving the East Asian lineage; (ii) the possibility of a recent genetic admixture of Americans of African origin with their African ancestors and individuals of European or East Asia origins. The SNP dataset and the compared scenarios are further detailed in the Supplementary Information. We used all the summary statistics provided by DIYABC for SNP markers (<ref type="bibr" target="#b16">Cornuet et al., 2014</ref>), namely 112 statistics in this setting complemented by the five LDA axes as additional statistics. To discriminate between the six scenarios of Supplementary<ref type="figure" target="#fig_4">Figure S12</ref>, RF and other classifiers have been trained on three refer ence tables of different sizes. The estimated prior error rates are reported in<ref type="figure" target="#tab_2">Table 2</ref>. Unlike the previous example, the information carried here by the 50 000 SNP markers is much higher, because it induces better separated simulations on the LDA axes (<ref type="figure" target="#fig_5">Fig. 2</ref>) and much lower prior error rates (<ref type="figure" target="#tab_2">Table 2</ref>). RF using both the initial summaries and the LDA axes provides the best results. The ABC-RF algorithm selects Scenario 2 as the predicted scenario on the Human dataset, an answer which is not visually obvious on the LDA projections of<ref type="figure" target="#fig_5">Figure 2</ref>in which Scenario 2 corresponds to the blue color. But considering previous population genetics studies in the field, it is not surprising that this scenario, which includes a single out-of-Africa colonization event giving an ancestral out-ofAfrica population with a secondarily split into one European and one East Asian population lineage and a recent genetic admixture of Americans of African origin with their African ancestors and European individuals, was selected. Using Algorithm 3, we got an estimate of the posterior probability of scenario 2 equal to 0.998, corresponding to a high level of confidence in choosing scenario 2. Computation time is a particularly important issue in the present example. Simulating the 10 000 SNP datasets used to train the classification methods requires 7 h on a computer with 32 processors (Intel Xeon(R) CPU 2 GHz). In that context, it is worth stressing that RF trained on the DIYABC summaries and the LDA axes of a 10 000 reference table has a smaller prior error rate than all other classifiers, even when they are trained on a 50 000 reference table. In practice, standard ABC treatments for model choice are based on reference tables of substantially larger sizes [i.e. 10 5 to 10 6 simulations per scenario (<ref type="bibr" target="#b7">Bertorelle et al., 2010;</ref><ref type="bibr" target="#b20">Estoup et al., 2012)]</ref>. For the above setting in which six scenarios are compared, standard ABC treatments would hence request a minimum computation time of 17 days (using the same computation resources). According to the comparative tests that we carried out on various example datasets, we found that RF globally allowed a minimum computation speed gain around a factor of 50 in comparison to standard ABC treatments: see also Supplementary Section S4 for other considerations regarding computation speed gain.Note. Performances of classifiers used in stage (B) of Algorithm 1. A set of 10 000 prior simulations was used to calibrate the number of neighbors k in both standard ABC and local logistic regression. Prior error rates are estimated as average misclassification errors on an independent set of 10 000 prior simulations, constant over methods and sizes of the reference tables. N ref corresponds to the number of simulations included in the reference table.</p><formula>0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 p(m=2|x) p(m=2|S(x))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Practical recommendations regarding the implementation of the algorithms</head><p>We develop here several points, formalized as questions, which should help users seeking to apply our methodology on their dataset for statistical model choice.</p><p>Are my models and/or associated priors compatible with the observed dataset? This question is of prime interest and applies to any type of ABC treatment, including both standard ABC treatments and treatments based on ABC RF. Basically, if none of the proposed model-prior combinations produces some simulated datasets in a reasonable vicinity of the observed dataset, it is a signal of incompatibility and we consider it is then useless to attempt model choice inference. In such situations, we strongly advise reformulating the compared models and/or the associated prior distributions to achieve some compatibility in the above sense. We propose here a visual way to address this issue, namely through the simultaneous projection of the simulated reference table datasets and of the observed dataset on the first LDA axes. Such a graphical assessment can be achieved using the R package abcrf associated with this paper. In the LDA projection, the observed dataset need be located reasonably within the clouds of simulated datasets (see<ref type="figure" target="#fig_5">Fig. 2</ref>Did I simulate enough datasets for my reference table? A rule of thumb is to simulate between 5000 and 10 000 datasets per model among those compared. For instance, in the example dealing with Human population history (Section 3.3), we have simulated a total of 50 000 datasets from six models (i.e. about 8300 datasets per model). To evaluate whether or not this number is sufficient for RF analysis, we recommend to compute global prior error rates from both the entire reference table and a subset of the reference table (for instance from a subset of 40 000 simulated datasets if the reference table includes a total of 50 000 simulated datasets). If the prior error rate value obtained from the subset of the reference table is similar, or only lightly higher, than the value obtained from the entire reference table, one can consider that the reference table contains enough simulated datasets. If a substantial difference is observed between both values, then we recommend an increase in the number of datasets in the reference table. For instance, in the Human population history example, we obtained prior error rate values of 4.22% and 4.18% when computed from a subset of 40 000 simulated datasets and the entire 50 000 datasets of the reference table, respectively. In this case, the benefit of producing more simulated dataset in the reference table seems negligible.</p><p>Did my forest grow enough trees? According to our experience, a forest made of 500 trees usually constitutes an interesting trade-off between computation efficiency and statistical precision (<ref type="bibr" target="#b11">Breiman, 2001</ref>). To evaluate whether or not this number is sufficient, we recommend to plot the estimated values of the prior error rate and/or the posterior probability of the best model as a function of the number of trees in the forest. The shapes of the curves provide a visual diagnostic of whether such key quantities stabilize when the number of trees tends to 500. We provide illustrations of such visual representations in the case of the example dealing with Human population history in<ref type="figure" target="#fig_2">Figure 3</ref>. Such a graphical assessment can be achieved using the R package abcrf associated with this paper</p><p>How do I set N boot and n try for classification and regression? For a reference table with up to 100 000 datasets and 250 summary statistics, we recommend keeping the entire reference table, that is, N boot ¼ N when building the trees. For larger reference tables, the value of N boot can be calibrated against the prior error rate, starting with a value of N boot ¼ 50 000 and doubling it until the estimated prior error rate is stabilized. For the number n try of summary statistics sampled at each of the nodes, we see no reason to modify the default number of covariates n try which is chosen as ffiffiffi d p for classification and d=3 for regression when d is the total number of predictors (<ref type="bibr" target="#b11">Breiman, 2001</ref>). Finally, when the number of summary statistics is lower than 15, one might reduce N boot to N=10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>This article is purposely focused on selecting a statistical model, which can be rephrased as a classification problem trained on ABC simulations. We defend here the paradigm shift of assessing the best fitting model via a RF classification and in evaluating our confidence in the selected model by a secondary RF procedure, resulting in a different approach to precisely estimate the posterior probability of the selected model. We further provide a calibrating principle for this approach, in that the prior error rate provides a rational way to select the classifier and the set of summary statistics which leads to results closer to a true Bayesian analysis. Compared with past ABC implementations, ABC-RF offers improvements at least at four levels: (i) on all experiments we studied, it has a lower prior error rate; (ii) it is robust to the size and choice of summary statistics, as RF can handle many superfluous statistics with no impact on the performance rates (which mostly depend on the intrinsic dimension of the classification problem (<ref type="bibr" target="#b8">Biau, 2012;</ref><ref type="bibr" target="#b33">Scornet et al., 2015</ref>), a characteristic confirmed by our results); (iii) the computing effort is considerably reduced as RF requires a much smaller reference table compared with alternatives (i.e. a few thousands versus hundred thousands to billions of simulations) and (iv) the method is associated with an embedded and error-freeReliable ABC model choiceevaluation which assesses the reliability of ABC-RF analysis. As a consequence, ABC-RF allows for a more robust handling of the degree of uncertainty in the choice between models, possibly in contrast with earlier and over-optimistic assessments. Because of a massive gain in computing and simulation efforts, ABC-RF will extend the range and complexity of datasets (e.g. number of markers in population genetics) and models handled by ABC. In particular, we believe that ABC-RF will be of considerable interest for the statistical processing of massive SNP datasets whose production rapidly increases within the field of population genetics for both model and non-model organisms. Once a given model has been chosen and confidence evaluated by ABC-RF, it becomes possible to estimate parameter distribution under this (single) model using standard ABC techniques (<ref type="bibr" target="#b5">Beaumont et al., 2002</ref>) or alternative methods such as those proposed by<ref type="bibr" target="#b21">Excoffier et al. (2013).</ref></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><figDesc>Algorithm 2: ABC-RF (A) Generate a reference table including N ref simulation ðm; SðxÞÞ from pðmÞpðhjmÞf ðxjm; hÞ (B) Construct N tree randomized CART which predict m using SðxÞ for b ¼ 1 to N tree do draw a bootstrap (sub-)sample of size N boot from the reference table grow a randomized CART T b (Supplementary Algorithm S1) end for (C) Determine the predicted indexes for Sðx 0 Þ and the trees fT b ; b ¼ 1;. .. ; N tree g (D) Sðx 0 Þ according to a majority vote among the predicted indexes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm3:</head><figDesc>Estimating the posterior probability of the selected model (a) Use the RF produced by Algorithm 2 to compute the outof-bag classifiers of all terms in the reference table and deduce the associated binary model prediction error (b) Use the reference table to build a RF regression function .ðsÞ regressing the model prediction error on the summary statistics (c) Return the value of 1 À .ðSðx 0 ÞÞ as the RF regression estimate of P½m ¼ ^ mðSðx 0 ÞÞjSðx 0 Þ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><figDesc>Harmonia axyridis) for the first recorded outbreak of this species in eastern North America. The dataset, first analyzed in Lombaert et al. (2011) and Estoup et al. (2012) via ABC, includes samples from three natural and two biocontrol populations genotyped at 18 microsatellite markers. The model selection requires the formalization and comparison of 10 complex competing scenarios corresponding to various possible routes of introduction [see Supplementary Information for details and analysis 1 in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.1.</head><figDesc>Fig. 1. Illustration of the discrepancy between posterior probabilities based on the whole data and based on a summary. The aim is to choose between two nested time series models, namely moving averages of order 1 and 2 [denoted MA(1) and MA(2), respectively; see Supplementary Information for more details]. Each point of the plot gives two posterior probabilities of MA(2) for a dataset simulated either from the MA(1) (blue) or MA(2) model (orange), based on the whole data (x-axis) and on only the first two autocorrelations (y-axis)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.2.</head><figDesc>Fig. 2. Human SNP data: projection of the reference table on the first four LDA axes. Colors correspond to model indices. The location of the additional datasets is indicated by a large black star</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig.3.</head><figDesc>Fig. 3. Human SNP data: evolution of the ABC-RF prior error rate when N ref ¼ 50 000 with respect to the number of trees in the forest</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>V C The Author 2015. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 859 Bioinformatics, 32(6), 2016, 859–866 doi: 10.1093/bioinformatics/btv684 Advance Access Publication Date: 20 November 2015 Original Paper</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>table, see Biau et al. (2015) or Stoehr et al. (2015). Algorithm 1. ABC model choice algorithm</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><figDesc>Table 1. Harlequin ladybird data: estimated prior error rates for various classification methods and sizes of the reference table</figDesc><table>Classification method 
trained on 

Prior error rates (%) 

N ref ¼ 10 000 N ref ¼ 20 000 N ref ¼ 50 000 

LDA 
39.91 
39.30 
39.04 
Standard ABC (k-nn) on 
DIYABC summaries 

57.46 
53.76 
51.03 

Standard ABC (k-nn) on 
LDA axes 

39.18 
38.46 
37.91 

Local logistic regression 
on LDA axes 

41.04 
37.08 
36.05 

RF on DIYABC 
summaries 

40.18 
38.94 
37.63 

RF on DIYABC summa-
ries and LDA axes 

36.86 
35.62 
34.44 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>Table 2.</figDesc><table>Human SNP data: estimated prior error rates for classifica-
tion methods and three sizes of reference table 

Classification method 
trained on 

Prior error rates (%) 

N ref ¼ 10 000 N ref ¼ 20 000 N ref ¼ 50 000 

LDA 
9.91 
9.97 
10.03 
Standard ABC (k-nn) 
using DYIABC 
summaries 

23.18 
20.55 
17.76 

Standard ABC (k-nn) 
using only LDA axes 

6.29 
5.76 
5.70 

Local logistic regression 
on LDA axes 

6.85 
6.42 
6.07 

RF using DYIABC initial 
summaries 

8.84 
7.32 
6.34 

RF using both DIYABC 
summaries and LDA 
axes 

5.01 
4.66 
4.18 

Note. Same comments as in Table 1. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>as an illustration). Note that visual representations of a similar type (although based on PCA) as well as computation for each summary statistics and for each model of the probabilities of the observed values in the prior distributions have been proposed by Cornuet et al. (2010) and are already automatically provided by the DIYABC software.</figDesc><table></table></figure>

			<note place="foot">at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">P.Pudlo et al. at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors are grateful to the referees for their supportive and constructive comments throughout the editorial process. The use of random forests was suggested to J.-M.M. and C.P.R. by Bin Yu during a visit at CREST, Paris. We are grateful to Gérard Biau for his help about the asymptotics of random forests. Some parts of the research were conducted at BIRS, Banff, Canada, and the authors (P.P. and C.P.R.) took advantage of this congenial research environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This research was partly supported by the ERA-Net BiodivERsA2013-48 (EXOTIC), with the national funders FRB, ANR, 25 MEDDE, BELSPO, PTDLR and DFG, part of the 2012–2013 BiodivERsA call for research proposals. This work was also supported by the Labex NUMEV. Conflict of Interest: none declared.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">A novel approach for choosing summary statistics in approximate Bayesian computation</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Aeschbacher</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetics</title>
		<imprint>
			<biblScope unit="volume">192</biblScope>
			<biblScope unit="page" from="1027" to="1047" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">CodABC: a computational framework to coestimate recombination, substitution, and molecular adaptation rates by approximate Bayesian computation</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Arenas</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Biol. Evol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1109" to="1112" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Considerate approaches to constructing summary statistics for ABC model selection</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Barnes</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Comput</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1181" to="1197" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint determination of topology, divergence time and immigration in population trees</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Beaumont</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simulations, Genetics and Human Prehistory. McDonald Institute Monographs, McDonald Institute for Archaeological Research</title>
		<editor>Matsumura,S. et al.</editor>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="134" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Approximate Bayesian computation in evolution and ecology</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Beaumont</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Ecol. Evol. Syst</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="379" to="406" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Approximate Bayesian computation in population genetics</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Beaumont</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetics</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="2025" to="2035" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<monogr>
		<title level="m" type="main">Statistical Decision Theory and Bayesian Analysis, second edition</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Berger</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">ABC as a flexible framework to estimate demography over space and time: some cons, many pros</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Bertorelle</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Ecol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2609" to="2625" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Analysis of a random forest model</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Biau</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1063" to="1095" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<monogr>
		<title level="m" type="main">New insights into approximate Bayesian computation. Annales de l&apos;Institut Henri Poincaré B Probability Stat</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Biau</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="376" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">A comparative review of dimension reduction methods in approximate Bayesian computation</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Blum</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Sci</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="189" to="208" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<monogr>
		<title level="m" type="main">Random forests. Machine Learn</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Breiman</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Detecting concerted demographic response across community assemblages using hierarchical approximate Bayesian computation</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Chan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Biol. Evol</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2501" to="2515" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimating admixture proportions with microsatellites: comparison of methods based on simulated data</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Choisy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Ecol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="955" to="968" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Inferring population history with DIY ABC: a user-friendly approach to approximate Bayesian computation</title>
		<author>
			<persName>
				<forename type="first">J.-M</forename>
				<surname>Cornuet</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2713" to="2719" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">Inference on population history and model checking using DNA sequence and microsatellite data with the software DIYABC (v1.0)</title>
		<author>
			<persName>
				<forename type="first">J.-M</forename>
				<surname>Cornuet</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">DIYABC v2.0: a software to make approximate Bayesian computation inferences about population history using single nucleotide polymorphism, DNA sequence and microsatellite data</title>
		<author>
			<persName>
				<forename type="first">J.-M</forename>
				<surname>Cornuet</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1187" to="1189" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Approximate Bayesian computation (ABC) in practice</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Csillèry</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Ecol. Evol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="410" to="418" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">A Probabilistic Theory of Pattern Recognition</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Devroye</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applications of Mathematics</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="1996" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Likelihood-free estimation of model evidence</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Didelot</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Anal</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="48" to="76" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Estimation of demo-genetic model probabilities with approximate Bayesian computation using linear discriminant analysis on summary statistics</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Estoup</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Ecol. Resour</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="846" to="855" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust demographic inference from genomic and SNP data</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Excoffier</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Genet</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">1003905</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Constructing summary statistics for approximate Bayesian computation: semi-automatic approximate Bayesian computation</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Fearnhead</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Prangle</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B (Stat. Methodol</title>
		<imprint>
			<biblScope unit="page" from="74" to="419" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Likelihood-free methods for model choice in Gibbs random fields</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Grelaud</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Anal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="427" to="442" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<monogr>
		<title level="m" type="main">The Elements of Statistical Learning. Data Mining, Inference, and Prediction, second edition</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hastie</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Inferring the origin of populations introduced from a genetically structured native range by approximate Bayesian computation: case study of the invasive ladybird Harmonia axyridis</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Lombaert</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Ecol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="4654" to="4670" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Approximate Bayesian computational methods</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Marin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Comput</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1167" to="1180" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<analytic>
		<title level="a" type="main">Relevant statistics for Bayesian model choice</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Marin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B (Stat. Methodol.)</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="833" to="859" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-automatic selection of summary statistics for ABC model choice</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Prangle</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Appl. Genet. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="67" to="82" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Population growth of human Y chrom osomes: a study of Y chromosome microsatellites</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Pritchard</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Biol. Evol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1791" to="1798" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<monogr>
		<title level="m" type="main">The Bayesian Choice, second edition</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Robert</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Lack of confidence in ABC model choice</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Robert</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl Acad. Sci. USA</title>
		<meeting>. Natl Acad. Sci. USA</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="15112" to="15117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">Bayesianly justifiable and relevant frequency calculations for the applied statistician</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Rubin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1151" to="1172" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Consistency of random forests</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Scornet</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1716" to="1741" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptive ABC model choice and geometric summary statistics for hidden Gibbs random fields</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Stoehr</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Comput</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="129" to="141" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">Inferring coalescence times from DNA sequence data</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Tavaré</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetics</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="505" to="518" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<analytic>
		<title level="a" type="main">An integrated map of genetic variation from 1 092 human genomes</title>
	</analytic>
	<monogr>
		<title level="m">The 1000 Genomes Project Consortium</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="56" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b37">
	<analytic>
		<title level="a" type="main">Inferring the history of population size change from genome-wide SNP data</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Theunert</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Biol. Evol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3653" to="3667" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b38">
	<analytic>
		<title level="a" type="main">Approximate Bayesian computation scheme for parameter inference and model selection in dynamical systems</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Toni</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Soc. Interface</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="187" to="202" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>