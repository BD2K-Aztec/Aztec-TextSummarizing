
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FastLSU: a more practical approach for the Benjamini–Hochberg FDR controlling procedure for huge-scale testing problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Vered</forename>
								<surname>Madar</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Statistical and Applied Mathematical Sciences Institute</orgName>
								<address>
									<postCode>27709</postCode>
									<settlement>Research Triangle Park</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Sandra</forename>
								<surname>Batista</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FastLSU: a more practical approach for the Benjamini–Hochberg FDR controlling procedure for huge-scale testing problems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btw029</idno>
					<note type="submission">Received on 24 July 2015; revised on 21 December 2015; accepted on 15 January 2016</note>
					<note>Gene expression *To whom correspondence should be addressed. Associate Editor: Ziv Bar-Joseph Contact: sbatista@cs.princeton.edu Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: We address a common problem in large-scale data analysis, and especially the field of genetics, the huge-scale testing problem, where millions to billions of hypotheses are tested together creating a computational challenge to control the inflation of the false discovery rate. As a solution we propose an alternative algorithm for the famous Linear Step Up procedure of Benjamini and Hochberg. Results: Our algorithm requires linear time and does not require any P-value ordering. It permits separating huge-scale testing problems arbitrarily into computationally feasible sets or chunks. Results from the chunks are combined by our algorithm to produce the same results as the controlling procedure on the entire set of tests, thus controlling the global false discovery rate even when P-values are arbitrarily divided. The practical memory usage may also be determined arbitrarily by the size of available memory. Availability and implementation: R code is provided in the supplementary material.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In many fields the substantially increased scale of data available has resulted in a significant increase in the size of multiple hypotheses testing problems. In genetics, in particular, typical GWAS studies consist of 10 5 À 10 6 SNPs (<ref type="bibr" target="#b9">Hindorff et al., 2015</ref>) while eQTL studies (<ref type="bibr" target="#b17">Wright et al., 2014;</ref><ref type="bibr">Xia et al., 2012</ref>), newly advanced methylation studies (<ref type="bibr" target="#b12">Smith et al., 2014</ref>), and imaging studies (<ref type="bibr" target="#b13">Stein et al., 2010</ref>) usually start with 10 9 tests. These testing problems are hugescale as opposed to large-scale used by<ref type="bibr" target="#b6">Efron (2004)</ref>to describe studies consisting of hundreds to thousands of hypotheses. It is preferable to control the false discovery proportion rather than the number of false positives for a huge-scale testing problem. Therefore the FDR or the pFDR approaches are favored and both tend to offer larger, more powerful sets of results than those yielded by the conservative FWER control. These huge-scale multiple hypotheses testing problems create numerous computational challenges when many tests, say of the order 10 6 , are performed with all of the P-values of more or less equal importance. As a result some simpler testing procedures such as rigid P-value thresholds may be used that sacrifice power and correctness. Alternatively tests may be separated or chunked into smaller sets or chunks that are more computationally feasible.<ref type="bibr" target="#b7">Efron (2008)</ref>notes that the problem of separating hypotheses tests has not received great attention and warns of some pitfalls in chunking P-values, but focuses on grouping tests that share a biological property rather than arbitrary, computationally feasible chunks. Cai and Sun (2009) and later (<ref type="bibr" target="#b0">Benjamini and Bogomolov, 2014</ref>) propose alternative solutions to Efron's grouping problem but do not address the problem of arbitrary, computationally feasible chunking. We confront the computationally feasible chunking problem for theBenjamini–Hochberg false discovery rate (<ref type="bibr" target="#b1">Benjamini and Hochberg, 1995</ref>). We show on data from Stranger's HapMap study (<ref type="bibr" target="#b16">Stranger et al., 2007;</ref><ref type="bibr">Xia et al., 2012</ref>) that if results from separate tests are not combined correctly, there is considerable inflation of type I error, offer an explication for this occurrence, and propose our algorithm as a solution. Consider a huge-scale testing problem of size m where our goal is to select exactly R ! 0 significant tests. Of the R significant discoveries, exactly V ! 0 tests will be false discoveries (i.e. truly nonsignificant tests that are declared significant). A common approach in multiple hypotheses testing problems is to control the family-wise error rate, FWER ¼ PrðV ! 1Þ, the probability of selecting at least one false discovery. For huge-scale testing a more favorable alternative is to control the false discovery proportion, FDP ¼ V=maxðR; 1Þ, the proportion of truly false tests among the significant R. Some will prefer to control the positive FDR, pFDR ¼ EðFDPjR &gt; 0Þ, the expectation of the FDP when significant tests are selected, while others will opt to control the false discovery rate, FDR, the expectation of the FDP, E(FDP). The FDR is always of a potentially smaller magnitude than the FWER and of the pFDR (FDR ¼ pFDR Á PrðR &gt; 0Þ). Yet, in reality for huge-scale testing, FWER &gt; &gt; FDR and sometimes, pFDR % FDR. Therefore both FDR and pFDR control approaches tend to offer larger, more powerful sets of results than those that might be offered by the conservative FWER control. For a further discussion about FWER, FDR and pFDR refer to Farcomeni (2004). In huge-scale testing when the m P-values are partitioned into chunks, it is challenging to control the FWER, pFDR or FDR over the entire collection of m P-values. Controlling these error rates on a per chunk basis, if not done correctly, may interfere with the overall results by introducing more false discoveries. Although the same difficulty arises for the FDR and pFDR control (<ref type="bibr" target="#b0">Benjamini and Bogomolov, 2014;</ref><ref type="bibr" target="#b7">Efron, 2008</ref>), it is easier to illustrate this for the FWER. Consider, for instance, an example of FWER control using the Bonferroni approach by collecting all P-values less than a=m. Assuming that the number m of P-values happen to be very large so that m should be divided into k chunks, each of of size m i so that m ¼ P k i¼1 m i. Applying Bonferroni in chunks of size m i will tend to select more significant results than applying it over the entire set of m ¼ P m i P-values since a=m is less than a=m i. In the case of FWER control, using a fixed bound of a=m for all the chunks is theoretically preferred but often yields no significant results. A stricter constant cut-off on all sets of tests as suggested by Dudbridge and Gusnanto (2008) for GWAS was developed based on results from simulated GWAS. However such an ad-hoc approach eliminates from the entire multiple hypotheses testing problem any knowledge of the actual significance level a used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">The Benjamini–Hochberg linear step up procedure for controlling the false discovery rate</head><p>The Benjamini–Hochberg Linear Step Up (LSU) procedure is designed to control the FDR at desired signficance level a (<ref type="bibr" target="#b1">Benjamini and Hochberg, 1995</ref>). As a result for huge-scale multiple hypotheses tests of equal importance, controlling a proportion of false discoveries, especially on the average, has increased power over procedures that control the FWER such as Bonferroni or a rigid cut-off bound such as 5 Â 10 À8 suggested for GWAS (<ref type="bibr" target="#b5">Dudbridge and Gusnanto, 2008</ref>). The larger the multiple hypotheses testing problem is the more powerful the LSU is over procedures that control the FWER. While the LSU procedure is still one of the most cited procedures, its application had required sorting all P-values in decreasing order to look for the largest P-value that satisfies a simple condition. In face of a huge-scale testing problem rather than apply LSU, some researchers had preferred to use harsh P-value cut-offs as mentioned above or to divide their huge-scale set of tests into computationally feasible smaller chunks and apply a multiple hypotheses testing procedure on each chunk selecting as the final significant results the union of the results in each of the chunks.<ref type="bibr" target="#b7">Efron (2008)</ref>warns of the danger in such aggregation from the perspective of pFDR, pointing out that some chunks might have a larger proportion of significant results than others, and aggregating the significant results can yield misleading estimates. Moreover different chunking of tests might yield different sets of significant results. Analysis done by different groups of populations or chromosomes may not give the same number of significant tests as analysis that is applied on equal sized subsets; for example, it is well known that chromosome 6 (<ref type="bibr" target="#b11">Mungall et al., 2003</ref>) has a higher proportion of significant HLA SNPs than other chromosomes. We shall show in Section 2 that sorting the P-values, arbitrary thresholds, and arbitrary aggregation of results are not necessary and do not improve computational time efficiency compared to our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">A faster algorithm for LSU</head><p>Our alternative algorithm to the Benjamini–Hochberg LSU, FastLSU, performs linear scans instead of sorting P-values, but takes into account the overall size of the testing problem. FastLSU tiles the LSU procedure to give one global set of results that does not differ from applying LSU to the entire set of tests. Our algorithm addresses the same objective function as the original LSU. Our approach is provably faster than the conventional approach that relies on sorting P-values. It may also be used on arbitrary chunks of arbitrary size with an arbitrary space constraint in order to return the same set of significant results as those from applying LSU to the entire set of tests. In the following section we address the difference between grouping and chunking tests and the difficulty in arbitrarily chunking tests by giving examples of inflation of type I error. In Section 3 we present FastLSU on a single set of tests and prove its equivalence to LSU, time efficiency and space efficiency. We present FastLSU on arbitrary chunks and also show its correctness and efficiency in Section 4. We offer suggestions for finalizing the report of significant tests in Section 5 and conclude with discussion in Section 6. R Code for an implementation of the algorithm is given in the supplemen tary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>An exercise of three HapMap groups and their chunking for stranger's cis-eQTL study FastLSU controls the global FDR, not a family-based bound or a group-based bound. Our final results are not affected by the procedure of separating tests. This is an important distinction because chunking is arbitrary and based primarily on computational efficiency without any consideration for relationships between the hypotheses being tested. This is in contrast to group-testing procedures where for example, hypotheses may be grouped based on experimental knowledge such as all the tests from the same chromosome, the back and front parts of the brain (<ref type="bibr" target="#b7">Efron, 2008</ref>), or different population groups in HapMap (<ref type="bibr" target="#b16">Stranger et al., 2007</ref>). In Section 3.2 we will show how the FastLSU algorithm can even improve the efficiency of a group-based controlling procedure. FastLSU is particularly suited to the current applications in genetics because we typically seek the significant set of SNPs or genes, and the family structure is usually of less importance than managing the FastLSUcomputational burden. Genetic family structures typically do not require any correction for family selection because each genetic family tends to contribute significant results of its own. (This is the case for the following example of 3 families of HapMap). For this reason in the remainder of this section, we will give motivating examples of applying FastLSU on the three groups of approximately 14 Â 10 6 cis-eQTLs of Stranger's HapMap study (<ref type="bibr" target="#b16">Stranger et al., 2007;</ref><ref type="bibr">Xia et al., 2012</ref>) in order to demonstrate the problems with arbitrary chunking without combining the results as FastLSU does.<ref type="bibr" target="#b16">Stranger et al. (2007)</ref>presents an eQTL study over 4 HapMap population samples: 30 Central Europeans(CEU), 45 Chinese (CHB), 45 Japanese(JPT) and 30 trios from Nigerians(YRI). To increase power each group is analyzed separately. We follow the recommendations of the SeeQTL website (<ref type="bibr">Xia et al., 2012</ref>) and consider the CHB and JPT together. We define cis-eQTLs as within 1 Mb upstream or downstream of a gene. Each population has a set of approximately 14 million P-values that were split into chunks of the following sizes: 1M, 900K, 800K, 700K, 600K, 500K, 250K, 100K, 50K, 25K, 10K and 5K. We contrast the differences in the number of significant results when the results are combined by taking the union of the results of LSU on each chunk versus those selected by FastLSU Algorithm 2 in<ref type="figure" target="#fig_0">Figure 1</ref>. Applying the FastLSU yields 9228 significant results for CEU, 6497 for YRI and 33 507 for the CHB &amp; JPT group. Most notable is that the results for FastLSU Algorithm 2 do not change across the chunk sizes whereas the alternative of taking the union of the results on each chunk increases not only the number of significant tests selected, but also the maximal P-value reported as the chunk size decreases. For example, applying FastLSU at level 10% for the complete chunk of approximately 14M P-values yields 9228 discoveries for the CEU. However, applying LSU on 1370 chunks of size 10K yields 16 925 significances which is equivalent to an overall FDR control of 23.75%. When 10K sized chunks are used for the CHB-JPT group, 41 587 P-values are selected as significant and this would require overall FDR control of a ¼ 15:58% For the YRI group when 10K sized chunks, there are 10 330 significant P-values and an a ¼ 21:1% would be required for this overall FDR control. This implies that performing the analysis using 1300–1400 chunks of size 10K rather than a single chunk inflates the type I error by 50%. While this is compelling experimental evidence, a more compelling explanation is that the union of partial orderings on subsets of a set is in general not equal to the partial ordering on the entire set. Moreover when the size of the chunks decreases, the significance interval is being divided into larger sub-intervals with each more likely containing more P-values spanning a greater range of values, so that the set of selected significant P-values will more likely contain a larger cut-off of P-values, and therefore a considerably larger value for the maximal significant P-value. The maximal value of significant P-values we obtain for the case of a single chunk is 6:7Â 10 À5 for CEU, 4:4 Â 10 À5 for YRI and 2:5 Â 10 À4 for CHB-JPT. For chunks of the size 100K (about 13 to 14 chunks), the max P-values are slightly higher especially for the CHB-JPT group: 1:7Â 10 À4 for CEU, 3:5 Â 10 À4 for YRI and 1:2 Â 10 À3 for CHB-JPT. For chunks of 10K size (about 130–140 chunks) the maximal P-values are considerably higher still: 0.0063 for CEU, 0.0034 for YRI and 0.011 for CHB-JPT. As we mentioned, there is no need to apply any correction for group or family selection as required by<ref type="bibr" target="#b0">Benjamini and Bogomolov (2014)</ref>since all three groups in the HapMap example give significant results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The FastLSU algorithm</head><p>The usual way to apply the LSU (<ref type="bibr" target="#b1">Benjamini and Hochberg, 1995</ref>) at level 0 a 1 is to sort the P-values in descending order, p ðmÞ ! p ðmÀ1Þ ! Á Á Á ! p ð1Þ. Starting from the largest P-value to the smallest, we need to look for the kth largest P-value that satisfies p ðkÞ &lt; ka=m. One may view the LSU algorithm (<ref type="bibr" target="#b2">Benjamini and Yekutieli, 2001</ref>) as a search for optimal index r r ¼ argmaxfi : p ðiÞ &lt; ia=mg:</p><formula>(1)</formula><p>This observation motivates the following Fast Linear Step Up (FastLSU) algorithm that controls the FDR at level a:At the first step, we find 9 P-values &lt; 0.05. That is, r 1 ¼ 9. At the second step, r 2 ¼ 7, 7 P-values are &lt; 9 Á 0:05=15 ¼ 0:03. At the third step, r 3 ¼ 5 P-values are &lt; 7 Á 0:05=15 ¼ 0:2333. We stop with r 4 ¼ 4 P-values that satisfy &lt; 4 Á 0:05=15 ¼ 0:01333. The selected P-values are 0:0001; 0:0019; 0:0004; 0:0095, and the reader can check that these 4 P-values are exactly the ones selected using the original LSU P-value sorting algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The equivalence to LSU and computational efficiency</head><p>THEOREM 1. For a significance level a, the Algorithm 1 maximizes the same objective function (1) that is used by the LSU. Therefore,the FastLSU controls the FDR at level a and gives the same selected set of significant results as would be obtained by applying the Benjamini–Hochberg LSU FDR controlling procedure (<ref type="bibr" target="#b1">Benjamini and Hochberg, 1995</ref>). PROOF. Consider a batch of P-values which we will denote as C. Let 0 t 1 and define Sðt : CÞ ¼ fp &lt; t : p is PÀvalue 2 Cg to be the number of P-values from C smaller than t. The set S ia m À Á consists of all P-values from C that are smaller than ia m. Hence, for the case of a single batch, it is possible to verify that the search for r ¼ argmaxfi : i ¼ #S ia=m ð Þg</p><formula>(2)</formula><p>is equivalent to looking for the largest rth P-value satisfying p ðrÞ ra m , as requested by (1). THEOREM 2. The FastLSU Algorithm 1 requires O(m) time and O(m) space where m is the number of P-values to be considered. The proof to Theorem 2 is given in the Appendix A with accompanying pseudocode for procedural language implementations. The main observation behind the linear time algorithm is that it is a constant time check where, in terms of what proportion of units of size a=m, a P-value is relative to a. It is then only a single scan to count the number of P-values that fall within each range relative to a and to find the range of P-values that satisfy the LSU condition. This is not commonly how statisticians consider LSU because we are not comparing the P-values directly to each other; we are comparing them by examining the range in which they fall relative to a and only implicitly to each other. We demonstrate the linear scans used in the linear algorithm in the following example. Example 2. To demonstrate this equivalence we apply FastLSU on the example appearing in the original 1995 FDR paper (<ref type="bibr" target="#b1">Benjamini and Hochberg, 1995</ref>) with a ¼ 0:05. Instead of sorting all 15 P-values we apply Algorithm 1 using 3 linear scans and find 4 significant P-values. Consider the 15 P-values from the example given in the original LSU paper (<ref type="bibr" target="#b1">Benjamini and Hochberg, 1995</ref>) from Example 1. In the first linear scan, we label each P-value with what interval k, k ¼ mp a AE Ç , such that the P-value is at most ka=m respectively as follows:</p><p>þ; þ<ref type="bibr">; 9; þ; 14; 9; 1; 1; 1; 7; þ; þ; þ; 3; 11</ref>For P-values greater than a, a label þ is used. During the same linear scan, we can also maintain counts for the number of P-values labeled with k from 1 to m ¼ 15 as follows:</p><formula>3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Applying FastLSU over families or groups of P-values of equal relevance</head><p>Efron (2008) distinguishes between the groups of imaging P-values that arise from the front brain and the back brain, and develops an empirical Bayes set-up to combine the significant P-values from the two families.<ref type="bibr" target="#b0">Benjamini and Bogomolov (2014)</ref>extended Efron's idea of groups into the voxel families of MRI where each set of P-values from a specific voxel of a certain location are treated as a separate, homogeneous group of relevance. For the first step each voxel group is analyzed by applying the LSU at level a. For the second step the number of significant groups is collected and LSU is performed again with another significance level a Ã to correct for the selection of groups. If, for example, S groups out of G are shown to have at least one significant P-value for LSU of level a, a Ã is set to Sa=G for each of the groups (<ref type="bibr" target="#b0">Benjamini and Bogomolov, 2014</ref>). Given the equivalence between LSU and FastLSU, FastLSU may be used for each step in the approach of Benjamini and Bogomolov. The second step needs only be done for groups that have at least one significant result. Since only significant P-values for each group need to be considered, FastLSU may be applied to each group by using a ÃÃ ¼<ref type="bibr" target="#b6">Efron, 2004;</ref><ref type="bibr" target="#b15">Storey, 2002</ref>). To be more precise, the complete grid of k values should be collected in each step and then the q-values should be corrected accordingly.An example of Algorithm 2 applied to the example appearing in the original 1995 FDR paper (<ref type="bibr" target="#b1">Benjamini and Hochberg, 1995</ref>) is given in the Appendix A. There are many ways one can alter Algorithm 2. For example, P-values that do not satisfy the condition in the iterative step that preserves the LSU may be either flagged or dropped. Alternatively Algorithm 1 may be applied to each chunk independently starting with the initial value for r 0 ¼ m À m i þ r ð1Þ i and still tiling by the total number of tests, m. The advantage of this is that this may be done in parallel if desired. The results of the chunks may then be combined into a single set, if space permits, and Algorithm 1 applied to this set to return the final set of significant results. If space does not permit the results of the chunks to be combined, they may be combined up to the space limit. Then the iterative step of Algorithm 2 above may be applied to the resultant chunks starting by checking for P-values less than m 0 a=m where m 0 is the size of the union of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SrgðaÞ</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Applying LSU on arbitrary chunks of P-values</head><formula>P n i¼1 r ðkÞ i a =m, and let r ðkþ1Þ i</formula><p>be the count for the chunk at this step.</p><formula>Repeat Step k þ 1 until k þ 1 ¼ m or P n i¼1 r ðkþ1Þ i ¼ P n i¼1 r ðkÞ i .</formula><p>Mark these P-values as significant under LSU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FastLSU</head><p>THEOREM 3. For a significance level a, and a collection of c chunks the Algorithm 2 gives the same selected set of significant results as would be obtained by applying the Benjamini–Hochberg (<ref type="bibr" target="#b1">Benjamini and Hochberg, 1995</ref>) LSU FDR controlling procedure over the set of all P-values from the c chunks. PROOF.<ref type="bibr">[Proof for Algorithm 2]</ref>Suppose we have exactly c disjoint chunks of P-values C 1 ; C 2 ;. .. ; C c , with sizes m i (i ¼ 1; 2;. .. ; c) such that C ¼ [ c i¼1 C i and jCj ¼ m ¼ P c i¼1 m i P-values. Let r C be the number of selected significant P-values that satisfy the LSU objective (1):</p><formula>r C ¼ argmax r : r ¼ #S ra m : C n o : (3)</formula><p>Algorithm 2 can be applied to each c chunks and the last step of Algorithm 2 is to finalize the selection. Accordingly for chunk C i we search for the largest r Ã i that satisfy the objective:</p><formula>r Ã i ¼ argmax r : r ¼ #S ðr þ m À m i Þa m : C i &amp; ' : (4)</formula><p>We claim that</p><formula>r C r Ã i þ m À m i ; for any i ¼ 1;. .. ; c (5)</formula><p>This implies that each of the P-values selected significant from the search for (3) within chunk C i must be selected while searching for argmax</p><formula>r Ã i as in (4) since p r C Á a=m implies p ðr Ã i þ m À m i Þa=m.</formula><p>It is possible to verify that p ðk:CiÞ the kth P-value in chunk C i cannot be larger than, p ðkþmÀmiÞ , the ðk þ m À m i Þ th P-value in the union C:</p><formula>p ðk:CiÞ p ðkþmÀmiÞ for all k ¼ 1;. .. ; m i : (6)</formula><p>In particular, by the condition (4), let p ðr Ã i Þ be the largest P-value</p><formula>in C i that satisfies p ðr Ã i :CiÞ &lt; ðr Ã i þ m À m i Þa=m. When m i &gt; r Ã i , it follows that, for k ¼ 1;. .. ; m i À r Ã i , ðr Ã i þ k þ m À m i Þa=m &lt; p ðr Ã i þk:CiÞ p ðr Ã i þkþmÀmiÞ : (7)</formula><formula>(When m i ¼ r Ã i (5)</formula><p>holds trivially since at most the entire set of tests may be selected as significant). To prove (5) by contradiction, let us compare between p ðrC Þ to p ðr Ã i þkþmÀmiÞ. If we assume that</p><formula>r C &gt; r Ã i þ m À m i</formula><p>, then we can define a positive integer</p><formula>k ¼ r C À ðr Ã i þm À m i Þ</formula><p>for which following the inequality in (7) holds,</p><formula>r C Á a=m ¼ ðr Ã i þ k þ m À m i Þa=m &lt; p ðr Ã i þk:CiÞ p ðr Ã i þkþmÀmiÞ ¼ p ðrCÞ :</formula><p>However, this is in contradiction to the condition (3) that provides p ðrC Þ &lt; r C Á a=m. In conclusion we show that applying FastLSU over the global union of P-values give the exact same selection of significant P-values. We search for r Ã C over the union of the results on each chunk of size</p><formula>P c i¼1 r Ã i r Ã C ¼ #S r Ã C Á a m : Sðr Ã 1 : C 1 Þ [ Á Á Á [ Sðr Ã c : C c Þ :</formula><p>Since the inequality (5) ensures that P-values that were not selected under the condition (4) would have not been selected under the condition (3), we conclude that r Ã C ¼ r C. ¶ THEOREM 4. The FastLSU Algorithm 2 requires O(m) time and O(m) space where m is the number of P-values to be considered. The practical memory usage may be restricted to an arbitrary limit for the largest chunk size, m Ã. Proof for the running time and space limitations are shown in the Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Useful tips for finalizing the report of significant P-values</head><p>If we assume that the R tests were declared significant by applying either the LSU or FastLSU, we offer tips for finalizing the set of significant P-values. In the remainder of this section we explain how to protect the FDR control of FastLSU against dependency structures and when such correction is actually needed. We will also explain how to compute q-values (adjusted P-values) for the final results without keeping the entire set of P-values and show how to add a set of simultaneous confidence intervals for the significant test statistics while accounting for the selection effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Correcting against general case of dependence</head><p>The LSU procedure is conservative under the general type of positive regression dependence on subsets (PRDS) (<ref type="bibr" target="#b2">Benjamini and Yekutieli, 2001</ref>), so applying the LSU at significance level of a always ensures FDR a for PRDS P-values. The PRDS class contains a larger set of structures, among them, the independent case and any positively associated P-values such as P-values obtained from a two-sided t test. Since the LSU and FastLSU are equivalent, applying the FastLSU at level a on PRDS P-values will control the FDR at level-a, as well. For other types of non PRDS dependency, such as in the case of pairwise comparisons, it is recommended to use the Benjamini– Yekutieli procedure (<ref type="bibr" target="#b2">Benjamini and Yekutieli, 2001</ref>) that applies LSU using a Ã ¼ a=c instead of a where c ¼ P m k¼1 1=k % ln m þ 0:5772. By the same argument the FastLSU under non PRDS will have FDR a when applied under a Ã ¼ a ln mþ0:5772. As a matter of practice, we suggest to first perform FastLSU using the significance level a. Then, if the P-values are non PRDS, correct the R selected P-values by applying FastLSU again over the single batch consisting of R P-values using a ÃÃ ¼ Ra Ã m. This approach provides FDR a since a Ã &lt; a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">How to compute q-values to a selected subset of significant tests</head><p>When it is preferable to report q-values or adjusted P-values, we suggest how this may be done more efficiently. If we assume that R tests were selected as significant, let the P-value p ðRÞ be the largest P-value satisfying p ðRÞ m=R &lt; a. All P-values larger than p ðRÞ were not selected as significant since p ðiÞ m=i &gt; a for i &gt; R and have q-values &gt; a. Therefore it is sufficient to consider only the set of selected R P-values. The LSU q-value, q ðiÞ , for the P-value, p ðiÞ has the form (<ref type="bibr" target="#b19">Yekutieli and Benjamini 1999)</ref>q ðiÞ ¼ min j¼R;RÀ1...;iþ1 fp ðiÞ m=i; q ðjÞ g; for i &lt; R q ðRÞ ¼ p R ð Þ m=R; for i ¼ R:</p><formula>(8)</formula><p>From this we can see that the algorithms presented by (<ref type="bibr" target="#b19">Yekutieli et al., 1999</ref>) and (<ref type="bibr" target="#b15">Storey, 2002</ref>) are OðRlogRÞ. One needs only sort the R selected P-values in descending order and then beginning from largest P-value assign the corresponding q-value in a final linear scan recording the minimum q-value assigned thus far. The q-values will also be descending assigned in this way and there is no need to compare previous values except the minimum q-value thus far. The value for the q-value for q ðiÞ will only change when it is less than the minimum seen thus far and the minimum q-values will span a range of P-values until either a sufficient number of P-values have been covered or a sufficient range in P-values have been covered. Alternatively, one can use existing procedures (such as the function p.adjust in R software or PROC MULTTEST in SAS) to compute adjusted P-values for the set of R selected P-values and multiply the results by R/m to correct them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Confidence intervals for selected subset of significant results</head><p>A less common approach in genetic studies is to report the significant test results by constructing a set of confidence intervals for the test statistics. While P-value is merely a measure of the magnitude of the test statistic, a confidence interval may offer the additional information about the dispersion of that magnitude. The selection adjusted confidence intervals (<ref type="bibr" target="#b3">Benjamini and Yekutieli, 2005</ref>) offer an appropriate construction that corrects against the false coverage effect of selection. For a useful example see (<ref type="bibr" target="#b10">Jung et al., 2011</ref>) for how this method is used for the significant log-fold changes of RNA Microarrays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1. PROOFS AND CODES</head><formula>¼ mp Ã a l m or k ¼ k þ 1 if mp Ã a equals mp Ã a l m</formula><p>where a is the significance level and p Ã is the given P-value being labeled. If p Ã is labeled with bin k less than or equal to m, increment the count for bin k and increment current P-value count, m Ã. If a p Ã has label k greater than m, it may be filtered, so no counts need to be incremented for such P-values (although they can be labeled with arbitrarily large values for k). Running time and memory: Labeling each P-value and incrementing the labeled bin count requires only constant time and a single pass through the P-values. In addition to storing the P-values, the P-value labels and bin counts also need to be stored, also requiring O(m) space each, and a variable for the current P-value count,</p><formula>m Ã .</formula><p>Accumulate. In this step we find the r Ã the significant bin to return all P-values in bins less than or equal to this bin as significant. To do so, starting from the highest labeled bin's count, i.e. for m, keep a partial sum of the total number of P-values in the bins thus far. If the current bin's has a non-zero bin count and its value is equal to m Ã minus then the current partial sum, then return the current bin as r Ã the significant bin. Running time and memory: This step can be done in a single scan of the bin counts and only requires additional variables for the significant bin, r Ã and the partial sum.</p><p>Return. Return as significant all the P-values that were labeled with a k less than or equal to r Ã. Running time and memory: This requires only a single scan of the P-value labels and no additional space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.2 Pseudocode for proof of theorem 2</head><p>Let p_vals<ref type="bibr">[m]</ref><ref type="bibr" target="#b1">Benjamini and Hochberg, 1995</ref>). Further assume that for some reason the 15 P-values are divided into two chunks. The first chunk, say C 1 , consists of the first 8 P-values:Next, we follow the last step of Algorithm 2 which is a combination step and applies on the collection of 8 P-values selected at the former steps: f0:0298; 0:0278; 0:0001; 0:0019; 0:0004; 0:0201; 0:0095; 0:0344g: All 8 selected P-values are clearly &lt; 0.05. Second, 5 P-values &lt; 8 Á 0:05=15 ¼ 0:0267, and third scan finds out 4 P-values &lt; 5 Á0:05=15 ¼ 0:0167 that also satisfy &lt; 4 Á 0:05=15. The selected P-values are, again, 0:0001; 0:0019; 0:0004; 0:0095.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.4 Proof of theorem 4</head><p>Proof. [Proof of Theorem 4] For m P-values arbitrarily divided into n chunks of size m c for c ¼ 1; :::; n such that the maximum chunk size is m Ã , we show that the algorithm is still linear in m and never uses more than m Ã space.</p><p>Bin. This step is as for Algorithm 1. It is important to note that the binning is done relative to m and not the size of the chunk. The only important difference is that bin sums are not maintained because of the m Ã space limitation. Labels need not be stored either. This step also counts the sum, m 0 , of all P-values across all groups that are less than a. Running time and memory: This requires a linear scan. A count variable can be kept for each group in order to get m 0. Accumulate. For each group, find the bin sums for the largest m Ã partition of bins not covered yet, i.e. find bin sums for bins m 0 À j m Ã þ 1 to m À ðj À 1Þm Ã for j ¼ 1; :::; n and we do both of the following before incrementing j. Accumulate bin sums across the chunks. This can be done in a linear scan of the chunks and a single array accumulating bin sums across chunks. Finding rÃ is as in Algorithm 1 Step 2. However, now only m Ã bins may be checked at a pass before needing to increment j. The subtotal of P-values counted thus far is maintained after j is incremented. Running time and memory: This step must be repeated at most n times and requires a linear scan of the data. At any point at most m Ã space plus several count variables are used.</p><p>Return. This step is as in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FastLSU</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Example1.</head><figDesc>We demonstrate Algorithm 1 on the example appearing in the original 1995 LSU paper (Benjamini and Hochberg, 1995) with a ¼ 0:05. Consider the 15 P-values: 0:6528; 0:7590; 0:0298; 0:4262; 0:0459; 0:0278; 0:0001; 0:0019; 0:0004; 0:0201; 1:0000; 0:5719; 0:3240; 0:0095; 0:0344</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.1.</head><figDesc>Fig. 1. Varying chunk size over Stranger's HapMap populations. Dashed lines show the consistent result of applying the FastLSU as in Algorithm 2. The triangle, dot, and plus shapes represent results of applying the LSU under 10%</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><figDesc>; 0; 1; 0; 0; 0; 1; 0; 2; 0; 1; 0; 0; 1; 0 In the second linear scan, we check for k from m ¼ 15 to 1 if the the number of significant P-values remaining, 9 in this case, minus the number of P-values in ranges examined so far equals to k as follows: 9; 8; 8; 8; 7; 7; 5; 5; 4; 4; 4; 4 This occurs when k ¼ 4. On the third and final scan, we return as significant those P-values with interval, r 4. The selected P-values are 0:0001; 0:0019; 0:0004; 0:0095 as in Example 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><figDesc>of which r g ðaÞ were declared significant of level a. Since FastLSU achieves efficiency by the simple recalling and correction of the constants m (or m g ) for the number of tests, a similar approach also works for Storey-Efron's positive FDR approach (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><figDesc>Algorithm 1 may be extended for when P-values are divided into an arbitrary number of chunks of arbitrary size. The size of the chunks may be determined by practical memory constraints or any other criteria. The iterative steps are applied on each chunk. The significant results from each chunk are combined using another iterative step to form the final set of significant results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm2.</head><figDesc>FastLSU for the case of two or more chunks of P-values Given a set of m tests and P-values divided into n separate chunks of sizes m i P-values for i ¼ 1; :::; n such that m ¼ P n i¼1 m i : Step 1. On each of the chunks, count the number of P-values less than a. Denote this count as r ð1Þ i. Step k þ 1. On each of the chunks (starting from k ¼ 1) count the number of P-values less than</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>A1.1</head><figDesc>Proof of theorem 2 PROOF. [Proof of Theorem 2] We present the algorithm here in three steps with running time for each step and accompanying pseudocode. Bin. Classify all P-values into the bins of the interval, ½0; a each of size 1=m and keep a total of all P-values with value less than a, m Ã. Label each P-value with a value k such that k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><figDesc>for a ¼ 0:05 has three steps: In Step 1, we scan the 8 P-values in C 1 and seek for the largest P-value, p ði:C1Þ satisfying &lt; ði þ 15 À 8Þa=15. This can be done in a similar manner to applying Algorithm 1. First scan gives 5 P-values &lt; ð8 þ 15 À 8Þ0:05= 15 ¼ 0:05. Second scan gives 4 P-values &lt; ð5 þ 15 À 8Þ0:05=15 ¼ 0:04 and stops with the 4 P-values &lt; ð4 þ 15 À 8Þ0:05=15 ¼ 0:0367. The selected P-values for the first step are Sð0:0367 : C 1 Þ ¼ f0:0298; 0:0278; 0:0001; 0:0019g; In Step 2, we scan the 7 P-values in C 2 and seek for the largest p ði:C2Þ &lt; ði þ 15 À 7Þ0:05=15. First scan gives 4 P-values &lt; ð7 þ 15 À7Þ0:05=15 ¼ 0:05 and immediately stops with these 4 P-values &lt; ð4 þ 15 À 7Þ0:05=15 ¼ 0:04. The selected P-values are Sð0:04 : C 2 Þ ¼ f0:0004; 0:0201; 0:0095; 0:0344g:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><figDesc>V C The Author 2016. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com 1716 Bioinformatics, 32(11), 2016, 1716–1723 doi: 10.1093/bioinformatics/btw029 Advance Access Publication Date: 30 January 2016 Original Paper</figDesc><table></table></figure>

			<note place="foot">at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">V.Madar and S.Batista at University of California, Los Angeles on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="6"> Discussion We presented an efficient algorithm to apply correctly the Benjamini– Hochberg Linear Step Up FDR controlling procedure in a huge-scale testing problem. Since we have shown that our algorithm requires only linear time, our algorithm is provably not any more computationally burdensome than even using a rigid Bonferroni cut-off for control. However, unlike the rigid Bonferroni cut-off, our approach ensures the FDR control at level a and this is a more powerful alternative to controlling the FWER, especially when the multiple testing problem is of huge-scale. Our approach is also scalable to any subsetting or chunking of the overall subset of P-values. In addition we offered tips for performing the LSU over a hugescale multiple hypotheses testing problem, such as showing how to correct for dependency or how to compute q-values directly from the subset of LSU significant results. We hope that these algorithms and tips offer better insight into the huge-scale testing problem rather than just black-box solutions. We encourage altering the steps in performing Algorithm 2, for instance, either by applying it sequentially or in parallel or a mixture of both. The amount of inflated type I error we observed during the exercise on different chunk sizes of the HapMap populations strongly suggests the need for greater diligence in correctly separating hypotheses tests, whether the objective is to control the FWER, the FDR, or pFDR. This observation was also reported by Efron (2008), but a full investigation on real data with decreasing chunk sizes was not performed. Our suggested approach solves this for the case of the Benjamini–Hochberg FDR. As we have alluded, a similar approach can be adopted to control correctly the Efron–Storey&apos;s pFDR (Storey, 2002) and this remains open for further research. In addition the severe phenomena of inflated type I error we observed suggests more care may be required in reporting and interpreting results in genetics literature especially in the case of GWAS and eQTL studies. Namely to properly understand the significance results there is a need for consistent consideration of the algorithms or software used for controlling and separating the hypotheses tests and for recording the chunk sizes used for a study.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank to the referees and wish to thank their colleagues for their many helpful comments. Special thanks are reserved for Dr. Kai Xia for his great help with the HapMap data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This material was based upon work partially supported by the National Science Foundation under Grant DMS-1127914 to the Statistical and Applied Mathematical Sciences Institute. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict</head><p>of Interest: none declared.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Selective inference on multiple families of hypotheses</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Benjamini</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Bogomolov</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="297" to="318" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Controlling the false discovery rate: a practical and powerful approach to multiple testing</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Benjamini</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Hochberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">The control of the false discovery rate in multiple testing under dependency</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Benjamini</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Yekutieli</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1165" to="1188" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">False discovery rate-adjusted multiple confidence intervals for selected parameters</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Benjamini</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Yekutieli</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="71" to="93" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Simultaneous testing of grouped hypotheses: finding needles in multiple haystacks</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">T</forename>
				<surname>Cai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Sun</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="1467" to="1481" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">P-value less than say 5 Â 10 À8 can be regarded as convincingly significant</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Dudbridge</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Gusnanto</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genet. Epidemiol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="227" to="234" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale simultaneous hypothesis testing: the choice of a null hypothesis</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Efron</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="96" to="104" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Simultaneous inference: when should hypothesis testing problems be combined? Ann</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Efron</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="197" to="223" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiple Testing Procedures Under Dependence, With Applications</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Farcomeni</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dipartimento di Statistica, Probabilita&apos; e statistiche applicate</title>
		<meeting><address><addrLine>Universita&apos; di Roma ; La Sapienza</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<monogr>
		<title level="m" type="main">A Catalog of Published Genome-Wide Association Studies Available at: www.genome.gov/gwastudies</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">A</forename>
				<surname>Hindorff</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Reporting FDR analogous confidence intervals for the log fold change of differentially expressed genes</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Jung</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinf</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">288</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">The DNA sequence and analysis of human chromosome 6</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Mungall</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">425</biblScope>
			<biblScope unit="page" from="805" to="811" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<analytic>
		<title level="a" type="main">Methylation quantitative trait loci (meQTLs) are consistently detected across ancestry, developmental stage, and tissue type</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">K</forename>
				<surname>Smith</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Genomics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">148</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<monogr>
		<title level="m" type="main">Voxelwise genome-wide association study (vGWAS)</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">L</forename>
				<surname>Stein</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1160" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<analytic>
		<title level="a" type="main">A direct approach to false discovery rates</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Storey</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Ser. B</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="479" to="498" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Population genomics of human gene expression</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">E</forename>
				<surname>Stranger</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1217" to="1224" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">Heritability and genomics of gene expression in peripheral blood</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">A</forename>
				<surname>Wright</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="430" to="437" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<analytic>
		<title level="a" type="main">2012) seeQTL: a searchable database for human eQTLs</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Xia</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="451" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Resampling-based false discovery rate controlling multiple test procedures for correlated test statistics</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Yekutieli</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Benjamini</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Plan. Inference</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="171" to="196" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>