
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining ShapePheno: unsupervised extraction of shape phenotypes from biological image collections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012">7 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Theofanis</forename>
								<surname>Karaletsos</surname>
							</persName>
							<email>Contact: theofanis.karaletsos@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems and Max Planck Institute for Developmental Biology</orgName>
								<orgName type="laboratory">Machine Learning and Computational Biology Research Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Oliver</forename>
								<surname>Stegle</surname>
							</persName>
							<email>oliver.stegle@ tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems and Max Planck Institute for Developmental Biology</orgName>
								<orgName type="laboratory">Machine Learning and Computational Biology Research Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Christine</forename>
								<surname>Dreyer</surname>
							</persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Molecular Biology</orgName>
								<orgName type="department" key="dep2">Max Planck Institute for Developmental Biology</orgName>
								<address>
									<postCode>72076</postCode>
									<settlement>Tübingen, Germany</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">John</forename>
								<surname>Winn</surname>
							</persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Machine Learning and Perception Group</orgName>
								<orgName type="institution">Microsoft Research Ltd</orgName>
								<address>
									<postCode>CB3 0FB</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Karsten</forename>
								<forename type="middle">M</forename>
								<surname>Borgwardt</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems and Max Planck Institute for Developmental Biology</orgName>
								<orgName type="laboratory">Machine Learning and Computational Biology Research Group</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Zentrum für Bioinformatik</orgName>
								<orgName type="institution" key="instit2">Eberhard Karls Universität</orgName>
								<address>
									<postCode>72076</postCode>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining ShapePheno: unsupervised extraction of shape phenotypes from biological image collections</title>
					</analytic>
					<monogr>
						<title level="j" type="main">BIOINFORMATICS ORIGINAL PAPER</title>
						<imprint>
							<biblScope unit="volume">28</biblScope>
							<biblScope unit="page" from="1001" to="1008"/>
							<date type="published" when="2012">7 2012</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/bts081</idno>
					<note type="submission">Received on September 22, 2011; revised on February 7, 2012; accepted on February 9, 2012</note>
					<note>Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [12:54 12/3/2012 Bioinformatics-bts081.tex] Page: 1001 1001–1008 Associate Editor: Jonathan Wren Supplementary information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: Accurate large-scale phenotyping has recently gained considerable importance in biology. For example, in genome-wide association studies technological advances have rendered genotyping cheap, leaving phenotype acquisition as the major bottleneck. Automatic image analysis is one major strategy to phenotype individuals in large numbers. Current approaches for visual phenotyping focus predominantly on summarizing statistics and geometric measures, such as height and width of an individual, or color histograms and patterns. However, more subtle, but biologically informative phenotypes, such as the local deformation of the shape of an individual with respect to the population mean cannot be automatically extracted and quantified by current techniques. Results: We propose a probabilistic machine learning model that allows for the extraction of deformation phenotypes from biological images, making them available as quantitative traits for downstream analysis. Our approach jointly models a collection of images using a learned common template that is mapped onto each image through a deformable smooth transformation. In a case study, we analyze the shape deformations of 388 guppy fish (Poecilia reticulata). We find that the flexible shape phenotypes our model extracts are complementary to basic geometric measures. Moreover, these quantitative traits assort the observations into distinct groups and can be mapped to polymorphic genetic loci of the sample set.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the advent of high-throughput genotyping techniques an unprecedented breadth of genotypic datasets can be generated, opening doors to large-scale association studies, promising sufficient power to understand the genetic underpinning of more subtle phenotypes that characterize the sample. As phenotyping often * To whom correspondence should be addressed. requires manual labor and expert knowledge, a major bottleneck now lies with the identification and quantification of informative traits. Currently, the quantification of phenotypic traits is predominantly done in a semi-manual fashion, rendering the task of analyzing large datasets expensive, time-consuming and error-prone. In order to address these shortcomings, the automated analysis of biological images has become a staple in modern biology. High-throughput imaging techniques for various types of microscopy and other imaging modalities have become common in the experimental environment. Automated image analysis for bioimaging attempts to deal with the flood of data and subsumes a large variety of tasks and methods; for a comprehensive review, see Peng (2008) and<ref type="bibr">Walter et al. (2010)</ref>. Common tasks include the counting of cells in microscopy images and differential analysis of distinct cell types (<ref type="bibr" target="#b4">Fuchs et al., 2010;</ref><ref type="bibr" target="#b8">Pau et al., 2010</ref>). Key challenges in bioimage informatics stem from the breadth and individuality of natural variation within these images and dealing with the inherent noise in biological imaging tasks. In order to deal with these factors, machine learning techniques have raised considerable attention and are used to tackle various complicated tasks in realistic settings (<ref type="bibr" target="#b7">Ning et al., 2005;</ref><ref type="bibr" target="#b11">Shamir et al., 2010</ref>). For example, in the analysis of appearance phenotypes machine vision has been used to quantify the extent of existence of predefined visual features or detect interesting appearance features that characterize the data (<ref type="bibr">Whibley et al., 2006</ref>). Visual appearance features usually pertain to specific local properties of the depicted objects. However, more general visual phenotypes often are also biologically informative, such as the description of the shape of an object and the quantification of global (including size and height) as well as local (i.e. locally deformed parts of an image) shape variations. An example where such a method is useful is the characterization of the shapes of guppy fish, which so far can only be analyzed by labor-intensive manual geometric phenotype measurements on hundreds of fish, as performed in (<ref type="bibr">Tripathi et al., 2009</ref>). Our goal is to automatically determine and quantify differences among observed shapes in biological images in order to interpret them as shape phenotypes and facilitate downstream analysis, for instance association tests of traits with putative causal factors in the genome. In this work, we propose an unsupervised machine learning method to quantify shape variations of a given object class depictedin a set of images, one per individual or sample. We postulate the existence of an unobserved reference shape, called a template. We proceed with joint learning of this shared template and the imagespecific shape deviation from this reference, allowing every image to be aligned to it. The resulting template iteratively converges to an idealized mean image from which the observed images are generated through deformation fields that explain the variation in shape of each image (<ref type="figure">Fig. 1</ref>). The converged model can also be run backwards, yielding a reconstruction of every image from the template and the mapping fields (R in<ref type="figure">Fig. 1</ref>). The general task of aligning two or more images is also known as registration, where a correspondence between pixels of one image and pixels of another image is established. For example<ref type="bibr" target="#b10">Saalfeld et al. (2010)</ref>perform a simpler form of registration, where images are aligned to a known template. In contrast to previous studies, our method does not require explicit knowledge of the template a priori; neither is supervision like setting of landmarks or outline selection/binarization on each image required. Instead, ShapePheno discovers and objectively quantifies deformation phenotypes on unannotated images in a fully unsupervised fashion while retaining interpretable features and results. Thus, our approach facilitates obtaining accurate non-trivial measurements on large datasets where human labor is costly and error-prone. In Section 3, we present a case study of our method on guppy fish, Poecilia reticulata. The individuals in this dataset are subject to variation in appearance and shape. Interestingly, both appearance and shape variability have previously been shown to exhibit considerable genetic components (<ref type="bibr">Tripathi et al., 2009</ref>). In Section 3.3, we describe the basic approach of quantifying shape phenotypes using our model. We demonstrate that these quantitative traits are orthogonal to traditional geometric measures (Section 3.4) and show practical utility of these traits in the context of two fundamental types of downstream analyses. First, in Section 3.5, we show how learned shape features allow for grouping the observed images into plausible similarly shaped or deformed subgroups based on characteristic deformation patterns. Second, in Section 3.6, we show that quantitative shape phenotypes can be associated to variable genetic loci in the guppy genome. This analysis serves both as a step towards obtaining further knowledge as to the underlying biological processes that lead to variation of these phenotypes as well as a natural validation step, suggesting that the automatically determined phenotypes are biologically relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head><p>Extraction and quantification of shape features is carried out in two steps. In Section 2.1, we discuss how a graphical model based on Markov random fields (MRF) can be used to simultaneously learn the unknown template and recover smooth mapping fields, performing a flexible variant of deformable registration. We decompose the mapping fields into a sum of a technical translation component and shape-related deformation fields, both specific to each image. The overall setup of our probabilistic model largely follows the jigsaw model (<ref type="bibr" target="#b6">Kannan et al., 2007</ref>). Under this model, a set of N observed images I i , i = 1,...,N is explained by a common latent template image T. The training images are explained as function of the template through learned mapping vectors L i between pixels in the template and observed pixels in each image i. The coordinate mapping accounts for an overall shift of the image with respect to the template, as well as local deformations, compressing or stretching specific parts of each image to match the common reference (Section 2.2). Subsequently, once the template and the deformation fields are learned, we extract quantitative traits from the information captured in the deformation fields. For this purpose, we employ linear dimensionality reduction (Section 2.3), yielding a compact set of features that explain the major axes of variation in the deformation fields for each image. For comparison, we also show how our model can be used to quantify length vectors within images, which can be directly related to established manual measurements of shape traits. Both types of features can be used for downstream analyses.i ={1,...,N}, where L i is a matrix with entries l i (x,y) ∈ Z 2 for each image i.</p><p>The size of template image T is set to (t (x) ×t (y)</p><p>). The model parameters μ 0 ,a,b and β define a prior on the template T. The parameters determining the smoothness prior are given as: the prior on the translation component of the mapping field is parametrized by an energy constant γ t ; the deformable component of model is defined by as based cost at saturation, γ 0 , the maximal allowed deformation distance ρ as well as α, parametrizing the order of the metric. U is describing the pixel block coupling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Markov random fields for deformable registration</head><p>Each image pixel I i (x,y) is related to the common template T, linked by a transformation M i that maps image coordinates (x,y) to the corresponding coordinates within T:</p><formula>M i (x,y) = (x,y)−l i (x,y) .</formula><formula>(1)</formula><p>The mapping is parametrized using a field of relative shifts</p><formula>L i = L i (0,0) ,...,L i (dx ,dy)</formula><p>, where contiguous (smooth) mappings of neighboring pixels corresponds to constant entries in L i. For a given mapping field, L i , the generative model (<ref type="figure" target="#fig_0">Fig. 2a</ref>) of each image is then</p><formula>I i = T M i +ψ,</formula><formula>(2)</formula><p>where M i is parameterized by L i and ψ denotes the reconstruction error of each pixel. Both, the template map T and the mapping fields L i are unknown a priori and hence need to be learned from the image data alone. For a common template T with set dimensions t (x) ,t (y) , a set of N observed images I i and corresponding mapping fields L i , the joint probability under our model is</p><formula>P(T,{I i ,L i } N i=1 ) = P(T) template prior N i=1 P(I i |T, L i ) likelihood P(L i ) mapping prior .</formula><formula>(3)</formula><p>The likelihood of the observation model corresponding to Equation (2) is a Gaussian mixture model independent for each image pixel (x,y):</p><formula>P(I i |T,L i ) = (x,y) (1−π) N I i (x,y) |μ M i (x,y) ,τ −1 M i (x,y) +π Uniform . (4)</formula><p>Here, μ and τ correspond to the means and the precisions at each position of the template image T and π is the mixture coefficient of the uniform background model to explain outliers that are not compatible with the template image. To ensure that the template is well-behaved, we choose a normal-gamma prior on the values of the template,</p><formula>P(T) = (x,y) N μ (x,y) |μ 0 ,(βτ (x,y) ) −1 Gamma(τ (x,y) |a,b).</formula><formula>(5)</formula><p>Smoothness of the mapping fields L i is encouraged through the choice of a Markov random field prior that couples neighboring mapping offsets in each image</p><formula>P(L i ) ∝ exp ⎡ ⎣ − (x,y),(x ,y )∈E (x,y) E V (l (x,y) ,l (x ,y ) ) ⎤ ⎦ .</formula><formula>(6)</formula><p>Here, E (x,y) denotes the set of pixels in the direct neighborhood of (x,y) and the pairwise energy term E V penalizes non-contiguous offsets of neighboring pixels. In ShapePheno, the mapping fields L i are decomposed into a translation and deformation component with individual smoothness priors. The specific modeling choices will be discussed in Section 2.2. Inference in the joint model implied by Equations (3–6) is feasible by means of iterative learning using expectation maximization. In this approach, we alternate between maximizing the joint probability (<ref type="bibr">Equation (3)</ref>) with respect to mapping fields L i and the unknown template T, keeping the other variables fixed. The first task, determining the most probable templatê T essentially boils down to parameter inference in a normal-gamma model, where closed form updates for mean and precisions of the template (μ,τ ) are available (<ref type="bibr" target="#b1">Bishop, 2006</ref>). Alternatively, for a fixed template T, the most probable mappingsˆLmappingsˆ mappingsˆL</p><formula>i = argmax L i p(I i |T,L i )p(L i</formula><p>) can be determined efficiently using graph cuts for each image (see<ref type="bibr" target="#b6">Kannan et al. (2007)</ref>and<ref type="bibr" target="#b2">Boykov et al. (2001)</ref>for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Design choices for MRF energy functions</head><p>Since the alignment of template and observed images requires a combination of translation and deformation, we choose L to be the sum of a translation and deformation field component L = L t +L d. In this stacked two-layer Markov random field, P(L t ) accounts for the global shift of images to the template and P(L d ) specifies the prior probability of local deformations (see also<ref type="figure" target="#fig_0">Fig. 2.1</ref>). The joint prior probability of the mapping-field components can be expressed as</p><formula>P(L,L t ,L d ) = δ(L,(L d +L t ))P(L d )P(L t ),</formula><p>where δ() denotes the Dirac delta function. Accounting for both prior contributions, the effective energy term in</p><formula>E VT (l p ,l q ) = γ t δ(l p ,l q )</formula><p>. Here, the cost parameter γ t is set to large value, such that all mappings L i are forced to take on identical values, solely accounting for a constant overall image shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Deformable model:</head><p>For the deformable prior P(L d ), we employ a non-rigid smoothness prior that encourages smooth deformation fields. In contrast to the rigid Pott's model, the energy costs is distant-dependent, favoring short-range deformations. More specifically, the energy function E VD scales linearly with a particular choice of distance norm of order α:</p><formula>E VD (l p ,l q ) = γ def l p −l q α for l p −l q α ≤ ρ ∞ otherwise.</formula><formula>(7)</formula><p>Here, ρ denotes the maximum permitted range of deformation, and α is a power (or order) where α ∈{1,2n} for n ∈ N &gt;0 and scaling parameter</p><formula>γ def = γ 0 max(1,ρ 1/α )</formula><p>. Intuitively, one can imagine this function to apply elastic bands connecting pieces (in our case pixels) to their four neighbours with the E VD part of the mapping costs being the equivalent of the elastic potential of the bands between all pieces.<ref type="figure" target="#fig_0">Figure 2b</ref>shows the energy function for two choices of α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Robustifying deformable registration:</head><p>We use various constraints on the registration to further improve the robustness of our method against noise and non-standardized images and reliably produce good results. We apply our deformation fields on pixel blocks, meaning that we constrain groups of pixels of block-size to obtain the same mapping via the prior U shown in<ref type="figure" target="#fig_0">Figure 2a</ref>. This leads to piecewise smooth deformation fields. Additionally, we constrain the parameter ρ of the deformation field itself. This makes large jumps prohibitively expensive and drives the model to use smoothly varying local deformation patches. A positive side effect of this constraint is a significant boost in computational efficiency and robustness against outlier-mistakes since the solution space is reduced. We also robustify alignments against appearance outliers with a mixture of densities used as the observation model in Equation (4), which allows for a background class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Feature representation for deformation maps</head><p>The deformation fields L d at pixel resolution, described in Section 2.2.2, capture the relevant information to explain local shape deformations of the samples in each image. Comparing deformation fields with non-equal objects at non-equal positions is hard, since we face the problem of correspondence. However, in our framework this problem can be elegantly circumvented using the common template all images are aligned to. To render individual deformation fields comparable between images, we first project these maps from observation space into the common reference coordinate system on the template. For this purpose, we apply the same mapping operator we used for image pixels now to the mappings themselves</p><formula>D i M i = L i d ,</formula><p>resulting in deformation field representations in the object-centered template space. Thus, we obtain an easily interpretable shift-corrected field D i of deformation for each image-mapping L i d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Low-rank representations of deformation fields:</head><p>In order to extract meaningful features from the high-dimensional deformation fields, we reduce their dimensionality by representing individual deformation fields via a set of coefficients over a small number of bases obtained via standard principal component analysis (PCA) (<ref type="figure" target="#fig_1">Fig. 3</ref>). Prior to running PCA, we can apply a binary mask to the template to select only relevant template regions for consideration as variance components. The resulting individual bases can be visualized and constitute local deformation factors.Here, these areas correspond to compression and stretching of fish tails and the main body (</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Geometric measurements as shape traits:</head><p>We also use our method to measure distances in the images by exploiting the mapping fields to the common template. We measure geometric traits by selecting points of interest on the template and measuring their distance per image by projecting the annotated template-points into the reconstructed image through the mapping fields. Thus, a single annotation of the shared template yields an exhaustive annotation of all images explained by the model. These annotations can be used to measure geometric distances in the images, for example. Importantly, these geometric measurements are local measurements on an image in contrast to the holistic descriptor of shape we introduce in Section 2.3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>We applied ShapePheno to a dataset that shows the lateral aspect of male guppy fish, P. reticulata. The goal was to obtain local deformation patterns that are informative about typical distortions of the shape among the individuals, which also display considerable variation of appearance traits and size. We demonstrate further use of our method in two tasks: clustering of populations according to deformation patterns and association studies to link genotypes to deformation phenotypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>The</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ShapePheno</head><p>This cross (157 Quare x Cumaná) has been subject of a genotyping project to establishing a comprehensive genetic map and to initiate conventional QTL (quantitative trait locus) mapping (<ref type="bibr">Tripathi et al., 2009</ref>). The raw images were rescaled such that the ratio of pixels to physical length is constant across the dataset. Due to rescaling, the image size was variable, ranging between 75×226 and 83×250 pixel size. To account for images taken at slightly varying distance, we chose to embed all images according to original size of the fish into empty images of the chosen format (83× 250). For this particular experiment, there were few outliers and thus setting the outlier ratio π = 0 yielded good results. For each of the 388 individuals, the dataset included matching genotype information, covering a total of 1063 genome-wide single nucleotide polymorphisms (SNPs). After filtering, removing rare SNPs with a minimum rare allele frequency &lt;5%, we obtained 814 polymorphisms that were considered for analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental settings</head><p>We chose the following parameters for the deformation model: γ 0 = 40, ρ = 10 and α = 1, which resulted in robust registration in a series of test runs. We applied the deformation field to 2×2 pixel blocks in order to locally tie together image pixels to correct for appearance differences and to prevent excessive local deformation. The normal-gamma prior parameters (Equation (5)) were set such that the prior reflects the first and second empirical moments of the distribution of the raw image pixels (see also<ref type="bibr" target="#b6">Kannan et al. (2007)</ref>). We ran a Python-based parallelized implementation of ShapePheno on an 8-core Intel Xeon machine where the full dataset could be run to convergence within 3 days. After convergence, we manually segmented the template fish from the background template to facilitate all downstream analyses (clustering and association mapping on foreground information only).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Shape phenotype determination</head><p>The converged ShapePheno model yielded a sharp template that resembles an average fish and mapping fields L i for every image in the dataset (<ref type="figure">Fig. 1</ref>). The model perceives the shapes of the fish in individual images as locally stretched or smoothly distorted versions of the template and smoothly bypasses appearance differences that would counteract shape alignment. This suggests that the deformation fields L i d capture shape information corrupted by noise stemming from the difference in sizes of images and the background color similarity to the fish corpora. Next, we used linear dimension reduction (Section 2.3.1) to determine the corresponding deformation factors of the converged model;<ref type="figure" target="#fig_1">Figure 3</ref>depicts the first three PCA-bases. These three main deformation features appear to divide the fish into the anterior and posterior part. Inflated anterior parts at the belly region as well as distorted posterior trunks are the main sources of shape variation. We also observed that local structure in the bases matches appearance features of the template that get distorted frequently. These findings reflect the set-up of the experiment in<ref type="bibr">Tripathi et al. (2009)</ref>in agreement with our expectations, as the parents were originally chosen to exhibit these shape differences and the offspring shows strong variation at these features. Supplementary<ref type="figure">Figure S1</ref>provides examples of inference results for extreme outliers within the data, here a singleton shapemutant in our training set. Since the method is unsupervised, itrequires shape mutants to be well-represented in the data in order to model their shape accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Quantification of geometric measurement accuracy</head><p>After the qualitative evaluation of the reconstructed shape template, we next characterized the accuracy of the shape representation captured by the model in a quantitative manner. For this purpose, we used the converged model to automatically measure geometric distances in images (Section 2.3.2). We comparatively evaluated eight geometric trait measurements (described in Figs 4 and 6), whose choice was motivated by primary analyses of the Guppy dataset (<ref type="bibr">Tripathi et al., 2009</ref>). Manual quantification was done on 50 individuals from our dataset chosen at random, measuring all 8 geometric distances in each raw image by 3 independent experts, as well as using the fully automated approach provided by the ShapePheno model. We assessed the correlation between manual and automated measurements, comparing the ShapePheno prediction to the mean of the manual quantification runs (<ref type="figure" target="#fig_1">Fig. 3.4</ref>). Encouragingly, all automated geometric measurements were in good agreement with the corresponding manual annotation. The correlation score for pairs of corresponding automated and manual measurements ranged between 0.65 (A2 versus M2) and 0.84 (A6 versus M6) with a mean correlation score of 0.76. To better understand the magnitude of the variation between automated and manual measurements, we also considered the pairwise correlation between two of the three manual runs (<ref type="figure">Fig. 5b</ref>), yielding comparable results. Pairwise correlations here ranged from 0.81 (M7) up to 0.96 (M3) with a mean correlation score of 0.87. This suggests that ShapePheno captures true variability in images and yields high levels of accuracy when used to quantify geometric measurements in place of a human expert. Detailed scatter plots, showing the correlations between manually and automatically determined traits are shown in<ref type="figure" target="#fig_3">Figure 6</ref>. From either of the correlation analyses, it was also notable that geometric measurements correlated well with each other, reflecting the biological relatedness of growth-phenotypes that underlie the geometric measurements under consideration. In contrast to this observation, the correlation to the new PCA-deformation phenotypes described in Section 3.3 was weak, which shows that they capture orthogonal aspects of shape variation and hence are complementary to geometric measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Clustering of populations based on deformation traits</head><p>We clustered populations of guppy fish according to their characteristic local deformation patterns, without any prior<ref type="figure">Fig. 5</ref>. Quantitative evaluation and comparison of the geometric traits as determined by ShapePheno (A1–A8), manually measured counterparts from a human expert (L1–L8) and novel PCA-deformation phenotypes (P X1 −P Y 2 ). (a) Correlation between automatic geometric measurements, manual measurements and PCA-deformation phenotypes. (b) Correlation between two manual quantification runs by an expert user (M1–M8 versus L1–L8). The results show that the reproducibility of manual expert labels is similar to automated measurements by ShapePheno, suggesting the model is able to achieve human-like results. Weak correlation between PCAdeformation phenotypes and geometric phenotypes shows that these new quantitative traits complement established measurements.where each point corresponds to an instance of the 50 samples chosen for quantification. Error bars show ±1SD, estimated separately for each quantification approach. For manual quantification, error bars correspond to the empirical variation between three independent annotation runs. For automated quantification, uncertainty estimates stem from the variation of the 15 nearest neighbor assignments on the template to the selected point and measuring their SD. The green diagonals show the expected ideal correlation. knowledge of their genetic constitution. Morphometric prototypes for the guppy have previously been determined from hand-annotated images and correlated to sex and environmental factors (<ref type="bibr" target="#b5">Hendry et al., 2006</ref>). We clustered deformation fields according to a linear kernel between low-rank projections of D i (as described in Section 2.3.1) using affinity propagation (<ref type="bibr" target="#b3">Frey and Dueck, 2007</ref>), a non-parametric clustering technique that uses deformation kernel values as inputs and yields a flexible number of clusters |C|. Reconstructing the mean low-rank vector field of each cluster given by its embedding in deformation space yields cluster-specific morphological deformation bases.<ref type="figure" target="#fig_5">Figure 7</ref>provides a comparative overview of three characteristic clusters, indicating that independent factors can influence the shape of the anterior and posterior trunk of the examined guppy fish. Deformation bases correspond to low-rank projections of the cluster means, where only the characteristic local deformations per cluster are considered as shared elements of cluster members. The shape seen in an example image representing the median deformation field per cluster corresponds to our expectation given the profiles. Different clusters portray significant variability between their profiles, such as the regional focus and the extent of expected local deformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Association study of shape factors to genotype</head><p>Finally, we performed a genome-wide association study using the previously learned phenotypes and their measurements. The phenotypic measurements y are the per-image coefficients w i of PCA-deformation bases (Sections 2.3.1 and 3.3). We used a linear model that assesses how well a particular phenotypic value is modeled when genetic factors are taken into account, compared to when they are ignored. The relevant quantity is the log-odds (LOD) score,</p><formula>log 10 ⎧ ⎨ ⎩ j P(y j |s j ,θ) P(y j |θ bck ) ⎫ ⎬ ⎭</formula><p>where s j is a SNP measurement and y j the phenotypic expression value for the j-th individual. The terms θ, θ bck are parameters for the genetic and background models, respectively. We thus obtain LOD score plots over a large genomic region to obtain an association plot. We used Storey's method (<ref type="bibr">Storey and Tibshirani, 2003</ref>), a variant of Benjamini Hochberg, to assess genome-wide significance. Although the available data has sparse genetic marker coverage, we still obtained statistically meaningful peaks as can be seen in<ref type="figure">Figure 8</ref>. Previous genetic QTL mapping in overlapping data has suggested markers of the proximal region of linkage group 12 (LG12) as relevant for size and body shape traits in male guppies, and in addition phenotypic sex has impact on these traits (<ref type="bibr">Tripathi et al., 2009</ref>). Among the significant hits in our mapping, Markers 398 (lod 7.7 on LG12) and 442 (lod 10.3, LG12) are found in the proximal region of LG12 while marker 229 (lod 11.9, LG12) is the most distal and closest to the putative male sex-determining locus. Depending on the trait analyzed, significant QTL were suggested within a region spanning ∼ 6 cM (∼ 7 Mb (<ref type="bibr">Tripathi et al., 2009</ref>)) in cross 157. Marker 442 was supported as a QTL for area of the posterior trunk for cross 158 (<ref type="bibr">Tripathi, 2009</ref>). Additional loci were detected with good statistical support, in agreement with the observation that co-factors on various linkage groups contribute to complex traits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head><p>We have proposed a generative probabilistic model that extracts deformation phenotypes by registering images to a latent, learned template in an unsupervised fashion. Our method presents a novel, clean framework for researchers to quantify and describe subtle local deformation patterns and use them for downstream analyses, like clustering or genetic association tests. We applied our method to a bioimaging task, where we discovered significant deformation patterns in images of guppy fish. We also showed that ShapePheno can be used for automated quantification of geometric measurements and showed good correspondence to manually labeled data. More important than accurate geometric traits, ShapePheno yielded deformation fields that characterize the variability in shape and could be used to identify low-rank PCA factors<ref type="figure">8</ref>. Genome-wide association plot, showing the association strengths with the first two PCA-deformation phenotypes corresponding to the X and Y direction. The significance threshold of 10% false discovery rate (FDR) is shown as thin line in the diagram. SNPs are plotted in order of linkage groups, while significant hits on LG12 as described in Section 3.6 are highlighted. LG13 contains markers with function in sex determination yielding additional informative peaks for shape determination. of shape variability. While simple distance measurements intercorrelate strongly, the deformation phenotypes we propose describe orthogonal shape factors and are thus novel holistic descriptors of shape. We showed practical utility of these PCA-deformation phenotypes in the context of clustering, grouping the data into clusters exhibiting characteristic deformation. We also performed a GWAs with the same traits, which yielded biologically sound results in agreement with previous results on geometric approximations of shape (see<ref type="bibr">Tripathi et al. (2009)</ref>and unpublished observations of C.D.). We are convinced that comprehensive genomic analyses on larger datasets can be performed by using this method with a rigorous treatment of image acquisition, higher image resolution and higher marker density. Unsupervised extraction and quantification of subtle morphological phenotypes, as done here, is the logical next step in automated image analysis. The relevance of these new types of methods is expected to rise quickly as dataset sizes increase, providing the necessary statistical power to identify and quantify complex phenotypic variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T.Karaletsos et al.</head><p>Storey,J.D. and Tibshirani,R.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig.2.</head><figDesc>Fig. 2. Illustration of deformable registration model and deformation cost function. (a) Graphical model representation of the core ShapePheno model. Observed images I 1 ,...,I N are modeled through common template image T and a coordinate mapping function M. The mapping is parametrized by smooth deformation fields L i (x,y) , denoting the coordinate offset between each image pixel and the template. The prior belief of smoothness of L is parameterized by an energy function for translation (E VT and deformation E VD ) defined on pixel blocks U. (b) Example choices of mapping energy function E VD of order α, inducing the cost of non-neighboring mappings as a function of mapping distance. At maximum allowed deformation ρ, the energy cost diverges, restricting the effective range of deformations learned by the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.3.</head><figDesc>Fig. 3. Illustration of the first three PCA-bases of deformation fields in the X and Y deformation direction. Individual highlighted image parts such as black stripes correspond to image parts with most pronounced deformation. Here, these areas correspond to compression and stretching of fish tails and the main body (Section 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.4.</head><figDesc>Fig. 4. Overview of the chosen points of interest on the fish and the template and the measured geometric phenotypes with corresponding names M1–M8. These measurements are chosen to capture shape differences between anatomically fixed points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.6.</head><figDesc>Fig. 6. Scatter plots with error bars, showing the relationship between manual measurements and automated geometric measurements as obtained by ShapePheno. Shown is data for each of the 8 length phenotypes, where each point corresponds to an instance of the 50 samples chosen for quantification. Error bars show ±1SD, estimated separately for each quantification approach. For manual quantification, error bars correspond to the empirical variation between three independent annotation runs. For automated quantification, uncertainty estimates stem from the variation of the 15 nearest neighbor assignments on the template to the selected point and measuring their SD. The green diagonals show the expected ideal correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><figDesc>Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [12:54 12/3/2012 Bioinformatics-bts081.tex] Page: 1007 1001–1008 ShapePheno</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.7.</head><figDesc>Fig. 7. Illustration of the clustering results obtained when using the PCA-deformation phenotypes from ShapePheno. From top to bottom the figure shows: exemplar of the cluster, illustrating the representative fish chosen by Affinity propagation to represent the given cluster. Deformation basis, showing the corresponding deformation of the exemplar projected onto the PCA-deformation fields used for clustering. The exemplars approximately correspond to visual categories of inflated anterior body, elongated anterior body and deformed tail, as present in the image collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig.</head><figDesc>Fig. 8. Genome-wide association plot, showing the association strengths with the first two PCA-deformation phenotypes corresponding to the X and Y direction. The significance threshold of 10% false discovery rate (FDR) is shown as thin line in the diagram. SNPs are plotted in order of linkage groups, while significant hits on LG12 as described in Section 3.6 are highlighted. LG13 contains markers with function in sex determination yielding additional informative peaks for shape determination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><figDesc>Funding: T.K. was supported by a Microsoft Research Cambridge stipend and O.S. was supported by a fellowship from the Volkswagen Foundation. Conflict of Interest: none declared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><figDesc>Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [12:54 12/3/2012 Bioinformatics-bts081.tex] Page: 1008 1001–1008</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><figDesc>(2003) Statistical significance for genomewide studies. Proc. Natl. Acad. Sci., 100, 9440–9445. Tripathi,N. et al. (2009) Genetic linkage map of the guppy, poecilia reticulata, and quantitative trait loci analysis of male size and colour variation. P. Roy. Soc. B Bio., 276, 2195–2208. Tripathi,N. (2009) PhD Thesis, University of Tuebingen. Walter,T. et al. (2010) Visualization of image data from cells to organisms. Nat. Methods, 7(Suppl. 3), S26–S41. Whibley,A.C. et al. (2006) Evolutionary paths underlying flower color variation in antirrhinum. Science, 313, 963–966.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>Summary overview of model parameters: In our model, we assume a set of N images I i and corresponding mapping fields L i of dimension (d (x) ×d (y) ) for</figDesc><table>Copyedited by: TRJ 

MANUSCRIPT CATEGORY: ORIGINAL PAPER 

[12:54 12/3/2012 Bioinformatics-bts081.tex] 
Page: 1003 1001–1008 

ShapePheno 

(a) 

(b) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><figDesc>Equation (6) becomes E V = E VD +E VT. Inference in the full model is done iteratively within the mapping updates, by first keeping L d = 0 fixed and updating L t. Next, L t is kept at the learned value while updating L d .</figDesc><table>Both update steps can be done following 
the standard jigsaw inference (Section 2.1 and Kannan et al. (2007)). In 
the following, we will explain the modeling choices of each mapping prior 
separately. Copyedited by: TRJ 

MANUSCRIPT CATEGORY: ORIGINAL PAPER 

[12:54 12/3/2012 Bioinformatics-bts081.tex] 
Page: 1004 1001–1008 

T.Karaletsos et al. 

2.2.1 Translation (rigid) model: Having defined a template that is of equal 
size as the images, the goal is to register images to it. In order to allow the 
shift field L t to incorporate translation behavior, we employ a Pott's Model 
prior with energy function </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><figDesc>Section 3). Building on the PCA basis functions, we use the corresponding coefficients for every image as a quantitative trait that characterizes the deformation field in a given sample. Formally, for a matrix of k linearized orthogonal bases of dimensionality p × (d (x) ·d (y) ) (dimensions of an image), a coefficient matrix W of dimensionality N×k and a N×(d (x) ·d (y) ) observation matrix of linearized matrices D i that contains the k-rank approximations to the observed, corrected and template-projected deformation fields D i as described in Section 2.3, the low-dimensional projections per image have the form: D i = w i ·.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><figDesc>388 available individuals are second generation progeny (F2) of two parents representing geographically and genetically distant populations whose visual appearances also differ significantly. The male parent from Cumaná (Ve) (Alexander and Breden, 2004) has a slimmer posterior trunk and brighter orange ornaments as compared to the maternal population from the Quare river in East Trinidad.</figDesc><table>Copyedited by: TRJ 

MANUSCRIPT CATEGORY: ORIGINAL PAPER 

[12:54 12/3/2012 Bioinformatics-bts081.tex] 
Page: 1005 1001–1008 

</table></figure>

			<note place="foot">© The Author 2012. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com</note>

			<note place="foot">at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">Sexual isolation and extreme morphological divergence in the cuman guppy: a possible case of incipient speciation</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">J</forename>
				<surname>Alexander</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Breden</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Evolution. Biol</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1238" to="1254" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Bishop</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Boykov</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1222" to="1239" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Clustering by passing messages between data points</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">J</forename>
				<surname>Frey</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Dueck</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">315</biblScope>
			<biblScope unit="page" from="972" to="976" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Clustering phenotype populations by genome-wide RNAi and multiparametric imaging</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Fuchs</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Syst. Biol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">370</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">Parallel evolution of the sexes? Effects of predation and habitat features on the size and shape of wild guppies</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">P</forename>
				<surname>Hendry</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Evolution. Biol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="741" to="754" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Clustering appearance and shape by learning jigsaws</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kannan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Schölkopf,B. et al.</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Toward automatic phenotyping of developing embryos from videos</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Ning</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1360" to="1371" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Ebimage—an R package for image processing with applications to cellular phenotypes</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Pau</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="979" to="981" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Bioimage informatics: a new area of engineering biology</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Peng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1827" to="1836" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">As-rigid-as-possible mosaicking and serial section registration of large system datasets</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Saalfeld</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="57" to="63" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Pattern recognition software and techniques for biological image analysis</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Shamir</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1000974</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>