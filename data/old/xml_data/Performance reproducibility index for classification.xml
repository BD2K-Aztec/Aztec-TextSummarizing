
<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/joey/Project/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.2-SNAPSHOT" ident="GROBID" when="2017-08-10T23:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data and text mining Performance reproducibility index for classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012">. 21 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Mohammadmahdi</forename>
								<forename type="middle">R</forename>
								<surname>Yousefi</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<postCode>77843</postCode>
									<settlement>College Station</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Edward</forename>
								<forename type="middle">R</forename>
								<surname>Dougherty</surname>
							</persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<postCode>77843</postCode>
									<settlement>College Station</settlement>
									<region>TX</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Computational Biology Division</orgName>
								<orgName type="institution">Translational Genomics Research Institute</orgName>
								<address>
									<postCode>85004</postCode>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data and text mining Performance reproducibility index for classification</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">28</biblScope>
							<biblScope unit="page" from="2824" to="2833"/>
							<date type="published" when="2012">. 21 2012</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/bts509</idno>
					<note type="submission">Received on July 3, 2012; revised on August 6, 2012; accepted on August 10, 2012</note>
					<note>BIOINFORMATICS ORIGINAL PAPER Associate Editor: Martin Bishop Contact: edward@ece.tamu.edu Supplementary Information: Supplementary data are available at Bioinformatics online.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Motivation: A common practice in biomarker discovery is to decide whether a large laboratory experiment should be carried out based on the results of a preliminary study on a small set of specimens. Consideration of the efficacy of this approach motivates the introduction of a probabilistic measure, for whether a classifier showing promising results in a small-sample preliminary study will perform similarly on a large independent sample. Given the error estimate from the preliminary study, if the probability of reproducible error is low, then there is really no purpose in substantially allocating more resources to a large follow-on study. Indeed, if the probability of the preliminary study providing likely reproducible results is small, then why even perform the preliminary study? Results: This article introduces a reproducibility index for classification , measuring the probability that a sufficiently small error estimate on a small sample will motivate a large follow-on study. We provide a simulation study based on synthetic distribution models that possess known intrinsic classification difficulties and emulate real-world scenarios. We also set up similar simulations on four real datasets to show the consistency of results. The reproducibility indices for different dis-tributional models, real datasets and classification schemes are empirically calculated. The effects of reporting and multiple-rule biases on the reproducibility index are also analyzed. Availability: We have implemented in C code the synthetic data distribution model, classification rules, feature selection routine and error estimation methods. The source code is available at http://gsp.tamu .edu/Publications/supplementary/yousefi12a/. Supplementary simulation results are also included.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Perhaps no problem in translational genomics has received more attention than the discovery of biomarkers for phenotypic discrimination. To date, there has been little success in developing clinically useful biomarkers and much has been said concerning the lack of reproducibility in biomarker discovery (<ref type="bibr" target="#b2">Boulesteix and Slawski, 2009;</ref><ref type="bibr" target="#b23">Ioannidis, 2005;</ref><ref type="bibr" target="#b28">Sabel et al., 2011;</ref><ref type="bibr" target="#b33">Zhang et al., 2008</ref>). In particular, recently a report concerning comments made by US Food and Drug Administration (FDA) drug division head Janet Woodcock stated: Janet Woodcock, drug division head at the FDA, this week expressed cautious optimism for the future of personalized drug development, noting that 'we may be out of the general skepticism phase, but we're in the long slog phase.. .'. The 'major barrier' to personalized medicine, as Woodcock sees it is 'coming up with the right diagnostics'. The reason for this problem is the dearth of valid biomarkers linked to disease prognosis and drug response. Based on conversations Woodcock has had with genomics researchers, she estimated that as much as 75% of published biomarker associations are not replicable. 'This poses a huge challenge for industry in biomarker identification and diagnostics development', she said (<ref type="bibr" target="#b27">Ray, 2011</ref>). Evaluating the consistency of biomarker discoveries across different platforms, experiments and datasets has attracted the attention of researchers. The studies addressing this issue mainly revolve around the reproducibility of signals (for example, lists of differentially expressed genes), their significance scores and rankings in a prepared list. They try to answer the following question: Do the same genes appear differentially expressed when the experiment is re-run?<ref type="bibr" target="#b2">Boulesteix and Slawski (2009</ref><ref type="bibr" target="#b25">), Li et al. (2011</ref><ref type="bibr" target="#b34">), Zhang et al. (2009</ref>and the references therein suggest several solutions to this and related questions. Our interest is different. A prototypical reproducibility paradigm arises when a classifier is designed on a preliminary study based on a small sample, and, based on promising reported results, a follow-on study is performed using a large independent data sample to check whether the classifier performs well as reported in the preliminary study. Many issues affect reproducibility, including the measurement platform, specimen handling, data normalization and sample compatibility between the original and subsequent studies. These may be categorized as laboratory issues; note that here we are not talking about the issue of providing access to data and software for follow-up studies on published results (<ref type="bibr" target="#b22">Hothorn and Leisch, 2011</ref>). One can conjecture mitigation of these issues as laboratory technique improves; however, there is a more fundamental methodological issue, namely, error estimation. In particular, inaccurate error estimation can lead to 'overoptimism' in reported results (<ref type="bibr" target="#b1">Boulesteix, 2010;</ref><ref type="bibr" target="#b5">Castaldi et al., 2011</ref>).The typical analysis proceeds in the following fashion: (i) based on the data, a feature set is chosen from the 30 000; (ii) a classifier is designed, with feature selection perhaps being performed in conjunction with classifier design and (iii) the classification error is measured by some procedure using the same sample data upon which feature selection and classifier design have been performed. Given no lack of reproducibility owing to laboratory issues, if the error estimate is sufficiently deemed small and a follow-on study with 1000 independent data specimens is carried out, can we expect the preliminary error estimate on a sample of 50 to be reproduced on a test sample of size 1000? Since the root-mean-square (RMS) error between the true and estimated errors for independent-test-data error estimation is bounded by ð2 ffiffiffiffi m p Þ À1 , where m is the size of the test sample (<ref type="bibr" target="#b12">Devroye et al., 1996</ref>), a test sample of 1000 insures RMS 0:016, so that the test-sample estimate can be taken as the true error. There are two fundamental related questions (Dougherty, 2012): (i) Given the reported estimate from the preliminary study, is it prudent to commit large resources to the follow-on study in the hope that a new biomarker diagnostic will result? (ii) Prior to that, is it possible that the preliminary study can obtain an error estimate that would warrant a decision to perform a follow-on study? A large follow-on study requires substantially more resources than those required for a preliminary study. If the preliminary study has a very low probability of producing reproducible results, then there is really no purpose in doing it. We propose a reproducibility index that simultaneously addresses both questions posed earlier. Our focal point is not that independent validation data should be used—this has been well argued, for instance, in the context of bioinformatics to avoid overoptimism (<ref type="bibr" target="#b24">Jelizarow et al., 2010</ref>); rather, the issue addressed by the reproducibility index is the efficacy of small-sample preliminary studies to determine whether a large validating study should be performed. We set up a simulation study on synthetic models that emulate real-world scenarios and on some real datasets. We calculate the reproducibility index for different distributional models (and real datasets) and classification schemes. We consider two other scenarios: (i) multiple independent preliminary studies with small samples are carried out and only the best results (minimum errors) reported and (ii) multiple classification schemes are applied to the preliminary study with small samples and only the results (minimum errors) of the best classfier are reported. A decision is made for a large follow-on study because the reported errors show very good performance.<ref type="bibr" target="#b30">Yousefi et al. (2010</ref><ref type="bibr" target="#b31">Yousefi et al. ( , 2011</ref>show that there is a poor statistical relationship between the reported results and true classifier performance in these scenarios, namely, there is a potential for significant optimistic 'reporting bias' or 'multiple-rule bias'. These two biases can substantially impact the reproducibility index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SYSTEMS AND METHODS</head><p>We define a classifier rule model as a pair ðÉ, ÄÞ, where É is a classification rule, possibly including feature selection, and Ä is a training-data error estimation rule on a feature-label distribution F. Given a random sample S n of size n drawn from F, the designed classifier is n ¼ ÉðS n Þ: The true error of n is given by " n ¼ Pð n ðXÞ 6 ¼ YÞ, where ðX, YÞ is a feature-label pair. The error estimation rule Ä provides an error estimate, ^ " n ¼ ÄðS n Þ, for n : To characterize reproducibility, we postulate a preliminary study in which a classifier, n , is designed from a sample of size n and its error is estimated. We say that the original study is reproducible with accuracy ! 0 if " n ^ " n þ : One could require that the true error lies in an interval about the estimated error, but our interest is in whether the proposed classifier is as good as claimed in the original study, which means that we only care whether its true performance is below some tolerable bound of the small-sample estimated error. Given a preliminary study, not any error estimate will lead to a follow-on study: the estimate has to be sufficiently small to motivate the follow-on. This means there is a threshold, , such that the second study occurs if and only if ^ " n : We define the reproducibility index by R n ð, Þ ¼ Pð" n ^ " n þ j ^ " n Þ: R n ð, Þ depends on the classification rule, É, the error estimation rule, Ä, and the feature-label distribution F. Clearly, 1 2 implies R n ð 1 , Þ R n ð 2 , Þ: If jj" n À ^ " n jj almost surely, meaning Pðjj" n À ^ " n jj Þ ¼ 1, then R n ð, Þ ¼ 1 for all : This means that, if the true and estimated errors are sufficiently close, then, no matter the decision threshold, the reproducibility index is 1. In practice, this ideal situation does not occur. Often, the true error greatly exceeds the estimated error when the latter is small, thereby driving down R n ð, Þ for small : We are interested in the relationship between reproducibility and classification difficulty. Fixing É and Ä makes R n ð, Þ dependent on F. If we parameterize F, and call it FðÞ, then we can write the reproducibility index as R n ð, ; Þ: If we select so there is a 1-1 monotonic relation between and the Bayes error, " bay , then there is a direct relationship between reproducibility and intrinsic classification difficulty, R n ð, ; " bay Þ: If we know the joint distribution between the true and estimated errors, then the entire analysis can be done analytically, for instance, in the case of the 1D Gaussian model with linear discriminant analysis (LDA) classification and leave-one-out (LOO) error estimation (<ref type="bibr" target="#b35">Zollanvari et al., 2010</ref>). Fixing the variances and letting the means be zero and gives the desired scenario. We can now analytically derive the reproducibility index R n ð, ; Þ: Unfortunately, there are very few distributions for which the joint distribution between the true and estimated errors is known. If not, then we use simulation to compute R n ð, ; Þ:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Analysis</head><p>To analyze the reproducibility index as a function of and tie this to the joint distribution of the true and estimated errors, we expand R n ð, Þ to obtain R n ð, Þ 1 1 þ Pð ^ "n5"nÀ Þ Pð"nÀ ^ "n Þ :</p><p>Consider the special case in which ¼ 0, and let rðÞ denote the probability fraction in the denominator. Then we have the upper bound R n ð0, Þ " n on ^ " n is a horizontal line, which has been observed approximately in many situations (<ref type="bibr" target="#b19">Hanczar et al., 2007</ref><ref type="bibr" target="#b20">Hanczar et al., , 2010</ref>). This means that " n and ^ " n are uncorrelated, which have also been observed for many cases. Then, let us approximate the resulting joint distribution by a Gaussian distribution with common mean (unbiased estimation) and ¼ 0 (according to our assumptions). Finally, let us assume that the standard deviation of " n is less than the standard deviation of ^ " n , as is typically the case for cross-validation. Letting fð" n , ^ " n Þ denote the joint Gaussian density,<ref type="figure" target="#fig_1">Figure 1</ref>shows a pictorial of a single-level cut of the joint distribution, along with the horizontal regression line and the " n ¼ ^ " n axis. Relative to the level cut, the dark gray and light gray regions correspond to the regions of integration for the numerator and denominator, respectively, of rðÞ: It is clear that for small , rðÞ becomes large, thereby making R n ð0, Þ small.</p><formula>rðÞ ¼ R 0 R "n 0 fð" n , ^ " n Þd ^ " n d" n R 0 R ^ "n 0 fð" n , ^ " n Þd" n d ^ " n :</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Synthetic model</head><p>A model is adopted for generating synthetic data, which is built upon parameterized multivariate distributions, each representing a class of observations (phenotype, prognosis condition, etc.). The model is designed to reflect a scenario in which there are subnetworks (pathways) for which genes within a given subnetwork are correlated but there is no (negligible) correlation between genes in different subnetworks. The situation is modeled by assuming a block covariance matrix (<ref type="bibr" target="#b20">Hanczar et al., 2010;</ref><ref type="bibr" target="#b21">Hua et al., 2005;</ref><ref type="bibr" target="#b30">Yousefi et al., 2010</ref><ref type="bibr" target="#b31">Yousefi et al., , 2011</ref>). Sample points are generated from two equally likely classes, Y ¼ 0 and Y ¼ 1 with d features. Therefore, each sample point is specified by a feature vector X 2 R d and a label Y 2 f0, 1g: The class conditional densities are multivariate Gaussian with fðxjY ¼ yÞ $ N d ð y , 2 AE y Þ, for y ¼ 0, 1, where 0 ¼ ½0, 0, 0,. .. , 0 T and 1 ¼ ½0, 0, 0,. .. ,  T are d Â 1 column vectors (for d ¼ 1, we have 0 ¼ 0 and 1 ¼ ), and AE y is a d Â d block matrix with off-diagonal block matrices equal to 0 and l Â l on-diagonal block matrices AE y being 1 on the diagonal and y off the diagonal. Three classes of Bayes optimal classifiers can be defined depending on 0 and 1 : If the features are uncorrelated, i.e. 0 ¼ 1 ¼ 0, the Bayes classifier takes its simplest form: a future point is assigned to the class to which it has the closest Euclidian distance. When 0 ¼ 1 6 ¼ 0, the Bayes classifier is a hyperplane in R d , which must pass through the midpoint between the means of two classes. If 0 6 ¼ 1 , the Bayes classifier takes a quadratic form, and decision surfaces are hyperquadrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Real data</head><p>We consider four microarray real datasets, each having more than 150 arrays: pediatric acute lymphoblastic leukemia (ALL) (<ref type="bibr" target="#b29">Yeoh et al., 2002</ref>), multiple myeloma (<ref type="bibr" target="#b32">Zhan et al., 2006</ref>), hepatocellular carcinoma (HCC) (<ref type="bibr" target="#b6">Chen et al., 2004</ref>), and a dataset for drugs and toxicant response on rats (<ref type="bibr" target="#b26">Natsoulis et al., 2005</ref>). We follow the data preparation instructions reported in the cited articles.<ref type="figure" target="#tab_1">Table 1</ref>provides a summary of the four real datasets. A detailed description can be found in the Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Classifier rule models</head><p>Three classification rules, two linear and one non-linear, are considered: LDA, linear support vector machine (L-SVM) and radial basis function SVM (RBF-SVM). Three error estimation methods are considered: 0.632 bootstrap, LOO and 5-fold cross-validation (5F-CV). In total, we have nine classifier rule models. LDA is a plug-in rule for the optimal classifier in a Gaussian model with common covariance matrix. The sample means and pooled sample covariance matrix obtained from the data are plugged into the discriminant. Assuming equally likely classes, LDA assigns a sample point x to class 1 if and only if</p><formula>ðx À ^ 1 Þ T ^ AE À1 ðx À ^ 1 Þ ðx À ^ 0 Þ T ^ AE À1 ðx À ^ 0 Þ, where ^ y</formula><p>is the sample mean for class y 2 f0, 1g, and ^ AE is the pooled sample covariance matrix. LDA usually provides good results even when the assumptions of Gaussianity with common covariances are mildly violated. Given a set of training sample points, the goal of support vector machine classifier is to find a maximal margin hyperplane. When the data are not linearly separable, one can introduce some slack variables in the optimization procedure allowing for mislabeled sample points and solve the dual problem. This classifier is called L-SVM, which is essentially a hyperplane in the feature space. Alternatively, using a transformation the data canbe projected into a higher-dimensional space, where it becomes linearly separable. One can avoid using the transformation and work with a kernel function that is expressible as an inner product in a feature space. The equivalent classifier back in the original feature space will generally be non-linear (<ref type="bibr" target="#b0">Boser et al., 1992;</ref><ref type="bibr" target="#b7">Cortes and Vapnik, 1995</ref>). When the kernel function is a Gaussian radial basis function, the corresponding classifier is referred to as RBF-SVM. In general, the 0.632 bootstrap error estimator can be written as, ^ " boot ¼ 0:368 ^ " resub þ 0:632 ^ " zero ,</p><p>where ^ " resub and ^ " zero are the resubstitution and bootstrap zero estimators. The resubstitution uses the empirical distribution by putting mass 1/n on each of the n sample points in the original data. A bootstrap sample is made by drawing n equally likely points with replacement from the original data. A classifier is designed on the bootstrap sample, and its error is calculated by counting the misclassified original sample points not in the bootstrap sample. The basic bootstrap zero estimator is the expectation of this error with respect to the bootstrap sampling distribution. This expectation is usually approximated by a Monte-Carlo estimate based on a number of independent bootstrap samples (between 25 and 200 is typical, we use 100). In 5F-CV, the sample S n is randomly partitioned into five folds S ðiÞ n , for i ¼ 1, 2,.. ., 5. Each fold is held out of the classifier design process in turn as the test set, a (surrogate) classifier ðiÞ n is designed on the remaining sets S n n S ðiÞ n , and the error of ðiÞ n is estimated by counting the misclassified sample points in S ðiÞ n : The 5F-CV estimate is the average error counted on all folds. Beside the variance arising from the sampling process, there is 'internal variance' resulting from the random selection of the partitions. To reduce this variance, we consider 5F-CV with 10 repetitions, meaning that we also average the cross-validation estimates of 10 randomly generated partitions over S n : LOO error estimation is a special case of cross-validation with n folds, where each fold consists of a single point. Therefore, LOO has no internal variance since there is only a single way to partition the data into n folds. With small samples, cross-validation tends to be inaccurate owing to high overall variance (Braga<ref type="bibr" target="#b3">Neto and Dougherty, 2004</ref>) and poor correlation with the true error (<ref type="bibr" target="#b19">Hanczar et al., 2007</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Simulation design</head><p>For the synthetic data, we assume that the features have multivariate Gaussian distributions as described in Section 2.2. We choose d 2 f1, 2, 5, 10, 15g, l ¼ d if d55 and l ¼ 5 if d ! 5: We also assume that ¼ 0:6 and the pair f 0 , 1 g takes three different values: {0, 0}, {0.8, 0.8} and {0.4, 0.8}. For fixed , f 0 , 1 g and d, we choose so that the Bayes error equals some desired values; specifically, from 0.025 to 0.4 (or the maximum possible value depending on f 0 , 1 g) with increments of 0.025. This will define a large class of different distribution models in our simulation. From each distribution model, we also generate random samples of size 30, 60 and 120 (half from each class) to emulate real-world problems, where only a small number of sample points are available. Due to the large number of simulations in this study, we have limited the dimension of the cases studied; however, the reproducibility index is not limited by dimension and, owing to the increased estimation variance, one can expect that, with larger dimensions, reproducibility can be expected to be even more problematic in such circumstances. For each model, we generate 10 000 random samples. For each sample, the true and estimated error pairs of all classifier rule models are calculated. The true errors of the designed classifiers are found exactly if analytical expressions are available. Otherwise, they are calculated via a very large independent sample (10 000 points) generated from the same distribution model. For each 2 f0:0005, 0:01, 0:05, 0:1g, 2 f0, 1=60, 2=60,. .. , 0:5g and classifier rule model, we empirically calculate R n ð, ; " bay Þ from 10 000 true and estimated error pairs. The real-data simulation is essentially the same as for the synthetic data, except that each real dataset now serves as a high-dimensional distribution model. Thus, there is a need for feature selection, which is part of the classification rule. Another difference is in calculating the true error: at each iteration, n ¼ 60 sample points are randomly picked for training, and a feature-selection step is carried out where d ¼ 5 features with highest t-scores are selected. Then a classifier is designed and its error estimated. The remaining held-out sample points are used to calculate the true error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS AND DISCUSSION</head><p>The complete set of simulation results can be found in the companion website of this article, including graphs for the joint distributions and reproducibility indices for different distribution models, real datasets and classifier rule models. Here, we provide some results that represent the general trends observed in the simulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Joint distribution</head><p>The joint distributions between ^ " n and " n are estimated with a density estimation method that uses bivariate Gaussian kernels. Here we present the results for only two synthetic distribution models with d ¼ 5 features and different sample sizes. For the first model, the class-conditional covariance matrices are equal and the features are uncorrelated. The target Bayes error is set to 0.2, being equivalent to ¼ 1:0. The results are shown for LDA and 5F-CV in<ref type="figure">Figure 2</ref>(a–c). For the second model, the class-conditional covariance matrices are assumed to be unequal and the features are correlated (f 0 , 1 g ¼ f0:4, 0:8g). The target Bayes error is 0.1, which results in ¼ 0:82.<ref type="figure">Figure 2</ref>(d–f) shows the corresponding graphs when RBF-SVM and LOO are used. Each plot also includes the regression line (dotted) and a small circle, indicating the sample mean of the joint distribution. Lack of regression and correlation, slightly high-bias and very high-variance for the estimated error are evident for small sample sizes. These graphs, which are consistent with ones in previous studies (<ref type="bibr" target="#b16">Dougherty et al., 2010;</ref><ref type="bibr" target="#b19">Hanczar et al., 2007</ref>), show a resemblance to<ref type="figure" target="#fig_1">Figure 1</ref>, indicating that our analysis in Section 2.1 is suitable for the synthetic model. The expected true errors for different classification rules applied to different real datasets are listed in<ref type="figure">Table 2</ref>. Similar to the synthetic data, the joint distributions for the real data are estimated using a bivariate Gaussian-kernel density estimation method. Here we only present the joint distribution results for the multiple myeloma dataset (<ref type="bibr" target="#b32">Zhan et al., 2006</ref>) with LDA as the classification rule in<ref type="figure" target="#fig_2">Figure 3</ref>(a–c), and for the HCC dataset (<ref type="bibr" target="#b6">Chen et al., 2004</ref>), when RBF-SVM is used, in<ref type="figure" target="#fig_2">Figure 3</ref>(d–f). In all cases, correlation between the true and estimated errors is small in absolute value and negative, the regression line having negative slope, which means that the conditional expectation of the true error decreases as the estimate increases. This behavior is not anomalous (<ref type="bibr" target="#b16">Dougherty et al., 2010</ref>); indeed, a negative correlation has been shown analytically for LOO with discrete classification (Braga<ref type="bibr" target="#b4">Neto and Dougherty, 2010</ref>).<ref type="figure">Figure 4</ref>shows the reproducibility index as a function of ð, " bay Þ for different sample sizes and : We assume d ¼ 5 uncorrelated features with equal class-conditional covariance matrices. The classification rule is LDA and error estimation is 5F-CV. Note that LDA is a consistent classification rule for this distribution model. As the Bayes error increases, a higher reproducibility index is achieved only for larger ; however, for ¼ 0:0005, which is close to zero, the upper bound for the reproducibility index is about 0.5, which is consistent with our analysis in Section 2.1. It is also notable that the rate of change in the reproducibility index gets slower for higher Bayes error and smaller sample size. Even though the rate of change in the reproducibility index is faster for sample size 120, the maximum is almost identical to what we have for sample size 60. This phenomenon can be attributed to the difficulty of classification (for higher Bayes error), high-variance of the error estimate, flat regression and lack of correlation between the true and estimated errors due to the small-sample nature of the problem. An irony appears in<ref type="figure">Figure 4</ref>when one tries to be prudent by only doing a large-scale experiment if the estimated error is small<ref type="figure">Fig. 2</ref>. Joint distribution of the true and estimated errors for n ¼ 30, 60, 120, d ¼ 5, two classification rules (LDA and RBF-SVM) and two error estimation methods (5F-CV and LOO). The covariance matrices are equal with features uncorrelated for LDA and unequal with features correlated for RBF-SVM: (a) n ¼ 30, LDA, 5F-CV, " bay ¼ 0:2; (b) n ¼ 60, LDA, 5F-CV, " bay ¼ 0:2; (c) n ¼ 120, LDA, 5F-CV, " bay ¼ 0:2; (d) n ¼ 30, RBF-SVM, LOO, " bay ¼ 0:1; (e) n ¼ 60, RBF-SVM, LOO, " bay ¼ 0:1; (f) n ¼ 120, RBF-SVM, LOO, " bay ¼ 0:1: The white line shows the " n ¼ ^ " n axis, the dotted line shows the regression line and the circle indicates the sample mean of the joint distribution<ref type="figure">Fig. 4</ref>. Reproducibility index for n ¼ 60, 120, LDA classification rule and 5F-CV error estimation. d ¼ 5 and the covariance matrices are equal with features uncorrelated. (a) n ¼ 60, ¼ 0:0005; (b) n ¼ 60, ¼ 0:05; (c) n ¼ 120, ¼ 0:0005; (d) n ¼ 120, ¼ 0:05in the preliminary study, that is, only proceeding to a follow-on study if ^ " n and is small. From the ridges in the figure, we see that the reproducibility index drops off once is chosen below a certain point. While small decreases the likelihood of a follow-on study, it increases the likelihood that the preliminary results are not reproducible. Seeming prudence is undermined by poor error estimation. For larger , reproducibility improves; however, for " bay ¼ 0:2 and 60 sample points, which is very typical in real-world classification problems, even for very large , say ¼ 0:3, we have R 60 ð0:05, 0:3Þ ¼ 0:832:<ref type="figure">Figure 5</ref>shows the results for the case of a distribution model with correlated features, unequal covariance matrices and ¼ 0:01: The classification rules are LDA and RBF-SVM. 5F-CV serves as the error estimation rule. LDA is no longer a consistent rule for this model, and we expect RBF-SVM to produce classifiers that, on average, perform better. If so, we would then expect the reproducibility index to be better for RBF-SVM because lower Bayes error usually means more accurate cross-validation error estimation, at least for the Gaussian model (<ref type="bibr" target="#b15">Dougherty et al., 2011</ref>). The graphs confirm this: as the reproducibility index for higher Bayes error is uniformly (slightly) better for RBF-SVM. The improvement is notable for RBF-SVM, compared with LDA, for larger sample size: for " bay ¼ 0:1725, we have R 120 ð0:01, 0:3Þ ¼ 0:575 for RBFSVM while R 120 ð0:01, 0:3Þ ¼ 0:244 for LDA.<ref type="figure">Figure 6</ref>presents the reproducibility index results for the real data when LDA and RBF-SVM are used, and their errors are estimated with 5F-CV and LOO. The trends are very similar to those in the synthetic data. The reproducibility index for 5F-CV is highly variable among datasets, specifically the datasets with higher expected true error have lower reproducibility index for small-to mid-range values of ð, Þ: The situation is worse for LOO due to its high variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reproducibility index</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reporting bias effect</head><p>Suppose that a study has tested a proposed classification rule on several datasets and reported only the best results, i.e. the ones on the datasets with the lowest estimated errors.<ref type="bibr" target="#b30">Yousefi et al. (2010)</ref>have shown that, for a very large class of problems, this practice introduces a major source of (reporting) bias to the results. Let fS 1 n , S 2 n ,. .. , S m n g be a family of m i.i.d. samples of size n, randomly drawn from a single distribution. Given a fixed classifier rule model, for each S i n , a classifier is designed, its error estimated and the true error of the designed classifier is also calculated. Assume that instead of reporting all the estimated errors, only the minimum estimated error is reported:</p><formula>^ " min n ¼ minf ^ " 1 n , ^ " 2 n ,. .. , ^ " m n g: Letting S i min n</formula><p>denote the sample on which the minimum error estimate occurs, the corresponding true error is then " i min n : In this case, the reported estimated error is ^ " min n for the dataset S i min n. Hence, the reproducibility index is computed for the pair " i min n and ^ " min n , and the reported study is reproducible with accuracy ! 0 from the m performed studies if</p><formula>" i min n ^ " min n þ :</formula><p>The reproducibility index for m independent datasets takes the form</p><formula>R m n ð, Þ ¼ Pð" i min n ^ " min n þ j ^ " min n Þ:</formula><p>R m n ð, Þ depends on the number of datasets, the classification rule, the error estimation rule and the feature-label distribution,<ref type="figure">Fig. 6</ref>. Reproducibility index for the real datasets, two classification rules (LDA and RBF-SVM) and two error estimation methods (5F-CV and LOO). The training sample size is 60, and d ¼ 5 features are selected using t-test feature selection method: (a) LDA, 5F-CV; (b) RBF-SVM, 5F-CV; (c) LDA, LOO; (d) RBF-SVM, LOO<ref type="figure">Fig. 5</ref>. Reproducibility index for ¼ 0:01, n ¼ 60, 120, two classification rules (LDA and RBF-SVM) and 5F-CV error estimation. d ¼ 5 and the covariance matrices are unequal with features correlated ( 0 ¼ 0:4 andthese being m, É, Ä, and F, respectively. Quantities such as R m n ð, ; Þ and R m n ð, ; " bay Þ are defined before. To illustrate the effect of reporting bias, for a fixed m 2 f1, 2,. .. , 5g, we randomly draw m pairs from the previously generated 10 000 error pairs. The minimum estimated error and its corresponding true error are found and recorded. This process is repeated to generate 10 000 new error pairs. Now similar to R n ð, ; " bay Þ, we calculate R m n ð, ; " bay Þ for each m, , and classifier rule model. Figures 7 and 8 show the effect of reporting bias on the reproducibility index for m ¼ 2, 5, d ¼ 5, LDA and 5F-CV when the covariance matrices are equal and the features are uncorrelated. Compare<ref type="figure">Figure 8</ref>with<ref type="figure">Figure 4</ref>. Strikingly, but not surprisingly, we do not need more than m ¼ 5 samples to observe a rapid drop (almost half) of reproducibility for ¼ 0:05 as the Bayes error and increase. Moreover, for ¼ 0:0005, the reproducibility index is almost zero independent of the sample size. As m increases, the reporting bias, E Sn ½" i min n À ^ " min n , also increases. Pictorially, the wide flat distribution in<ref type="figure" target="#fig_1">Figure 1</ref>becomes more circular with smaller variance and gets shifted to the left side of the " n ¼ ^ " n axis. Thus, the probability that</p><formula>" i min n is smaller than ^ " min n þ diminishes to 0 even though ^ " min n for all :</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multiple-rule bias effect</head><p>Suppose r classification rules are considered in the preliminary study and only the results of the best one are considered. In this case, a random small sample is drawn from the feature-label distribution F, and r classifiers are designed. Assuming F is unknown, the errors of the designed classifiers are estimated from sample data using s different error estimation methods, and the classification rule leading to the classifier with minimum estimated error is chosen as 'best'. This practice has been shown to introduce substantial optimistic bias (<ref type="bibr" target="#b31">Yousefi et al., 2011</ref>). Denote r classification rules by É 1 , É 2 ,. .. , É r , and s error estimation rules by Ä 1 , Ä 2 ,. .. , Ä s : In total, there are m ¼ rs classifier rule models:</p><formula>ðÉ 1 , Ä 1 Þ, ðÉ 1 , Ä 2 Þ,. .. , ðÉ 1 , Ä s Þ, ðÉ 2 , Ä 1 Þ, ðÉ 2 , Ä 2 Þ,. .. , ðÉ r , Ä s Þ</formula><p>: Given a random sample S n drawn from F, the classification rules yield r designed classifiers:</p><formula>i ¼ É i ðS n Þ for i ¼ 1, 2</formula><p>,.. ., r. The true error of i is denoted by " i n : Let ^ " i, j n denote the jth estimated error for i , where j ¼ 1, 2,.. ., s. The minimum estimated error is</p><formula>^ " min n ¼ minf ^ " 1, 1 n , ^ " 1, 2 n ,. .. , ^ " 1, s n , ^ " 2, 1 n ,. .. , ^ " r, s n g:</formula><p>Letting i min and j min denote the classifier number and error estimator number, respectively, for which the error estimate is minimum, we have ^</p><formula>" min n ¼ ^ " i min , j min n :</formula><p>The corresponding true error is then " imin n :</p><p>The reproducibility index is now computed for the pair " i min n and ^ " min n and the reported study is reproducible with accuracy ! 0 from the m performed studies if</p><formula>" i min n ^ " min n þ :</formula><p>The reproducibility index for m classifier rule models is defined by</p><formula>R m n ð, Þ ¼ Pð" imin n ^ " min n þ j ^ " min n Þ:</formula><p>Quantities such as R m n ð, ; Þ and R m n ð, ; " bay Þ are defined as before. We use the original true and estimated error pairs described in Section 2.5 and consider three classification rules (LDA, L-SVM and RBF-SVM) and three error estimation methods (0.632 bootstrap, LOO and 5F-CV). Therefore, we can have r ¼ 1, 2, 3. We generate all 3 r À Á possible collections of classification rules of size r, each associated with three error estimation rules, resulting in 3 r À Á collections of classifier rule models of size m ¼ 3r. For each collection of size m, we find the true and estimated error pairs from the original error pairs and record the minimum estimated error and its corresponding true error. We repeat this process 10 000 times. Now, similar to R n ð, ; " bay Þ, we calculate R m n ð, ; " bay Þ for each m, and :<ref type="figure">Figure 9</ref>shows the effect of multiple-rule bias on the reproducibility index for m ¼ 3, d ¼ 5, LDA and 5F-CV when the covariance matrices are equal and the features are uncorrelated. The cases for m ¼ 6, 9 are given on the companion website. Similar observations to those of reporting bias can be made here. The reproducibility index decreases for increasing m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Application methodology</head><p>Application of the reproducibility index in practice requires that the defining probability be computed, or at least approximated,<ref type="figure">Fig. 7</ref>. Reporting bias effect on the reproducibility index for m ¼ 2, n ¼ 60, 120, LDA classification rule and 5F-CV error estimation. d ¼ 5 and the covariance matrices are equal with features uncorrelated: (a) n ¼ 60, ¼ 0:0005; (b) n ¼ 60, ¼ 0:05; (c) n ¼ 120, ¼ 0:0005; (d) n ¼ 120, ¼ 0:05<ref type="figure">Fig. 8</ref>. Reporting bias effect on the reproducibility index for m ¼ 5, n ¼ 60, 120, LDA classification rule and 5F-CV error estimation. d ¼ 5 and the covariance matrices are equal with features uncorrelated: (a) n ¼ 60, ¼ 0:0005; (b) n ¼ 60, ¼ 0:05; (c) n ¼ 120, ¼ 0:0005; (d) n ¼ 120, ¼ 0:05 beforehand. This requires prior knowledge regarding the feature-label distribution. If the feature-label distribution was known and the corresponding theory regarding the joint distribution of the true and estimated errors developed, then R n ð, ; Þ could be directly computed for different values of n, and : For instance, in the case of LDA in the Gaussian model with known covariance matrix, the joint distribution is known exactly in the univariate case and can be approximated in the multivariate case (<ref type="bibr" target="#b35">Zollanvari et al., 2010</ref>). Of course, if the feature-label distribution was known, then there would be no reason to collect any data; just derive the Bayes classifier from the model. Thus, when we speak of prior knowledge, we mean the assumption that the feature-label distribution belongs to an uncertainty class of feature-label distributions. Considering our earlier remarks about parameterizing the feature-label distribution by , thereby treating it as FðÞ, the uncertainty class can be denoted by Â, with each 2 Â determining a possible feature-label distribution. Furthermore, taking a Bayesian perspective, we can put a prior distribution, ðÞ, perhaps non-informative, on Â: Assuming an uncertainty class in the case of reproducibility is pragmatic because reproducibility concerns error-estimation accuracy and virtually nothing practical can be said concerning error-estimation accuracy in the absence of prior knowledge (<ref type="bibr" target="#b15">Dougherty et al., 2011</ref>). For instance, the most common measure of error-estimation accuracy is the RMS between the true and estimated errors, and, without distributional assumptions, the RMS cannot be usefully bounded in the case of trainingdata-based error estimators unless the sample size is very large, well beyond practical biological circumstances and beyond what is needed to split the data into training and testing data. As noted by Fisher in 1925, 'Only by systematically tackling small sample problems on their merits does it seem possible to apply accurate tests to practical data' (<ref type="bibr" target="#b18">Fisher, 1925</ref>). Large-sample bounds do not help. Owing to this limitation, optimal error estimation relative to a prior distribution on an uncertainty class of feature-label distributions has been developed (<ref type="bibr" target="#b8">Dalton and Dougherty, 2011a</ref>) and applied in gene-expression classification (<ref type="bibr" target="#b9">Dalton and Dougherty, 2011b</ref>). Given an uncertainty class and prior distribution, the problem is to ensure a desired level of reproducibility before experimental design; that is, determine n, and so that the desired level is achieved. A conservative approach would be to ensure min 2Â R n ð, ; Þ4r, where r is the desired level of reproducibility. If we assume that and are given, satisfaction of the inequality would yield a required sample size n. The weakness of this approach is that the minimization requirement is determined by worst-case values of : A less conservative approach, and the one we take here, is to require that E ½R n ð, ; Þ4r: One could apply other (more conservative) criteria, such as E ½R n ð, ; Þ À 2SD ½R n ð, ; Þ4r, where SD denotes standard deviation with respect to. As noted, for demonstration purposes, we stay with E ½R n ð, ; Þ4r: Rarely can this inequality be evaluated analytically. We demonstrate a Monte-Carlo approach to find the minimum sample size yielding a desired reproducibility index for classification rule É with error estimation rule Ä: Assume, from our prior knowledge, that the feature-label distribution generating the experimental samples, after processing the data, can be approximated with the synthetic model introduced in Section 2.2, with d ¼ 2 features, ¼ 0:6, f 0 , 1 g ¼ f0:4, 0:8g and being normally distributed with mean 1.167 and variance 2 =5d ¼ 0:036 ( % 1:167 corresponding to " bay % 0:1). For given and , and for fixed n, generate random i $ Nð1:167, 0:036Þ, i ¼ 1,. .. , 1000: For each i , draw random samples S j n , j ¼ 1,. .. , 5000, from the distribution model Fð i Þ: For each sample S j n , design a classifier j n ¼ ÉðS j n Þ, calculate its true error using an independent large sample drawn from the same distribution Fð i Þ and estimate its error by ÄðS j n Þ: Now calculate R n ð, ; i Þ empirically from these 5000 pairs of true and estimated errors and approximate E ½R n ð, ; Þ by averaging over R n ð, ; i Þ: Repeat the procedure for different n until E ½R n ð, ; Þ4r for a given r.<ref type="figure" target="#fig_1">Figure 10</ref>shows the expected reproducibility index for LDA, RBF-SVM and 0.632 bootstrap error estimation with respect to different sample size, and : If r ¼ 0.6, ¼ 0:01, ¼ 0:2 and the<ref type="figure">Fig. 9</ref>. Multiple-rule bias effect on the reproducibility index for m ¼ 3, n ¼ 60, 120, LDA classification rule, and 5F-CV error estimation. d ¼ 5 and the covariance matrices are equal with features uncorrelated: (a) n ¼ 60, ¼ 0:0005; (b) n ¼ 60, ¼ 0:05; (c) n ¼ 120, ¼ 0:0005; (d) n ¼ 120, ¼ 0:05 classification rule is LDA,<ref type="figure" target="#fig_1">Figure 10a</ref>shows that n must exceed 82. As another example, the graph in<ref type="figure" target="#fig_1">Figure 10d</ref>shows that, for r ¼ 0.8, ¼ 0:05, ¼ 0:15 and RBF-SVM, n460:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Concluding remarks</head><p>Performance reproducibility is an epistemological issue: What knowledge is provided by a study? Ultimately, we are led back to the core epistemological issue in biomarker prediction, accuracy of the error estimate. To the extent that the estimated classifier error differs from the true error on the feature-label distribution, there is lack of knowledge at the conclusion of the first study. If there is virtually no reproducibility, then there is virtually no knowledge. Thus, there is no justification for a large study based on the preliminary study. Indeed, why proceed with the preliminary study if there is no reason to believe that its results will be reproducible? The issue of reproducibility should be settled before any study, small or large. The proposed reproducibility index provides the needed determination. Ultimately, the reproducibility index depends on the accuracy of the error estimator, and if we judge accuracy by the RMS, then the deviation variance of the estimator plays a crucial rule since RMS ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi</p><formula>Var dev ½ ^ " n  þ Bias 2 ½ ^ " n  p</formula><p>, where the bias and deviation variance are defined by</p><formula>Bias½ ^ " n  ¼ E½ ^ " n À " n  and Var dev ½ ^ " n  ¼ Var½ ^ " n À " n </formula><p>, respectively. When the bias is small, as in the case of LOO,</p><formula>RMS % ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi Var dev ½ ^ " n  p ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi Var½ ^ " n  þ Var½" n  À 2 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi Var½ ^ " n Var½" n  p q</formula><p>where is the correlation coefficient between the true and estimated errors. As we see in<ref type="figure" target="#fig_2">Figures 2 and 3</ref>, Var½ ^ " n  tends to be large and tends to be very small or even negative (Braga<ref type="bibr" target="#b4">Neto and Dougherty, 2010;</ref><ref type="bibr" target="#b19">Hanczar et al., 2007</ref>). This large variance and lack of positive correlation results in lack of reproducibility for small samples. Let us conclude with some remarks concerning validation, which, in our particular circumstance, means validation of the classifier error from the original small-sample study. For complex models, such as stochastic dynamical networks, validation of the full network is typically beyond hope, and one must be content with validating some characteristic of the network, such as its steady-state solution, by comparing it to empirical observations (). As for how close the theoretical and the corresponding empirical characteristic must be to warrant acceptance, closeness must be defined by some quantitative criterion understood by all. The intersubjectivity of validation resides in the fact that some group has agreed on the measure of closeness (and the requisite experimental protocol), although they might disagree on the degree of closeness required for acceptance (<ref type="bibr" target="#b15">Dougherty and Bittner, 2011</ref>). In the case of classification (as noted in the Introduction), when applying a classifier on an independent test set, the RMS possesses a distribution-free bound of ð2 ffiffiffiffi m p Þ À1 : Agreeing to using the RMS as the closeness criterion and using this bound, one can determine a test sample size to achieve a desired degree of accuracy, thereby validating (or not validating) the performance claims made in the original experiment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><figDesc>Consider a protocol in which the expressions of 30 000 genes are measured from 50 patients, each suffering from a different stage of breast cancer—30,000 features and a sample size of 50. *To whom correspondence should be addressed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.1.</head><figDesc>Fig. 1. A single-level cut of the joint distribution and corresponding probabilities</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.3.</head><figDesc>Fig. 3. Joint distribution of the true and estimated errors for two real datasets and different classifier rule models. The training size is 60, and d ¼ 5 features are selected using t-test feature selection method: (a) multiple myeloma, LDA and 5F-CV; (b) multiple myeloma, LDA and LOO; (c) multiple myeloma, LDA and 0.632 bootstrap; (d) HCC, RBF-SVM and 5F-CV; (e) HCC, RBF-SVM and LOO; (f) HCC, RBF-SVM and 0.632 bootstrap. The white line shows the " n ¼ ^ " n axis, the dotted line shows the regression line and the circle indicates the sample mean of the joint distribution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig.10.</head><figDesc>Fig. 10. Expected reproducibility index for LDA and RBF-SVM classification rules, and 0.632 bootstrap error estimation as a function of n: (a) ¼ 0:01, LDA; (b) ¼ 0:01, RBF-SVM; (c) ¼ 0:05, LDA; (d) ¼ 0:05, RBF-SVM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><figDesc>Table 1. Four microarray real datasets used in this study</figDesc><table>Dataset 
Dataset type 
Feature—sample size 

Yeoh et al. (2002) 
Pediatric ALL 
5077—149/99 
Zhan et al. (2006) 
Multiple myeloma 
54 613—156/78 
Chen et al. (2004) 
HCC 
10 237—75/82 
Natsoulis et al. (2005) Drugs response on rats 8491—120/61 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><figDesc>Table 2.</figDesc><table>Expected true errors of three classification rules used on the real 
datasets 

Dataset 
LDA 
L-SVM 
RBF-SVM 

Yeoh et al. (2002) 
0.080 
0.083 
0.080 
Zhan et al. (2006) 
0.186 
0.193 
0.188 
Chen et al. (2004) 
0.154 
0.151 
0.140 
Natsoulis et al. (2005) 
0.247 
0.258 
0.301 </table></figure>

			<note place="foot">ß The Author 2012. Published by Oxford University Press. All rights reserved. For Permissions, please e-mail: journals.permissions@oup.com at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="1"> 1þrðÞ : To gain insight into this bound, we postulate a geometrically simple model whose assumptions are not unrealistic. First, we assume that the linear regression of</note>

			<note place="foot">Performance reproducibility index at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot">M.R.Yousefi and E.R.Dougherty at :: on August 30, 2016 http://bioinformatics.oxfordjournals.org/ Downloaded from</note>

			<note place="foot" n="1"> ¼ 0:8). (a) n ¼ 60, LDA; (b) n ¼ 120, LDA; (c) n ¼ 60, RBF-SVM; (d) n ¼ 120, RBF-SVM</note>

			<note place="foot">The situation is much more subtle when using the RMS on the training data. In very few cases are any distribution-free bounds known and, when known, they are useless for small samples. To obtain useful RMS bounds, one must apply prior distributional knowledge. There is no option. Given prior (partial) distributional knowledge, one can determine a sample size to achieve a desired RMS (Zollanvari et al., 2012). Furthermore, given a prior distribution on the uncertainty class, one can find an exact expression for the RMS given the sample, meaning that one can use a censored sampling approach to sample just long enough to achieve the desired RMS (Dalton and Dougherty, 2012a). Prior knowledge can also be used to calibrate ad hoc error estimators such as resubstitution and LOO to gain improved estimation accuracy (Dalton and Dougherty, 2012b). One might argue that assuming prior knowledge carries risk because the knowledge could be erroneous. But if one does not bring sufficient knowledge to an experiment to achieve meaningful results, then he or she is not ready to do the experiment. Pragmatism requires prior knowledge. The prior knowledge is uncertain, and our formulation of it must include a measure of that uncertainty. The more uncertain we are, the less impact the knowledge will have on our conclusions. In the case of the reproducibility index, we have introduced a few criteria by which one can decide whether, in the framework of this uncertainty, a desired level is achieved. A key point regarding uncertainty in the context of reproducibility is that, should the prior distribution on the uncertainty class be optimistic, it may result in carrying out a second study without sufficient justification but it will not lead to an overoptimistic conclusion because the conclusion will be based on the independent larger follow-on study in which the prior knowledge is not employed. This is far better than basing the decision to proceed with a large independent study on a meaningless error estimate.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>The authors thank the High-Performance Biocomputing Center of TGen for providing the clustered computing resources used in this study; this includes the Saguaro-2 cluster supercomputer, partially funded by NIH grant 1S10RR025056-01. Conflict of Interest: none declared.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct   xml:id="b0">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">E</forename>
				<surname>Boser</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT &apos;92: Proceedings of the Fifth Annual Workshop on Computational Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b1">
	<analytic>
		<title level="a" type="main">Over-optimism in bioinformatics research</title>
		<author>
			<persName>
				<forename type="first">A.-L</forename>
				<surname>Boulesteix</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="437" to="439" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b2">
	<analytic>
		<title level="a" type="main">Stability and aggregation of ranked gene lists</title>
		<author>
			<persName>
				<forename type="first">A.-L</forename>
				<surname>Boulesteix</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Slawski</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brief. Bioinform</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="556" to="568" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b3">
	<analytic>
		<title level="a" type="main">Is cross-validation valid for small-sample microarray classification?</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<forename type="middle">M</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="374" to="380" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b4">
	<analytic>
		<title level="a" type="main">Exact correlation between actual and estimated errors in discrete classification</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<forename type="middle">M</forename>
				<surname>Braga-Neto</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="407" to="413" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b5">
	<analytic>
		<title level="a" type="main">An empirical assessment of validation practices for molecular classifiers</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">J</forename>
				<surname>Castaldi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brief. Bioinform</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="189" to="202" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b6">
	<analytic>
		<title level="a" type="main">Novel endothelial cell markers in hepatocellular carcinoma</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Chen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Modern Pathol</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1198" to="1210" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b7">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Cortes</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<forename type="middle">N</forename>
				<surname>Vapnik</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b8">
	<analytic>
		<title level="a" type="main">Bayesian minimum mean-square error estimation for classification error–Part I: Definition and the Bayesian MMSE error estimator for discrete classification</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">A</forename>
				<surname>Dalton</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="115" to="129" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b9">
	<analytic>
		<title level="a" type="main">Application of the Bayesian MMSE error estimator for classification error to gene-expression microarray data</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">A</forename>
				<surname>Dalton</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1822" to="1831" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b10">
	<analytic>
		<title level="a" type="main">Exact MSE performance of the Bayesian MMSE estimator for classification error–Part II: Consistency and performance analysis</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">A</forename>
				<surname>Dalton</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="2588" to="2603" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimal MSE calibration of error estimators under Bayesian models</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">A</forename>
				<surname>Dalton</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2308" to="2320" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b12">
	<monogr>
		<title level="m" type="main">A Probabilistic Theory of Pattern Recognition</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Devroye</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b13">
	<analytic>
		<title level="a" type="main">Validation of gene regulatory networks: scientific and inferential</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brief. Bioinform</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="245" to="252" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b14">
	<analytic>
		<title level="a" type="main">Prudence, risk, and reproducibility in biomarker discovery</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioEssays</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="277" to="279" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b15">
	<monogr>
		<title level="m" type="main">Epistemology of the Cell: A Systems Perspective on Biological Knowledge</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">L</forename>
				<surname>Bittner</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>John Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b16">
	<analytic>
		<title level="a" type="main">Performance of error estimators for classification</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Bioinform</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="53" to="67" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b17">
	<analytic>
		<title level="a" type="main">The illusion of distribution-free small-sample classification in genomics</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Dougherty</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Genomics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="333" to="341" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b18">
	<monogr>
		<title level="m" type="main">Statistical Methods for Research Workers</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">A</forename>
				<surname>Fisher</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1925" />
			<publisher>Oliver and Boyd</publisher>
			<pubPlace>Edinburg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b19">
	<analytic>
		<title level="a" type="main">Decorrelation of the true and estimated classifier errors in high-dimensional settings</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Hanczar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Bioinform. Syst. Biol</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b20">
	<analytic>
		<title level="a" type="main">Small-sample precision of ROC-related estimates</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Hanczar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="822" to="830" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimal number of features as a function of sample size for various classification rules</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Hua</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1509" to="1515" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b22">
	<analytic>
		<title level="a" type="main">Case studies in reproducibility</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hothorn</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Leisch</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brief. Bioinform</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="288" to="300" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b23">
	<analytic>
		<title level="a" type="main">Why most published research findings are false</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">P A</forename>
				<surname>Ioannidis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Med</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">124</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b24">
	<analytic>
		<title level="a" type="main">Over-optimism in bioinformatics: an illustration</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Jelizarow</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b25">
	<analytic>
		<title level="a" type="main">Measuring reproducibility of high-throughput experiments</title>
		<author>
			<persName>
				<forename type="first">Q</forename>
				<surname>Li</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1752" to="1779" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b26">
	<analytic>
		<title level="a" type="main">Classification of a large microarray data set: algorithm comparison and analysis of drug signatures</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Natsoulis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="724" to="736" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b27">
	<monogr>
		<title level="m" type="main">FDA&apos;s Woodcock says personalized drug development entering &apos;long slog&apos; phase. Pharmacogen. Rep., http://www.genomeweb.com/mdx/fdaswoodcock-says-personalized-drug-development-entering-long-slog-phase</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Ray</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2011-10-26" />
		</imprint>
	</monogr>
	<note>date. last accessed</note>
</biblStruct>

<biblStruct   xml:id="b28">
	<analytic>
		<title level="a" type="main">Proteomics in melanoma biomarker discovery: great potential, many obstacles</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">S</forename>
				<surname>Sabel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Proteom</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b29">
	<analytic>
		<title level="a" type="main">Classification, subtype discovery, and prediction of outcome in pediatric acute lymphoblastic leukemia by gene expression profiling</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">J</forename>
				<surname>Yeoh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Cell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="133" to="143" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b30">
	<analytic>
		<title level="a" type="main">Reporting bias when using real data sets to analyze classification performance</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">R</forename>
				<surname>Yousefi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="68" to="76" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiple-rule bias in the comparison of classification rules</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">R</forename>
				<surname>Yousefi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1675" to="1683" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b32">
	<analytic>
		<title level="a" type="main">The molecular classification of multiple myeloma</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Zhan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Blood</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="2020" to="2028" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b33">
	<analytic>
		<title level="a" type="main">Apparently low reproducibility of true differential expression discoveries in microarray studies</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2057" to="2063" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b34">
	<analytic>
		<title level="a" type="main">Evaluating reproducibility of differential expression discoveries in microarray studies by considering correlated molecular changes</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1662" to="1668" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint sampling distribution between actual and estimated classification errors for linear discriminant analysis</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Zollanvari</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="784" to="804" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct   xml:id="b36">
	<analytic>
		<title level="a" type="main">Exact representation of the second-order moments for resubstitution and leave-one-out error estimation for linear discriminant analysis in the univariate heteroskedastic Gaussian model</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Zollanvari</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="908" to="917" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>